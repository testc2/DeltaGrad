dataset name::MNIST5
deletion rate::0.00167
python3 generate_rand_ids 0.00167  MNIST5 1
tensor([51718, 58888, 12296,  9740, 34844,    30, 42014, 12326, 55335, 30249,
        36396, 18994, 37433,  8781,  9293,  5711, 37973,  2645, 41559, 33371,
        30816, 29283, 43625, 38511, 21628, 36990, 12429, 23700, 10388, 47770,
        43163, 15515, 59038, 26785, 30370, 11426, 16047, 36534, 22527, 16572,
        32448, 14535, 13006, 41168, 49366, 28891, 23773, 36575, 15096, 13566,
        45827, 45316, 17677, 16653, 40208, 21266, 28949, 21787, 57629, 22310,
        37161, 10029,  2350, 18221, 37677, 48946, 28486,   856, 19289, 46429,
        58725,  7527, 33130, 55146, 45425, 51589, 11664, 14738, 17810,  3986,
        23447, 34221, 55213, 12222, 10174,  9664, 39879, 27087,  6101, 42965,
        31193, 51677, 22497, 58343, 54767, 36336, 23023, 41974, 54266, 48127])
python3 generate_dataset_train_test.py Logistic_regression MNIST5 16384 32 10
repetition 0
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 2.315239
Train - Epoch 1, Batch: 0, Loss: 1.931006
Train - Epoch 2, Batch: 0, Loss: 1.661958
Train - Epoch 3, Batch: 0, Loss: 1.458471
Train - Epoch 4, Batch: 0, Loss: 1.312817
Train - Epoch 5, Batch: 0, Loss: 1.195454
Train - Epoch 6, Batch: 0, Loss: 1.106796
Train - Epoch 7, Batch: 0, Loss: 1.030307
Train - Epoch 8, Batch: 0, Loss: 0.969640
Train - Epoch 9, Batch: 0, Loss: 0.922973
Train - Epoch 10, Batch: 0, Loss: 0.884900
Train - Epoch 11, Batch: 0, Loss: 0.844942
Train - Epoch 12, Batch: 0, Loss: 0.823049
Train - Epoch 13, Batch: 0, Loss: 0.784530
Train - Epoch 14, Batch: 0, Loss: 0.765388
Train - Epoch 15, Batch: 0, Loss: 0.750378
Train - Epoch 16, Batch: 0, Loss: 0.728425
Train - Epoch 17, Batch: 0, Loss: 0.711945
Train - Epoch 18, Batch: 0, Loss: 0.699222
Train - Epoch 19, Batch: 0, Loss: 0.684475
Train - Epoch 20, Batch: 0, Loss: 0.671459
Train - Epoch 21, Batch: 0, Loss: 0.657667
Train - Epoch 22, Batch: 0, Loss: 0.642137
Train - Epoch 23, Batch: 0, Loss: 0.646085
Train - Epoch 24, Batch: 0, Loss: 0.628178
Train - Epoch 25, Batch: 0, Loss: 0.626751
Train - Epoch 26, Batch: 0, Loss: 0.613949
Train - Epoch 27, Batch: 0, Loss: 0.609608
Train - Epoch 28, Batch: 0, Loss: 0.602509
Train - Epoch 29, Batch: 0, Loss: 0.599498
Train - Epoch 30, Batch: 0, Loss: 0.591005
Train - Epoch 31, Batch: 0, Loss: 0.579938
Test Avg. Loss: 0.000055, Accuracy: 0.873800
training_time:: 3.2555441856384277
training time full:: 3.2556090354919434
provenance prepare time:: 5.9604644775390625e-06
Test Avg. Loss: 0.000055, Accuracy: 0.873800
baseline::
python3 incremental_updates_base_line_lr_multi.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Benchmark_experiments/benchmark_exp.py:1298: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  curr_delta_id = torch.tensor(delta_id)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 100
max_epoch:: 32
batch_size:: 16384
Num of deletion:: 0, running time baseline::2.421104
Num of deletion:: 10, running time baseline::25.750822
Num of deletion:: 20, running time baseline::49.213454
Num of deletion:: 30, running time baseline::72.639349
Num of deletion:: 40, running time baseline::96.109516
Num of deletion:: 50, running time baseline::119.609387
Num of deletion:: 60, running time baseline::143.180926
Num of deletion:: 70, running time baseline::166.797173
Num of deletion:: 80, running time baseline::190.448285
Num of deletion:: 90, running time baseline::214.086216
training time is 235.28860473632812
overhead:: 0
overhead2:: 235.28360962867737
overhead3:: 4.522143602371216
time_baseline:: 235.2887477874756
curr_diff: 0 tensor(0.0050, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0050, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.873900
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance5_lr_multi.py 10 5 0 0.00167 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 100
max_epoch:: 32
Num of deletion:: 0, running time provenance::0.968585
Num of deletion:: 10, running time provenance::10.170084
Num of deletion:: 20, running time provenance::19.277249
Num of deletion:: 30, running time provenance::28.454189
Num of deletion:: 40, running time provenance::37.637937
Num of deletion:: 50, running time provenance::46.938162
Num of deletion:: 60, running time provenance::56.051211
Num of deletion:: 70, running time provenance::65.454532
Num of deletion:: 80, running time provenance::74.928884
Num of deletion:: 90, running time provenance::84.375659
overhead:: 0
overhead2:: 0
overhead3:: 92.62875604629517
overhead4:: 0
overhead5:: 5.031837224960327
memory usage:: 2992529408
time_provenance:: 92.63010215759277
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0051, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0051, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.873900
repetition 1
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 1 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 2.293827
Train - Epoch 1, Batch: 0, Loss: 1.924059
Train - Epoch 2, Batch: 0, Loss: 1.657167
Train - Epoch 3, Batch: 0, Loss: 1.454296
Train - Epoch 4, Batch: 0, Loss: 1.309139
Train - Epoch 5, Batch: 0, Loss: 1.194327
Train - Epoch 6, Batch: 0, Loss: 1.098176
Train - Epoch 7, Batch: 0, Loss: 1.026780
Train - Epoch 8, Batch: 0, Loss: 0.973298
Train - Epoch 9, Batch: 0, Loss: 0.919325
Train - Epoch 10, Batch: 0, Loss: 0.884006
Train - Epoch 11, Batch: 0, Loss: 0.840185
Train - Epoch 12, Batch: 0, Loss: 0.818217
Train - Epoch 13, Batch: 0, Loss: 0.792519
Train - Epoch 14, Batch: 0, Loss: 0.756965
Train - Epoch 15, Batch: 0, Loss: 0.743277
Train - Epoch 16, Batch: 0, Loss: 0.729185
Train - Epoch 17, Batch: 0, Loss: 0.707656
Train - Epoch 18, Batch: 0, Loss: 0.694925
Train - Epoch 19, Batch: 0, Loss: 0.681406
Train - Epoch 20, Batch: 0, Loss: 0.666958
Train - Epoch 21, Batch: 0, Loss: 0.651351
Train - Epoch 22, Batch: 0, Loss: 0.647027
Train - Epoch 23, Batch: 0, Loss: 0.632970
Train - Epoch 24, Batch: 0, Loss: 0.636662
Train - Epoch 25, Batch: 0, Loss: 0.614254
Train - Epoch 26, Batch: 0, Loss: 0.609720
Train - Epoch 27, Batch: 0, Loss: 0.597417
Train - Epoch 28, Batch: 0, Loss: 0.597399
Train - Epoch 29, Batch: 0, Loss: 0.590398
Train - Epoch 30, Batch: 0, Loss: 0.580237
Train - Epoch 31, Batch: 0, Loss: 0.575669
Test Avg. Loss: 0.000055, Accuracy: 0.875300
training_time:: 3.2606348991394043
training time full:: 3.2607057094573975
provenance prepare time:: 4.291534423828125e-06
Test Avg. Loss: 0.000055, Accuracy: 0.875300
baseline::
python3 incremental_updates_base_line_lr_multi.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Benchmark_experiments/benchmark_exp.py:1298: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  curr_delta_id = torch.tensor(delta_id)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 100
max_epoch:: 32
batch_size:: 16384
Num of deletion:: 0, running time baseline::2.398292
Num of deletion:: 10, running time baseline::25.815183
Num of deletion:: 20, running time baseline::49.207245
Num of deletion:: 30, running time baseline::72.602715
Num of deletion:: 40, running time baseline::95.979318
Num of deletion:: 50, running time baseline::119.403732
Num of deletion:: 60, running time baseline::142.851457
Num of deletion:: 70, running time baseline::166.312057
Num of deletion:: 80, running time baseline::189.779346
Num of deletion:: 90, running time baseline::213.223708
training time is 234.3835654258728
overhead:: 0
overhead2:: 234.37821769714355
overhead3:: 4.626243829727173
time_baseline:: 234.38370370864868
curr_diff: 0 tensor(0.0050, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0050, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875400
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance5_lr_multi.py 10 5 1 0.00167 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 100
max_epoch:: 32
Num of deletion:: 0, running time provenance::0.963566
Num of deletion:: 10, running time provenance::10.101042
Num of deletion:: 20, running time provenance::19.221074
Num of deletion:: 30, running time provenance::28.430541
Num of deletion:: 40, running time provenance::37.596139
Num of deletion:: 50, running time provenance::46.993470
Num of deletion:: 60, running time provenance::56.309141
Num of deletion:: 70, running time provenance::65.645497
Num of deletion:: 80, running time provenance::74.892108
Num of deletion:: 90, running time provenance::84.039822
overhead:: 0
overhead2:: 0
overhead3:: 92.37240839004517
overhead4:: 0
overhead5:: 5.033286094665527
memory usage:: 2990190592
time_provenance:: 92.37377214431763
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0050, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0050, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875300
repetition 2
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 2 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 2.319599
Train - Epoch 1, Batch: 0, Loss: 1.946566
Train - Epoch 2, Batch: 0, Loss: 1.667449
Train - Epoch 3, Batch: 0, Loss: 1.467530
Train - Epoch 4, Batch: 0, Loss: 1.310697
Train - Epoch 5, Batch: 0, Loss: 1.199905
Train - Epoch 6, Batch: 0, Loss: 1.106088
Train - Epoch 7, Batch: 0, Loss: 1.035228
Train - Epoch 8, Batch: 0, Loss: 0.982157
Train - Epoch 9, Batch: 0, Loss: 0.929350
Train - Epoch 10, Batch: 0, Loss: 0.893292
Train - Epoch 11, Batch: 0, Loss: 0.847549
Train - Epoch 12, Batch: 0, Loss: 0.826183
Train - Epoch 13, Batch: 0, Loss: 0.797082
Train - Epoch 14, Batch: 0, Loss: 0.774374
Train - Epoch 15, Batch: 0, Loss: 0.745874
Train - Epoch 16, Batch: 0, Loss: 0.723417
Train - Epoch 17, Batch: 0, Loss: 0.718588
Train - Epoch 18, Batch: 0, Loss: 0.700937
Train - Epoch 19, Batch: 0, Loss: 0.681442
Train - Epoch 20, Batch: 0, Loss: 0.667525
Train - Epoch 21, Batch: 0, Loss: 0.655385
Train - Epoch 22, Batch: 0, Loss: 0.650524
Train - Epoch 23, Batch: 0, Loss: 0.639420
Train - Epoch 24, Batch: 0, Loss: 0.633844
Train - Epoch 25, Batch: 0, Loss: 0.623347
Train - Epoch 26, Batch: 0, Loss: 0.606952
Train - Epoch 27, Batch: 0, Loss: 0.608867
Train - Epoch 28, Batch: 0, Loss: 0.600840
Train - Epoch 29, Batch: 0, Loss: 0.585122
Train - Epoch 30, Batch: 0, Loss: 0.582475
Train - Epoch 31, Batch: 0, Loss: 0.578839
Test Avg. Loss: 0.000055, Accuracy: 0.877900
training_time:: 3.266904830932617
training time full:: 3.2669730186462402
provenance prepare time:: 5.7220458984375e-06
Test Avg. Loss: 0.000055, Accuracy: 0.877900
baseline::
python3 incremental_updates_base_line_lr_multi.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Benchmark_experiments/benchmark_exp.py:1298: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  curr_delta_id = torch.tensor(delta_id)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 100
max_epoch:: 32
batch_size:: 16384
Num of deletion:: 0, running time baseline::2.399892
Num of deletion:: 10, running time baseline::25.697835
Num of deletion:: 20, running time baseline::48.957706
Num of deletion:: 30, running time baseline::72.272053
Num of deletion:: 40, running time baseline::95.605240
Num of deletion:: 50, running time baseline::118.962979
Num of deletion:: 60, running time baseline::142.364953
Num of deletion:: 70, running time baseline::165.755365
Num of deletion:: 80, running time baseline::189.140646
Num of deletion:: 90, running time baseline::212.397632
training time is 233.38068866729736
overhead:: 0
overhead2:: 233.37577509880066
overhead3:: 4.501879930496216
time_baseline:: 233.38083004951477
curr_diff: 0 tensor(0.0051, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0051, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.877700
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance5_lr_multi.py 10 5 2 0.00167 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 100
max_epoch:: 32
Num of deletion:: 0, running time provenance::0.958608
Num of deletion:: 10, running time provenance::10.115969
Num of deletion:: 20, running time provenance::19.228957
Num of deletion:: 30, running time provenance::28.561391
Num of deletion:: 40, running time provenance::37.941324
Num of deletion:: 50, running time provenance::47.388243
Num of deletion:: 60, running time provenance::56.767953
Num of deletion:: 70, running time provenance::66.233763
Num of deletion:: 80, running time provenance::75.583839
Num of deletion:: 90, running time provenance::85.073137
overhead:: 0
overhead2:: 0
overhead3:: 93.76853585243225
overhead4:: 0
overhead5:: 5.051758766174316
memory usage:: 2991362048
time_provenance:: 93.76983857154846
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0051, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0051, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.877800
repetition 3
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 3 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 2.319269
Train - Epoch 1, Batch: 0, Loss: 1.928068
Train - Epoch 2, Batch: 0, Loss: 1.653975
Train - Epoch 3, Batch: 0, Loss: 1.449656
Train - Epoch 4, Batch: 0, Loss: 1.301074
Train - Epoch 5, Batch: 0, Loss: 1.192755
Train - Epoch 6, Batch: 0, Loss: 1.097543
Train - Epoch 7, Batch: 0, Loss: 1.029181
Train - Epoch 8, Batch: 0, Loss: 0.968608
Train - Epoch 9, Batch: 0, Loss: 0.927604
Train - Epoch 10, Batch: 0, Loss: 0.875342
Train - Epoch 11, Batch: 0, Loss: 0.846306
Train - Epoch 12, Batch: 0, Loss: 0.816816
Train - Epoch 13, Batch: 0, Loss: 0.790696
Train - Epoch 14, Batch: 0, Loss: 0.761677
Train - Epoch 15, Batch: 0, Loss: 0.746579
Train - Epoch 16, Batch: 0, Loss: 0.729023
Train - Epoch 17, Batch: 0, Loss: 0.716140
Train - Epoch 18, Batch: 0, Loss: 0.692059
Train - Epoch 19, Batch: 0, Loss: 0.686998
Train - Epoch 20, Batch: 0, Loss: 0.667607
Train - Epoch 21, Batch: 0, Loss: 0.659400
Train - Epoch 22, Batch: 0, Loss: 0.644052
Train - Epoch 23, Batch: 0, Loss: 0.633613
Train - Epoch 24, Batch: 0, Loss: 0.628924
Train - Epoch 25, Batch: 0, Loss: 0.618154
Train - Epoch 26, Batch: 0, Loss: 0.611429
Train - Epoch 27, Batch: 0, Loss: 0.599452
Train - Epoch 28, Batch: 0, Loss: 0.602760
Train - Epoch 29, Batch: 0, Loss: 0.578186
Train - Epoch 30, Batch: 0, Loss: 0.579823
Train - Epoch 31, Batch: 0, Loss: 0.580026
Test Avg. Loss: 0.000055, Accuracy: 0.875000
training_time:: 3.3135247230529785
training time full:: 3.3135933876037598
provenance prepare time:: 4.76837158203125e-06
Test Avg. Loss: 0.000055, Accuracy: 0.875000
baseline::
python3 incremental_updates_base_line_lr_multi.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Benchmark_experiments/benchmark_exp.py:1298: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  curr_delta_id = torch.tensor(delta_id)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 100
max_epoch:: 32
batch_size:: 16384
Num of deletion:: 0, running time baseline::2.383946
Num of deletion:: 10, running time baseline::25.711406
Num of deletion:: 20, running time baseline::49.001799
Num of deletion:: 30, running time baseline::72.714253
Num of deletion:: 40, running time baseline::96.065585
Num of deletion:: 50, running time baseline::119.445852
Num of deletion:: 60, running time baseline::142.841722
Num of deletion:: 70, running time baseline::166.252769
Num of deletion:: 80, running time baseline::189.663328
Num of deletion:: 90, running time baseline::213.102676
training time is 234.25169348716736
overhead:: 0
overhead2:: 234.24661707878113
overhead3:: 4.501265525817871
time_baseline:: 234.2518355846405
curr_diff: 0 tensor(0.0050, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0050, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874800
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance5_lr_multi.py 10 5 3 0.00167 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 100
max_epoch:: 32
Num of deletion:: 0, running time provenance::0.965560
Num of deletion:: 10, running time provenance::10.082110
Num of deletion:: 20, running time provenance::19.281652
Num of deletion:: 30, running time provenance::28.468324
Num of deletion:: 40, running time provenance::37.601431
Num of deletion:: 50, running time provenance::47.297402
Num of deletion:: 60, running time provenance::56.553631
Num of deletion:: 70, running time provenance::65.777245
Num of deletion:: 80, running time provenance::75.015593
Num of deletion:: 90, running time provenance::84.326068
overhead:: 0
overhead2:: 0
overhead3:: 92.588205575943
overhead4:: 0
overhead5:: 5.035924673080444
memory usage:: 2991091712
time_provenance:: 92.58957195281982
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0051, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0051, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874800
repetition 4
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 4 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 2.308584
Train - Epoch 1, Batch: 0, Loss: 1.926464
Train - Epoch 2, Batch: 0, Loss: 1.659653
Train - Epoch 3, Batch: 0, Loss: 1.450628
Train - Epoch 4, Batch: 0, Loss: 1.304267
Train - Epoch 5, Batch: 0, Loss: 1.193216
Train - Epoch 6, Batch: 0, Loss: 1.103961
Train - Epoch 7, Batch: 0, Loss: 1.037062
Train - Epoch 8, Batch: 0, Loss: 0.972682
Train - Epoch 9, Batch: 0, Loss: 0.922995
Train - Epoch 10, Batch: 0, Loss: 0.881254
Train - Epoch 11, Batch: 0, Loss: 0.844546
Train - Epoch 12, Batch: 0, Loss: 0.819683
Train - Epoch 13, Batch: 0, Loss: 0.789169
Train - Epoch 14, Batch: 0, Loss: 0.772449
Train - Epoch 15, Batch: 0, Loss: 0.748586
Train - Epoch 16, Batch: 0, Loss: 0.727842
Train - Epoch 17, Batch: 0, Loss: 0.718215
Train - Epoch 18, Batch: 0, Loss: 0.695639
Train - Epoch 19, Batch: 0, Loss: 0.680707
Train - Epoch 20, Batch: 0, Loss: 0.668949
Train - Epoch 21, Batch: 0, Loss: 0.660894
Train - Epoch 22, Batch: 0, Loss: 0.645250
Train - Epoch 23, Batch: 0, Loss: 0.638531
Train - Epoch 24, Batch: 0, Loss: 0.628130
Train - Epoch 25, Batch: 0, Loss: 0.620974
Train - Epoch 26, Batch: 0, Loss: 0.610987
Train - Epoch 27, Batch: 0, Loss: 0.610235
Train - Epoch 28, Batch: 0, Loss: 0.593285
Train - Epoch 29, Batch: 0, Loss: 0.578086
Train - Epoch 30, Batch: 0, Loss: 0.584343
Train - Epoch 31, Batch: 0, Loss: 0.582985
Test Avg. Loss: 0.000055, Accuracy: 0.876600
training_time:: 3.263352155685425
training time full:: 3.2634215354919434
provenance prepare time:: 4.5299530029296875e-06
Test Avg. Loss: 0.000055, Accuracy: 0.876600
baseline::
python3 incremental_updates_base_line_lr_multi.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Benchmark_experiments/benchmark_exp.py:1298: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  curr_delta_id = torch.tensor(delta_id)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 100
max_epoch:: 32
batch_size:: 16384
Num of deletion:: 0, running time baseline::2.369691
Num of deletion:: 10, running time baseline::25.570506
Num of deletion:: 20, running time baseline::48.755890
Num of deletion:: 30, running time baseline::71.991888
Num of deletion:: 40, running time baseline::95.225995
Num of deletion:: 50, running time baseline::118.466754
Num of deletion:: 60, running time baseline::141.791193
Num of deletion:: 70, running time baseline::165.111559
Num of deletion:: 80, running time baseline::188.527633
Num of deletion:: 90, running time baseline::211.962818
training time is 233.06311893463135
overhead:: 0
overhead2:: 233.05783796310425
overhead3:: 4.488490104675293
time_baseline:: 233.06326031684875
curr_diff: 0 tensor(0.0050, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0050, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876800
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance5_lr_multi.py 10 5 4 0.00167 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 100
max_epoch:: 32
Num of deletion:: 0, running time provenance::0.952916
Num of deletion:: 10, running time provenance::10.160106
Num of deletion:: 20, running time provenance::19.352417
Num of deletion:: 30, running time provenance::28.516287
Num of deletion:: 40, running time provenance::37.615290
Num of deletion:: 50, running time provenance::47.167444
Num of deletion:: 60, running time provenance::56.570606
Num of deletion:: 70, running time provenance::66.014649
Num of deletion:: 80, running time provenance::75.431724
Num of deletion:: 90, running time provenance::84.594937
overhead:: 0
overhead2:: 0
overhead3:: 92.93089652061462
overhead4:: 0
overhead5:: 5.139852046966553
memory usage:: 2991853568
time_provenance:: 92.93227887153625
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0050, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0050, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876800
repetition 5
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 5 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 2.345831
Train - Epoch 1, Batch: 0, Loss: 1.960987
Train - Epoch 2, Batch: 0, Loss: 1.680062
Train - Epoch 3, Batch: 0, Loss: 1.472922
Train - Epoch 4, Batch: 0, Loss: 1.320571
Train - Epoch 5, Batch: 0, Loss: 1.198309
Train - Epoch 6, Batch: 0, Loss: 1.109355
Train - Epoch 7, Batch: 0, Loss: 1.036576
Train - Epoch 8, Batch: 0, Loss: 0.978965
Train - Epoch 9, Batch: 0, Loss: 0.925744
Train - Epoch 10, Batch: 0, Loss: 0.883043
Train - Epoch 11, Batch: 0, Loss: 0.856453
Train - Epoch 12, Batch: 0, Loss: 0.821864
Train - Epoch 13, Batch: 0, Loss: 0.789850
Train - Epoch 14, Batch: 0, Loss: 0.767692
Train - Epoch 15, Batch: 0, Loss: 0.745422
Train - Epoch 16, Batch: 0, Loss: 0.729040
Train - Epoch 17, Batch: 0, Loss: 0.715407
Train - Epoch 18, Batch: 0, Loss: 0.694928
Train - Epoch 19, Batch: 0, Loss: 0.684336
Train - Epoch 20, Batch: 0, Loss: 0.671819
Train - Epoch 21, Batch: 0, Loss: 0.664380
Train - Epoch 22, Batch: 0, Loss: 0.646626
Train - Epoch 23, Batch: 0, Loss: 0.641428
Train - Epoch 24, Batch: 0, Loss: 0.626055
Train - Epoch 25, Batch: 0, Loss: 0.625273
Train - Epoch 26, Batch: 0, Loss: 0.610199
Train - Epoch 27, Batch: 0, Loss: 0.612913
Train - Epoch 28, Batch: 0, Loss: 0.599620
Train - Epoch 29, Batch: 0, Loss: 0.591659
Train - Epoch 30, Batch: 0, Loss: 0.587786
Train - Epoch 31, Batch: 0, Loss: 0.579304
Test Avg. Loss: 0.000055, Accuracy: 0.875300
training_time:: 3.2912919521331787
training time full:: 3.2913599014282227
provenance prepare time:: 5.0067901611328125e-06
Test Avg. Loss: 0.000055, Accuracy: 0.875300
baseline::
python3 incremental_updates_base_line_lr_multi.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Benchmark_experiments/benchmark_exp.py:1298: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  curr_delta_id = torch.tensor(delta_id)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 100
max_epoch:: 32
batch_size:: 16384
Num of deletion:: 0, running time baseline::2.399016
Num of deletion:: 10, running time baseline::26.168206
Num of deletion:: 20, running time baseline::50.055037
Num of deletion:: 30, running time baseline::73.454381
Num of deletion:: 40, running time baseline::96.877558
Num of deletion:: 50, running time baseline::120.316967
Num of deletion:: 60, running time baseline::143.763728
Num of deletion:: 70, running time baseline::167.171633
Num of deletion:: 80, running time baseline::190.600928
Num of deletion:: 90, running time baseline::214.047468
training time is 235.21164107322693
overhead:: 0
overhead2:: 235.2064700126648
overhead3:: 4.53374171257019
time_baseline:: 235.2117862701416
curr_diff: 0 tensor(0.0051, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0051, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875200
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance5_lr_multi.py 10 5 5 0.00167 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 100
max_epoch:: 32
Num of deletion:: 0, running time provenance::0.966210
Num of deletion:: 10, running time provenance::10.225649
Num of deletion:: 20, running time provenance::19.585360
Num of deletion:: 30, running time provenance::28.791160
Num of deletion:: 40, running time provenance::38.027648
Num of deletion:: 50, running time provenance::47.407909
Num of deletion:: 60, running time provenance::56.640535
Num of deletion:: 70, running time provenance::65.935891
Num of deletion:: 80, running time provenance::75.098051
Num of deletion:: 90, running time provenance::84.367179
overhead:: 0
overhead2:: 0
overhead3:: 92.65673542022705
overhead4:: 0
overhead5:: 5.014709711074829
memory usage:: 2991292416
time_provenance:: 92.65810489654541
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0051, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0051, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.875200
repetition 6
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 6 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 2.320657
Train - Epoch 1, Batch: 0, Loss: 1.929843
Train - Epoch 2, Batch: 0, Loss: 1.658912
Train - Epoch 3, Batch: 0, Loss: 1.456339
Train - Epoch 4, Batch: 0, Loss: 1.309735
Train - Epoch 5, Batch: 0, Loss: 1.187750
Train - Epoch 6, Batch: 0, Loss: 1.111586
Train - Epoch 7, Batch: 0, Loss: 1.036122
Train - Epoch 8, Batch: 0, Loss: 0.973065
Train - Epoch 9, Batch: 0, Loss: 0.930572
Train - Epoch 10, Batch: 0, Loss: 0.879247
Train - Epoch 11, Batch: 0, Loss: 0.847539
Train - Epoch 12, Batch: 0, Loss: 0.814654
Train - Epoch 13, Batch: 0, Loss: 0.798794
Train - Epoch 14, Batch: 0, Loss: 0.763145
Train - Epoch 15, Batch: 0, Loss: 0.750345
Train - Epoch 16, Batch: 0, Loss: 0.734108
Train - Epoch 17, Batch: 0, Loss: 0.714487
Train - Epoch 18, Batch: 0, Loss: 0.694677
Train - Epoch 19, Batch: 0, Loss: 0.680821
Train - Epoch 20, Batch: 0, Loss: 0.675241
Train - Epoch 21, Batch: 0, Loss: 0.659651
Train - Epoch 22, Batch: 0, Loss: 0.646006
Train - Epoch 23, Batch: 0, Loss: 0.634647
Train - Epoch 24, Batch: 0, Loss: 0.622049
Train - Epoch 25, Batch: 0, Loss: 0.618895
Train - Epoch 26, Batch: 0, Loss: 0.624173
Train - Epoch 27, Batch: 0, Loss: 0.604331
Train - Epoch 28, Batch: 0, Loss: 0.600969
Train - Epoch 29, Batch: 0, Loss: 0.590982
Train - Epoch 30, Batch: 0, Loss: 0.581112
Train - Epoch 31, Batch: 0, Loss: 0.573968
Test Avg. Loss: 0.000055, Accuracy: 0.874300
training_time:: 3.274310350418091
training time full:: 3.2743756771087646
provenance prepare time:: 4.5299530029296875e-06
Test Avg. Loss: 0.000055, Accuracy: 0.874300
baseline::
python3 incremental_updates_base_line_lr_multi.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Benchmark_experiments/benchmark_exp.py:1298: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  curr_delta_id = torch.tensor(delta_id)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 100
max_epoch:: 32
batch_size:: 16384
Num of deletion:: 0, running time baseline::2.412646
Num of deletion:: 10, running time baseline::25.960459
Num of deletion:: 20, running time baseline::49.353011
Num of deletion:: 30, running time baseline::72.820617
Num of deletion:: 40, running time baseline::96.484950
Num of deletion:: 50, running time baseline::120.091237
Num of deletion:: 60, running time baseline::143.710542
Num of deletion:: 70, running time baseline::167.299237
Num of deletion:: 80, running time baseline::190.946634
Num of deletion:: 90, running time baseline::214.474636
training time is 235.71863389015198
overhead:: 0
overhead2:: 235.71358394622803
overhead3:: 4.6762778759002686
time_baseline:: 235.71877360343933
curr_diff: 0 tensor(0.0050, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0050, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874300
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance5_lr_multi.py 10 5 6 0.00167 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 100
max_epoch:: 32
Num of deletion:: 0, running time provenance::0.986689
Num of deletion:: 10, running time provenance::10.320047
Num of deletion:: 20, running time provenance::19.507503
Num of deletion:: 30, running time provenance::29.192977
Num of deletion:: 40, running time provenance::38.429822
Num of deletion:: 50, running time provenance::47.758179
Num of deletion:: 60, running time provenance::56.978696
Num of deletion:: 70, running time provenance::66.435367
Num of deletion:: 80, running time provenance::75.851400
Num of deletion:: 90, running time provenance::85.093813
overhead:: 0
overhead2:: 0
overhead3:: 93.55940413475037
overhead4:: 0
overhead5:: 5.05442214012146
memory usage:: 2991063040
time_provenance:: 93.56075167655945
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0050, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0050, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874300
repetition 7
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 7 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 2.316539
Train - Epoch 1, Batch: 0, Loss: 1.936414
Train - Epoch 2, Batch: 0, Loss: 1.668950
Train - Epoch 3, Batch: 0, Loss: 1.463699
Train - Epoch 4, Batch: 0, Loss: 1.313799
Train - Epoch 5, Batch: 0, Loss: 1.196666
Train - Epoch 6, Batch: 0, Loss: 1.105677
Train - Epoch 7, Batch: 0, Loss: 1.039128
Train - Epoch 8, Batch: 0, Loss: 0.975115
Train - Epoch 9, Batch: 0, Loss: 0.927451
Train - Epoch 10, Batch: 0, Loss: 0.883650
Train - Epoch 11, Batch: 0, Loss: 0.856605
Train - Epoch 12, Batch: 0, Loss: 0.825510
Train - Epoch 13, Batch: 0, Loss: 0.797002
Train - Epoch 14, Batch: 0, Loss: 0.768783
Train - Epoch 15, Batch: 0, Loss: 0.750880
Train - Epoch 16, Batch: 0, Loss: 0.723636
Train - Epoch 17, Batch: 0, Loss: 0.714766
Train - Epoch 18, Batch: 0, Loss: 0.701322
Train - Epoch 19, Batch: 0, Loss: 0.682019
Train - Epoch 20, Batch: 0, Loss: 0.667880
Train - Epoch 21, Batch: 0, Loss: 0.657442
Train - Epoch 22, Batch: 0, Loss: 0.654464
Train - Epoch 23, Batch: 0, Loss: 0.638322
Train - Epoch 24, Batch: 0, Loss: 0.632361
Train - Epoch 25, Batch: 0, Loss: 0.620104
Train - Epoch 26, Batch: 0, Loss: 0.610072
Train - Epoch 27, Batch: 0, Loss: 0.599141
Train - Epoch 28, Batch: 0, Loss: 0.598245
Train - Epoch 29, Batch: 0, Loss: 0.597277
Train - Epoch 30, Batch: 0, Loss: 0.581540
Train - Epoch 31, Batch: 0, Loss: 0.578770
Test Avg. Loss: 0.000055, Accuracy: 0.874500
training_time:: 3.291980504989624
training time full:: 3.292053699493408
provenance prepare time:: 4.291534423828125e-06
Test Avg. Loss: 0.000055, Accuracy: 0.874500
baseline::
python3 incremental_updates_base_line_lr_multi.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Benchmark_experiments/benchmark_exp.py:1298: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  curr_delta_id = torch.tensor(delta_id)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 100
max_epoch:: 32
batch_size:: 16384
Num of deletion:: 0, running time baseline::2.418545
Num of deletion:: 10, running time baseline::25.811171
Num of deletion:: 20, running time baseline::49.170760
Num of deletion:: 30, running time baseline::72.430315
Num of deletion:: 40, running time baseline::95.752879
Num of deletion:: 50, running time baseline::119.244133
Num of deletion:: 60, running time baseline::142.670683
Num of deletion:: 70, running time baseline::166.065742
Num of deletion:: 80, running time baseline::189.514256
Num of deletion:: 90, running time baseline::212.911014
training time is 233.98376893997192
overhead:: 0
overhead2:: 233.97858452796936
overhead3:: 4.526954174041748
time_baseline:: 233.98392057418823
curr_diff: 0 tensor(0.0051, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0051, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874500
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance5_lr_multi.py 10 5 7 0.00167 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 100
max_epoch:: 32
Num of deletion:: 0, running time provenance::0.987482
Num of deletion:: 10, running time provenance::10.199967
Num of deletion:: 20, running time provenance::19.633858
Num of deletion:: 30, running time provenance::28.958989
Num of deletion:: 40, running time provenance::38.171949
Num of deletion:: 50, running time provenance::47.362318
Num of deletion:: 60, running time provenance::56.758315
Num of deletion:: 70, running time provenance::66.023203
Num of deletion:: 80, running time provenance::75.212422
Num of deletion:: 90, running time provenance::84.460242
overhead:: 0
overhead2:: 0
overhead3:: 92.89026999473572
overhead4:: 0
overhead5:: 5.044692277908325
memory usage:: 2998181888
time_provenance:: 92.8916368484497
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0051, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0051, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874600
repetition 8
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 8 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 2.334292
Train - Epoch 1, Batch: 0, Loss: 1.955604
Train - Epoch 2, Batch: 0, Loss: 1.676143
Train - Epoch 3, Batch: 0, Loss: 1.473460
Train - Epoch 4, Batch: 0, Loss: 1.320435
Train - Epoch 5, Batch: 0, Loss: 1.199327
Train - Epoch 6, Batch: 0, Loss: 1.113483
Train - Epoch 7, Batch: 0, Loss: 1.048464
Train - Epoch 8, Batch: 0, Loss: 0.988595
Train - Epoch 9, Batch: 0, Loss: 0.927051
Train - Epoch 10, Batch: 0, Loss: 0.886580
Train - Epoch 11, Batch: 0, Loss: 0.851994
Train - Epoch 12, Batch: 0, Loss: 0.824824
Train - Epoch 13, Batch: 0, Loss: 0.799575
Train - Epoch 14, Batch: 0, Loss: 0.768145
Train - Epoch 15, Batch: 0, Loss: 0.754332
Train - Epoch 16, Batch: 0, Loss: 0.733824
Train - Epoch 17, Batch: 0, Loss: 0.711289
Train - Epoch 18, Batch: 0, Loss: 0.703395
Train - Epoch 19, Batch: 0, Loss: 0.683915
Train - Epoch 20, Batch: 0, Loss: 0.674563
Train - Epoch 21, Batch: 0, Loss: 0.665239
Train - Epoch 22, Batch: 0, Loss: 0.642378
Train - Epoch 23, Batch: 0, Loss: 0.639840
Train - Epoch 24, Batch: 0, Loss: 0.628267
Train - Epoch 25, Batch: 0, Loss: 0.616711
Train - Epoch 26, Batch: 0, Loss: 0.609151
Train - Epoch 27, Batch: 0, Loss: 0.599244
Train - Epoch 28, Batch: 0, Loss: 0.604431
Train - Epoch 29, Batch: 0, Loss: 0.595361
Train - Epoch 30, Batch: 0, Loss: 0.581986
Train - Epoch 31, Batch: 0, Loss: 0.587376
Test Avg. Loss: 0.000055, Accuracy: 0.875000
training_time:: 3.3150384426116943
training time full:: 3.315110206604004
provenance prepare time:: 4.0531158447265625e-06
Test Avg. Loss: 0.000055, Accuracy: 0.875000
baseline::
python3 incremental_updates_base_line_lr_multi.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Benchmark_experiments/benchmark_exp.py:1298: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  curr_delta_id = torch.tensor(delta_id)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 100
max_epoch:: 32
batch_size:: 16384
Num of deletion:: 0, running time baseline::2.414069
Num of deletion:: 10, running time baseline::25.961315
Num of deletion:: 20, running time baseline::49.347509
Num of deletion:: 30, running time baseline::72.760427
Num of deletion:: 40, running time baseline::96.189374
Num of deletion:: 50, running time baseline::119.695091
Num of deletion:: 60, running time baseline::143.055901
Num of deletion:: 70, running time baseline::166.485112
Num of deletion:: 80, running time baseline::189.904548
Num of deletion:: 90, running time baseline::213.319983
training time is 234.40563368797302
overhead:: 0
overhead2:: 234.40025854110718
overhead3:: 4.549485206604004
time_baseline:: 234.40578389167786
curr_diff: 0 tensor(0.0050, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0050, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874800
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance5_lr_multi.py 10 5 8 0.00167 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 100
max_epoch:: 32
Num of deletion:: 0, running time provenance::0.954252
Num of deletion:: 10, running time provenance::10.177665
Num of deletion:: 20, running time provenance::19.446535
Num of deletion:: 30, running time provenance::28.676503
Num of deletion:: 40, running time provenance::37.928407
Num of deletion:: 50, running time provenance::47.357623
Num of deletion:: 60, running time provenance::56.671026
Num of deletion:: 70, running time provenance::65.898574
Num of deletion:: 80, running time provenance::75.364779
Num of deletion:: 90, running time provenance::84.997639
overhead:: 0
overhead2:: 0
overhead3:: 93.38717198371887
overhead4:: 0
overhead5:: 5.054920434951782
memory usage:: 2991665152
time_provenance:: 93.38854241371155
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0051, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0051, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874800
repetition 9
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 9 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 2.274804
Train - Epoch 1, Batch: 0, Loss: 1.897578
Train - Epoch 2, Batch: 0, Loss: 1.628586
Train - Epoch 3, Batch: 0, Loss: 1.434890
Train - Epoch 4, Batch: 0, Loss: 1.282802
Train - Epoch 5, Batch: 0, Loss: 1.173984
Train - Epoch 6, Batch: 0, Loss: 1.092033
Train - Epoch 7, Batch: 0, Loss: 1.020116
Train - Epoch 8, Batch: 0, Loss: 0.966125
Train - Epoch 9, Batch: 0, Loss: 0.915030
Train - Epoch 10, Batch: 0, Loss: 0.875055
Train - Epoch 11, Batch: 0, Loss: 0.839221
Train - Epoch 12, Batch: 0, Loss: 0.816020
Train - Epoch 13, Batch: 0, Loss: 0.796016
Train - Epoch 14, Batch: 0, Loss: 0.767723
Train - Epoch 15, Batch: 0, Loss: 0.753405
Train - Epoch 16, Batch: 0, Loss: 0.727170
Train - Epoch 17, Batch: 0, Loss: 0.710161
Train - Epoch 18, Batch: 0, Loss: 0.697658
Train - Epoch 19, Batch: 0, Loss: 0.684475
Train - Epoch 20, Batch: 0, Loss: 0.666204
Train - Epoch 21, Batch: 0, Loss: 0.658206
Train - Epoch 22, Batch: 0, Loss: 0.636767
Train - Epoch 23, Batch: 0, Loss: 0.636210
Train - Epoch 24, Batch: 0, Loss: 0.629698
Train - Epoch 25, Batch: 0, Loss: 0.625262
Train - Epoch 26, Batch: 0, Loss: 0.608638
Train - Epoch 27, Batch: 0, Loss: 0.601181
Train - Epoch 28, Batch: 0, Loss: 0.594772
Train - Epoch 29, Batch: 0, Loss: 0.592490
Train - Epoch 30, Batch: 0, Loss: 0.584454
Train - Epoch 31, Batch: 0, Loss: 0.577644
Test Avg. Loss: 0.000055, Accuracy: 0.875100
training_time:: 3.3040597438812256
training time full:: 3.3041296005249023
provenance prepare time:: 5.0067901611328125e-06
Test Avg. Loss: 0.000055, Accuracy: 0.875100
baseline::
python3 incremental_updates_base_line_lr_multi.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Benchmark_experiments/benchmark_exp.py:1298: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  curr_delta_id = torch.tensor(delta_id)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 100
max_epoch:: 32
batch_size:: 16384
Num of deletion:: 0, running time baseline::2.376876
Num of deletion:: 10, running time baseline::25.772763
Num of deletion:: 20, running time baseline::49.146618
Num of deletion:: 30, running time baseline::72.520645
Num of deletion:: 40, running time baseline::95.918473
Num of deletion:: 50, running time baseline::119.322910
Num of deletion:: 60, running time baseline::142.784338
Num of deletion:: 70, running time baseline::166.291829
Num of deletion:: 80, running time baseline::189.733832
Num of deletion:: 90, running time baseline::213.195960
training time is 234.32362246513367
overhead:: 0
overhead2:: 234.3185110092163
overhead3:: 4.512155055999756
time_baseline:: 234.3237648010254
curr_diff: 0 tensor(0.0050, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0050, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874900
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance5_lr_multi.py 10 5 9 0.00167 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 100
max_epoch:: 32
Num of deletion:: 0, running time provenance::0.960179
Num of deletion:: 10, running time provenance::10.202282
Num of deletion:: 20, running time provenance::19.269379
Num of deletion:: 30, running time provenance::28.545268
Num of deletion:: 40, running time provenance::37.756710
Num of deletion:: 50, running time provenance::47.182068
Num of deletion:: 60, running time provenance::56.477743
Num of deletion:: 70, running time provenance::65.653192
Num of deletion:: 80, running time provenance::75.074741
Num of deletion:: 90, running time provenance::84.573398
overhead:: 0
overhead2:: 0
overhead3:: 92.94937658309937
overhead4:: 0
overhead5:: 5.077291011810303
memory usage:: 3005427712
time_provenance:: 92.95073127746582
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0050, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0050, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.874900
dataset name::covtype
deletion rate::0.00019123
python3 generate_rand_ids 0.00019123  covtype 1
start loading data...
normalization start!!
tensor([ 31753,  34313, 374281, 300052, 411670, 362011, 247835, 313895, 464424,
        396842, 203307, 480813, 498738, 216627, 350773, 243267, 294467, 164421,
        331858, 510559,  88160,  87650, 347753, 190069, 391292, 353916,  27774,
        140418, 156809, 223883, 126114, 235173,  23721, 402615, 442553, 434364,
        406716, 517311, 293573,  67278, 305878, 198871, 101598, 490723, 520421,
         87286, 517376, 231175, 222474, 203532, 314132, 296232, 272171, 146732,
         54067,  14131, 470326, 501560,  56124,  68925,  61246, 138047, 440147,
        223059, 487765,   4440, 229216, 215911, 356200, 107369, 231788, 226677,
         82295, 522625,  50561, 347011, 347523,  75151, 142226, 312219,  64927,
        338337, 464296, 174001,  61361, 249270, 285628, 282054,  77271, 479194,
         84443, 123361, 334823, 333288, 474089,  16367, 293361, 503282, 345084])
python3 generate_dataset_train_test.py Logistic_regression covtype 16384 32 10
start loading data...
normalization start!!
repetition 0
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 0 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 1.828203
Train - Epoch 0, Batch: 10, Loss: 1.342927
Train - Epoch 0, Batch: 20, Loss: 1.226275
Train - Epoch 0, Batch: 30, Loss: 1.173706
Train - Epoch 1, Batch: 0, Loss: 1.169433
Train - Epoch 1, Batch: 10, Loss: 1.159628
Train - Epoch 1, Batch: 20, Loss: 1.135221
Train - Epoch 1, Batch: 30, Loss: 1.122509
Train - Epoch 2, Batch: 0, Loss: 1.119734
Train - Epoch 2, Batch: 10, Loss: 1.108936
Train - Epoch 2, Batch: 20, Loss: 1.100795
Train - Epoch 2, Batch: 30, Loss: 1.091251
Train - Epoch 3, Batch: 0, Loss: 1.091561
Train - Epoch 3, Batch: 10, Loss: 1.086643
Train - Epoch 3, Batch: 20, Loss: 1.082088
Train - Epoch 3, Batch: 30, Loss: 1.062633
Train - Epoch 4, Batch: 0, Loss: 1.070189
Train - Epoch 4, Batch: 10, Loss: 1.069086
Train - Epoch 4, Batch: 20, Loss: 1.058182
Train - Epoch 4, Batch: 30, Loss: 1.061333
Train - Epoch 5, Batch: 0, Loss: 1.055232
Train - Epoch 5, Batch: 10, Loss: 1.049129
Train - Epoch 5, Batch: 20, Loss: 1.043648
Train - Epoch 5, Batch: 30, Loss: 1.041404
Train - Epoch 6, Batch: 0, Loss: 1.037069
Train - Epoch 6, Batch: 10, Loss: 1.026642
Train - Epoch 6, Batch: 20, Loss: 1.023196
Train - Epoch 6, Batch: 30, Loss: 1.025491
Train - Epoch 7, Batch: 0, Loss: 1.029212
Train - Epoch 7, Batch: 10, Loss: 1.009322
Train - Epoch 7, Batch: 20, Loss: 1.017368
Train - Epoch 7, Batch: 30, Loss: 1.016634
Train - Epoch 8, Batch: 0, Loss: 1.010588
Train - Epoch 8, Batch: 10, Loss: 0.995639
Train - Epoch 8, Batch: 20, Loss: 1.001332
Train - Epoch 8, Batch: 30, Loss: 1.002046
Train - Epoch 9, Batch: 0, Loss: 0.998337
Train - Epoch 9, Batch: 10, Loss: 0.986027
Train - Epoch 9, Batch: 20, Loss: 0.987949
Train - Epoch 9, Batch: 30, Loss: 0.987930
Train - Epoch 10, Batch: 0, Loss: 0.983251
Train - Epoch 10, Batch: 10, Loss: 0.994541
Train - Epoch 10, Batch: 20, Loss: 0.982866
Train - Epoch 10, Batch: 30, Loss: 0.981169
Train - Epoch 11, Batch: 0, Loss: 0.982425
Train - Epoch 11, Batch: 10, Loss: 0.982439
Train - Epoch 11, Batch: 20, Loss: 0.979439
Train - Epoch 11, Batch: 30, Loss: 0.974772
Train - Epoch 12, Batch: 0, Loss: 0.980511
Train - Epoch 12, Batch: 10, Loss: 0.966340
Train - Epoch 12, Batch: 20, Loss: 0.964783
Train - Epoch 12, Batch: 30, Loss: 0.969772
Train - Epoch 13, Batch: 0, Loss: 0.974464
Train - Epoch 13, Batch: 10, Loss: 0.969872
Train - Epoch 13, Batch: 20, Loss: 0.962167
Train - Epoch 13, Batch: 30, Loss: 0.967232
Train - Epoch 14, Batch: 0, Loss: 0.963805
Train - Epoch 14, Batch: 10, Loss: 0.948169
Train - Epoch 14, Batch: 20, Loss: 0.943698
Train - Epoch 14, Batch: 30, Loss: 0.952631
Train - Epoch 15, Batch: 0, Loss: 0.961644
Train - Epoch 15, Batch: 10, Loss: 0.948133
Train - Epoch 15, Batch: 20, Loss: 0.949900
Train - Epoch 15, Batch: 30, Loss: 0.943423
Train - Epoch 16, Batch: 0, Loss: 0.960178
Train - Epoch 16, Batch: 10, Loss: 0.953752
Train - Epoch 16, Batch: 20, Loss: 0.946517
Train - Epoch 16, Batch: 30, Loss: 0.943921
Train - Epoch 17, Batch: 0, Loss: 0.947972
Train - Epoch 17, Batch: 10, Loss: 0.928484
Train - Epoch 17, Batch: 20, Loss: 0.942169
Train - Epoch 17, Batch: 30, Loss: 0.938211
Train - Epoch 18, Batch: 0, Loss: 0.940029
Train - Epoch 18, Batch: 10, Loss: 0.931322
Train - Epoch 18, Batch: 20, Loss: 0.931176
Train - Epoch 18, Batch: 30, Loss: 0.942809
Train - Epoch 19, Batch: 0, Loss: 0.925766
Train - Epoch 19, Batch: 10, Loss: 0.925739
Train - Epoch 19, Batch: 20, Loss: 0.931907
Train - Epoch 19, Batch: 30, Loss: 0.933839
Train - Epoch 20, Batch: 0, Loss: 0.922992
Train - Epoch 20, Batch: 10, Loss: 0.930608
Train - Epoch 20, Batch: 20, Loss: 0.931790
Train - Epoch 20, Batch: 30, Loss: 0.929662
Train - Epoch 21, Batch: 0, Loss: 0.920878
Train - Epoch 21, Batch: 10, Loss: 0.921832
Train - Epoch 21, Batch: 20, Loss: 0.927575
Train - Epoch 21, Batch: 30, Loss: 0.922864
Train - Epoch 22, Batch: 0, Loss: 0.922350
Train - Epoch 22, Batch: 10, Loss: 0.920002
Train - Epoch 22, Batch: 20, Loss: 0.917545
Train - Epoch 22, Batch: 30, Loss: 0.904838
Train - Epoch 23, Batch: 0, Loss: 0.920445
Train - Epoch 23, Batch: 10, Loss: 0.914480
Train - Epoch 23, Batch: 20, Loss: 0.919740
Train - Epoch 23, Batch: 30, Loss: 0.910481
Train - Epoch 24, Batch: 0, Loss: 0.925098
Train - Epoch 24, Batch: 10, Loss: 0.901319
Train - Epoch 24, Batch: 20, Loss: 0.920594
Train - Epoch 24, Batch: 30, Loss: 0.901082
Train - Epoch 25, Batch: 0, Loss: 0.912922
Train - Epoch 25, Batch: 10, Loss: 0.904293
Train - Epoch 25, Batch: 20, Loss: 0.911413
Train - Epoch 25, Batch: 30, Loss: 0.908364
Train - Epoch 26, Batch: 0, Loss: 0.905898
Train - Epoch 26, Batch: 10, Loss: 0.899366
Train - Epoch 26, Batch: 20, Loss: 0.908972
Train - Epoch 26, Batch: 30, Loss: 0.915223
Train - Epoch 27, Batch: 0, Loss: 0.906351
Train - Epoch 27, Batch: 10, Loss: 0.907963
Train - Epoch 27, Batch: 20, Loss: 0.901518
Train - Epoch 27, Batch: 30, Loss: 0.902404
Train - Epoch 28, Batch: 0, Loss: 0.890969
Train - Epoch 28, Batch: 10, Loss: 0.904021
Train - Epoch 28, Batch: 20, Loss: 0.903228
Train - Epoch 28, Batch: 30, Loss: 0.903675
Train - Epoch 29, Batch: 0, Loss: 0.898000
Train - Epoch 29, Batch: 10, Loss: 0.899328
Train - Epoch 29, Batch: 20, Loss: 0.906320
Train - Epoch 29, Batch: 30, Loss: 0.896556
Train - Epoch 30, Batch: 0, Loss: 0.899429
Train - Epoch 30, Batch: 10, Loss: 0.899421
Train - Epoch 30, Batch: 20, Loss: 0.887588
Train - Epoch 30, Batch: 30, Loss: 0.900043
Train - Epoch 31, Batch: 0, Loss: 0.905685
Train - Epoch 31, Batch: 10, Loss: 0.903992
Train - Epoch 31, Batch: 20, Loss: 0.893736
Train - Epoch 31, Batch: 30, Loss: 0.880298
Test Avg. Loss: 0.000070, Accuracy: 0.628548
training_time:: 3.937865734100342
training time full:: 3.9379327297210693
provenance prepare time:: 8.821487426757812e-06
Test Avg. Loss: 0.000070, Accuracy: 0.628548
baseline::
python3 incremental_updates_base_line_lr_multi.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Benchmark_experiments/benchmark_exp.py:1298: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  curr_delta_id = torch.tensor(delta_id)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 99
max_epoch:: 32
batch_size:: 16384
Num of deletion:: 0, running time baseline::2.269526
Num of deletion:: 10, running time baseline::22.855784
Num of deletion:: 20, running time baseline::43.922200
Num of deletion:: 30, running time baseline::65.258864
Num of deletion:: 40, running time baseline::85.465760
Num of deletion:: 50, running time baseline::105.302172
Num of deletion:: 60, running time baseline::125.479254
Num of deletion:: 70, running time baseline::145.251586
Num of deletion:: 80, running time baseline::165.181341
Num of deletion:: 90, running time baseline::184.384992
training time is 200.1277232170105
overhead:: 0
overhead2:: 200.1216504573822
overhead3:: 8.020976781845093
time_baseline:: 200.12870168685913
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628548
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance5_lr_multi.py 10 5 0 0.00019123 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 99
max_epoch:: 32
Num of deletion:: 0, running time provenance::1.059509
Num of deletion:: 10, running time provenance::11.704692
Num of deletion:: 20, running time provenance::24.380778
Num of deletion:: 30, running time provenance::36.470443
Num of deletion:: 40, running time provenance::46.381108
Num of deletion:: 50, running time provenance::58.232755
Num of deletion:: 60, running time provenance::69.153801
Num of deletion:: 70, running time provenance::82.024986
Num of deletion:: 80, running time provenance::93.897862
Num of deletion:: 90, running time provenance::104.686016
overhead:: 0
overhead2:: 0
overhead3:: 113.74654769897461
overhead4:: 0
overhead5:: 9.890087604522705
memory usage:: 3153760256
time_provenance:: 113.74904298782349
curr_diff: 0 tensor(1.6817e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6817e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628548
repetition 1
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 1 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 1.769800
Train - Epoch 0, Batch: 10, Loss: 1.341836
Train - Epoch 0, Batch: 20, Loss: 1.233534
Train - Epoch 0, Batch: 30, Loss: 1.177191
Train - Epoch 1, Batch: 0, Loss: 1.174332
Train - Epoch 1, Batch: 10, Loss: 1.153787
Train - Epoch 1, Batch: 20, Loss: 1.137826
Train - Epoch 1, Batch: 30, Loss: 1.134623
Train - Epoch 2, Batch: 0, Loss: 1.124573
Train - Epoch 2, Batch: 10, Loss: 1.122975
Train - Epoch 2, Batch: 20, Loss: 1.108830
Train - Epoch 2, Batch: 30, Loss: 1.090453
Train - Epoch 3, Batch: 0, Loss: 1.088975
Train - Epoch 3, Batch: 10, Loss: 1.090086
Train - Epoch 3, Batch: 20, Loss: 1.084231
Train - Epoch 3, Batch: 30, Loss: 1.075149
Train - Epoch 4, Batch: 0, Loss: 1.071673
Train - Epoch 4, Batch: 10, Loss: 1.062391
Train - Epoch 4, Batch: 20, Loss: 1.057826
Train - Epoch 4, Batch: 30, Loss: 1.049856
Train - Epoch 5, Batch: 0, Loss: 1.047887
Train - Epoch 5, Batch: 10, Loss: 1.047801
Train - Epoch 5, Batch: 20, Loss: 1.050542
Train - Epoch 5, Batch: 30, Loss: 1.042100
Train - Epoch 6, Batch: 0, Loss: 1.044497
Train - Epoch 6, Batch: 10, Loss: 1.029115
Train - Epoch 6, Batch: 20, Loss: 1.027373
Train - Epoch 6, Batch: 30, Loss: 1.025992
Train - Epoch 7, Batch: 0, Loss: 1.015780
Train - Epoch 7, Batch: 10, Loss: 1.007112
Train - Epoch 7, Batch: 20, Loss: 1.026242
Train - Epoch 7, Batch: 30, Loss: 1.013179
Train - Epoch 8, Batch: 0, Loss: 1.002298
Train - Epoch 8, Batch: 10, Loss: 1.009284
Train - Epoch 8, Batch: 20, Loss: 1.004348
Train - Epoch 8, Batch: 30, Loss: 1.002661
Train - Epoch 9, Batch: 0, Loss: 1.001661
Train - Epoch 9, Batch: 10, Loss: 1.004095
Train - Epoch 9, Batch: 20, Loss: 0.997960
Train - Epoch 9, Batch: 30, Loss: 0.985214
Train - Epoch 10, Batch: 0, Loss: 0.987449
Train - Epoch 10, Batch: 10, Loss: 0.978125
Train - Epoch 10, Batch: 20, Loss: 0.988995
Train - Epoch 10, Batch: 30, Loss: 0.977616
Train - Epoch 11, Batch: 0, Loss: 0.975588
Train - Epoch 11, Batch: 10, Loss: 0.990032
Train - Epoch 11, Batch: 20, Loss: 0.967845
Train - Epoch 11, Batch: 30, Loss: 0.966038
Train - Epoch 12, Batch: 0, Loss: 0.981365
Train - Epoch 12, Batch: 10, Loss: 0.972644
Train - Epoch 12, Batch: 20, Loss: 0.978221
Train - Epoch 12, Batch: 30, Loss: 0.959309
Train - Epoch 13, Batch: 0, Loss: 0.968508
Train - Epoch 13, Batch: 10, Loss: 0.961040
Train - Epoch 13, Batch: 20, Loss: 0.964807
Train - Epoch 13, Batch: 30, Loss: 0.956641
Train - Epoch 14, Batch: 0, Loss: 0.971606
Train - Epoch 14, Batch: 10, Loss: 0.956584
Train - Epoch 14, Batch: 20, Loss: 0.954285
Train - Epoch 14, Batch: 30, Loss: 0.947244
Train - Epoch 15, Batch: 0, Loss: 0.961172
Train - Epoch 15, Batch: 10, Loss: 0.951881
Train - Epoch 15, Batch: 20, Loss: 0.952063
Train - Epoch 15, Batch: 30, Loss: 0.957939
Train - Epoch 16, Batch: 0, Loss: 0.939379
Train - Epoch 16, Batch: 10, Loss: 0.940320
Train - Epoch 16, Batch: 20, Loss: 0.943372
Train - Epoch 16, Batch: 30, Loss: 0.940604
Train - Epoch 17, Batch: 0, Loss: 0.951332
Train - Epoch 17, Batch: 10, Loss: 0.945541
Train - Epoch 17, Batch: 20, Loss: 0.948667
Train - Epoch 17, Batch: 30, Loss: 0.938041
Train - Epoch 18, Batch: 0, Loss: 0.930357
Train - Epoch 18, Batch: 10, Loss: 0.939930
Train - Epoch 18, Batch: 20, Loss: 0.927135
Train - Epoch 18, Batch: 30, Loss: 0.933060
Train - Epoch 19, Batch: 0, Loss: 0.942093
Train - Epoch 19, Batch: 10, Loss: 0.933382
Train - Epoch 19, Batch: 20, Loss: 0.936990
Train - Epoch 19, Batch: 30, Loss: 0.929083
Train - Epoch 20, Batch: 0, Loss: 0.931555
Train - Epoch 20, Batch: 10, Loss: 0.930917
Train - Epoch 20, Batch: 20, Loss: 0.929120
Train - Epoch 20, Batch: 30, Loss: 0.919322
Train - Epoch 21, Batch: 0, Loss: 0.929664
Train - Epoch 21, Batch: 10, Loss: 0.934425
Train - Epoch 21, Batch: 20, Loss: 0.927435
Train - Epoch 21, Batch: 30, Loss: 0.936446
Train - Epoch 22, Batch: 0, Loss: 0.916705
Train - Epoch 22, Batch: 10, Loss: 0.927569
Train - Epoch 22, Batch: 20, Loss: 0.916572
Train - Epoch 22, Batch: 30, Loss: 0.913932
Train - Epoch 23, Batch: 0, Loss: 0.912019
Train - Epoch 23, Batch: 10, Loss: 0.913491
Train - Epoch 23, Batch: 20, Loss: 0.922330
Train - Epoch 23, Batch: 30, Loss: 0.916241
Train - Epoch 24, Batch: 0, Loss: 0.917414
Train - Epoch 24, Batch: 10, Loss: 0.906815
Train - Epoch 24, Batch: 20, Loss: 0.906670
Train - Epoch 24, Batch: 30, Loss: 0.913796
Train - Epoch 25, Batch: 0, Loss: 0.919472
Train - Epoch 25, Batch: 10, Loss: 0.913234
Train - Epoch 25, Batch: 20, Loss: 0.915934
Train - Epoch 25, Batch: 30, Loss: 0.912505
Train - Epoch 26, Batch: 0, Loss: 0.911672
Train - Epoch 26, Batch: 10, Loss: 0.905639
Train - Epoch 26, Batch: 20, Loss: 0.912051
Train - Epoch 26, Batch: 30, Loss: 0.908463
Train - Epoch 27, Batch: 0, Loss: 0.909179
Train - Epoch 27, Batch: 10, Loss: 0.916737
Train - Epoch 27, Batch: 20, Loss: 0.898290
Train - Epoch 27, Batch: 30, Loss: 0.905925
Train - Epoch 28, Batch: 0, Loss: 0.899579
Train - Epoch 28, Batch: 10, Loss: 0.901160
Train - Epoch 28, Batch: 20, Loss: 0.904671
Train - Epoch 28, Batch: 30, Loss: 0.904090
Train - Epoch 29, Batch: 0, Loss: 0.905107
Train - Epoch 29, Batch: 10, Loss: 0.903403
Train - Epoch 29, Batch: 20, Loss: 0.894662
Train - Epoch 29, Batch: 30, Loss: 0.894385
Train - Epoch 30, Batch: 0, Loss: 0.903772
Train - Epoch 30, Batch: 10, Loss: 0.903076
Train - Epoch 30, Batch: 20, Loss: 0.898647
Train - Epoch 30, Batch: 30, Loss: 0.894614
Train - Epoch 31, Batch: 0, Loss: 0.896202
Train - Epoch 31, Batch: 10, Loss: 0.892323
Train - Epoch 31, Batch: 20, Loss: 0.898019
Train - Epoch 31, Batch: 30, Loss: 0.898383
Test Avg. Loss: 0.000070, Accuracy: 0.628152
training_time:: 4.256208181381226
training time full:: 4.256273031234741
provenance prepare time:: 6.67572021484375e-06
Test Avg. Loss: 0.000070, Accuracy: 0.628152
baseline::
python3 incremental_updates_base_line_lr_multi.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Benchmark_experiments/benchmark_exp.py:1298: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  curr_delta_id = torch.tensor(delta_id)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 99
max_epoch:: 32
batch_size:: 16384
Num of deletion:: 0, running time baseline::2.494683
Num of deletion:: 10, running time baseline::25.004743
Num of deletion:: 20, running time baseline::45.060575
Num of deletion:: 30, running time baseline::66.672012
Num of deletion:: 40, running time baseline::87.079692
Num of deletion:: 50, running time baseline::107.537200
Num of deletion:: 60, running time baseline::127.516214
Num of deletion:: 70, running time baseline::147.354823
Num of deletion:: 80, running time baseline::167.434345
Num of deletion:: 90, running time baseline::187.293476
training time is 203.1248962879181
overhead:: 0
overhead2:: 203.11905360221863
overhead3:: 8.023449182510376
time_baseline:: 203.1259024143219
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628169
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance5_lr_multi.py 10 5 1 0.00019123 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 99
max_epoch:: 32
Num of deletion:: 0, running time provenance::1.172881
Num of deletion:: 10, running time provenance::11.461332
Num of deletion:: 20, running time provenance::22.643065
Num of deletion:: 30, running time provenance::34.340766
Num of deletion:: 40, running time provenance::46.024786
Num of deletion:: 50, running time provenance::57.023643
Num of deletion:: 60, running time provenance::69.662409
Num of deletion:: 70, running time provenance::80.875229
Num of deletion:: 80, running time provenance::92.569925
Num of deletion:: 90, running time provenance::104.756042
overhead:: 0
overhead2:: 0
overhead3:: 113.41306614875793
overhead4:: 0
overhead5:: 9.772937059402466
memory usage:: 3163131904
time_provenance:: 113.41581964492798
curr_diff: 0 tensor(2.1463e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1463e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628169
repetition 2
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 2 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 1.918898
Train - Epoch 0, Batch: 10, Loss: 1.384921
Train - Epoch 0, Batch: 20, Loss: 1.252599
Train - Epoch 0, Batch: 30, Loss: 1.195003
Train - Epoch 1, Batch: 0, Loss: 1.201320
Train - Epoch 1, Batch: 10, Loss: 1.164359
Train - Epoch 1, Batch: 20, Loss: 1.154913
Train - Epoch 1, Batch: 30, Loss: 1.123024
Train - Epoch 2, Batch: 0, Loss: 1.132321
Train - Epoch 2, Batch: 10, Loss: 1.122489
Train - Epoch 2, Batch: 20, Loss: 1.119266
Train - Epoch 2, Batch: 30, Loss: 1.095747
Train - Epoch 3, Batch: 0, Loss: 1.105091
Train - Epoch 3, Batch: 10, Loss: 1.092053
Train - Epoch 3, Batch: 20, Loss: 1.091458
Train - Epoch 3, Batch: 30, Loss: 1.073156
Train - Epoch 4, Batch: 0, Loss: 1.079364
Train - Epoch 4, Batch: 10, Loss: 1.073242
Train - Epoch 4, Batch: 20, Loss: 1.072385
Train - Epoch 4, Batch: 30, Loss: 1.064434
Train - Epoch 5, Batch: 0, Loss: 1.070169
Train - Epoch 5, Batch: 10, Loss: 1.047401
Train - Epoch 5, Batch: 20, Loss: 1.048626
Train - Epoch 5, Batch: 30, Loss: 1.036046
Train - Epoch 6, Batch: 0, Loss: 1.042957
Train - Epoch 6, Batch: 10, Loss: 1.034717
Train - Epoch 6, Batch: 20, Loss: 1.035534
Train - Epoch 6, Batch: 30, Loss: 1.028460
Train - Epoch 7, Batch: 0, Loss: 1.035948
Train - Epoch 7, Batch: 10, Loss: 1.014248
Train - Epoch 7, Batch: 20, Loss: 1.015826
Train - Epoch 7, Batch: 30, Loss: 1.006503
Train - Epoch 8, Batch: 0, Loss: 1.019844
Train - Epoch 8, Batch: 10, Loss: 1.005722
Train - Epoch 8, Batch: 20, Loss: 1.011555
Train - Epoch 8, Batch: 30, Loss: 1.004533
Train - Epoch 9, Batch: 0, Loss: 1.004464
Train - Epoch 9, Batch: 10, Loss: 1.000085
Train - Epoch 9, Batch: 20, Loss: 0.991682
Train - Epoch 9, Batch: 30, Loss: 0.995213
Train - Epoch 10, Batch: 0, Loss: 0.989593
Train - Epoch 10, Batch: 10, Loss: 0.983233
Train - Epoch 10, Batch: 20, Loss: 0.986286
Train - Epoch 10, Batch: 30, Loss: 0.986531
Train - Epoch 11, Batch: 0, Loss: 0.985514
Train - Epoch 11, Batch: 10, Loss: 0.984349
Train - Epoch 11, Batch: 20, Loss: 0.975597
Train - Epoch 11, Batch: 30, Loss: 0.973232
Train - Epoch 12, Batch: 0, Loss: 0.976421
Train - Epoch 12, Batch: 10, Loss: 0.967606
Train - Epoch 12, Batch: 20, Loss: 0.974041
Train - Epoch 12, Batch: 30, Loss: 0.969620
Train - Epoch 13, Batch: 0, Loss: 0.969042
Train - Epoch 13, Batch: 10, Loss: 0.963881
Train - Epoch 13, Batch: 20, Loss: 0.960112
Train - Epoch 13, Batch: 30, Loss: 0.961640
Train - Epoch 14, Batch: 0, Loss: 0.974770
Train - Epoch 14, Batch: 10, Loss: 0.957759
Train - Epoch 14, Batch: 20, Loss: 0.952705
Train - Epoch 14, Batch: 30, Loss: 0.960247
Train - Epoch 15, Batch: 0, Loss: 0.949576
Train - Epoch 15, Batch: 10, Loss: 0.944656
Train - Epoch 15, Batch: 20, Loss: 0.962996
Train - Epoch 15, Batch: 30, Loss: 0.957682
Train - Epoch 16, Batch: 0, Loss: 0.950157
Train - Epoch 16, Batch: 10, Loss: 0.951190
Train - Epoch 16, Batch: 20, Loss: 0.948645
Train - Epoch 16, Batch: 30, Loss: 0.941087
Train - Epoch 17, Batch: 0, Loss: 0.935911
Train - Epoch 17, Batch: 10, Loss: 0.936661
Train - Epoch 17, Batch: 20, Loss: 0.940388
Train - Epoch 17, Batch: 30, Loss: 0.954888
Train - Epoch 18, Batch: 0, Loss: 0.940781
Train - Epoch 18, Batch: 10, Loss: 0.939216
Train - Epoch 18, Batch: 20, Loss: 0.928355
Train - Epoch 18, Batch: 30, Loss: 0.934753
Train - Epoch 19, Batch: 0, Loss: 0.937130
Train - Epoch 19, Batch: 10, Loss: 0.940583
Train - Epoch 19, Batch: 20, Loss: 0.923522
Train - Epoch 19, Batch: 30, Loss: 0.936650
Train - Epoch 20, Batch: 0, Loss: 0.924928
Train - Epoch 20, Batch: 10, Loss: 0.928489
Train - Epoch 20, Batch: 20, Loss: 0.922383
Train - Epoch 20, Batch: 30, Loss: 0.934760
Train - Epoch 21, Batch: 0, Loss: 0.920154
Train - Epoch 21, Batch: 10, Loss: 0.924672
Train - Epoch 21, Batch: 20, Loss: 0.924853
Train - Epoch 21, Batch: 30, Loss: 0.922370
Train - Epoch 22, Batch: 0, Loss: 0.926884
Train - Epoch 22, Batch: 10, Loss: 0.919425
Train - Epoch 22, Batch: 20, Loss: 0.925433
Train - Epoch 22, Batch: 30, Loss: 0.919677
Train - Epoch 23, Batch: 0, Loss: 0.913629
Train - Epoch 23, Batch: 10, Loss: 0.910356
Train - Epoch 23, Batch: 20, Loss: 0.920631
Train - Epoch 23, Batch: 30, Loss: 0.938502
Train - Epoch 24, Batch: 0, Loss: 0.909876
Train - Epoch 24, Batch: 10, Loss: 0.910245
Train - Epoch 24, Batch: 20, Loss: 0.917927
Train - Epoch 24, Batch: 30, Loss: 0.910768
Train - Epoch 25, Batch: 0, Loss: 0.911279
Train - Epoch 25, Batch: 10, Loss: 0.903925
Train - Epoch 25, Batch: 20, Loss: 0.911381
Train - Epoch 25, Batch: 30, Loss: 0.911473
Train - Epoch 26, Batch: 0, Loss: 0.901189
Train - Epoch 26, Batch: 10, Loss: 0.909282
Train - Epoch 26, Batch: 20, Loss: 0.903119
Train - Epoch 26, Batch: 30, Loss: 0.909904
Train - Epoch 27, Batch: 0, Loss: 0.902466
Train - Epoch 27, Batch: 10, Loss: 0.896205
Train - Epoch 27, Batch: 20, Loss: 0.905600
Train - Epoch 27, Batch: 30, Loss: 0.905285
Train - Epoch 28, Batch: 0, Loss: 0.896114
Train - Epoch 28, Batch: 10, Loss: 0.902059
Train - Epoch 28, Batch: 20, Loss: 0.907085
Train - Epoch 28, Batch: 30, Loss: 0.893783
Train - Epoch 29, Batch: 0, Loss: 0.902072
Train - Epoch 29, Batch: 10, Loss: 0.886558
Train - Epoch 29, Batch: 20, Loss: 0.905485
Train - Epoch 29, Batch: 30, Loss: 0.910352
Train - Epoch 30, Batch: 0, Loss: 0.894573
Train - Epoch 30, Batch: 10, Loss: 0.904014
Train - Epoch 30, Batch: 20, Loss: 0.900512
Train - Epoch 30, Batch: 30, Loss: 0.885994
Train - Epoch 31, Batch: 0, Loss: 0.891750
Train - Epoch 31, Batch: 10, Loss: 0.894374
Train - Epoch 31, Batch: 20, Loss: 0.903451
Train - Epoch 31, Batch: 30, Loss: 0.897976
Test Avg. Loss: 0.000070, Accuracy: 0.628617
training_time:: 4.322988986968994
training time full:: 4.323055982589722
provenance prepare time:: 6.67572021484375e-06
Test Avg. Loss: 0.000070, Accuracy: 0.628617
baseline::
python3 incremental_updates_base_line_lr_multi.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Benchmark_experiments/benchmark_exp.py:1298: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  curr_delta_id = torch.tensor(delta_id)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 99
max_epoch:: 32
batch_size:: 16384
Num of deletion:: 0, running time baseline::1.963708
Num of deletion:: 10, running time baseline::22.689759
Num of deletion:: 20, running time baseline::42.762351
Num of deletion:: 30, running time baseline::63.802267
Num of deletion:: 40, running time baseline::83.685855
Num of deletion:: 50, running time baseline::103.367155
Num of deletion:: 60, running time baseline::123.470999
Num of deletion:: 70, running time baseline::143.081310
Num of deletion:: 80, running time baseline::162.613065
Num of deletion:: 90, running time baseline::182.408659
training time is 198.460595369339
overhead:: 0
overhead2:: 198.45462369918823
overhead3:: 8.098220348358154
time_baseline:: 198.46155881881714
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628599
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance5_lr_multi.py 10 5 2 0.00019123 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 99
max_epoch:: 32
Num of deletion:: 0, running time provenance::1.125509
Num of deletion:: 10, running time provenance::12.175387
Num of deletion:: 20, running time provenance::22.554838
Num of deletion:: 30, running time provenance::34.826529
Num of deletion:: 40, running time provenance::48.263394
Num of deletion:: 50, running time provenance::59.004100
Num of deletion:: 60, running time provenance::69.815814
Num of deletion:: 70, running time provenance::80.306124
Num of deletion:: 80, running time provenance::93.457277
Num of deletion:: 90, running time provenance::105.517331
overhead:: 0
overhead2:: 0
overhead3:: 113.82834362983704
overhead4:: 0
overhead5:: 9.446484804153442
memory usage:: 3162198016
time_provenance:: 113.83171486854553
curr_diff: 0 tensor(1.5952e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5952e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628599
repetition 3
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 3 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 1.944293
Train - Epoch 0, Batch: 10, Loss: 1.374752
Train - Epoch 0, Batch: 20, Loss: 1.226834
Train - Epoch 0, Batch: 30, Loss: 1.194029
Train - Epoch 1, Batch: 0, Loss: 1.171397
Train - Epoch 1, Batch: 10, Loss: 1.154562
Train - Epoch 1, Batch: 20, Loss: 1.138282
Train - Epoch 1, Batch: 30, Loss: 1.131547
Train - Epoch 2, Batch: 0, Loss: 1.119933
Train - Epoch 2, Batch: 10, Loss: 1.114309
Train - Epoch 2, Batch: 20, Loss: 1.095848
Train - Epoch 2, Batch: 30, Loss: 1.100132
Train - Epoch 3, Batch: 0, Loss: 1.091366
Train - Epoch 3, Batch: 10, Loss: 1.083519
Train - Epoch 3, Batch: 20, Loss: 1.081776
Train - Epoch 3, Batch: 30, Loss: 1.064607
Train - Epoch 4, Batch: 0, Loss: 1.065370
Train - Epoch 4, Batch: 10, Loss: 1.059302
Train - Epoch 4, Batch: 20, Loss: 1.057487
Train - Epoch 4, Batch: 30, Loss: 1.042947
Train - Epoch 5, Batch: 0, Loss: 1.052523
Train - Epoch 5, Batch: 10, Loss: 1.049352
Train - Epoch 5, Batch: 20, Loss: 1.047024
Train - Epoch 5, Batch: 30, Loss: 1.039241
Train - Epoch 6, Batch: 0, Loss: 1.037869
Train - Epoch 6, Batch: 10, Loss: 1.029887
Train - Epoch 6, Batch: 20, Loss: 1.016157
Train - Epoch 6, Batch: 30, Loss: 1.019985
Train - Epoch 7, Batch: 0, Loss: 1.021400
Train - Epoch 7, Batch: 10, Loss: 1.015726
Train - Epoch 7, Batch: 20, Loss: 1.011920
Train - Epoch 7, Batch: 30, Loss: 1.003945
Train - Epoch 8, Batch: 0, Loss: 1.011424
Train - Epoch 8, Batch: 10, Loss: 1.002560
Train - Epoch 8, Batch: 20, Loss: 1.003315
Train - Epoch 8, Batch: 30, Loss: 1.001291
Train - Epoch 9, Batch: 0, Loss: 1.005914
Train - Epoch 9, Batch: 10, Loss: 0.990190
Train - Epoch 9, Batch: 20, Loss: 0.992238
Train - Epoch 9, Batch: 30, Loss: 0.994812
Train - Epoch 10, Batch: 0, Loss: 0.991523
Train - Epoch 10, Batch: 10, Loss: 0.990795
Train - Epoch 10, Batch: 20, Loss: 0.980364
Train - Epoch 10, Batch: 30, Loss: 0.980768
Train - Epoch 11, Batch: 0, Loss: 0.986831
Train - Epoch 11, Batch: 10, Loss: 0.975536
Train - Epoch 11, Batch: 20, Loss: 0.986236
Train - Epoch 11, Batch: 30, Loss: 0.969283
Train - Epoch 12, Batch: 0, Loss: 0.973651
Train - Epoch 12, Batch: 10, Loss: 0.979654
Train - Epoch 12, Batch: 20, Loss: 0.974667
Train - Epoch 12, Batch: 30, Loss: 0.972546
Train - Epoch 13, Batch: 0, Loss: 0.970109
Train - Epoch 13, Batch: 10, Loss: 0.973853
Train - Epoch 13, Batch: 20, Loss: 0.955945
Train - Epoch 13, Batch: 30, Loss: 0.962549
Train - Epoch 14, Batch: 0, Loss: 0.957192
Train - Epoch 14, Batch: 10, Loss: 0.966861
Train - Epoch 14, Batch: 20, Loss: 0.958150
Train - Epoch 14, Batch: 30, Loss: 0.952907
Train - Epoch 15, Batch: 0, Loss: 0.953755
Train - Epoch 15, Batch: 10, Loss: 0.960777
Train - Epoch 15, Batch: 20, Loss: 0.944478
Train - Epoch 15, Batch: 30, Loss: 0.951195
Train - Epoch 16, Batch: 0, Loss: 0.951100
Train - Epoch 16, Batch: 10, Loss: 0.947664
Train - Epoch 16, Batch: 20, Loss: 0.938205
Train - Epoch 16, Batch: 30, Loss: 0.945109
Train - Epoch 17, Batch: 0, Loss: 0.939352
Train - Epoch 17, Batch: 10, Loss: 0.948035
Train - Epoch 17, Batch: 20, Loss: 0.939366
Train - Epoch 17, Batch: 30, Loss: 0.936842
Train - Epoch 18, Batch: 0, Loss: 0.929703
Train - Epoch 18, Batch: 10, Loss: 0.938892
Train - Epoch 18, Batch: 20, Loss: 0.935753
Train - Epoch 18, Batch: 30, Loss: 0.935179
Train - Epoch 19, Batch: 0, Loss: 0.932949
Train - Epoch 19, Batch: 10, Loss: 0.914949
Train - Epoch 19, Batch: 20, Loss: 0.930100
Train - Epoch 19, Batch: 30, Loss: 0.924265
Train - Epoch 20, Batch: 0, Loss: 0.925502
Train - Epoch 20, Batch: 10, Loss: 0.913102
Train - Epoch 20, Batch: 20, Loss: 0.924142
Train - Epoch 20, Batch: 30, Loss: 0.923726
Train - Epoch 21, Batch: 0, Loss: 0.916916
Train - Epoch 21, Batch: 10, Loss: 0.924540
Train - Epoch 21, Batch: 20, Loss: 0.929313
Train - Epoch 21, Batch: 30, Loss: 0.928086
Train - Epoch 22, Batch: 0, Loss: 0.926147
Train - Epoch 22, Batch: 10, Loss: 0.916234
Train - Epoch 22, Batch: 20, Loss: 0.918018
Train - Epoch 22, Batch: 30, Loss: 0.921163
Train - Epoch 23, Batch: 0, Loss: 0.915265
Train - Epoch 23, Batch: 10, Loss: 0.916397
Train - Epoch 23, Batch: 20, Loss: 0.910469
Train - Epoch 23, Batch: 30, Loss: 0.917420
Train - Epoch 24, Batch: 0, Loss: 0.911679
Train - Epoch 24, Batch: 10, Loss: 0.911214
Train - Epoch 24, Batch: 20, Loss: 0.907708
Train - Epoch 24, Batch: 30, Loss: 0.920701
Train - Epoch 25, Batch: 0, Loss: 0.909076
Train - Epoch 25, Batch: 10, Loss: 0.913651
Train - Epoch 25, Batch: 20, Loss: 0.909128
Train - Epoch 25, Batch: 30, Loss: 0.911015
Train - Epoch 26, Batch: 0, Loss: 0.911267
Train - Epoch 26, Batch: 10, Loss: 0.912654
Train - Epoch 26, Batch: 20, Loss: 0.908966
Train - Epoch 26, Batch: 30, Loss: 0.914443
Train - Epoch 27, Batch: 0, Loss: 0.904251
Train - Epoch 27, Batch: 10, Loss: 0.910367
Train - Epoch 27, Batch: 20, Loss: 0.898286
Train - Epoch 27, Batch: 30, Loss: 0.896121
Train - Epoch 28, Batch: 0, Loss: 0.902853
Train - Epoch 28, Batch: 10, Loss: 0.901376
Train - Epoch 28, Batch: 20, Loss: 0.898887
Train - Epoch 28, Batch: 30, Loss: 0.916417
Train - Epoch 29, Batch: 0, Loss: 0.914393
Train - Epoch 29, Batch: 10, Loss: 0.902574
Train - Epoch 29, Batch: 20, Loss: 0.904635
Train - Epoch 29, Batch: 30, Loss: 0.894772
Train - Epoch 30, Batch: 0, Loss: 0.890088
Train - Epoch 30, Batch: 10, Loss: 0.895192
Train - Epoch 30, Batch: 20, Loss: 0.906665
Train - Epoch 30, Batch: 30, Loss: 0.898275
Train - Epoch 31, Batch: 0, Loss: 0.890418
Train - Epoch 31, Batch: 10, Loss: 0.890511
Train - Epoch 31, Batch: 20, Loss: 0.901619
Train - Epoch 31, Batch: 30, Loss: 0.901569
Test Avg. Loss: 0.000070, Accuracy: 0.628926
training_time:: 4.28019642829895
training time full:: 4.2802581787109375
provenance prepare time:: 6.4373016357421875e-06
Test Avg. Loss: 0.000070, Accuracy: 0.628926
baseline::
python3 incremental_updates_base_line_lr_multi.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Benchmark_experiments/benchmark_exp.py:1298: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  curr_delta_id = torch.tensor(delta_id)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 99
max_epoch:: 32
batch_size:: 16384
Num of deletion:: 0, running time baseline::2.039690
Num of deletion:: 10, running time baseline::22.473141
Num of deletion:: 20, running time baseline::42.378107
Num of deletion:: 30, running time baseline::62.440187
Num of deletion:: 40, running time baseline::83.174438
Num of deletion:: 50, running time baseline::103.487186
Num of deletion:: 60, running time baseline::123.489043
Num of deletion:: 70, running time baseline::143.213492
Num of deletion:: 80, running time baseline::163.298487
Num of deletion:: 90, running time baseline::182.849296
training time is 198.8337595462799
overhead:: 0
overhead2:: 198.827641248703
overhead3:: 7.980318546295166
time_baseline:: 198.83478951454163
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628944
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance5_lr_multi.py 10 5 3 0.00019123 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 99
max_epoch:: 32
Num of deletion:: 0, running time provenance::1.158134
Num of deletion:: 10, running time provenance::13.096139
Num of deletion:: 20, running time provenance::25.332532
Num of deletion:: 30, running time provenance::37.209957
Num of deletion:: 40, running time provenance::49.449995
Num of deletion:: 50, running time provenance::61.759947
Num of deletion:: 60, running time provenance::74.176604
Num of deletion:: 70, running time provenance::85.653130
Num of deletion:: 80, running time provenance::96.149380
Num of deletion:: 90, running time provenance::106.396295
overhead:: 0
overhead2:: 0
overhead3:: 116.0701072216034
overhead4:: 0
overhead5:: 10.208117723464966
memory usage:: 3154960384
time_provenance:: 116.0728600025177
curr_diff: 0 tensor(1.7776e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.7776e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628944
repetition 4
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 4 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 2.012698
Train - Epoch 0, Batch: 10, Loss: 1.416859
Train - Epoch 0, Batch: 20, Loss: 1.255730
Train - Epoch 0, Batch: 30, Loss: 1.214843
Train - Epoch 1, Batch: 0, Loss: 1.202869
Train - Epoch 1, Batch: 10, Loss: 1.179405
Train - Epoch 1, Batch: 20, Loss: 1.164344
Train - Epoch 1, Batch: 30, Loss: 1.156401
Train - Epoch 2, Batch: 0, Loss: 1.142524
Train - Epoch 2, Batch: 10, Loss: 1.133360
Train - Epoch 2, Batch: 20, Loss: 1.115852
Train - Epoch 2, Batch: 30, Loss: 1.119559
Train - Epoch 3, Batch: 0, Loss: 1.108279
Train - Epoch 3, Batch: 10, Loss: 1.092033
Train - Epoch 3, Batch: 20, Loss: 1.096167
Train - Epoch 3, Batch: 30, Loss: 1.076754
Train - Epoch 4, Batch: 0, Loss: 1.081129
Train - Epoch 4, Batch: 10, Loss: 1.082564
Train - Epoch 4, Batch: 20, Loss: 1.065966
Train - Epoch 4, Batch: 30, Loss: 1.063583
Train - Epoch 5, Batch: 0, Loss: 1.060017
Train - Epoch 5, Batch: 10, Loss: 1.050041
Train - Epoch 5, Batch: 20, Loss: 1.055006
Train - Epoch 5, Batch: 30, Loss: 1.040715
Train - Epoch 6, Batch: 0, Loss: 1.056050
Train - Epoch 6, Batch: 10, Loss: 1.038332
Train - Epoch 6, Batch: 20, Loss: 1.037902
Train - Epoch 6, Batch: 30, Loss: 1.020050
Train - Epoch 7, Batch: 0, Loss: 1.025687
Train - Epoch 7, Batch: 10, Loss: 1.030172
Train - Epoch 7, Batch: 20, Loss: 1.017575
Train - Epoch 7, Batch: 30, Loss: 1.015689
Train - Epoch 8, Batch: 0, Loss: 1.011364
Train - Epoch 8, Batch: 10, Loss: 1.003831
Train - Epoch 8, Batch: 20, Loss: 1.009192
Train - Epoch 8, Batch: 30, Loss: 1.004500
Train - Epoch 9, Batch: 0, Loss: 1.005236
Train - Epoch 9, Batch: 10, Loss: 1.008019
Train - Epoch 9, Batch: 20, Loss: 0.995295
Train - Epoch 9, Batch: 30, Loss: 0.993776
Train - Epoch 10, Batch: 0, Loss: 1.002865
Train - Epoch 10, Batch: 10, Loss: 0.971913
Train - Epoch 10, Batch: 20, Loss: 0.994584
Train - Epoch 10, Batch: 30, Loss: 0.984550
Train - Epoch 11, Batch: 0, Loss: 0.981513
Train - Epoch 11, Batch: 10, Loss: 0.977156
Train - Epoch 11, Batch: 20, Loss: 0.985307
Train - Epoch 11, Batch: 30, Loss: 0.980657
Train - Epoch 12, Batch: 0, Loss: 0.978832
Train - Epoch 12, Batch: 10, Loss: 0.967052
Train - Epoch 12, Batch: 20, Loss: 0.971455
Train - Epoch 12, Batch: 30, Loss: 0.967409
Train - Epoch 13, Batch: 0, Loss: 0.965423
Train - Epoch 13, Batch: 10, Loss: 0.969797
Train - Epoch 13, Batch: 20, Loss: 0.967901
Train - Epoch 13, Batch: 30, Loss: 0.962124
Train - Epoch 14, Batch: 0, Loss: 0.963862
Train - Epoch 14, Batch: 10, Loss: 0.953148
Train - Epoch 14, Batch: 20, Loss: 0.957724
Train - Epoch 14, Batch: 30, Loss: 0.960467
Train - Epoch 15, Batch: 0, Loss: 0.954371
Train - Epoch 15, Batch: 10, Loss: 0.962969
Train - Epoch 15, Batch: 20, Loss: 0.951004
Train - Epoch 15, Batch: 30, Loss: 0.944334
Train - Epoch 16, Batch: 0, Loss: 0.942770
Train - Epoch 16, Batch: 10, Loss: 0.941543
Train - Epoch 16, Batch: 20, Loss: 0.942730
Train - Epoch 16, Batch: 30, Loss: 0.943395
Train - Epoch 17, Batch: 0, Loss: 0.947306
Train - Epoch 17, Batch: 10, Loss: 0.933922
Train - Epoch 17, Batch: 20, Loss: 0.940973
Train - Epoch 17, Batch: 30, Loss: 0.937522
Train - Epoch 18, Batch: 0, Loss: 0.944143
Train - Epoch 18, Batch: 10, Loss: 0.931135
Train - Epoch 18, Batch: 20, Loss: 0.939220
Train - Epoch 18, Batch: 30, Loss: 0.928870
Train - Epoch 19, Batch: 0, Loss: 0.939051
Train - Epoch 19, Batch: 10, Loss: 0.925846
Train - Epoch 19, Batch: 20, Loss: 0.929387
Train - Epoch 19, Batch: 30, Loss: 0.933247
Train - Epoch 20, Batch: 0, Loss: 0.922051
Train - Epoch 20, Batch: 10, Loss: 0.919020
Train - Epoch 20, Batch: 20, Loss: 0.922177
Train - Epoch 20, Batch: 30, Loss: 0.925424
Train - Epoch 21, Batch: 0, Loss: 0.923090
Train - Epoch 21, Batch: 10, Loss: 0.922870
Train - Epoch 21, Batch: 20, Loss: 0.917073
Train - Epoch 21, Batch: 30, Loss: 0.911928
Train - Epoch 22, Batch: 0, Loss: 0.929660
Train - Epoch 22, Batch: 10, Loss: 0.910093
Train - Epoch 22, Batch: 20, Loss: 0.912281
Train - Epoch 22, Batch: 30, Loss: 0.916420
Train - Epoch 23, Batch: 0, Loss: 0.919257
Train - Epoch 23, Batch: 10, Loss: 0.917264
Train - Epoch 23, Batch: 20, Loss: 0.916380
Train - Epoch 23, Batch: 30, Loss: 0.907489
Train - Epoch 24, Batch: 0, Loss: 0.923707
Train - Epoch 24, Batch: 10, Loss: 0.920065
Train - Epoch 24, Batch: 20, Loss: 0.916077
Train - Epoch 24, Batch: 30, Loss: 0.913406
Train - Epoch 25, Batch: 0, Loss: 0.907535
Train - Epoch 25, Batch: 10, Loss: 0.910642
Train - Epoch 25, Batch: 20, Loss: 0.917069
Train - Epoch 25, Batch: 30, Loss: 0.905673
Train - Epoch 26, Batch: 0, Loss: 0.922169
Train - Epoch 26, Batch: 10, Loss: 0.905589
Train - Epoch 26, Batch: 20, Loss: 0.900654
Train - Epoch 26, Batch: 30, Loss: 0.902351
Train - Epoch 27, Batch: 0, Loss: 0.909485
Train - Epoch 27, Batch: 10, Loss: 0.905375
Train - Epoch 27, Batch: 20, Loss: 0.906525
Train - Epoch 27, Batch: 30, Loss: 0.904663
Train - Epoch 28, Batch: 0, Loss: 0.900845
Train - Epoch 28, Batch: 10, Loss: 0.908408
Train - Epoch 28, Batch: 20, Loss: 0.903628
Train - Epoch 28, Batch: 30, Loss: 0.903700
Train - Epoch 29, Batch: 0, Loss: 0.901890
Train - Epoch 29, Batch: 10, Loss: 0.896899
Train - Epoch 29, Batch: 20, Loss: 0.900273
Train - Epoch 29, Batch: 30, Loss: 0.902588
Train - Epoch 30, Batch: 0, Loss: 0.899124
Train - Epoch 30, Batch: 10, Loss: 0.897241
Train - Epoch 30, Batch: 20, Loss: 0.889316
Train - Epoch 30, Batch: 30, Loss: 0.901081
Train - Epoch 31, Batch: 0, Loss: 0.898104
Train - Epoch 31, Batch: 10, Loss: 0.894423
Train - Epoch 31, Batch: 20, Loss: 0.898697
Train - Epoch 31, Batch: 30, Loss: 0.898777
Test Avg. Loss: 0.000070, Accuracy: 0.628255
training_time:: 3.833350419998169
training time full:: 3.8334150314331055
provenance prepare time:: 7.62939453125e-06
Test Avg. Loss: 0.000070, Accuracy: 0.628255
baseline::
python3 incremental_updates_base_line_lr_multi.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Benchmark_experiments/benchmark_exp.py:1298: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  curr_delta_id = torch.tensor(delta_id)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 99
max_epoch:: 32
batch_size:: 16384
Num of deletion:: 0, running time baseline::2.068682
Num of deletion:: 10, running time baseline::22.853571
Num of deletion:: 20, running time baseline::43.482271
Num of deletion:: 30, running time baseline::64.276151
Num of deletion:: 40, running time baseline::85.199603
Num of deletion:: 50, running time baseline::105.541216
Num of deletion:: 60, running time baseline::125.984340
Num of deletion:: 70, running time baseline::146.250320
Num of deletion:: 80, running time baseline::165.470753
Num of deletion:: 90, running time baseline::184.880168
training time is 200.5618155002594
overhead:: 0
overhead2:: 200.55560445785522
overhead3:: 8.095475435256958
time_baseline:: 200.562814950943
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628255
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance5_lr_multi.py 10 5 4 0.00019123 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 99
max_epoch:: 32
Num of deletion:: 0, running time provenance::1.225099
Num of deletion:: 10, running time provenance::12.777063
Num of deletion:: 20, running time provenance::23.287607
Num of deletion:: 30, running time provenance::35.259581
Num of deletion:: 40, running time provenance::48.455731
Num of deletion:: 50, running time provenance::59.720658
Num of deletion:: 60, running time provenance::72.115673
Num of deletion:: 70, running time provenance::84.880366
Num of deletion:: 80, running time provenance::97.747763
Num of deletion:: 90, running time provenance::109.250819
overhead:: 0
overhead2:: 0
overhead3:: 117.4516019821167
overhead4:: 0
overhead5:: 10.283316850662231
memory usage:: 3154358272
time_provenance:: 117.45413517951965
curr_diff: 0 tensor(2.5798e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.5798e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628255
repetition 5
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 5 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 1.899203
Train - Epoch 0, Batch: 10, Loss: 1.374489
Train - Epoch 0, Batch: 20, Loss: 1.227934
Train - Epoch 0, Batch: 30, Loss: 1.182634
Train - Epoch 1, Batch: 0, Loss: 1.180215
Train - Epoch 1, Batch: 10, Loss: 1.150152
Train - Epoch 1, Batch: 20, Loss: 1.131418
Train - Epoch 1, Batch: 30, Loss: 1.130102
Train - Epoch 2, Batch: 0, Loss: 1.132814
Train - Epoch 2, Batch: 10, Loss: 1.109995
Train - Epoch 2, Batch: 20, Loss: 1.105000
Train - Epoch 2, Batch: 30, Loss: 1.094909
Train - Epoch 3, Batch: 0, Loss: 1.099072
Train - Epoch 3, Batch: 10, Loss: 1.087815
Train - Epoch 3, Batch: 20, Loss: 1.075472
Train - Epoch 3, Batch: 30, Loss: 1.076950
Train - Epoch 4, Batch: 0, Loss: 1.075238
Train - Epoch 4, Batch: 10, Loss: 1.059206
Train - Epoch 4, Batch: 20, Loss: 1.066304
Train - Epoch 4, Batch: 30, Loss: 1.043822
Train - Epoch 5, Batch: 0, Loss: 1.054515
Train - Epoch 5, Batch: 10, Loss: 1.045336
Train - Epoch 5, Batch: 20, Loss: 1.035272
Train - Epoch 5, Batch: 30, Loss: 1.038080
Train - Epoch 6, Batch: 0, Loss: 1.032327
Train - Epoch 6, Batch: 10, Loss: 1.029732
Train - Epoch 6, Batch: 20, Loss: 1.025808
Train - Epoch 6, Batch: 30, Loss: 1.018172
Train - Epoch 7, Batch: 0, Loss: 1.025352
Train - Epoch 7, Batch: 10, Loss: 1.012669
Train - Epoch 7, Batch: 20, Loss: 1.005224
Train - Epoch 7, Batch: 30, Loss: 1.019497
Train - Epoch 8, Batch: 0, Loss: 1.014440
Train - Epoch 8, Batch: 10, Loss: 1.009093
Train - Epoch 8, Batch: 20, Loss: 1.003061
Train - Epoch 8, Batch: 30, Loss: 0.994126
Train - Epoch 9, Batch: 0, Loss: 1.000422
Train - Epoch 9, Batch: 10, Loss: 0.997023
Train - Epoch 9, Batch: 20, Loss: 0.991877
Train - Epoch 9, Batch: 30, Loss: 0.997394
Train - Epoch 10, Batch: 0, Loss: 0.989767
Train - Epoch 10, Batch: 10, Loss: 0.973594
Train - Epoch 10, Batch: 20, Loss: 0.986526
Train - Epoch 10, Batch: 30, Loss: 0.973265
Train - Epoch 11, Batch: 0, Loss: 0.979658
Train - Epoch 11, Batch: 10, Loss: 0.985228
Train - Epoch 11, Batch: 20, Loss: 0.970846
Train - Epoch 11, Batch: 30, Loss: 0.983029
Train - Epoch 12, Batch: 0, Loss: 0.971374
Train - Epoch 12, Batch: 10, Loss: 0.976684
Train - Epoch 12, Batch: 20, Loss: 0.980010
Train - Epoch 12, Batch: 30, Loss: 0.962332
Train - Epoch 13, Batch: 0, Loss: 0.972397
Train - Epoch 13, Batch: 10, Loss: 0.968832
Train - Epoch 13, Batch: 20, Loss: 0.964384
Train - Epoch 13, Batch: 30, Loss: 0.956744
Train - Epoch 14, Batch: 0, Loss: 0.967097
Train - Epoch 14, Batch: 10, Loss: 0.950270
Train - Epoch 14, Batch: 20, Loss: 0.957992
Train - Epoch 14, Batch: 30, Loss: 0.957716
Train - Epoch 15, Batch: 0, Loss: 0.949908
Train - Epoch 15, Batch: 10, Loss: 0.950992
Train - Epoch 15, Batch: 20, Loss: 0.948787
Train - Epoch 15, Batch: 30, Loss: 0.949686
Train - Epoch 16, Batch: 0, Loss: 0.948729
Train - Epoch 16, Batch: 10, Loss: 0.944193
Train - Epoch 16, Batch: 20, Loss: 0.952008
Train - Epoch 16, Batch: 30, Loss: 0.938417
Train - Epoch 17, Batch: 0, Loss: 0.946806
Train - Epoch 17, Batch: 10, Loss: 0.931703
Train - Epoch 17, Batch: 20, Loss: 0.938359
Train - Epoch 17, Batch: 30, Loss: 0.942066
Train - Epoch 18, Batch: 0, Loss: 0.930632
Train - Epoch 18, Batch: 10, Loss: 0.934788
Train - Epoch 18, Batch: 20, Loss: 0.934177
Train - Epoch 18, Batch: 30, Loss: 0.928563
Train - Epoch 19, Batch: 0, Loss: 0.926480
Train - Epoch 19, Batch: 10, Loss: 0.924836
Train - Epoch 19, Batch: 20, Loss: 0.936081
Train - Epoch 19, Batch: 30, Loss: 0.915609
Train - Epoch 20, Batch: 0, Loss: 0.938469
Train - Epoch 20, Batch: 10, Loss: 0.930053
Train - Epoch 20, Batch: 20, Loss: 0.925904
Train - Epoch 20, Batch: 30, Loss: 0.927139
Train - Epoch 21, Batch: 0, Loss: 0.924877
Train - Epoch 21, Batch: 10, Loss: 0.931910
Train - Epoch 21, Batch: 20, Loss: 0.917526
Train - Epoch 21, Batch: 30, Loss: 0.918771
Train - Epoch 22, Batch: 0, Loss: 0.920485
Train - Epoch 22, Batch: 10, Loss: 0.920773
Train - Epoch 22, Batch: 20, Loss: 0.919273
Train - Epoch 22, Batch: 30, Loss: 0.926319
Train - Epoch 23, Batch: 0, Loss: 0.914461
Train - Epoch 23, Batch: 10, Loss: 0.920261
Train - Epoch 23, Batch: 20, Loss: 0.920020
Train - Epoch 23, Batch: 30, Loss: 0.919243
Train - Epoch 24, Batch: 0, Loss: 0.918452
Train - Epoch 24, Batch: 10, Loss: 0.909384
Train - Epoch 24, Batch: 20, Loss: 0.910341
Train - Epoch 24, Batch: 30, Loss: 0.906991
Train - Epoch 25, Batch: 0, Loss: 0.907590
Train - Epoch 25, Batch: 10, Loss: 0.911574
Train - Epoch 25, Batch: 20, Loss: 0.910807
Train - Epoch 25, Batch: 30, Loss: 0.904650
Train - Epoch 26, Batch: 0, Loss: 0.913710
Train - Epoch 26, Batch: 10, Loss: 0.914810
Train - Epoch 26, Batch: 20, Loss: 0.907538
Train - Epoch 26, Batch: 30, Loss: 0.910214
Train - Epoch 27, Batch: 0, Loss: 0.911330
Train - Epoch 27, Batch: 10, Loss: 0.908115
Train - Epoch 27, Batch: 20, Loss: 0.908930
Train - Epoch 27, Batch: 30, Loss: 0.905159
Train - Epoch 28, Batch: 0, Loss: 0.905923
Train - Epoch 28, Batch: 10, Loss: 0.904224
Train - Epoch 28, Batch: 20, Loss: 0.908490
Train - Epoch 28, Batch: 30, Loss: 0.897298
Train - Epoch 29, Batch: 0, Loss: 0.904495
Train - Epoch 29, Batch: 10, Loss: 0.895231
Train - Epoch 29, Batch: 20, Loss: 0.899906
Train - Epoch 29, Batch: 30, Loss: 0.906505
Train - Epoch 30, Batch: 0, Loss: 0.905288
Train - Epoch 30, Batch: 10, Loss: 0.902360
Train - Epoch 30, Batch: 20, Loss: 0.889523
Train - Epoch 30, Batch: 30, Loss: 0.903660
Train - Epoch 31, Batch: 0, Loss: 0.896086
Train - Epoch 31, Batch: 10, Loss: 0.891303
Train - Epoch 31, Batch: 20, Loss: 0.894060
Train - Epoch 31, Batch: 30, Loss: 0.896883
Test Avg. Loss: 0.000070, Accuracy: 0.628169
training_time:: 4.283137559890747
training time full:: 4.283204078674316
provenance prepare time:: 7.3909759521484375e-06
Test Avg. Loss: 0.000070, Accuracy: 0.628169
baseline::
python3 incremental_updates_base_line_lr_multi.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Benchmark_experiments/benchmark_exp.py:1298: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  curr_delta_id = torch.tensor(delta_id)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 99
max_epoch:: 32
batch_size:: 16384
Num of deletion:: 0, running time baseline::2.022844
Num of deletion:: 10, running time baseline::23.071300
Num of deletion:: 20, running time baseline::43.701408
Num of deletion:: 30, running time baseline::64.030194
Num of deletion:: 40, running time baseline::84.305866
Num of deletion:: 50, running time baseline::103.771488
Num of deletion:: 60, running time baseline::123.930878
Num of deletion:: 70, running time baseline::143.475791
Num of deletion:: 80, running time baseline::163.160471
Num of deletion:: 90, running time baseline::182.425900
training time is 198.5474705696106
overhead:: 0
overhead2:: 198.54161667823792
overhead3:: 8.276348352432251
time_baseline:: 198.5484471321106
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628186
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance5_lr_multi.py 10 5 5 0.00019123 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 99
max_epoch:: 32
Num of deletion:: 0, running time provenance::1.164515
Num of deletion:: 10, running time provenance::13.434553
Num of deletion:: 20, running time provenance::24.520491
Num of deletion:: 30, running time provenance::37.620776
Num of deletion:: 40, running time provenance::49.367131
Num of deletion:: 50, running time provenance::60.432950
Num of deletion:: 60, running time provenance::71.803847
Num of deletion:: 70, running time provenance::83.018711
Num of deletion:: 80, running time provenance::95.332894
Num of deletion:: 90, running time provenance::106.489828
overhead:: 0
overhead2:: 0
overhead3:: 114.89823246002197
overhead4:: 0
overhead5:: 10.052183151245117
memory usage:: 3157090304
time_provenance:: 114.90093970298767
curr_diff: 0 tensor(1.3636e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3636e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628186
repetition 6
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 6 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 1.901212
Train - Epoch 0, Batch: 10, Loss: 1.381425
Train - Epoch 0, Batch: 20, Loss: 1.245880
Train - Epoch 0, Batch: 30, Loss: 1.190307
Train - Epoch 1, Batch: 0, Loss: 1.184839
Train - Epoch 1, Batch: 10, Loss: 1.160596
Train - Epoch 1, Batch: 20, Loss: 1.144724
Train - Epoch 1, Batch: 30, Loss: 1.132547
Train - Epoch 2, Batch: 0, Loss: 1.133948
Train - Epoch 2, Batch: 10, Loss: 1.122559
Train - Epoch 2, Batch: 20, Loss: 1.113173
Train - Epoch 2, Batch: 30, Loss: 1.099882
Train - Epoch 3, Batch: 0, Loss: 1.101431
Train - Epoch 3, Batch: 10, Loss: 1.087673
Train - Epoch 3, Batch: 20, Loss: 1.086156
Train - Epoch 3, Batch: 30, Loss: 1.084479
Train - Epoch 4, Batch: 0, Loss: 1.086900
Train - Epoch 4, Batch: 10, Loss: 1.069301
Train - Epoch 4, Batch: 20, Loss: 1.068849
Train - Epoch 4, Batch: 30, Loss: 1.059487
Train - Epoch 5, Batch: 0, Loss: 1.063775
Train - Epoch 5, Batch: 10, Loss: 1.039494
Train - Epoch 5, Batch: 20, Loss: 1.055658
Train - Epoch 5, Batch: 30, Loss: 1.040046
Train - Epoch 6, Batch: 0, Loss: 1.041556
Train - Epoch 6, Batch: 10, Loss: 1.037644
Train - Epoch 6, Batch: 20, Loss: 1.034708
Train - Epoch 6, Batch: 30, Loss: 1.028466
Train - Epoch 7, Batch: 0, Loss: 1.018156
Train - Epoch 7, Batch: 10, Loss: 1.022583
Train - Epoch 7, Batch: 20, Loss: 1.010437
Train - Epoch 7, Batch: 30, Loss: 1.010159
Train - Epoch 8, Batch: 0, Loss: 1.014860
Train - Epoch 8, Batch: 10, Loss: 1.000286
Train - Epoch 8, Batch: 20, Loss: 1.005629
Train - Epoch 8, Batch: 30, Loss: 1.001927
Train - Epoch 9, Batch: 0, Loss: 1.011220
Train - Epoch 9, Batch: 10, Loss: 1.002381
Train - Epoch 9, Batch: 20, Loss: 0.983974
Train - Epoch 9, Batch: 30, Loss: 0.982405
Train - Epoch 10, Batch: 0, Loss: 0.980945
Train - Epoch 10, Batch: 10, Loss: 0.995306
Train - Epoch 10, Batch: 20, Loss: 0.984058
Train - Epoch 10, Batch: 30, Loss: 0.981159
Train - Epoch 11, Batch: 0, Loss: 0.974781
Train - Epoch 11, Batch: 10, Loss: 0.971088
Train - Epoch 11, Batch: 20, Loss: 0.979590
Train - Epoch 11, Batch: 30, Loss: 0.974668
Train - Epoch 12, Batch: 0, Loss: 0.966140
Train - Epoch 12, Batch: 10, Loss: 0.967295
Train - Epoch 12, Batch: 20, Loss: 0.974204
Train - Epoch 12, Batch: 30, Loss: 0.963162
Train - Epoch 13, Batch: 0, Loss: 0.951723
Train - Epoch 13, Batch: 10, Loss: 0.965337
Train - Epoch 13, Batch: 20, Loss: 0.969440
Train - Epoch 13, Batch: 30, Loss: 0.964480
Train - Epoch 14, Batch: 0, Loss: 0.955580
Train - Epoch 14, Batch: 10, Loss: 0.944666
Train - Epoch 14, Batch: 20, Loss: 0.955077
Train - Epoch 14, Batch: 30, Loss: 0.954766
Train - Epoch 15, Batch: 0, Loss: 0.953017
Train - Epoch 15, Batch: 10, Loss: 0.951525
Train - Epoch 15, Batch: 20, Loss: 0.955098
Train - Epoch 15, Batch: 30, Loss: 0.945355
Train - Epoch 16, Batch: 0, Loss: 0.948568
Train - Epoch 16, Batch: 10, Loss: 0.944126
Train - Epoch 16, Batch: 20, Loss: 0.940667
Train - Epoch 16, Batch: 30, Loss: 0.944399
Train - Epoch 17, Batch: 0, Loss: 0.949090
Train - Epoch 17, Batch: 10, Loss: 0.943200
Train - Epoch 17, Batch: 20, Loss: 0.939687
Train - Epoch 17, Batch: 30, Loss: 0.932937
Train - Epoch 18, Batch: 0, Loss: 0.930977
Train - Epoch 18, Batch: 10, Loss: 0.938537
Train - Epoch 18, Batch: 20, Loss: 0.934635
Train - Epoch 18, Batch: 30, Loss: 0.924455
Train - Epoch 19, Batch: 0, Loss: 0.927988
Train - Epoch 19, Batch: 10, Loss: 0.926451
Train - Epoch 19, Batch: 20, Loss: 0.932888
Train - Epoch 19, Batch: 30, Loss: 0.923172
Train - Epoch 20, Batch: 0, Loss: 0.922529
Train - Epoch 20, Batch: 10, Loss: 0.926318
Train - Epoch 20, Batch: 20, Loss: 0.928450
Train - Epoch 20, Batch: 30, Loss: 0.923879
Train - Epoch 21, Batch: 0, Loss: 0.929033
Train - Epoch 21, Batch: 10, Loss: 0.912202
Train - Epoch 21, Batch: 20, Loss: 0.912112
Train - Epoch 21, Batch: 30, Loss: 0.918931
Train - Epoch 22, Batch: 0, Loss: 0.923580
Train - Epoch 22, Batch: 10, Loss: 0.915426
Train - Epoch 22, Batch: 20, Loss: 0.913683
Train - Epoch 22, Batch: 30, Loss: 0.914107
Train - Epoch 23, Batch: 0, Loss: 0.920151
Train - Epoch 23, Batch: 10, Loss: 0.909145
Train - Epoch 23, Batch: 20, Loss: 0.913185
Train - Epoch 23, Batch: 30, Loss: 0.914338
Train - Epoch 24, Batch: 0, Loss: 0.918127
Train - Epoch 24, Batch: 10, Loss: 0.917347
Train - Epoch 24, Batch: 20, Loss: 0.914359
Train - Epoch 24, Batch: 30, Loss: 0.913755
Train - Epoch 25, Batch: 0, Loss: 0.912578
Train - Epoch 25, Batch: 10, Loss: 0.910389
Train - Epoch 25, Batch: 20, Loss: 0.908178
Train - Epoch 25, Batch: 30, Loss: 0.905267
Train - Epoch 26, Batch: 0, Loss: 0.910788
Train - Epoch 26, Batch: 10, Loss: 0.902755
Train - Epoch 26, Batch: 20, Loss: 0.907451
Train - Epoch 26, Batch: 30, Loss: 0.907219
Train - Epoch 27, Batch: 0, Loss: 0.908925
Train - Epoch 27, Batch: 10, Loss: 0.900341
Train - Epoch 27, Batch: 20, Loss: 0.905911
Train - Epoch 27, Batch: 30, Loss: 0.900626
Train - Epoch 28, Batch: 0, Loss: 0.907161
Train - Epoch 28, Batch: 10, Loss: 0.893267
Train - Epoch 28, Batch: 20, Loss: 0.908904
Train - Epoch 28, Batch: 30, Loss: 0.901289
Train - Epoch 29, Batch: 0, Loss: 0.912385
Train - Epoch 29, Batch: 10, Loss: 0.901726
Train - Epoch 29, Batch: 20, Loss: 0.894209
Train - Epoch 29, Batch: 30, Loss: 0.896618
Train - Epoch 30, Batch: 0, Loss: 0.902142
Train - Epoch 30, Batch: 10, Loss: 0.892040
Train - Epoch 30, Batch: 20, Loss: 0.891937
Train - Epoch 30, Batch: 30, Loss: 0.901341
Train - Epoch 31, Batch: 0, Loss: 0.902362
Train - Epoch 31, Batch: 10, Loss: 0.897063
Train - Epoch 31, Batch: 20, Loss: 0.895810
Train - Epoch 31, Batch: 30, Loss: 0.893636
Test Avg. Loss: 0.000070, Accuracy: 0.628926
training_time:: 3.904770612716675
training time full:: 3.904834508895874
provenance prepare time:: 7.867813110351562e-06
Test Avg. Loss: 0.000070, Accuracy: 0.628926
baseline::
python3 incremental_updates_base_line_lr_multi.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Benchmark_experiments/benchmark_exp.py:1298: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  curr_delta_id = torch.tensor(delta_id)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 99
max_epoch:: 32
batch_size:: 16384
Num of deletion:: 0, running time baseline::2.164540
Num of deletion:: 10, running time baseline::22.767470
Num of deletion:: 20, running time baseline::43.813608
Num of deletion:: 30, running time baseline::63.826396
Num of deletion:: 40, running time baseline::84.493436
Num of deletion:: 50, running time baseline::104.454763
Num of deletion:: 60, running time baseline::124.157627
Num of deletion:: 70, running time baseline::144.215043
Num of deletion:: 80, running time baseline::163.631548
Num of deletion:: 90, running time baseline::182.987701
training time is 198.80114936828613
overhead:: 0
overhead2:: 198.7952811717987
overhead3:: 8.025470495223999
time_baseline:: 198.80215787887573
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628926
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance5_lr_multi.py 10 5 6 0.00019123 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 99
max_epoch:: 32
Num of deletion:: 0, running time provenance::1.096784
Num of deletion:: 10, running time provenance::13.805558
Num of deletion:: 20, running time provenance::25.621073
Num of deletion:: 30, running time provenance::38.120860
Num of deletion:: 40, running time provenance::48.816228
Num of deletion:: 50, running time provenance::60.646830
Num of deletion:: 60, running time provenance::72.253520
Num of deletion:: 70, running time provenance::84.228570
Num of deletion:: 80, running time provenance::95.223842
Num of deletion:: 90, running time provenance::107.270408
overhead:: 0
overhead2:: 0
overhead3:: 116.00209403038025
overhead4:: 0
overhead5:: 10.176319122314453
memory usage:: 3161579520
time_provenance:: 116.00486207008362
curr_diff: 0 tensor(1.3708e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3708e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628926
repetition 7
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 7 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 2.076129
Train - Epoch 0, Batch: 10, Loss: 1.407368
Train - Epoch 0, Batch: 20, Loss: 1.240883
Train - Epoch 0, Batch: 30, Loss: 1.189861
Train - Epoch 1, Batch: 0, Loss: 1.175058
Train - Epoch 1, Batch: 10, Loss: 1.156202
Train - Epoch 1, Batch: 20, Loss: 1.134215
Train - Epoch 1, Batch: 30, Loss: 1.120152
Train - Epoch 2, Batch: 0, Loss: 1.126258
Train - Epoch 2, Batch: 10, Loss: 1.114511
Train - Epoch 2, Batch: 20, Loss: 1.111539
Train - Epoch 2, Batch: 30, Loss: 1.092752
Train - Epoch 3, Batch: 0, Loss: 1.101645
Train - Epoch 3, Batch: 10, Loss: 1.081548
Train - Epoch 3, Batch: 20, Loss: 1.070651
Train - Epoch 3, Batch: 30, Loss: 1.063358
Train - Epoch 4, Batch: 0, Loss: 1.065270
Train - Epoch 4, Batch: 10, Loss: 1.070850
Train - Epoch 4, Batch: 20, Loss: 1.054606
Train - Epoch 4, Batch: 30, Loss: 1.051139
Train - Epoch 5, Batch: 0, Loss: 1.052248
Train - Epoch 5, Batch: 10, Loss: 1.051137
Train - Epoch 5, Batch: 20, Loss: 1.035000
Train - Epoch 5, Batch: 30, Loss: 1.034290
Train - Epoch 6, Batch: 0, Loss: 1.035283
Train - Epoch 6, Batch: 10, Loss: 1.026754
Train - Epoch 6, Batch: 20, Loss: 1.028528
Train - Epoch 6, Batch: 30, Loss: 1.023239
Train - Epoch 7, Batch: 0, Loss: 1.015726
Train - Epoch 7, Batch: 10, Loss: 1.011562
Train - Epoch 7, Batch: 20, Loss: 1.011226
Train - Epoch 7, Batch: 30, Loss: 1.006836
Train - Epoch 8, Batch: 0, Loss: 1.010899
Train - Epoch 8, Batch: 10, Loss: 0.995766
Train - Epoch 8, Batch: 20, Loss: 1.005254
Train - Epoch 8, Batch: 30, Loss: 0.999479
Train - Epoch 9, Batch: 0, Loss: 1.000863
Train - Epoch 9, Batch: 10, Loss: 0.991745
Train - Epoch 9, Batch: 20, Loss: 0.985871
Train - Epoch 9, Batch: 30, Loss: 0.993390
Train - Epoch 10, Batch: 0, Loss: 0.994195
Train - Epoch 10, Batch: 10, Loss: 0.978503
Train - Epoch 10, Batch: 20, Loss: 0.980123
Train - Epoch 10, Batch: 30, Loss: 0.977595
Train - Epoch 11, Batch: 0, Loss: 0.979940
Train - Epoch 11, Batch: 10, Loss: 0.973651
Train - Epoch 11, Batch: 20, Loss: 0.955656
Train - Epoch 11, Batch: 30, Loss: 0.964973
Train - Epoch 12, Batch: 0, Loss: 0.959195
Train - Epoch 12, Batch: 10, Loss: 0.974727
Train - Epoch 12, Batch: 20, Loss: 0.961047
Train - Epoch 12, Batch: 30, Loss: 0.970478
Train - Epoch 13, Batch: 0, Loss: 0.961603
Train - Epoch 13, Batch: 10, Loss: 0.965543
Train - Epoch 13, Batch: 20, Loss: 0.962242
Train - Epoch 13, Batch: 30, Loss: 0.954688
Train - Epoch 14, Batch: 0, Loss: 0.951213
Train - Epoch 14, Batch: 10, Loss: 0.942406
Train - Epoch 14, Batch: 20, Loss: 0.949667
Train - Epoch 14, Batch: 30, Loss: 0.952137
Train - Epoch 15, Batch: 0, Loss: 0.944351
Train - Epoch 15, Batch: 10, Loss: 0.944766
Train - Epoch 15, Batch: 20, Loss: 0.940054
Train - Epoch 15, Batch: 30, Loss: 0.945614
Train - Epoch 16, Batch: 0, Loss: 0.945585
Train - Epoch 16, Batch: 10, Loss: 0.949336
Train - Epoch 16, Batch: 20, Loss: 0.937436
Train - Epoch 16, Batch: 30, Loss: 0.944782
Train - Epoch 17, Batch: 0, Loss: 0.942926
Train - Epoch 17, Batch: 10, Loss: 0.944121
Train - Epoch 17, Batch: 20, Loss: 0.933939
Train - Epoch 17, Batch: 30, Loss: 0.929489
Train - Epoch 18, Batch: 0, Loss: 0.941329
Train - Epoch 18, Batch: 10, Loss: 0.929916
Train - Epoch 18, Batch: 20, Loss: 0.933550
Train - Epoch 18, Batch: 30, Loss: 0.928978
Train - Epoch 19, Batch: 0, Loss: 0.928161
Train - Epoch 19, Batch: 10, Loss: 0.931610
Train - Epoch 19, Batch: 20, Loss: 0.919537
Train - Epoch 19, Batch: 30, Loss: 0.929091
Train - Epoch 20, Batch: 0, Loss: 0.929324
Train - Epoch 20, Batch: 10, Loss: 0.917435
Train - Epoch 20, Batch: 20, Loss: 0.924818
Train - Epoch 20, Batch: 30, Loss: 0.931562
Train - Epoch 21, Batch: 0, Loss: 0.928444
Train - Epoch 21, Batch: 10, Loss: 0.926652
Train - Epoch 21, Batch: 20, Loss: 0.914089
Train - Epoch 21, Batch: 30, Loss: 0.915964
Train - Epoch 22, Batch: 0, Loss: 0.911053
Train - Epoch 22, Batch: 10, Loss: 0.913446
Train - Epoch 22, Batch: 20, Loss: 0.917642
Train - Epoch 22, Batch: 30, Loss: 0.920115
Train - Epoch 23, Batch: 0, Loss: 0.913818
Train - Epoch 23, Batch: 10, Loss: 0.922989
Train - Epoch 23, Batch: 20, Loss: 0.908842
Train - Epoch 23, Batch: 30, Loss: 0.910175
Train - Epoch 24, Batch: 0, Loss: 0.909255
Train - Epoch 24, Batch: 10, Loss: 0.917454
Train - Epoch 24, Batch: 20, Loss: 0.909857
Train - Epoch 24, Batch: 30, Loss: 0.918771
Train - Epoch 25, Batch: 0, Loss: 0.914195
Train - Epoch 25, Batch: 10, Loss: 0.904325
Train - Epoch 25, Batch: 20, Loss: 0.910328
Train - Epoch 25, Batch: 30, Loss: 0.908880
Train - Epoch 26, Batch: 0, Loss: 0.907233
Train - Epoch 26, Batch: 10, Loss: 0.904491
Train - Epoch 26, Batch: 20, Loss: 0.907023
Train - Epoch 26, Batch: 30, Loss: 0.897039
Train - Epoch 27, Batch: 0, Loss: 0.909430
Train - Epoch 27, Batch: 10, Loss: 0.905800
Train - Epoch 27, Batch: 20, Loss: 0.903670
Train - Epoch 27, Batch: 30, Loss: 0.902113
Train - Epoch 28, Batch: 0, Loss: 0.901029
Train - Epoch 28, Batch: 10, Loss: 0.899979
Train - Epoch 28, Batch: 20, Loss: 0.901613
Train - Epoch 28, Batch: 30, Loss: 0.887369
Train - Epoch 29, Batch: 0, Loss: 0.895334
Train - Epoch 29, Batch: 10, Loss: 0.903020
Train - Epoch 29, Batch: 20, Loss: 0.906178
Train - Epoch 29, Batch: 30, Loss: 0.895246
Train - Epoch 30, Batch: 0, Loss: 0.898001
Train - Epoch 30, Batch: 10, Loss: 0.900954
Train - Epoch 30, Batch: 20, Loss: 0.900736
Train - Epoch 30, Batch: 30, Loss: 0.886536
Train - Epoch 31, Batch: 0, Loss: 0.895239
Train - Epoch 31, Batch: 10, Loss: 0.901670
Train - Epoch 31, Batch: 20, Loss: 0.907193
Train - Epoch 31, Batch: 30, Loss: 0.893109
Test Avg. Loss: 0.000070, Accuracy: 0.630252
training_time:: 4.383511543273926
training time full:: 4.383577585220337
provenance prepare time:: 6.67572021484375e-06
Test Avg. Loss: 0.000070, Accuracy: 0.630252
baseline::
python3 incremental_updates_base_line_lr_multi.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Benchmark_experiments/benchmark_exp.py:1298: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  curr_delta_id = torch.tensor(delta_id)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 99
max_epoch:: 32
batch_size:: 16384
Num of deletion:: 0, running time baseline::2.072452
Num of deletion:: 10, running time baseline::23.284150
Num of deletion:: 20, running time baseline::43.967146
Num of deletion:: 30, running time baseline::64.853757
Num of deletion:: 40, running time baseline::84.672996
Num of deletion:: 50, running time baseline::104.753114
Num of deletion:: 60, running time baseline::124.246477
Num of deletion:: 70, running time baseline::144.502922
Num of deletion:: 80, running time baseline::164.306099
Num of deletion:: 90, running time baseline::184.546703
training time is 200.06866359710693
overhead:: 0
overhead2:: 200.06256198883057
overhead3:: 8.008875608444214
time_baseline:: 200.06952261924744
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630252
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance5_lr_multi.py 10 5 7 0.00019123 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 99
max_epoch:: 32
Num of deletion:: 0, running time provenance::1.184451
Num of deletion:: 10, running time provenance::14.240063
Num of deletion:: 20, running time provenance::26.274583
Num of deletion:: 30, running time provenance::38.017020
Num of deletion:: 40, running time provenance::49.085250
Num of deletion:: 50, running time provenance::60.447812
Num of deletion:: 60, running time provenance::71.768373
Num of deletion:: 70, running time provenance::82.769875
Num of deletion:: 80, running time provenance::93.994015
Num of deletion:: 90, running time provenance::105.520761
overhead:: 0
overhead2:: 0
overhead3:: 114.14266085624695
overhead4:: 0
overhead5:: 9.893568515777588
memory usage:: 3087872000
time_provenance:: 114.14802074432373
curr_diff: 0 tensor(1.2007e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2007e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630252
repetition 8
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 8 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 1.964102
Train - Epoch 0, Batch: 10, Loss: 1.393228
Train - Epoch 0, Batch: 20, Loss: 1.256849
Train - Epoch 0, Batch: 30, Loss: 1.197962
Train - Epoch 1, Batch: 0, Loss: 1.206351
Train - Epoch 1, Batch: 10, Loss: 1.169398
Train - Epoch 1, Batch: 20, Loss: 1.157745
Train - Epoch 1, Batch: 30, Loss: 1.145737
Train - Epoch 2, Batch: 0, Loss: 1.129966
Train - Epoch 2, Batch: 10, Loss: 1.132412
Train - Epoch 2, Batch: 20, Loss: 1.111793
Train - Epoch 2, Batch: 30, Loss: 1.112307
Train - Epoch 3, Batch: 0, Loss: 1.101781
Train - Epoch 3, Batch: 10, Loss: 1.105898
Train - Epoch 3, Batch: 20, Loss: 1.081050
Train - Epoch 3, Batch: 30, Loss: 1.083278
Train - Epoch 4, Batch: 0, Loss: 1.073134
Train - Epoch 4, Batch: 10, Loss: 1.074684
Train - Epoch 4, Batch: 20, Loss: 1.060008
Train - Epoch 4, Batch: 30, Loss: 1.054681
Train - Epoch 5, Batch: 0, Loss: 1.055530
Train - Epoch 5, Batch: 10, Loss: 1.060635
Train - Epoch 5, Batch: 20, Loss: 1.039974
Train - Epoch 5, Batch: 30, Loss: 1.044200
Train - Epoch 6, Batch: 0, Loss: 1.036441
Train - Epoch 6, Batch: 10, Loss: 1.039159
Train - Epoch 6, Batch: 20, Loss: 1.033976
Train - Epoch 6, Batch: 30, Loss: 1.026031
Train - Epoch 7, Batch: 0, Loss: 1.012835
Train - Epoch 7, Batch: 10, Loss: 1.014591
Train - Epoch 7, Batch: 20, Loss: 0.999654
Train - Epoch 7, Batch: 30, Loss: 1.002014
Train - Epoch 8, Batch: 0, Loss: 0.999386
Train - Epoch 8, Batch: 10, Loss: 1.002713
Train - Epoch 8, Batch: 20, Loss: 1.000239
Train - Epoch 8, Batch: 30, Loss: 0.996875
Train - Epoch 9, Batch: 0, Loss: 1.002098
Train - Epoch 9, Batch: 10, Loss: 0.991697
Train - Epoch 9, Batch: 20, Loss: 0.987414
Train - Epoch 9, Batch: 30, Loss: 0.987010
Train - Epoch 10, Batch: 0, Loss: 0.986135
Train - Epoch 10, Batch: 10, Loss: 0.983085
Train - Epoch 10, Batch: 20, Loss: 0.973185
Train - Epoch 10, Batch: 30, Loss: 0.980307
Train - Epoch 11, Batch: 0, Loss: 0.975964
Train - Epoch 11, Batch: 10, Loss: 0.979760
Train - Epoch 11, Batch: 20, Loss: 0.984491
Train - Epoch 11, Batch: 30, Loss: 0.971822
Train - Epoch 12, Batch: 0, Loss: 0.961391
Train - Epoch 12, Batch: 10, Loss: 0.959204
Train - Epoch 12, Batch: 20, Loss: 0.966263
Train - Epoch 12, Batch: 30, Loss: 0.962107
Train - Epoch 13, Batch: 0, Loss: 0.963494
Train - Epoch 13, Batch: 10, Loss: 0.967541
Train - Epoch 13, Batch: 20, Loss: 0.964598
Train - Epoch 13, Batch: 30, Loss: 0.960828
Train - Epoch 14, Batch: 0, Loss: 0.960468
Train - Epoch 14, Batch: 10, Loss: 0.953931
Train - Epoch 14, Batch: 20, Loss: 0.959912
Train - Epoch 14, Batch: 30, Loss: 0.950356
Train - Epoch 15, Batch: 0, Loss: 0.937978
Train - Epoch 15, Batch: 10, Loss: 0.951718
Train - Epoch 15, Batch: 20, Loss: 0.963250
Train - Epoch 15, Batch: 30, Loss: 0.960595
Train - Epoch 16, Batch: 0, Loss: 0.948942
Train - Epoch 16, Batch: 10, Loss: 0.946984
Train - Epoch 16, Batch: 20, Loss: 0.932672
Train - Epoch 16, Batch: 30, Loss: 0.941595
Train - Epoch 17, Batch: 0, Loss: 0.938619
Train - Epoch 17, Batch: 10, Loss: 0.935954
Train - Epoch 17, Batch: 20, Loss: 0.933462
Train - Epoch 17, Batch: 30, Loss: 0.946610
Train - Epoch 18, Batch: 0, Loss: 0.929529
Train - Epoch 18, Batch: 10, Loss: 0.936306
Train - Epoch 18, Batch: 20, Loss: 0.928260
Train - Epoch 18, Batch: 30, Loss: 0.922659
Train - Epoch 19, Batch: 0, Loss: 0.935635
Train - Epoch 19, Batch: 10, Loss: 0.928353
Train - Epoch 19, Batch: 20, Loss: 0.914070
Train - Epoch 19, Batch: 30, Loss: 0.919509
Train - Epoch 20, Batch: 0, Loss: 0.930992
Train - Epoch 20, Batch: 10, Loss: 0.928829
Train - Epoch 20, Batch: 20, Loss: 0.928651
Train - Epoch 20, Batch: 30, Loss: 0.937135
Train - Epoch 21, Batch: 0, Loss: 0.914996
Train - Epoch 21, Batch: 10, Loss: 0.934887
Train - Epoch 21, Batch: 20, Loss: 0.920004
Train - Epoch 21, Batch: 30, Loss: 0.915245
Train - Epoch 22, Batch: 0, Loss: 0.926623
Train - Epoch 22, Batch: 10, Loss: 0.917498
Train - Epoch 22, Batch: 20, Loss: 0.923537
Train - Epoch 22, Batch: 30, Loss: 0.913150
Train - Epoch 23, Batch: 0, Loss: 0.928844
Train - Epoch 23, Batch: 10, Loss: 0.913014
Train - Epoch 23, Batch: 20, Loss: 0.908840
Train - Epoch 23, Batch: 30, Loss: 0.906601
Train - Epoch 24, Batch: 0, Loss: 0.908518
Train - Epoch 24, Batch: 10, Loss: 0.926457
Train - Epoch 24, Batch: 20, Loss: 0.913086
Train - Epoch 24, Batch: 30, Loss: 0.902959
Train - Epoch 25, Batch: 0, Loss: 0.911516
Train - Epoch 25, Batch: 10, Loss: 0.910524
Train - Epoch 25, Batch: 20, Loss: 0.912858
Train - Epoch 25, Batch: 30, Loss: 0.894336
Train - Epoch 26, Batch: 0, Loss: 0.910624
Train - Epoch 26, Batch: 10, Loss: 0.902108
Train - Epoch 26, Batch: 20, Loss: 0.886678
Train - Epoch 26, Batch: 30, Loss: 0.897731
Train - Epoch 27, Batch: 0, Loss: 0.907018
Train - Epoch 27, Batch: 10, Loss: 0.907232
Train - Epoch 27, Batch: 20, Loss: 0.901288
Train - Epoch 27, Batch: 30, Loss: 0.894238
Train - Epoch 28, Batch: 0, Loss: 0.900033
Train - Epoch 28, Batch: 10, Loss: 0.901420
Train - Epoch 28, Batch: 20, Loss: 0.900923
Train - Epoch 28, Batch: 30, Loss: 0.902600
Train - Epoch 29, Batch: 0, Loss: 0.899366
Train - Epoch 29, Batch: 10, Loss: 0.898539
Train - Epoch 29, Batch: 20, Loss: 0.895775
Train - Epoch 29, Batch: 30, Loss: 0.898937
Train - Epoch 30, Batch: 0, Loss: 0.890429
Train - Epoch 30, Batch: 10, Loss: 0.893098
Train - Epoch 30, Batch: 20, Loss: 0.896550
Train - Epoch 30, Batch: 30, Loss: 0.903708
Train - Epoch 31, Batch: 0, Loss: 0.887103
Train - Epoch 31, Batch: 10, Loss: 0.880138
Train - Epoch 31, Batch: 20, Loss: 0.894419
Train - Epoch 31, Batch: 30, Loss: 0.892799
Test Avg. Loss: 0.000070, Accuracy: 0.629976
training_time:: 3.9170472621917725
training time full:: 3.917113780975342
provenance prepare time:: 6.198883056640625e-06
Test Avg. Loss: 0.000070, Accuracy: 0.629976
baseline::
python3 incremental_updates_base_line_lr_multi.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Benchmark_experiments/benchmark_exp.py:1298: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  curr_delta_id = torch.tensor(delta_id)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 99
max_epoch:: 32
batch_size:: 16384
Num of deletion:: 0, running time baseline::2.381740
Num of deletion:: 10, running time baseline::23.830553
Num of deletion:: 20, running time baseline::44.100578
Num of deletion:: 30, running time baseline::64.091087
Num of deletion:: 40, running time baseline::84.992926
Num of deletion:: 50, running time baseline::104.989707
Num of deletion:: 60, running time baseline::124.847575
Num of deletion:: 70, running time baseline::144.728749
Num of deletion:: 80, running time baseline::164.465267
Num of deletion:: 90, running time baseline::185.216119
training time is 200.75383758544922
overhead:: 0
overhead2:: 200.74783992767334
overhead3:: 8.15062665939331
time_baseline:: 200.7547047138214
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629976
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance5_lr_multi.py 10 5 8 0.00019123 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 99
max_epoch:: 32
Num of deletion:: 0, running time provenance::1.113831
Num of deletion:: 10, running time provenance::13.461601
Num of deletion:: 20, running time provenance::26.818070
Num of deletion:: 30, running time provenance::37.586868
Num of deletion:: 40, running time provenance::48.247683
Num of deletion:: 50, running time provenance::58.838716
Num of deletion:: 60, running time provenance::69.792083
Num of deletion:: 70, running time provenance::80.897581
Num of deletion:: 80, running time provenance::91.522447
Num of deletion:: 90, running time provenance::101.402911
overhead:: 0
overhead2:: 0
overhead3:: 110.51979279518127
overhead4:: 0
overhead5:: 9.727813243865967
memory usage:: 3162107904
time_provenance:: 110.52247834205627
curr_diff: 0 tensor(1.7844e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.7844e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629976
repetition 9
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 9 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 1.909304
Train - Epoch 0, Batch: 10, Loss: 1.375613
Train - Epoch 0, Batch: 20, Loss: 1.233783
Train - Epoch 0, Batch: 30, Loss: 1.197338
Train - Epoch 1, Batch: 0, Loss: 1.182896
Train - Epoch 1, Batch: 10, Loss: 1.163208
Train - Epoch 1, Batch: 20, Loss: 1.141667
Train - Epoch 1, Batch: 30, Loss: 1.145845
Train - Epoch 2, Batch: 0, Loss: 1.124082
Train - Epoch 2, Batch: 10, Loss: 1.125834
Train - Epoch 2, Batch: 20, Loss: 1.123193
Train - Epoch 2, Batch: 30, Loss: 1.094538
Train - Epoch 3, Batch: 0, Loss: 1.099313
Train - Epoch 3, Batch: 10, Loss: 1.096705
Train - Epoch 3, Batch: 20, Loss: 1.078589
Train - Epoch 3, Batch: 30, Loss: 1.082572
Train - Epoch 4, Batch: 0, Loss: 1.076717
Train - Epoch 4, Batch: 10, Loss: 1.076250
Train - Epoch 4, Batch: 20, Loss: 1.073437
Train - Epoch 4, Batch: 30, Loss: 1.062065
Train - Epoch 5, Batch: 0, Loss: 1.065189
Train - Epoch 5, Batch: 10, Loss: 1.050282
Train - Epoch 5, Batch: 20, Loss: 1.043470
Train - Epoch 5, Batch: 30, Loss: 1.031954
Train - Epoch 6, Batch: 0, Loss: 1.035970
Train - Epoch 6, Batch: 10, Loss: 1.040193
Train - Epoch 6, Batch: 20, Loss: 1.022727
Train - Epoch 6, Batch: 30, Loss: 1.037175
Train - Epoch 7, Batch: 0, Loss: 1.027632
Train - Epoch 7, Batch: 10, Loss: 1.027635
Train - Epoch 7, Batch: 20, Loss: 1.010466
Train - Epoch 7, Batch: 30, Loss: 1.015364
Train - Epoch 8, Batch: 0, Loss: 1.012834
Train - Epoch 8, Batch: 10, Loss: 1.009414
Train - Epoch 8, Batch: 20, Loss: 0.998567
Train - Epoch 8, Batch: 30, Loss: 0.996521
Train - Epoch 9, Batch: 0, Loss: 1.005519
Train - Epoch 9, Batch: 10, Loss: 0.995934
Train - Epoch 9, Batch: 20, Loss: 0.991409
Train - Epoch 9, Batch: 30, Loss: 0.986052
Train - Epoch 10, Batch: 0, Loss: 0.987921
Train - Epoch 10, Batch: 10, Loss: 0.982703
Train - Epoch 10, Batch: 20, Loss: 0.979827
Train - Epoch 10, Batch: 30, Loss: 0.980575
Train - Epoch 11, Batch: 0, Loss: 0.978239
Train - Epoch 11, Batch: 10, Loss: 0.969903
Train - Epoch 11, Batch: 20, Loss: 0.973178
Train - Epoch 11, Batch: 30, Loss: 0.962354
Train - Epoch 12, Batch: 0, Loss: 0.979085
Train - Epoch 12, Batch: 10, Loss: 0.968712
Train - Epoch 12, Batch: 20, Loss: 0.973148
Train - Epoch 12, Batch: 30, Loss: 0.971208
Train - Epoch 13, Batch: 0, Loss: 0.960943
Train - Epoch 13, Batch: 10, Loss: 0.964013
Train - Epoch 13, Batch: 20, Loss: 0.967069
Train - Epoch 13, Batch: 30, Loss: 0.964182
Train - Epoch 14, Batch: 0, Loss: 0.962650
Train - Epoch 14, Batch: 10, Loss: 0.966927
Train - Epoch 14, Batch: 20, Loss: 0.957234
Train - Epoch 14, Batch: 30, Loss: 0.949050
Train - Epoch 15, Batch: 0, Loss: 0.954906
Train - Epoch 15, Batch: 10, Loss: 0.953644
Train - Epoch 15, Batch: 20, Loss: 0.936615
Train - Epoch 15, Batch: 30, Loss: 0.950022
Train - Epoch 16, Batch: 0, Loss: 0.943725
Train - Epoch 16, Batch: 10, Loss: 0.950585
Train - Epoch 16, Batch: 20, Loss: 0.944455
Train - Epoch 16, Batch: 30, Loss: 0.952172
Train - Epoch 17, Batch: 0, Loss: 0.943096
Train - Epoch 17, Batch: 10, Loss: 0.943980
Train - Epoch 17, Batch: 20, Loss: 0.933638
Train - Epoch 17, Batch: 30, Loss: 0.932683
Train - Epoch 18, Batch: 0, Loss: 0.928133
Train - Epoch 18, Batch: 10, Loss: 0.936085
Train - Epoch 18, Batch: 20, Loss: 0.929756
Train - Epoch 18, Batch: 30, Loss: 0.929859
Train - Epoch 19, Batch: 0, Loss: 0.936991
Train - Epoch 19, Batch: 10, Loss: 0.932158
Train - Epoch 19, Batch: 20, Loss: 0.930293
Train - Epoch 19, Batch: 30, Loss: 0.937084
Train - Epoch 20, Batch: 0, Loss: 0.931276
Train - Epoch 20, Batch: 10, Loss: 0.927071
Train - Epoch 20, Batch: 20, Loss: 0.923754
Train - Epoch 20, Batch: 30, Loss: 0.928748
Train - Epoch 21, Batch: 0, Loss: 0.925042
Train - Epoch 21, Batch: 10, Loss: 0.918441
Train - Epoch 21, Batch: 20, Loss: 0.923751
Train - Epoch 21, Batch: 30, Loss: 0.912709
Train - Epoch 22, Batch: 0, Loss: 0.928662
Train - Epoch 22, Batch: 10, Loss: 0.921387
Train - Epoch 22, Batch: 20, Loss: 0.914624
Train - Epoch 22, Batch: 30, Loss: 0.916755
Train - Epoch 23, Batch: 0, Loss: 0.917771
Train - Epoch 23, Batch: 10, Loss: 0.914813
Train - Epoch 23, Batch: 20, Loss: 0.915623
Train - Epoch 23, Batch: 30, Loss: 0.912099
Train - Epoch 24, Batch: 0, Loss: 0.916127
Train - Epoch 24, Batch: 10, Loss: 0.916984
Train - Epoch 24, Batch: 20, Loss: 0.915480
Train - Epoch 24, Batch: 30, Loss: 0.907448
Train - Epoch 25, Batch: 0, Loss: 0.906925
Train - Epoch 25, Batch: 10, Loss: 0.905956
Train - Epoch 25, Batch: 20, Loss: 0.909812
Train - Epoch 25, Batch: 30, Loss: 0.909430
Train - Epoch 26, Batch: 0, Loss: 0.919206
Train - Epoch 26, Batch: 10, Loss: 0.901870
Train - Epoch 26, Batch: 20, Loss: 0.910913
Train - Epoch 26, Batch: 30, Loss: 0.907030
Train - Epoch 27, Batch: 0, Loss: 0.905597
Train - Epoch 27, Batch: 10, Loss: 0.909686
Train - Epoch 27, Batch: 20, Loss: 0.901719
Train - Epoch 27, Batch: 30, Loss: 0.904059
Train - Epoch 28, Batch: 0, Loss: 0.902393
Train - Epoch 28, Batch: 10, Loss: 0.896960
Train - Epoch 28, Batch: 20, Loss: 0.892061
Train - Epoch 28, Batch: 30, Loss: 0.899903
Train - Epoch 29, Batch: 0, Loss: 0.906455
Train - Epoch 29, Batch: 10, Loss: 0.897269
Train - Epoch 29, Batch: 20, Loss: 0.899951
Train - Epoch 29, Batch: 30, Loss: 0.896715
Train - Epoch 30, Batch: 0, Loss: 0.893184
Train - Epoch 30, Batch: 10, Loss: 0.902499
Train - Epoch 30, Batch: 20, Loss: 0.896796
Train - Epoch 30, Batch: 30, Loss: 0.896640
Train - Epoch 31, Batch: 0, Loss: 0.898111
Train - Epoch 31, Batch: 10, Loss: 0.891752
Train - Epoch 31, Batch: 20, Loss: 0.892832
Train - Epoch 31, Batch: 30, Loss: 0.896137
Test Avg. Loss: 0.000070, Accuracy: 0.629512
training_time:: 4.373092412948608
training time full:: 4.373159408569336
provenance prepare time:: 6.9141387939453125e-06
Test Avg. Loss: 0.000070, Accuracy: 0.629512
baseline::
python3 incremental_updates_base_line_lr_multi.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Benchmark_experiments/benchmark_exp.py:1298: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  curr_delta_id = torch.tensor(delta_id)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 99
max_epoch:: 32
batch_size:: 16384
Num of deletion:: 0, running time baseline::2.254890
Num of deletion:: 10, running time baseline::23.408300
Num of deletion:: 20, running time baseline::44.359849
Num of deletion:: 30, running time baseline::64.626419
Num of deletion:: 40, running time baseline::84.875070
Num of deletion:: 50, running time baseline::104.879809
Num of deletion:: 60, running time baseline::125.540367
Num of deletion:: 70, running time baseline::145.860270
Num of deletion:: 80, running time baseline::166.044198
Num of deletion:: 90, running time baseline::186.308508
training time is 202.73638772964478
overhead:: 0
overhead2:: 202.73024797439575
overhead3:: 8.261968612670898
time_baseline:: 202.73729252815247
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629529
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance5_lr_multi.py 10 5 9 0.00019123 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 99
max_epoch:: 32
Num of deletion:: 0, running time provenance::1.183000
Num of deletion:: 10, running time provenance::13.813669
Num of deletion:: 20, running time provenance::25.616267
Num of deletion:: 30, running time provenance::36.999223
Num of deletion:: 40, running time provenance::47.857331
Num of deletion:: 50, running time provenance::59.052028
Num of deletion:: 60, running time provenance::70.099134
Num of deletion:: 70, running time provenance::81.065098
Num of deletion:: 80, running time provenance::91.305453
Num of deletion:: 90, running time provenance::102.560644
overhead:: 0
overhead2:: 0
overhead3:: 111.2709608078003
overhead4:: 0
overhead5:: 9.647419214248657
memory usage:: 3077111808
time_provenance:: 111.27861166000366
curr_diff: 0 tensor(1.9252e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.9252e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629529
dataset name::higgs
deletion rate::0.000009524
python3 generate_rand_ids 0.000009524  higgs 1
start loading data...
normalization start!!
torch.Size([10500000, 29])
torch.Size([500000, 29])
tensor([ 8799236,  7985161,  1680408,   771101,  8031781,  1592358,  1090612,
         6543930,  2699840,  3062865,  8249429,  8281178,  3789403,  2009692,
         8207964,  5359714,  4104807,  1633897,  1342058,  7324267,  9937005,
         3233392,  6587521,  6353031,  1678986,  5446283,   855189,  2854045,
         4602025,  1314477,  2706611,  2302644,  3483836,  2132677,  7076041,
         7088845,  8842958,   212692,  4058837,  9289945,  1912544,  1626336,
         3468514,   789221, 10067712,  2012417,  8343303,  4593422,  7812367,
        10470159,  1390870,  7433499,  1040159,  1192736,  3920167,  2053928,
         6635819,  7350574,  6486319,  2492211,  2626359,  9868087, 10231099,
         2208578,  5799763,  9747284,  2513239,  7655271,  2059114,  7151981,
         3009390,  6450033,  9182066,   282491,  6323579,  3606416,  5835156,
         6212504,  2740122,  6346140,  3117984,  9198502,  7035307,  8199088,
         3250100,  6457784,   319935, 10466243,  3945414,  7781833,  4128201,
         6324176,  6456276,  2333142,  4957654,  6500832,  3895780,  4521957,
         6203876,  1678326])
python3 generate_dataset_train_test.py Logistic_regression higgs 16384 4 10
start loading data...
normalization start!!
torch.Size([10500000, 29])
torch.Size([500000, 29])
repetition 0
python3 benchmark_exp_lr.py 0.001 16384 4 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression higgs 0 0.005 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 0.732630
Train - Epoch 0, Batch: 10, Loss: 0.690229
Train - Epoch 0, Batch: 20, Loss: 0.691618
Train - Epoch 0, Batch: 30, Loss: 0.690598
Train - Epoch 0, Batch: 40, Loss: 0.689722
Train - Epoch 0, Batch: 50, Loss: 0.690730
Train - Epoch 0, Batch: 60, Loss: 0.689234
Train - Epoch 0, Batch: 70, Loss: 0.690666
Train - Epoch 0, Batch: 80, Loss: 0.689256
Train - Epoch 0, Batch: 90, Loss: 0.689229
Train - Epoch 0, Batch: 100, Loss: 0.689785
Train - Epoch 0, Batch: 110, Loss: 0.690138
Train - Epoch 0, Batch: 120, Loss: 0.688584
Train - Epoch 0, Batch: 130, Loss: 0.689678
Train - Epoch 0, Batch: 140, Loss: 0.688881
Train - Epoch 0, Batch: 150, Loss: 0.688812
Train - Epoch 0, Batch: 160, Loss: 0.688010
Train - Epoch 0, Batch: 170, Loss: 0.688242
Train - Epoch 0, Batch: 180, Loss: 0.689318
Train - Epoch 0, Batch: 190, Loss: 0.688269
Train - Epoch 0, Batch: 200, Loss: 0.688564
Train - Epoch 0, Batch: 210, Loss: 0.688273
Train - Epoch 0, Batch: 220, Loss: 0.687336
Train - Epoch 0, Batch: 230, Loss: 0.687433
Train - Epoch 0, Batch: 240, Loss: 0.688451
Train - Epoch 0, Batch: 250, Loss: 0.687642
Train - Epoch 0, Batch: 260, Loss: 0.688208
Train - Epoch 0, Batch: 270, Loss: 0.687528
Train - Epoch 0, Batch: 280, Loss: 0.687548
Train - Epoch 0, Batch: 290, Loss: 0.687794
Train - Epoch 0, Batch: 300, Loss: 0.687547
Train - Epoch 0, Batch: 310, Loss: 0.687435
Train - Epoch 0, Batch: 320, Loss: 0.687718
Train - Epoch 0, Batch: 330, Loss: 0.688569
Train - Epoch 0, Batch: 340, Loss: 0.687550
Train - Epoch 0, Batch: 350, Loss: 0.688113
Train - Epoch 0, Batch: 360, Loss: 0.687632
Train - Epoch 0, Batch: 370, Loss: 0.687442
Train - Epoch 0, Batch: 380, Loss: 0.688479
Train - Epoch 0, Batch: 390, Loss: 0.686179
Train - Epoch 0, Batch: 400, Loss: 0.687431
Train - Epoch 0, Batch: 410, Loss: 0.688586
Train - Epoch 0, Batch: 420, Loss: 0.687193
Train - Epoch 0, Batch: 430, Loss: 0.686909
Train - Epoch 0, Batch: 440, Loss: 0.687548
Train - Epoch 0, Batch: 450, Loss: 0.687242
Train - Epoch 0, Batch: 460, Loss: 0.687262
Train - Epoch 0, Batch: 470, Loss: 0.687089
Train - Epoch 0, Batch: 480, Loss: 0.686938
Train - Epoch 0, Batch: 490, Loss: 0.685767
Train - Epoch 0, Batch: 500, Loss: 0.686950
Train - Epoch 0, Batch: 510, Loss: 0.686837
Train - Epoch 0, Batch: 520, Loss: 0.688308
Train - Epoch 0, Batch: 530, Loss: 0.688157
Train - Epoch 0, Batch: 540, Loss: 0.686949
Train - Epoch 0, Batch: 550, Loss: 0.687286
Train - Epoch 0, Batch: 560, Loss: 0.685794
Train - Epoch 0, Batch: 570, Loss: 0.684847
Train - Epoch 0, Batch: 580, Loss: 0.686687
Train - Epoch 0, Batch: 590, Loss: 0.686594
Train - Epoch 0, Batch: 600, Loss: 0.686803
Train - Epoch 0, Batch: 610, Loss: 0.686568
Train - Epoch 0, Batch: 620, Loss: 0.685528
Train - Epoch 0, Batch: 630, Loss: 0.686952
Train - Epoch 0, Batch: 640, Loss: 0.686613
Train - Epoch 1, Batch: 0, Loss: 0.686506
Train - Epoch 1, Batch: 10, Loss: 0.685872
Train - Epoch 1, Batch: 20, Loss: 0.686569
Train - Epoch 1, Batch: 30, Loss: 0.686644
Train - Epoch 1, Batch: 40, Loss: 0.685604
Train - Epoch 1, Batch: 50, Loss: 0.685036
Train - Epoch 1, Batch: 60, Loss: 0.686518
Train - Epoch 1, Batch: 70, Loss: 0.685961
Train - Epoch 1, Batch: 80, Loss: 0.685439
Train - Epoch 1, Batch: 90, Loss: 0.686959
Train - Epoch 1, Batch: 100, Loss: 0.686962
Train - Epoch 1, Batch: 110, Loss: 0.685938
Train - Epoch 1, Batch: 120, Loss: 0.685755
Train - Epoch 1, Batch: 130, Loss: 0.686720
Train - Epoch 1, Batch: 140, Loss: 0.686007
Train - Epoch 1, Batch: 150, Loss: 0.685801
Train - Epoch 1, Batch: 160, Loss: 0.684408
Train - Epoch 1, Batch: 170, Loss: 0.686045
Train - Epoch 1, Batch: 180, Loss: 0.687243
Train - Epoch 1, Batch: 190, Loss: 0.685484
Train - Epoch 1, Batch: 200, Loss: 0.686189
Train - Epoch 1, Batch: 210, Loss: 0.686089
Train - Epoch 1, Batch: 220, Loss: 0.685973
Train - Epoch 1, Batch: 230, Loss: 0.686024
Train - Epoch 1, Batch: 240, Loss: 0.686833
Train - Epoch 1, Batch: 250, Loss: 0.684464
Train - Epoch 1, Batch: 260, Loss: 0.686209
Train - Epoch 1, Batch: 270, Loss: 0.685764
Train - Epoch 1, Batch: 280, Loss: 0.686068
Train - Epoch 1, Batch: 290, Loss: 0.685460
Train - Epoch 1, Batch: 300, Loss: 0.686161
Train - Epoch 1, Batch: 310, Loss: 0.684871
Train - Epoch 1, Batch: 320, Loss: 0.685571
Train - Epoch 1, Batch: 330, Loss: 0.684626
Train - Epoch 1, Batch: 340, Loss: 0.685072
Train - Epoch 1, Batch: 350, Loss: 0.686664
Train - Epoch 1, Batch: 360, Loss: 0.685679
Train - Epoch 1, Batch: 370, Loss: 0.686807
Train - Epoch 1, Batch: 380, Loss: 0.685787
Train - Epoch 1, Batch: 390, Loss: 0.685327
Train - Epoch 1, Batch: 400, Loss: 0.684975
Train - Epoch 1, Batch: 410, Loss: 0.684513
Train - Epoch 1, Batch: 420, Loss: 0.685904
Train - Epoch 1, Batch: 430, Loss: 0.685731
Train - Epoch 1, Batch: 440, Loss: 0.684220
Train - Epoch 1, Batch: 450, Loss: 0.685300
Train - Epoch 1, Batch: 460, Loss: 0.686108
Train - Epoch 1, Batch: 470, Loss: 0.686288
Train - Epoch 1, Batch: 480, Loss: 0.685494
Train - Epoch 1, Batch: 490, Loss: 0.684726
Train - Epoch 1, Batch: 500, Loss: 0.686755
Train - Epoch 1, Batch: 510, Loss: 0.686328
Train - Epoch 1, Batch: 520, Loss: 0.685037
Train - Epoch 1, Batch: 530, Loss: 0.686181
Train - Epoch 1, Batch: 540, Loss: 0.684993
Train - Epoch 1, Batch: 550, Loss: 0.685146
Train - Epoch 1, Batch: 560, Loss: 0.685549
Train - Epoch 1, Batch: 570, Loss: 0.684773
Train - Epoch 1, Batch: 580, Loss: 0.684776
Train - Epoch 1, Batch: 590, Loss: 0.685406
Train - Epoch 1, Batch: 600, Loss: 0.685560
Train - Epoch 1, Batch: 610, Loss: 0.684438
Train - Epoch 1, Batch: 620, Loss: 0.684881
Train - Epoch 1, Batch: 630, Loss: 0.685146
Train - Epoch 1, Batch: 640, Loss: 0.684326
Train - Epoch 2, Batch: 0, Loss: 0.685651
Train - Epoch 2, Batch: 10, Loss: 0.684989
Train - Epoch 2, Batch: 20, Loss: 0.685759
Train - Epoch 2, Batch: 30, Loss: 0.684824
Train - Epoch 2, Batch: 40, Loss: 0.684829
Train - Epoch 2, Batch: 50, Loss: 0.684722
Train - Epoch 2, Batch: 60, Loss: 0.685316
Train - Epoch 2, Batch: 70, Loss: 0.684502
Train - Epoch 2, Batch: 80, Loss: 0.684455
Train - Epoch 2, Batch: 90, Loss: 0.684574
Train - Epoch 2, Batch: 100, Loss: 0.685040
Train - Epoch 2, Batch: 110, Loss: 0.683278
Train - Epoch 2, Batch: 120, Loss: 0.685092
Train - Epoch 2, Batch: 130, Loss: 0.684239
Train - Epoch 2, Batch: 140, Loss: 0.684032
Train - Epoch 2, Batch: 150, Loss: 0.684893
Train - Epoch 2, Batch: 160, Loss: 0.684061
Train - Epoch 2, Batch: 170, Loss: 0.686194
Train - Epoch 2, Batch: 180, Loss: 0.685033
Train - Epoch 2, Batch: 190, Loss: 0.685334
Train - Epoch 2, Batch: 200, Loss: 0.685115
Train - Epoch 2, Batch: 210, Loss: 0.685228
Train - Epoch 2, Batch: 220, Loss: 0.683954
Train - Epoch 2, Batch: 230, Loss: 0.684328
Train - Epoch 2, Batch: 240, Loss: 0.684110
Train - Epoch 2, Batch: 250, Loss: 0.684899
Train - Epoch 2, Batch: 260, Loss: 0.684520
Train - Epoch 2, Batch: 270, Loss: 0.684954
Train - Epoch 2, Batch: 280, Loss: 0.685351
Train - Epoch 2, Batch: 290, Loss: 0.685079
Train - Epoch 2, Batch: 300, Loss: 0.684091
Train - Epoch 2, Batch: 310, Loss: 0.684732
Train - Epoch 2, Batch: 320, Loss: 0.683817
Train - Epoch 2, Batch: 330, Loss: 0.684071
Train - Epoch 2, Batch: 340, Loss: 0.684846
Train - Epoch 2, Batch: 350, Loss: 0.685033
Train - Epoch 2, Batch: 360, Loss: 0.684203
Train - Epoch 2, Batch: 370, Loss: 0.684849
Train - Epoch 2, Batch: 380, Loss: 0.684560
Train - Epoch 2, Batch: 390, Loss: 0.684519
Train - Epoch 2, Batch: 400, Loss: 0.685397
Train - Epoch 2, Batch: 410, Loss: 0.684307
Train - Epoch 2, Batch: 420, Loss: 0.684058
Train - Epoch 2, Batch: 430, Loss: 0.684488
Train - Epoch 2, Batch: 440, Loss: 0.684402
Train - Epoch 2, Batch: 450, Loss: 0.683576
Train - Epoch 2, Batch: 460, Loss: 0.684443
Train - Epoch 2, Batch: 470, Loss: 0.685547
Train - Epoch 2, Batch: 480, Loss: 0.684280
Train - Epoch 2, Batch: 490, Loss: 0.684525
Train - Epoch 2, Batch: 500, Loss: 0.683924
Train - Epoch 2, Batch: 510, Loss: 0.683964
Train - Epoch 2, Batch: 520, Loss: 0.685782
Train - Epoch 2, Batch: 530, Loss: 0.683365
Train - Epoch 2, Batch: 540, Loss: 0.683701
Train - Epoch 2, Batch: 550, Loss: 0.683479/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 2, Batch: 560, Loss: 0.686145
Train - Epoch 2, Batch: 570, Loss: 0.683221
Train - Epoch 2, Batch: 580, Loss: 0.685262
Train - Epoch 2, Batch: 590, Loss: 0.683766
Train - Epoch 2, Batch: 600, Loss: 0.684192
Train - Epoch 2, Batch: 610, Loss: 0.683816
Train - Epoch 2, Batch: 620, Loss: 0.683860
Train - Epoch 2, Batch: 630, Loss: 0.683853
Train - Epoch 2, Batch: 640, Loss: 0.686146
Train - Epoch 3, Batch: 0, Loss: 0.684705
Train - Epoch 3, Batch: 10, Loss: 0.684765
Train - Epoch 3, Batch: 20, Loss: 0.684099
Train - Epoch 3, Batch: 30, Loss: 0.684029
Train - Epoch 3, Batch: 40, Loss: 0.684632
Train - Epoch 3, Batch: 50, Loss: 0.685254
Train - Epoch 3, Batch: 60, Loss: 0.684544
Train - Epoch 3, Batch: 70, Loss: 0.683464
Train - Epoch 3, Batch: 80, Loss: 0.684156
Train - Epoch 3, Batch: 90, Loss: 0.684338
Train - Epoch 3, Batch: 100, Loss: 0.683544
Train - Epoch 3, Batch: 110, Loss: 0.684219
Train - Epoch 3, Batch: 120, Loss: 0.682569
Train - Epoch 3, Batch: 130, Loss: 0.685123
Train - Epoch 3, Batch: 140, Loss: 0.683856
Train - Epoch 3, Batch: 150, Loss: 0.684230
Train - Epoch 3, Batch: 160, Loss: 0.684621
Train - Epoch 3, Batch: 170, Loss: 0.683647
Train - Epoch 3, Batch: 180, Loss: 0.683824
Train - Epoch 3, Batch: 190, Loss: 0.683302
Train - Epoch 3, Batch: 200, Loss: 0.683636
Train - Epoch 3, Batch: 210, Loss: 0.683926
Train - Epoch 3, Batch: 220, Loss: 0.685044
Train - Epoch 3, Batch: 230, Loss: 0.684499
Train - Epoch 3, Batch: 240, Loss: 0.683870
Train - Epoch 3, Batch: 250, Loss: 0.683865
Train - Epoch 3, Batch: 260, Loss: 0.682741
Train - Epoch 3, Batch: 270, Loss: 0.684284
Train - Epoch 3, Batch: 280, Loss: 0.683856
Train - Epoch 3, Batch: 290, Loss: 0.684961
Train - Epoch 3, Batch: 300, Loss: 0.684030
Train - Epoch 3, Batch: 310, Loss: 0.684494
Train - Epoch 3, Batch: 320, Loss: 0.684355
Train - Epoch 3, Batch: 330, Loss: 0.683071
Train - Epoch 3, Batch: 340, Loss: 0.683309
Train - Epoch 3, Batch: 350, Loss: 0.684653
Train - Epoch 3, Batch: 360, Loss: 0.683330
Train - Epoch 3, Batch: 370, Loss: 0.683350
Train - Epoch 3, Batch: 380, Loss: 0.683367
Train - Epoch 3, Batch: 390, Loss: 0.684296
Train - Epoch 3, Batch: 400, Loss: 0.682533
Train - Epoch 3, Batch: 410, Loss: 0.682487
Train - Epoch 3, Batch: 420, Loss: 0.684358
Train - Epoch 3, Batch: 430, Loss: 0.684059
Train - Epoch 3, Batch: 440, Loss: 0.683582
Train - Epoch 3, Batch: 450, Loss: 0.684649
Train - Epoch 3, Batch: 460, Loss: 0.683434
Train - Epoch 3, Batch: 470, Loss: 0.684578
Train - Epoch 3, Batch: 480, Loss: 0.682767
Train - Epoch 3, Batch: 490, Loss: 0.683376
Train - Epoch 3, Batch: 500, Loss: 0.684100
Train - Epoch 3, Batch: 510, Loss: 0.683566
Train - Epoch 3, Batch: 520, Loss: 0.685237
Train - Epoch 3, Batch: 530, Loss: 0.684815
Train - Epoch 3, Batch: 540, Loss: 0.682493
Train - Epoch 3, Batch: 550, Loss: 0.684148
Train - Epoch 3, Batch: 560, Loss: 0.684077
Train - Epoch 3, Batch: 570, Loss: 0.684501
Train - Epoch 3, Batch: 580, Loss: 0.684238
Train - Epoch 3, Batch: 590, Loss: 0.683072
Train - Epoch 3, Batch: 600, Loss: 0.684212
Train - Epoch 3, Batch: 610, Loss: 0.683324
Train - Epoch 3, Batch: 620, Loss: 0.682969
Train - Epoch 3, Batch: 630, Loss: 0.683524
Train - Epoch 3, Batch: 640, Loss: 0.683241
Test Avg. Loss: 0.000042, Accuracy: 0.553970
training_time:: 10.300997734069824
training time full:: 10.301063060760498
provenance prepare time:: 4.76837158203125e-06
Test Avg. Loss: 0.000042, Accuracy: 0.553970
baseline::
python3 incremental_updates_base_line_lr_multi.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Benchmark_experiments/benchmark_exp.py:1298: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  curr_delta_id = torch.tensor(delta_id)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 100
max_epoch:: 4
batch_size:: 16384
Num of deletion:: 0, running time baseline::4.692943
Num of deletion:: 10, running time baseline::53.490705
Num of deletion:: 20, running time baseline::102.740056
Num of deletion:: 30, running time baseline::154.235609
Num of deletion:: 40, running time baseline::205.508783
Num of deletion:: 50, running time baseline::255.412571
Num of deletion:: 60, running time baseline::305.055789
Num of deletion:: 70, running time baseline::353.992373
Num of deletion:: 80, running time baseline::405.654346
Num of deletion:: 90, running time baseline::455.584795
training time is 501.7049732208252
overhead:: 0
overhead2:: 501.70268034935
overhead3:: 9.044955015182495
time_baseline:: 501.70606780052185
curr_diff: 0 tensor(2.6890e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.6890e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553988
period:: 3
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance5_lr_multi.py 300 3 0 0.000009524 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 100
max_epoch:: 4
Num of deletion:: 0, running time provenance::3.353086
Num of deletion:: 10, running time provenance::35.602088
Num of deletion:: 20, running time provenance::66.348573
Num of deletion:: 30, running time provenance::97.929331
Num of deletion:: 40, running time provenance::128.324382
Num of deletion:: 50, running time provenance::160.457648
Num of deletion:: 60, running time provenance::191.602001
Num of deletion:: 70, running time provenance::224.780647
Num of deletion:: 80, running time provenance::257.013120
Num of deletion:: 90, running time provenance::287.085050
overhead:: 0
overhead2:: 0
overhead3:: 314.2073550224304
overhead4:: 0
overhead5:: 9.734038591384888
memory usage:: 5987381248
time_provenance:: 314.2108132839203
curr_diff: 0 tensor(1.6479e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6479e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(2.6224e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.6224e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553988
repetition 1
python3 benchmark_exp_lr.py 0.001 16384 4 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression higgs 1 0.005 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 0.695784
Train - Epoch 0, Batch: 10, Loss: 0.693482
Train - Epoch 0, Batch: 20, Loss: 0.693361
Train - Epoch 0, Batch: 30, Loss: 0.693246
Train - Epoch 0, Batch: 40, Loss: 0.693526
Train - Epoch 0, Batch: 50, Loss: 0.692897
Train - Epoch 0, Batch: 60, Loss: 0.692737
Train - Epoch 0, Batch: 70, Loss: 0.691959
Train - Epoch 0, Batch: 80, Loss: 0.691585
Train - Epoch 0, Batch: 90, Loss: 0.690530
Train - Epoch 0, Batch: 100, Loss: 0.690616
Train - Epoch 0, Batch: 110, Loss: 0.690609
Train - Epoch 0, Batch: 120, Loss: 0.690779
Train - Epoch 0, Batch: 130, Loss: 0.689625
Train - Epoch 0, Batch: 140, Loss: 0.690115
Train - Epoch 0, Batch: 150, Loss: 0.690219
Train - Epoch 0, Batch: 160, Loss: 0.690158
Train - Epoch 0, Batch: 170, Loss: 0.690536
Train - Epoch 0, Batch: 180, Loss: 0.689700
Train - Epoch 0, Batch: 190, Loss: 0.689940
Train - Epoch 0, Batch: 200, Loss: 0.688899
Train - Epoch 0, Batch: 210, Loss: 0.689272
Train - Epoch 0, Batch: 220, Loss: 0.689581
Train - Epoch 0, Batch: 230, Loss: 0.689213
Train - Epoch 0, Batch: 240, Loss: 0.689653
Train - Epoch 0, Batch: 250, Loss: 0.689823
Train - Epoch 0, Batch: 260, Loss: 0.688592
Train - Epoch 0, Batch: 270, Loss: 0.689673
Train - Epoch 0, Batch: 280, Loss: 0.689352
Train - Epoch 0, Batch: 290, Loss: 0.689657
Train - Epoch 0, Batch: 300, Loss: 0.688431
Train - Epoch 0, Batch: 310, Loss: 0.688337
Train - Epoch 0, Batch: 320, Loss: 0.689578
Train - Epoch 0, Batch: 330, Loss: 0.687809
Train - Epoch 0, Batch: 340, Loss: 0.687705
Train - Epoch 0, Batch: 350, Loss: 0.688393
Train - Epoch 0, Batch: 360, Loss: 0.688818
Train - Epoch 0, Batch: 370, Loss: 0.687423
Train - Epoch 0, Batch: 380, Loss: 0.688175
Train - Epoch 0, Batch: 390, Loss: 0.688073
Train - Epoch 0, Batch: 400, Loss: 0.687696
Train - Epoch 0, Batch: 410, Loss: 0.688507
Train - Epoch 0, Batch: 420, Loss: 0.687600
Train - Epoch 0, Batch: 430, Loss: 0.687797
Train - Epoch 0, Batch: 440, Loss: 0.687351
Train - Epoch 0, Batch: 450, Loss: 0.687537
Train - Epoch 0, Batch: 460, Loss: 0.687866
Train - Epoch 0, Batch: 470, Loss: 0.687278
Train - Epoch 0, Batch: 480, Loss: 0.688708
Train - Epoch 0, Batch: 490, Loss: 0.687443
Train - Epoch 0, Batch: 500, Loss: 0.687217
Train - Epoch 0, Batch: 510, Loss: 0.686490
Train - Epoch 0, Batch: 520, Loss: 0.687941
Train - Epoch 0, Batch: 530, Loss: 0.687104
Train - Epoch 0, Batch: 540, Loss: 0.687002
Train - Epoch 0, Batch: 550, Loss: 0.686976
Train - Epoch 0, Batch: 560, Loss: 0.687764
Train - Epoch 0, Batch: 570, Loss: 0.687949
Train - Epoch 0, Batch: 580, Loss: 0.686602
Train - Epoch 0, Batch: 590, Loss: 0.687727
Train - Epoch 0, Batch: 600, Loss: 0.687323
Train - Epoch 0, Batch: 610, Loss: 0.687139
Train - Epoch 0, Batch: 620, Loss: 0.687144
Train - Epoch 0, Batch: 630, Loss: 0.686197
Train - Epoch 0, Batch: 640, Loss: 0.687947
Train - Epoch 1, Batch: 0, Loss: 0.687747
Train - Epoch 1, Batch: 10, Loss: 0.687185
Train - Epoch 1, Batch: 20, Loss: 0.686769
Train - Epoch 1, Batch: 30, Loss: 0.686945
Train - Epoch 1, Batch: 40, Loss: 0.687503
Train - Epoch 1, Batch: 50, Loss: 0.686492
Train - Epoch 1, Batch: 60, Loss: 0.687285
Train - Epoch 1, Batch: 70, Loss: 0.688065
Train - Epoch 1, Batch: 80, Loss: 0.687514
Train - Epoch 1, Batch: 90, Loss: 0.687027
Train - Epoch 1, Batch: 100, Loss: 0.686194
Train - Epoch 1, Batch: 110, Loss: 0.686575
Train - Epoch 1, Batch: 120, Loss: 0.686656
Train - Epoch 1, Batch: 130, Loss: 0.687461
Train - Epoch 1, Batch: 140, Loss: 0.687077
Train - Epoch 1, Batch: 150, Loss: 0.687224
Train - Epoch 1, Batch: 160, Loss: 0.686095
Train - Epoch 1, Batch: 170, Loss: 0.687453
Train - Epoch 1, Batch: 180, Loss: 0.686438
Train - Epoch 1, Batch: 190, Loss: 0.686964
Train - Epoch 1, Batch: 200, Loss: 0.685980
Train - Epoch 1, Batch: 210, Loss: 0.686368
Train - Epoch 1, Batch: 220, Loss: 0.685917
Train - Epoch 1, Batch: 230, Loss: 0.685787
Train - Epoch 1, Batch: 240, Loss: 0.686835
Train - Epoch 1, Batch: 250, Loss: 0.687477
Train - Epoch 1, Batch: 260, Loss: 0.686112
Train - Epoch 1, Batch: 270, Loss: 0.686369
Train - Epoch 1, Batch: 280, Loss: 0.686274
Train - Epoch 1, Batch: 290, Loss: 0.686133
Train - Epoch 1, Batch: 300, Loss: 0.686115
Train - Epoch 1, Batch: 310, Loss: 0.686008
Train - Epoch 1, Batch: 320, Loss: 0.685458
Train - Epoch 1, Batch: 330, Loss: 0.684920
Train - Epoch 1, Batch: 340, Loss: 0.686391
Train - Epoch 1, Batch: 350, Loss: 0.687319
Train - Epoch 1, Batch: 360, Loss: 0.685496
Train - Epoch 1, Batch: 370, Loss: 0.686834
Train - Epoch 1, Batch: 380, Loss: 0.686606
Train - Epoch 1, Batch: 390, Loss: 0.686419
Train - Epoch 1, Batch: 400, Loss: 0.685219
Train - Epoch 1, Batch: 410, Loss: 0.685432
Train - Epoch 1, Batch: 420, Loss: 0.684720
Train - Epoch 1, Batch: 430, Loss: 0.686070
Train - Epoch 1, Batch: 440, Loss: 0.684585
Train - Epoch 1, Batch: 450, Loss: 0.686235
Train - Epoch 1, Batch: 460, Loss: 0.685872
Train - Epoch 1, Batch: 470, Loss: 0.687072
Train - Epoch 1, Batch: 480, Loss: 0.687216
Train - Epoch 1, Batch: 490, Loss: 0.685098
Train - Epoch 1, Batch: 500, Loss: 0.686511
Train - Epoch 1, Batch: 510, Loss: 0.685699
Train - Epoch 1, Batch: 520, Loss: 0.686215
Train - Epoch 1, Batch: 530, Loss: 0.685376
Train - Epoch 1, Batch: 540, Loss: 0.686660
Train - Epoch 1, Batch: 550, Loss: 0.686031
Train - Epoch 1, Batch: 560, Loss: 0.686597
Train - Epoch 1, Batch: 570, Loss: 0.685042
Train - Epoch 1, Batch: 580, Loss: 0.686427
Train - Epoch 1, Batch: 590, Loss: 0.685688
Train - Epoch 1, Batch: 600, Loss: 0.686384
Train - Epoch 1, Batch: 610, Loss: 0.685625
Train - Epoch 1, Batch: 620, Loss: 0.686483
Train - Epoch 1, Batch: 630, Loss: 0.685625
Train - Epoch 1, Batch: 640, Loss: 0.684347
Train - Epoch 2, Batch: 0, Loss: 0.684960
Train - Epoch 2, Batch: 10, Loss: 0.685527
Train - Epoch 2, Batch: 20, Loss: 0.686304
Train - Epoch 2, Batch: 30, Loss: 0.685211
Train - Epoch 2, Batch: 40, Loss: 0.686082
Train - Epoch 2, Batch: 50, Loss: 0.685795
Train - Epoch 2, Batch: 60, Loss: 0.684468
Train - Epoch 2, Batch: 70, Loss: 0.683924
Train - Epoch 2, Batch: 80, Loss: 0.685734
Train - Epoch 2, Batch: 90, Loss: 0.684790
Train - Epoch 2, Batch: 100, Loss: 0.684997
Train - Epoch 2, Batch: 110, Loss: 0.685423
Train - Epoch 2, Batch: 120, Loss: 0.685517
Train - Epoch 2, Batch: 130, Loss: 0.685556
Train - Epoch 2, Batch: 140, Loss: 0.684484
Train - Epoch 2, Batch: 150, Loss: 0.685356
Train - Epoch 2, Batch: 160, Loss: 0.685318
Train - Epoch 2, Batch: 170, Loss: 0.685110
Train - Epoch 2, Batch: 180, Loss: 0.685522
Train - Epoch 2, Batch: 190, Loss: 0.685142
Train - Epoch 2, Batch: 200, Loss: 0.685769
Train - Epoch 2, Batch: 210, Loss: 0.685001
Train - Epoch 2, Batch: 220, Loss: 0.684459
Train - Epoch 2, Batch: 230, Loss: 0.685684
Train - Epoch 2, Batch: 240, Loss: 0.685083
Train - Epoch 2, Batch: 250, Loss: 0.686466
Train - Epoch 2, Batch: 260, Loss: 0.686371
Train - Epoch 2, Batch: 270, Loss: 0.684279
Train - Epoch 2, Batch: 280, Loss: 0.684900
Train - Epoch 2, Batch: 290, Loss: 0.684208
Train - Epoch 2, Batch: 300, Loss: 0.684276
Train - Epoch 2, Batch: 310, Loss: 0.686950
Train - Epoch 2, Batch: 320, Loss: 0.685604
Train - Epoch 2, Batch: 330, Loss: 0.684261
Train - Epoch 2, Batch: 340, Loss: 0.685968
Train - Epoch 2, Batch: 350, Loss: 0.685454
Train - Epoch 2, Batch: 360, Loss: 0.684376
Train - Epoch 2, Batch: 370, Loss: 0.685441
Train - Epoch 2, Batch: 380, Loss: 0.684110
Train - Epoch 2, Batch: 390, Loss: 0.684756
Train - Epoch 2, Batch: 400, Loss: 0.684616
Train - Epoch 2, Batch: 410, Loss: 0.685269
Train - Epoch 2, Batch: 420, Loss: 0.684380
Train - Epoch 2, Batch: 430, Loss: 0.684203
Train - Epoch 2, Batch: 440, Loss: 0.685000
Train - Epoch 2, Batch: 450, Loss: 0.685271
Train - Epoch 2, Batch: 460, Loss: 0.684400
Train - Epoch 2, Batch: 470, Loss: 0.684078
Train - Epoch 2, Batch: 480, Loss: 0.685712
Train - Epoch 2, Batch: 490, Loss: 0.684301
Train - Epoch 2, Batch: 500, Loss: 0.683902
Train - Epoch 2, Batch: 510, Loss: 0.684708
Train - Epoch 2, Batch: 520, Loss: 0.685183
Train - Epoch 2, Batch: 530, Loss: 0.684210
Train - Epoch 2, Batch: 540, Loss: 0.684146
Train - Epoch 2, Batch: 550, Loss: 0.685385/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 2, Batch: 560, Loss: 0.684367
Train - Epoch 2, Batch: 570, Loss: 0.686040
Train - Epoch 2, Batch: 580, Loss: 0.684133
Train - Epoch 2, Batch: 590, Loss: 0.684758
Train - Epoch 2, Batch: 600, Loss: 0.685414
Train - Epoch 2, Batch: 610, Loss: 0.684540
Train - Epoch 2, Batch: 620, Loss: 0.682977
Train - Epoch 2, Batch: 630, Loss: 0.683562
Train - Epoch 2, Batch: 640, Loss: 0.685423
Train - Epoch 3, Batch: 0, Loss: 0.685248
Train - Epoch 3, Batch: 10, Loss: 0.685836
Train - Epoch 3, Batch: 20, Loss: 0.685917
Train - Epoch 3, Batch: 30, Loss: 0.684719
Train - Epoch 3, Batch: 40, Loss: 0.683460
Train - Epoch 3, Batch: 50, Loss: 0.684019
Train - Epoch 3, Batch: 60, Loss: 0.683859
Train - Epoch 3, Batch: 70, Loss: 0.683948
Train - Epoch 3, Batch: 80, Loss: 0.683105
Train - Epoch 3, Batch: 90, Loss: 0.685098
Train - Epoch 3, Batch: 100, Loss: 0.684654
Train - Epoch 3, Batch: 110, Loss: 0.684189
Train - Epoch 3, Batch: 120, Loss: 0.683145
Train - Epoch 3, Batch: 130, Loss: 0.684629
Train - Epoch 3, Batch: 140, Loss: 0.684193
Train - Epoch 3, Batch: 150, Loss: 0.683386
Train - Epoch 3, Batch: 160, Loss: 0.684490
Train - Epoch 3, Batch: 170, Loss: 0.684591
Train - Epoch 3, Batch: 180, Loss: 0.683841
Train - Epoch 3, Batch: 190, Loss: 0.683999
Train - Epoch 3, Batch: 200, Loss: 0.684723
Train - Epoch 3, Batch: 210, Loss: 0.685133
Train - Epoch 3, Batch: 220, Loss: 0.684354
Train - Epoch 3, Batch: 230, Loss: 0.684872
Train - Epoch 3, Batch: 240, Loss: 0.684335
Train - Epoch 3, Batch: 250, Loss: 0.683984
Train - Epoch 3, Batch: 260, Loss: 0.684237
Train - Epoch 3, Batch: 270, Loss: 0.684104
Train - Epoch 3, Batch: 280, Loss: 0.684662
Train - Epoch 3, Batch: 290, Loss: 0.683511
Train - Epoch 3, Batch: 300, Loss: 0.684111
Train - Epoch 3, Batch: 310, Loss: 0.684184
Train - Epoch 3, Batch: 320, Loss: 0.684607
Train - Epoch 3, Batch: 330, Loss: 0.683099
Train - Epoch 3, Batch: 340, Loss: 0.683526
Train - Epoch 3, Batch: 350, Loss: 0.683801
Train - Epoch 3, Batch: 360, Loss: 0.684527
Train - Epoch 3, Batch: 370, Loss: 0.684523
Train - Epoch 3, Batch: 380, Loss: 0.682607
Train - Epoch 3, Batch: 390, Loss: 0.683646
Train - Epoch 3, Batch: 400, Loss: 0.684485
Train - Epoch 3, Batch: 410, Loss: 0.684084
Train - Epoch 3, Batch: 420, Loss: 0.683722
Train - Epoch 3, Batch: 430, Loss: 0.684270
Train - Epoch 3, Batch: 440, Loss: 0.683491
Train - Epoch 3, Batch: 450, Loss: 0.683265
Train - Epoch 3, Batch: 460, Loss: 0.684565
Train - Epoch 3, Batch: 470, Loss: 0.684177
Train - Epoch 3, Batch: 480, Loss: 0.685295
Train - Epoch 3, Batch: 490, Loss: 0.684560
Train - Epoch 3, Batch: 500, Loss: 0.683383
Train - Epoch 3, Batch: 510, Loss: 0.683900
Train - Epoch 3, Batch: 520, Loss: 0.684010
Train - Epoch 3, Batch: 530, Loss: 0.683568
Train - Epoch 3, Batch: 540, Loss: 0.684761
Train - Epoch 3, Batch: 550, Loss: 0.685361
Train - Epoch 3, Batch: 560, Loss: 0.683430
Train - Epoch 3, Batch: 570, Loss: 0.684379
Train - Epoch 3, Batch: 580, Loss: 0.684839
Train - Epoch 3, Batch: 590, Loss: 0.683842
Train - Epoch 3, Batch: 600, Loss: 0.683046
Train - Epoch 3, Batch: 610, Loss: 0.683756
Train - Epoch 3, Batch: 620, Loss: 0.684390
Train - Epoch 3, Batch: 630, Loss: 0.685076
Train - Epoch 3, Batch: 640, Loss: 0.683875
Test Avg. Loss: 0.000042, Accuracy: 0.552674
training_time:: 15.258618831634521
training time full:: 15.258708953857422
provenance prepare time:: 7.867813110351562e-06
Test Avg. Loss: 0.000042, Accuracy: 0.552674
baseline::
python3 incremental_updates_base_line_lr_multi.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Benchmark_experiments/benchmark_exp.py:1298: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  curr_delta_id = torch.tensor(delta_id)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 100
max_epoch:: 4
batch_size:: 16384
Num of deletion:: 0, running time baseline::4.803164
Num of deletion:: 10, running time baseline::52.521272
Num of deletion:: 20, running time baseline::99.679465
Num of deletion:: 30, running time baseline::146.691442
Num of deletion:: 40, running time baseline::194.482750
Num of deletion:: 50, running time baseline::240.817314
Num of deletion:: 60, running time baseline::288.900983
Num of deletion:: 70, running time baseline::336.528152
Num of deletion:: 80, running time baseline::384.497799
Num of deletion:: 90, running time baseline::432.609053
training time is 475.4595673084259
overhead:: 0
overhead2:: 475.45738101005554
overhead3:: 8.699681043624878
time_baseline:: 475.461341381073
curr_diff: 0 tensor(2.1397e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1397e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.552674
period:: 3
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance5_lr_multi.py 300 3 1 0.000009524 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 100
max_epoch:: 4
Num of deletion:: 0, running time provenance::3.467915
Num of deletion:: 10, running time provenance::31.650088
Num of deletion:: 20, running time provenance::64.076692
Num of deletion:: 30, running time provenance::95.545260
Num of deletion:: 40, running time provenance::124.716710
Num of deletion:: 50, running time provenance::155.781983
Num of deletion:: 60, running time provenance::187.726586
Num of deletion:: 70, running time provenance::219.637620
Num of deletion:: 80, running time provenance::251.129812
Num of deletion:: 90, running time provenance::280.955818
overhead:: 0
overhead2:: 0
overhead3:: 309.52052211761475
overhead4:: 0
overhead5:: 9.549188613891602
memory usage:: 5987966976
time_provenance:: 309.5239791870117
curr_diff: 0 tensor(1.5452e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5452e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(2.0798e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.0798e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.552672
repetition 2
python3 benchmark_exp_lr.py 0.001 16384 4 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression higgs 2 0.005 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 0.709393
Train - Epoch 0, Batch: 10, Loss: 0.691374
Train - Epoch 0, Batch: 20, Loss: 0.691990
Train - Epoch 0, Batch: 30, Loss: 0.692547
Train - Epoch 0, Batch: 40, Loss: 0.692778
Train - Epoch 0, Batch: 50, Loss: 0.689541
Train - Epoch 0, Batch: 60, Loss: 0.691756
Train - Epoch 0, Batch: 70, Loss: 0.690943
Train - Epoch 0, Batch: 80, Loss: 0.690633
Train - Epoch 0, Batch: 90, Loss: 0.691245
Train - Epoch 0, Batch: 100, Loss: 0.689270
Train - Epoch 0, Batch: 110, Loss: 0.690829
Train - Epoch 0, Batch: 120, Loss: 0.689855
Train - Epoch 0, Batch: 130, Loss: 0.689286
Train - Epoch 0, Batch: 140, Loss: 0.688658
Train - Epoch 0, Batch: 150, Loss: 0.690273
Train - Epoch 0, Batch: 160, Loss: 0.689755
Train - Epoch 0, Batch: 170, Loss: 0.689013
Train - Epoch 0, Batch: 180, Loss: 0.689530
Train - Epoch 0, Batch: 190, Loss: 0.689433
Train - Epoch 0, Batch: 200, Loss: 0.689419
Train - Epoch 0, Batch: 210, Loss: 0.690415
Train - Epoch 0, Batch: 220, Loss: 0.689974
Train - Epoch 0, Batch: 230, Loss: 0.688923
Train - Epoch 0, Batch: 240, Loss: 0.689122
Train - Epoch 0, Batch: 250, Loss: 0.689320
Train - Epoch 0, Batch: 260, Loss: 0.689796
Train - Epoch 0, Batch: 270, Loss: 0.688239
Train - Epoch 0, Batch: 280, Loss: 0.689144
Train - Epoch 0, Batch: 290, Loss: 0.689842
Train - Epoch 0, Batch: 300, Loss: 0.691356
Train - Epoch 0, Batch: 310, Loss: 0.688130
Train - Epoch 0, Batch: 320, Loss: 0.687748
Train - Epoch 0, Batch: 330, Loss: 0.689011
Train - Epoch 0, Batch: 340, Loss: 0.688451
Train - Epoch 0, Batch: 350, Loss: 0.688508
Train - Epoch 0, Batch: 360, Loss: 0.687637
Train - Epoch 0, Batch: 370, Loss: 0.689296
Train - Epoch 0, Batch: 380, Loss: 0.686589
Train - Epoch 0, Batch: 390, Loss: 0.688190
Train - Epoch 0, Batch: 400, Loss: 0.688857
Train - Epoch 0, Batch: 410, Loss: 0.688429
Train - Epoch 0, Batch: 420, Loss: 0.687106
Train - Epoch 0, Batch: 430, Loss: 0.688351
Train - Epoch 0, Batch: 440, Loss: 0.686753
Train - Epoch 0, Batch: 450, Loss: 0.688254
Train - Epoch 0, Batch: 460, Loss: 0.687376
Train - Epoch 0, Batch: 470, Loss: 0.687847
Train - Epoch 0, Batch: 480, Loss: 0.687296
Train - Epoch 0, Batch: 490, Loss: 0.687757
Train - Epoch 0, Batch: 500, Loss: 0.687934
Train - Epoch 0, Batch: 510, Loss: 0.686930
Train - Epoch 0, Batch: 520, Loss: 0.687011
Train - Epoch 0, Batch: 530, Loss: 0.687568
Train - Epoch 0, Batch: 540, Loss: 0.686848
Train - Epoch 0, Batch: 550, Loss: 0.687905
Train - Epoch 0, Batch: 560, Loss: 0.687293
Train - Epoch 0, Batch: 570, Loss: 0.686921
Train - Epoch 0, Batch: 580, Loss: 0.687124
Train - Epoch 0, Batch: 590, Loss: 0.687070
Train - Epoch 0, Batch: 600, Loss: 0.686859
Train - Epoch 0, Batch: 610, Loss: 0.687901
Train - Epoch 0, Batch: 620, Loss: 0.687121
Train - Epoch 0, Batch: 630, Loss: 0.687565
Train - Epoch 0, Batch: 640, Loss: 0.687176
Train - Epoch 1, Batch: 0, Loss: 0.687624
Train - Epoch 1, Batch: 10, Loss: 0.687663
Train - Epoch 1, Batch: 20, Loss: 0.688340
Train - Epoch 1, Batch: 30, Loss: 0.686071
Train - Epoch 1, Batch: 40, Loss: 0.686400
Train - Epoch 1, Batch: 50, Loss: 0.687059
Train - Epoch 1, Batch: 60, Loss: 0.686542
Train - Epoch 1, Batch: 70, Loss: 0.687523
Train - Epoch 1, Batch: 80, Loss: 0.687468
Train - Epoch 1, Batch: 90, Loss: 0.687294
Train - Epoch 1, Batch: 100, Loss: 0.687910
Train - Epoch 1, Batch: 110, Loss: 0.687256
Train - Epoch 1, Batch: 120, Loss: 0.686082
Train - Epoch 1, Batch: 130, Loss: 0.687003
Train - Epoch 1, Batch: 140, Loss: 0.685418
Train - Epoch 1, Batch: 150, Loss: 0.686979
Train - Epoch 1, Batch: 160, Loss: 0.688575
Train - Epoch 1, Batch: 170, Loss: 0.686919
Train - Epoch 1, Batch: 180, Loss: 0.686034
Train - Epoch 1, Batch: 190, Loss: 0.687445
Train - Epoch 1, Batch: 200, Loss: 0.685446
Train - Epoch 1, Batch: 210, Loss: 0.685700
Train - Epoch 1, Batch: 220, Loss: 0.687043
Train - Epoch 1, Batch: 230, Loss: 0.685127
Train - Epoch 1, Batch: 240, Loss: 0.687501
Train - Epoch 1, Batch: 250, Loss: 0.685856
Train - Epoch 1, Batch: 260, Loss: 0.686094
Train - Epoch 1, Batch: 270, Loss: 0.686549
Train - Epoch 1, Batch: 280, Loss: 0.685778
Train - Epoch 1, Batch: 290, Loss: 0.686482
Train - Epoch 1, Batch: 300, Loss: 0.686058
Train - Epoch 1, Batch: 310, Loss: 0.685564
Train - Epoch 1, Batch: 320, Loss: 0.687114
Train - Epoch 1, Batch: 330, Loss: 0.686063
Train - Epoch 1, Batch: 340, Loss: 0.685526
Train - Epoch 1, Batch: 350, Loss: 0.686600
Train - Epoch 1, Batch: 360, Loss: 0.686355
Train - Epoch 1, Batch: 370, Loss: 0.686295
Train - Epoch 1, Batch: 380, Loss: 0.686370
Train - Epoch 1, Batch: 390, Loss: 0.685829
Train - Epoch 1, Batch: 400, Loss: 0.686596
Train - Epoch 1, Batch: 410, Loss: 0.686619
Train - Epoch 1, Batch: 420, Loss: 0.684898
Train - Epoch 1, Batch: 430, Loss: 0.685838
Train - Epoch 1, Batch: 440, Loss: 0.685945
Train - Epoch 1, Batch: 450, Loss: 0.687047
Train - Epoch 1, Batch: 460, Loss: 0.687020
Train - Epoch 1, Batch: 470, Loss: 0.685356
Train - Epoch 1, Batch: 480, Loss: 0.686692
Train - Epoch 1, Batch: 490, Loss: 0.684675
Train - Epoch 1, Batch: 500, Loss: 0.685358
Train - Epoch 1, Batch: 510, Loss: 0.686828
Train - Epoch 1, Batch: 520, Loss: 0.685373
Train - Epoch 1, Batch: 530, Loss: 0.685599
Train - Epoch 1, Batch: 540, Loss: 0.686629
Train - Epoch 1, Batch: 550, Loss: 0.685935
Train - Epoch 1, Batch: 560, Loss: 0.686205
Train - Epoch 1, Batch: 570, Loss: 0.685307
Train - Epoch 1, Batch: 580, Loss: 0.685160
Train - Epoch 1, Batch: 590, Loss: 0.685140
Train - Epoch 1, Batch: 600, Loss: 0.685853
Train - Epoch 1, Batch: 610, Loss: 0.684715
Train - Epoch 1, Batch: 620, Loss: 0.686238
Train - Epoch 1, Batch: 630, Loss: 0.684979
Train - Epoch 1, Batch: 640, Loss: 0.686743
Train - Epoch 2, Batch: 0, Loss: 0.685146
Train - Epoch 2, Batch: 10, Loss: 0.686822
Train - Epoch 2, Batch: 20, Loss: 0.685312
Train - Epoch 2, Batch: 30, Loss: 0.685549
Train - Epoch 2, Batch: 40, Loss: 0.685729
Train - Epoch 2, Batch: 50, Loss: 0.684748
Train - Epoch 2, Batch: 60, Loss: 0.685542
Train - Epoch 2, Batch: 70, Loss: 0.684508
Train - Epoch 2, Batch: 80, Loss: 0.685336
Train - Epoch 2, Batch: 90, Loss: 0.685765
Train - Epoch 2, Batch: 100, Loss: 0.686162
Train - Epoch 2, Batch: 110, Loss: 0.685815
Train - Epoch 2, Batch: 120, Loss: 0.684146
Train - Epoch 2, Batch: 130, Loss: 0.685700
Train - Epoch 2, Batch: 140, Loss: 0.685053
Train - Epoch 2, Batch: 150, Loss: 0.685062
Train - Epoch 2, Batch: 160, Loss: 0.684631
Train - Epoch 2, Batch: 170, Loss: 0.685138
Train - Epoch 2, Batch: 180, Loss: 0.685293
Train - Epoch 2, Batch: 190, Loss: 0.685409
Train - Epoch 2, Batch: 200, Loss: 0.684544
Train - Epoch 2, Batch: 210, Loss: 0.684802
Train - Epoch 2, Batch: 220, Loss: 0.685635
Train - Epoch 2, Batch: 230, Loss: 0.685161
Train - Epoch 2, Batch: 240, Loss: 0.685696
Train - Epoch 2, Batch: 250, Loss: 0.683726
Train - Epoch 2, Batch: 260, Loss: 0.686386
Train - Epoch 2, Batch: 270, Loss: 0.686339
Train - Epoch 2, Batch: 280, Loss: 0.684917
Train - Epoch 2, Batch: 290, Loss: 0.684971
Train - Epoch 2, Batch: 300, Loss: 0.684802
Train - Epoch 2, Batch: 310, Loss: 0.685927
Train - Epoch 2, Batch: 320, Loss: 0.685417
Train - Epoch 2, Batch: 330, Loss: 0.685102
Train - Epoch 2, Batch: 340, Loss: 0.684415
Train - Epoch 2, Batch: 350, Loss: 0.685767
Train - Epoch 2, Batch: 360, Loss: 0.684713
Train - Epoch 2, Batch: 370, Loss: 0.683293
Train - Epoch 2, Batch: 380, Loss: 0.685314
Train - Epoch 2, Batch: 390, Loss: 0.685004
Train - Epoch 2, Batch: 400, Loss: 0.683804
Train - Epoch 2, Batch: 410, Loss: 0.683507
Train - Epoch 2, Batch: 420, Loss: 0.684789
Train - Epoch 2, Batch: 430, Loss: 0.684361
Train - Epoch 2, Batch: 440, Loss: 0.684924
Train - Epoch 2, Batch: 450, Loss: 0.685502
Train - Epoch 2, Batch: 460, Loss: 0.684771
Train - Epoch 2, Batch: 470, Loss: 0.685016
Train - Epoch 2, Batch: 480, Loss: 0.684614
Train - Epoch 2, Batch: 490, Loss: 0.684544
Train - Epoch 2, Batch: 500, Loss: 0.684324
Train - Epoch 2, Batch: 510, Loss: 0.684625
Train - Epoch 2, Batch: 520, Loss: 0.683502
Train - Epoch 2, Batch: 530, Loss: 0.684121
Train - Epoch 2, Batch: 540, Loss: 0.684999
Train - Epoch 2, Batch: 550, Loss: 0.683684/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 2, Batch: 560, Loss: 0.685195
Train - Epoch 2, Batch: 570, Loss: 0.685131
Train - Epoch 2, Batch: 580, Loss: 0.684506
Train - Epoch 2, Batch: 590, Loss: 0.685563
Train - Epoch 2, Batch: 600, Loss: 0.684746
Train - Epoch 2, Batch: 610, Loss: 0.685130
Train - Epoch 2, Batch: 620, Loss: 0.683219
Train - Epoch 2, Batch: 630, Loss: 0.684018
Train - Epoch 2, Batch: 640, Loss: 0.684954
Train - Epoch 3, Batch: 0, Loss: 0.685106
Train - Epoch 3, Batch: 10, Loss: 0.685179
Train - Epoch 3, Batch: 20, Loss: 0.684387
Train - Epoch 3, Batch: 30, Loss: 0.683268
Train - Epoch 3, Batch: 40, Loss: 0.684441
Train - Epoch 3, Batch: 50, Loss: 0.684132
Train - Epoch 3, Batch: 60, Loss: 0.684784
Train - Epoch 3, Batch: 70, Loss: 0.684550
Train - Epoch 3, Batch: 80, Loss: 0.684113
Train - Epoch 3, Batch: 90, Loss: 0.684464
Train - Epoch 3, Batch: 100, Loss: 0.684237
Train - Epoch 3, Batch: 110, Loss: 0.685289
Train - Epoch 3, Batch: 120, Loss: 0.684766
Train - Epoch 3, Batch: 130, Loss: 0.684774
Train - Epoch 3, Batch: 140, Loss: 0.684787
Train - Epoch 3, Batch: 150, Loss: 0.684041
Train - Epoch 3, Batch: 160, Loss: 0.684444
Train - Epoch 3, Batch: 170, Loss: 0.683437
Train - Epoch 3, Batch: 180, Loss: 0.683688
Train - Epoch 3, Batch: 190, Loss: 0.684214
Train - Epoch 3, Batch: 200, Loss: 0.684873
Train - Epoch 3, Batch: 210, Loss: 0.686382
Train - Epoch 3, Batch: 220, Loss: 0.683590
Train - Epoch 3, Batch: 230, Loss: 0.683874
Train - Epoch 3, Batch: 240, Loss: 0.685085
Train - Epoch 3, Batch: 250, Loss: 0.684467
Train - Epoch 3, Batch: 260, Loss: 0.684927
Train - Epoch 3, Batch: 270, Loss: 0.684848
Train - Epoch 3, Batch: 280, Loss: 0.683190
Train - Epoch 3, Batch: 290, Loss: 0.684187
Train - Epoch 3, Batch: 300, Loss: 0.683176
Train - Epoch 3, Batch: 310, Loss: 0.683863
Train - Epoch 3, Batch: 320, Loss: 0.686016
Train - Epoch 3, Batch: 330, Loss: 0.684202
Train - Epoch 3, Batch: 340, Loss: 0.684512
Train - Epoch 3, Batch: 350, Loss: 0.683899
Train - Epoch 3, Batch: 360, Loss: 0.683826
Train - Epoch 3, Batch: 370, Loss: 0.684716
Train - Epoch 3, Batch: 380, Loss: 0.685671
Train - Epoch 3, Batch: 390, Loss: 0.684612
Train - Epoch 3, Batch: 400, Loss: 0.683628
Train - Epoch 3, Batch: 410, Loss: 0.684384
Train - Epoch 3, Batch: 420, Loss: 0.684602
Train - Epoch 3, Batch: 430, Loss: 0.683587
Train - Epoch 3, Batch: 440, Loss: 0.683405
Train - Epoch 3, Batch: 450, Loss: 0.684162
Train - Epoch 3, Batch: 460, Loss: 0.684804
Train - Epoch 3, Batch: 470, Loss: 0.683421
Train - Epoch 3, Batch: 480, Loss: 0.684229
Train - Epoch 3, Batch: 490, Loss: 0.685002
Train - Epoch 3, Batch: 500, Loss: 0.685187
Train - Epoch 3, Batch: 510, Loss: 0.683813
Train - Epoch 3, Batch: 520, Loss: 0.685801
Train - Epoch 3, Batch: 530, Loss: 0.684200
Train - Epoch 3, Batch: 540, Loss: 0.684044
Train - Epoch 3, Batch: 550, Loss: 0.682674
Train - Epoch 3, Batch: 560, Loss: 0.684894
Train - Epoch 3, Batch: 570, Loss: 0.683935
Train - Epoch 3, Batch: 580, Loss: 0.683420
Train - Epoch 3, Batch: 590, Loss: 0.683336
Train - Epoch 3, Batch: 600, Loss: 0.683367
Train - Epoch 3, Batch: 610, Loss: 0.683286
Train - Epoch 3, Batch: 620, Loss: 0.683738
Train - Epoch 3, Batch: 630, Loss: 0.682405
Train - Epoch 3, Batch: 640, Loss: 0.683567
Test Avg. Loss: 0.000042, Accuracy: 0.552062
training_time:: 15.365097999572754
training time full:: 15.36516261100769
provenance prepare time:: 6.4373016357421875e-06
Test Avg. Loss: 0.000042, Accuracy: 0.552062
baseline::
python3 incremental_updates_base_line_lr_multi.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Benchmark_experiments/benchmark_exp.py:1298: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  curr_delta_id = torch.tensor(delta_id)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 100
max_epoch:: 4
batch_size:: 16384
Num of deletion:: 0, running time baseline::4.254485
Num of deletion:: 10, running time baseline::53.189901
Num of deletion:: 20, running time baseline::100.636024
Num of deletion:: 30, running time baseline::148.757153
Num of deletion:: 40, running time baseline::195.275748
Num of deletion:: 50, running time baseline::241.895961
Num of deletion:: 60, running time baseline::289.427664
Num of deletion:: 70, running time baseline::340.570778
Num of deletion:: 80, running time baseline::389.733505
Num of deletion:: 90, running time baseline::437.098383
training time is 480.1881968975067
overhead:: 0
overhead2:: 480.1859953403473
overhead3:: 8.931309700012207
time_baseline:: 480.1893033981323
curr_diff: 0 tensor(2.7067e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.7067e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.552066
period:: 3
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance5_lr_multi.py 300 3 2 0.000009524 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 100
max_epoch:: 4
Num of deletion:: 0, running time provenance::3.011682
Num of deletion:: 10, running time provenance::34.707473
Num of deletion:: 20, running time provenance::67.589426
Num of deletion:: 30, running time provenance::98.718524
Num of deletion:: 40, running time provenance::130.325380
Num of deletion:: 50, running time provenance::162.611983
Num of deletion:: 60, running time provenance::194.510816
Num of deletion:: 70, running time provenance::227.584476
Num of deletion:: 80, running time provenance::262.122094
Num of deletion:: 90, running time provenance::296.800521
overhead:: 0
overhead2:: 0
overhead3:: 326.1044023036957
overhead4:: 0
overhead5:: 10.14032793045044
memory usage:: 5988417536
time_provenance:: 326.1079292297363
curr_diff: 0 tensor(1.3672e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3672e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(2.6264e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.6264e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.552066
repetition 3
python3 benchmark_exp_lr.py 0.001 16384 4 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression higgs 3 0.005 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 0.712854
Train - Epoch 0, Batch: 10, Loss: 0.699381
Train - Epoch 0, Batch: 20, Loss: 0.698175
Train - Epoch 0, Batch: 30, Loss: 0.695572
Train - Epoch 0, Batch: 40, Loss: 0.693563
Train - Epoch 0, Batch: 50, Loss: 0.695509
Train - Epoch 0, Batch: 60, Loss: 0.693867
Train - Epoch 0, Batch: 70, Loss: 0.693499
Train - Epoch 0, Batch: 80, Loss: 0.692439
Train - Epoch 0, Batch: 90, Loss: 0.692076
Train - Epoch 0, Batch: 100, Loss: 0.691194
Train - Epoch 0, Batch: 110, Loss: 0.692151
Train - Epoch 0, Batch: 120, Loss: 0.691652
Train - Epoch 0, Batch: 130, Loss: 0.691351
Train - Epoch 0, Batch: 140, Loss: 0.691252
Train - Epoch 0, Batch: 150, Loss: 0.690946
Train - Epoch 0, Batch: 160, Loss: 0.690456
Train - Epoch 0, Batch: 170, Loss: 0.690661
Train - Epoch 0, Batch: 180, Loss: 0.690093
Train - Epoch 0, Batch: 190, Loss: 0.690178
Train - Epoch 0, Batch: 200, Loss: 0.690393
Train - Epoch 0, Batch: 210, Loss: 0.690559
Train - Epoch 0, Batch: 220, Loss: 0.689788
Train - Epoch 0, Batch: 230, Loss: 0.689718
Train - Epoch 0, Batch: 240, Loss: 0.690479
Train - Epoch 0, Batch: 250, Loss: 0.691880
Train - Epoch 0, Batch: 260, Loss: 0.689238
Train - Epoch 0, Batch: 270, Loss: 0.690361
Train - Epoch 0, Batch: 280, Loss: 0.690093
Train - Epoch 0, Batch: 290, Loss: 0.690193
Train - Epoch 0, Batch: 300, Loss: 0.689324
Train - Epoch 0, Batch: 310, Loss: 0.689985
Train - Epoch 0, Batch: 320, Loss: 0.689885
Train - Epoch 0, Batch: 330, Loss: 0.690064
Train - Epoch 0, Batch: 340, Loss: 0.689569
Train - Epoch 0, Batch: 350, Loss: 0.689522
Train - Epoch 0, Batch: 360, Loss: 0.689414
Train - Epoch 0, Batch: 370, Loss: 0.689268
Train - Epoch 0, Batch: 380, Loss: 0.689104
Train - Epoch 0, Batch: 390, Loss: 0.689203
Train - Epoch 0, Batch: 400, Loss: 0.689244
Train - Epoch 0, Batch: 410, Loss: 0.688222
Train - Epoch 0, Batch: 420, Loss: 0.688376
Train - Epoch 0, Batch: 430, Loss: 0.688314
Train - Epoch 0, Batch: 440, Loss: 0.688408
Train - Epoch 0, Batch: 450, Loss: 0.688388
Train - Epoch 0, Batch: 460, Loss: 0.688481
Train - Epoch 0, Batch: 470, Loss: 0.688255
Train - Epoch 0, Batch: 480, Loss: 0.688714
Train - Epoch 0, Batch: 490, Loss: 0.686324
Train - Epoch 0, Batch: 500, Loss: 0.688031
Train - Epoch 0, Batch: 510, Loss: 0.688949
Train - Epoch 0, Batch: 520, Loss: 0.688861
Train - Epoch 0, Batch: 530, Loss: 0.688567
Train - Epoch 0, Batch: 540, Loss: 0.687723
Train - Epoch 0, Batch: 550, Loss: 0.688612
Train - Epoch 0, Batch: 560, Loss: 0.687360
Train - Epoch 0, Batch: 570, Loss: 0.688956
Train - Epoch 0, Batch: 580, Loss: 0.688060
Train - Epoch 0, Batch: 590, Loss: 0.688716
Train - Epoch 0, Batch: 600, Loss: 0.688784
Train - Epoch 0, Batch: 610, Loss: 0.687927
Train - Epoch 0, Batch: 620, Loss: 0.688665
Train - Epoch 0, Batch: 630, Loss: 0.688278
Train - Epoch 0, Batch: 640, Loss: 0.686655
Train - Epoch 1, Batch: 0, Loss: 0.688086
Train - Epoch 1, Batch: 10, Loss: 0.688764
Train - Epoch 1, Batch: 20, Loss: 0.688602
Train - Epoch 1, Batch: 30, Loss: 0.687959
Train - Epoch 1, Batch: 40, Loss: 0.688219
Train - Epoch 1, Batch: 50, Loss: 0.687990
Train - Epoch 1, Batch: 60, Loss: 0.687529
Train - Epoch 1, Batch: 70, Loss: 0.687250
Train - Epoch 1, Batch: 80, Loss: 0.688905
Train - Epoch 1, Batch: 90, Loss: 0.687712
Train - Epoch 1, Batch: 100, Loss: 0.687995
Train - Epoch 1, Batch: 110, Loss: 0.687031
Train - Epoch 1, Batch: 120, Loss: 0.688249
Train - Epoch 1, Batch: 130, Loss: 0.688588
Train - Epoch 1, Batch: 140, Loss: 0.687615
Train - Epoch 1, Batch: 150, Loss: 0.686839
Train - Epoch 1, Batch: 160, Loss: 0.687105
Train - Epoch 1, Batch: 170, Loss: 0.686485
Train - Epoch 1, Batch: 180, Loss: 0.686831
Train - Epoch 1, Batch: 190, Loss: 0.687256
Train - Epoch 1, Batch: 200, Loss: 0.687067
Train - Epoch 1, Batch: 210, Loss: 0.686377
Train - Epoch 1, Batch: 220, Loss: 0.686326
Train - Epoch 1, Batch: 230, Loss: 0.686127
Train - Epoch 1, Batch: 240, Loss: 0.687466
Train - Epoch 1, Batch: 250, Loss: 0.687536
Train - Epoch 1, Batch: 260, Loss: 0.686587
Train - Epoch 1, Batch: 270, Loss: 0.685850
Train - Epoch 1, Batch: 280, Loss: 0.685580
Train - Epoch 1, Batch: 290, Loss: 0.686465
Train - Epoch 1, Batch: 300, Loss: 0.688005
Train - Epoch 1, Batch: 310, Loss: 0.685590
Train - Epoch 1, Batch: 320, Loss: 0.686044
Train - Epoch 1, Batch: 330, Loss: 0.686250
Train - Epoch 1, Batch: 340, Loss: 0.686304
Train - Epoch 1, Batch: 350, Loss: 0.686506
Train - Epoch 1, Batch: 360, Loss: 0.686744
Train - Epoch 1, Batch: 370, Loss: 0.686719
Train - Epoch 1, Batch: 380, Loss: 0.685288
Train - Epoch 1, Batch: 390, Loss: 0.685550
Train - Epoch 1, Batch: 400, Loss: 0.686168
Train - Epoch 1, Batch: 410, Loss: 0.686359
Train - Epoch 1, Batch: 420, Loss: 0.687034
Train - Epoch 1, Batch: 430, Loss: 0.686373
Train - Epoch 1, Batch: 440, Loss: 0.686167
Train - Epoch 1, Batch: 450, Loss: 0.686406
Train - Epoch 1, Batch: 460, Loss: 0.686403
Train - Epoch 1, Batch: 470, Loss: 0.687118
Train - Epoch 1, Batch: 480, Loss: 0.685050
Train - Epoch 1, Batch: 490, Loss: 0.686295
Train - Epoch 1, Batch: 500, Loss: 0.685767
Train - Epoch 1, Batch: 510, Loss: 0.686836
Train - Epoch 1, Batch: 520, Loss: 0.686951
Train - Epoch 1, Batch: 530, Loss: 0.686321
Train - Epoch 1, Batch: 540, Loss: 0.686217
Train - Epoch 1, Batch: 550, Loss: 0.685656
Train - Epoch 1, Batch: 560, Loss: 0.685261
Train - Epoch 1, Batch: 570, Loss: 0.684938
Train - Epoch 1, Batch: 580, Loss: 0.687767
Train - Epoch 1, Batch: 590, Loss: 0.686031
Train - Epoch 1, Batch: 600, Loss: 0.685525
Train - Epoch 1, Batch: 610, Loss: 0.686685
Train - Epoch 1, Batch: 620, Loss: 0.686753
Train - Epoch 1, Batch: 630, Loss: 0.685296
Train - Epoch 1, Batch: 640, Loss: 0.684628
Train - Epoch 2, Batch: 0, Loss: 0.685421
Train - Epoch 2, Batch: 10, Loss: 0.686246
Train - Epoch 2, Batch: 20, Loss: 0.686779
Train - Epoch 2, Batch: 30, Loss: 0.685795
Train - Epoch 2, Batch: 40, Loss: 0.685451
Train - Epoch 2, Batch: 50, Loss: 0.685373
Train - Epoch 2, Batch: 60, Loss: 0.685828
Train - Epoch 2, Batch: 70, Loss: 0.686432
Train - Epoch 2, Batch: 80, Loss: 0.685930
Train - Epoch 2, Batch: 90, Loss: 0.685754
Train - Epoch 2, Batch: 100, Loss: 0.684869
Train - Epoch 2, Batch: 110, Loss: 0.684961
Train - Epoch 2, Batch: 120, Loss: 0.684843
Train - Epoch 2, Batch: 130, Loss: 0.685992
Train - Epoch 2, Batch: 140, Loss: 0.685949
Train - Epoch 2, Batch: 150, Loss: 0.684285
Train - Epoch 2, Batch: 160, Loss: 0.685582
Train - Epoch 2, Batch: 170, Loss: 0.686367
Train - Epoch 2, Batch: 180, Loss: 0.684657
Train - Epoch 2, Batch: 190, Loss: 0.685202
Train - Epoch 2, Batch: 200, Loss: 0.686195
Train - Epoch 2, Batch: 210, Loss: 0.683887
Train - Epoch 2, Batch: 220, Loss: 0.686120
Train - Epoch 2, Batch: 230, Loss: 0.685826
Train - Epoch 2, Batch: 240, Loss: 0.686102
Train - Epoch 2, Batch: 250, Loss: 0.684523
Train - Epoch 2, Batch: 260, Loss: 0.686558
Train - Epoch 2, Batch: 270, Loss: 0.684842
Train - Epoch 2, Batch: 280, Loss: 0.686524
Train - Epoch 2, Batch: 290, Loss: 0.684619
Train - Epoch 2, Batch: 300, Loss: 0.683351
Train - Epoch 2, Batch: 310, Loss: 0.685398
Train - Epoch 2, Batch: 320, Loss: 0.685724
Train - Epoch 2, Batch: 330, Loss: 0.685763
Train - Epoch 2, Batch: 340, Loss: 0.685854
Train - Epoch 2, Batch: 350, Loss: 0.685508
Train - Epoch 2, Batch: 360, Loss: 0.685215
Train - Epoch 2, Batch: 370, Loss: 0.685561
Train - Epoch 2, Batch: 380, Loss: 0.685165
Train - Epoch 2, Batch: 390, Loss: 0.684168
Train - Epoch 2, Batch: 400, Loss: 0.685087
Train - Epoch 2, Batch: 410, Loss: 0.685311
Train - Epoch 2, Batch: 420, Loss: 0.686876
Train - Epoch 2, Batch: 430, Loss: 0.685118
Train - Epoch 2, Batch: 440, Loss: 0.684674
Train - Epoch 2, Batch: 450, Loss: 0.685771
Train - Epoch 2, Batch: 460, Loss: 0.685292
Train - Epoch 2, Batch: 470, Loss: 0.684263
Train - Epoch 2, Batch: 480, Loss: 0.684419
Train - Epoch 2, Batch: 490, Loss: 0.685211
Train - Epoch 2, Batch: 500, Loss: 0.685391
Train - Epoch 2, Batch: 510, Loss: 0.683769
Train - Epoch 2, Batch: 520, Loss: 0.686232
Train - Epoch 2, Batch: 530, Loss: 0.685210
Train - Epoch 2, Batch: 540, Loss: 0.685419
Train - Epoch 2, Batch: 550, Loss: 0.684374/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 2, Batch: 560, Loss: 0.683239
Train - Epoch 2, Batch: 570, Loss: 0.683033
Train - Epoch 2, Batch: 580, Loss: 0.684984
Train - Epoch 2, Batch: 590, Loss: 0.684790
Train - Epoch 2, Batch: 600, Loss: 0.686137
Train - Epoch 2, Batch: 610, Loss: 0.684725
Train - Epoch 2, Batch: 620, Loss: 0.684613
Train - Epoch 2, Batch: 630, Loss: 0.685327
Train - Epoch 2, Batch: 640, Loss: 0.683973
Train - Epoch 3, Batch: 0, Loss: 0.684503
Train - Epoch 3, Batch: 10, Loss: 0.684833
Train - Epoch 3, Batch: 20, Loss: 0.685026
Train - Epoch 3, Batch: 30, Loss: 0.684410
Train - Epoch 3, Batch: 40, Loss: 0.685537
Train - Epoch 3, Batch: 50, Loss: 0.685336
Train - Epoch 3, Batch: 60, Loss: 0.683945
Train - Epoch 3, Batch: 70, Loss: 0.684695
Train - Epoch 3, Batch: 80, Loss: 0.685149
Train - Epoch 3, Batch: 90, Loss: 0.685369
Train - Epoch 3, Batch: 100, Loss: 0.684093
Train - Epoch 3, Batch: 110, Loss: 0.685962
Train - Epoch 3, Batch: 120, Loss: 0.684680
Train - Epoch 3, Batch: 130, Loss: 0.684953
Train - Epoch 3, Batch: 140, Loss: 0.685427
Train - Epoch 3, Batch: 150, Loss: 0.684793
Train - Epoch 3, Batch: 160, Loss: 0.685253
Train - Epoch 3, Batch: 170, Loss: 0.684863
Train - Epoch 3, Batch: 180, Loss: 0.684304
Train - Epoch 3, Batch: 190, Loss: 0.684139
Train - Epoch 3, Batch: 200, Loss: 0.684066
Train - Epoch 3, Batch: 210, Loss: 0.684432
Train - Epoch 3, Batch: 220, Loss: 0.684073
Train - Epoch 3, Batch: 230, Loss: 0.684616
Train - Epoch 3, Batch: 240, Loss: 0.684529
Train - Epoch 3, Batch: 250, Loss: 0.685472
Train - Epoch 3, Batch: 260, Loss: 0.684158
Train - Epoch 3, Batch: 270, Loss: 0.684161
Train - Epoch 3, Batch: 280, Loss: 0.683997
Train - Epoch 3, Batch: 290, Loss: 0.684242
Train - Epoch 3, Batch: 300, Loss: 0.684658
Train - Epoch 3, Batch: 310, Loss: 0.684779
Train - Epoch 3, Batch: 320, Loss: 0.683964
Train - Epoch 3, Batch: 330, Loss: 0.683833
Train - Epoch 3, Batch: 340, Loss: 0.684193
Train - Epoch 3, Batch: 350, Loss: 0.683780
Train - Epoch 3, Batch: 360, Loss: 0.685107
Train - Epoch 3, Batch: 370, Loss: 0.685818
Train - Epoch 3, Batch: 380, Loss: 0.685365
Train - Epoch 3, Batch: 390, Loss: 0.684178
Train - Epoch 3, Batch: 400, Loss: 0.684612
Train - Epoch 3, Batch: 410, Loss: 0.685325
Train - Epoch 3, Batch: 420, Loss: 0.684845
Train - Epoch 3, Batch: 430, Loss: 0.683204
Train - Epoch 3, Batch: 440, Loss: 0.685331
Train - Epoch 3, Batch: 450, Loss: 0.683679
Train - Epoch 3, Batch: 460, Loss: 0.682867
Train - Epoch 3, Batch: 470, Loss: 0.684151
Train - Epoch 3, Batch: 480, Loss: 0.684822
Train - Epoch 3, Batch: 490, Loss: 0.684363
Train - Epoch 3, Batch: 500, Loss: 0.684598
Train - Epoch 3, Batch: 510, Loss: 0.684213
Train - Epoch 3, Batch: 520, Loss: 0.684416
Train - Epoch 3, Batch: 530, Loss: 0.684952
Train - Epoch 3, Batch: 540, Loss: 0.683523
Train - Epoch 3, Batch: 550, Loss: 0.684973
Train - Epoch 3, Batch: 560, Loss: 0.684690
Train - Epoch 3, Batch: 570, Loss: 0.684270
Train - Epoch 3, Batch: 580, Loss: 0.684330
Train - Epoch 3, Batch: 590, Loss: 0.684084
Train - Epoch 3, Batch: 600, Loss: 0.683641
Train - Epoch 3, Batch: 610, Loss: 0.684281
Train - Epoch 3, Batch: 620, Loss: 0.683056
Train - Epoch 3, Batch: 630, Loss: 0.682030
Train - Epoch 3, Batch: 640, Loss: 0.686011
Test Avg. Loss: 0.000042, Accuracy: 0.553924
training_time:: 16.107589960098267
training time full:: 16.10765314102173
provenance prepare time:: 6.198883056640625e-06
Test Avg. Loss: 0.000042, Accuracy: 0.553924
baseline::
python3 incremental_updates_base_line_lr_multi.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Benchmark_experiments/benchmark_exp.py:1298: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  curr_delta_id = torch.tensor(delta_id)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 100
max_epoch:: 4
batch_size:: 16384
Num of deletion:: 0, running time baseline::4.657361
Num of deletion:: 10, running time baseline::50.782909
Num of deletion:: 20, running time baseline::98.927825
Num of deletion:: 30, running time baseline::146.695921
Num of deletion:: 40, running time baseline::196.236968
Num of deletion:: 50, running time baseline::245.791022
Num of deletion:: 60, running time baseline::293.655257
Num of deletion:: 70, running time baseline::339.977984
Num of deletion:: 80, running time baseline::386.789472
Num of deletion:: 90, running time baseline::435.662059
training time is 477.31602692604065
overhead:: 0
overhead2:: 477.31370306015015
overhead3:: 9.152632713317871
time_baseline:: 477.3171465396881
curr_diff: 0 tensor(2.6301e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.6301e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553888
period:: 3
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance5_lr_multi.py 300 3 3 0.000009524 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 100
max_epoch:: 4
Num of deletion:: 0, running time provenance::3.243262
Num of deletion:: 10, running time provenance::34.469493
Num of deletion:: 20, running time provenance::67.324813
Num of deletion:: 30, running time provenance::95.437546
Num of deletion:: 40, running time provenance::128.370524
Num of deletion:: 50, running time provenance::159.385051
Num of deletion:: 60, running time provenance::191.316454
Num of deletion:: 70, running time provenance::222.068103
Num of deletion:: 80, running time provenance::253.006735
Num of deletion:: 90, running time provenance::285.013498
overhead:: 0
overhead2:: 0
overhead3:: 312.28688311576843
overhead4:: 0
overhead5:: 9.9598548412323
memory usage:: 5986942976
time_provenance:: 312.29036474227905
curr_diff: 0 tensor(2.0519e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.0519e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(2.5553e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.5553e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553888
repetition 4
python3 benchmark_exp_lr.py 0.001 16384 4 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression higgs 4 0.005 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 0.707674
Train - Epoch 0, Batch: 10, Loss: 0.692676
Train - Epoch 0, Batch: 20, Loss: 0.692524
Train - Epoch 0, Batch: 30, Loss: 0.690850
Train - Epoch 0, Batch: 40, Loss: 0.691429
Train - Epoch 0, Batch: 50, Loss: 0.690858
Train - Epoch 0, Batch: 60, Loss: 0.691197
Train - Epoch 0, Batch: 70, Loss: 0.690267
Train - Epoch 0, Batch: 80, Loss: 0.690031
Train - Epoch 0, Batch: 90, Loss: 0.691353
Train - Epoch 0, Batch: 100, Loss: 0.689598
Train - Epoch 0, Batch: 110, Loss: 0.689845
Train - Epoch 0, Batch: 120, Loss: 0.689226
Train - Epoch 0, Batch: 130, Loss: 0.688686
Train - Epoch 0, Batch: 140, Loss: 0.688842
Train - Epoch 0, Batch: 150, Loss: 0.689051
Train - Epoch 0, Batch: 160, Loss: 0.687928
Train - Epoch 0, Batch: 170, Loss: 0.688804
Train - Epoch 0, Batch: 180, Loss: 0.688612
Train - Epoch 0, Batch: 190, Loss: 0.688631
Train - Epoch 0, Batch: 200, Loss: 0.689410
Train - Epoch 0, Batch: 210, Loss: 0.688224
Train - Epoch 0, Batch: 220, Loss: 0.688288
Train - Epoch 0, Batch: 230, Loss: 0.689309
Train - Epoch 0, Batch: 240, Loss: 0.689024
Train - Epoch 0, Batch: 250, Loss: 0.687863
Train - Epoch 0, Batch: 260, Loss: 0.687552
Train - Epoch 0, Batch: 270, Loss: 0.688339
Train - Epoch 0, Batch: 280, Loss: 0.688159
Train - Epoch 0, Batch: 290, Loss: 0.687711
Train - Epoch 0, Batch: 300, Loss: 0.688216
Train - Epoch 0, Batch: 310, Loss: 0.688746
Train - Epoch 0, Batch: 320, Loss: 0.688062
Train - Epoch 0, Batch: 330, Loss: 0.687593
Train - Epoch 0, Batch: 340, Loss: 0.687364
Train - Epoch 0, Batch: 350, Loss: 0.686987
Train - Epoch 0, Batch: 360, Loss: 0.687683
Train - Epoch 0, Batch: 370, Loss: 0.686317
Train - Epoch 0, Batch: 380, Loss: 0.687021
Train - Epoch 0, Batch: 390, Loss: 0.687536
Train - Epoch 0, Batch: 400, Loss: 0.687508
Train - Epoch 0, Batch: 410, Loss: 0.687348
Train - Epoch 0, Batch: 420, Loss: 0.686743
Train - Epoch 0, Batch: 430, Loss: 0.688341
Train - Epoch 0, Batch: 440, Loss: 0.687647
Train - Epoch 0, Batch: 450, Loss: 0.687805
Train - Epoch 0, Batch: 460, Loss: 0.686482
Train - Epoch 0, Batch: 470, Loss: 0.686995
Train - Epoch 0, Batch: 480, Loss: 0.686936
Train - Epoch 0, Batch: 490, Loss: 0.686795
Train - Epoch 0, Batch: 500, Loss: 0.687960
Train - Epoch 0, Batch: 510, Loss: 0.687465
Train - Epoch 0, Batch: 520, Loss: 0.686529
Train - Epoch 0, Batch: 530, Loss: 0.687600
Train - Epoch 0, Batch: 540, Loss: 0.687327
Train - Epoch 0, Batch: 550, Loss: 0.687463
Train - Epoch 0, Batch: 560, Loss: 0.687442
Train - Epoch 0, Batch: 570, Loss: 0.688021
Train - Epoch 0, Batch: 580, Loss: 0.687482
Train - Epoch 0, Batch: 590, Loss: 0.686256
Train - Epoch 0, Batch: 600, Loss: 0.687025
Train - Epoch 0, Batch: 610, Loss: 0.686028
Train - Epoch 0, Batch: 620, Loss: 0.686531
Train - Epoch 0, Batch: 630, Loss: 0.686603
Train - Epoch 0, Batch: 640, Loss: 0.686518
Train - Epoch 1, Batch: 0, Loss: 0.685860
Train - Epoch 1, Batch: 10, Loss: 0.686797
Train - Epoch 1, Batch: 20, Loss: 0.687524
Train - Epoch 1, Batch: 30, Loss: 0.687069
Train - Epoch 1, Batch: 40, Loss: 0.686718
Train - Epoch 1, Batch: 50, Loss: 0.686611
Train - Epoch 1, Batch: 60, Loss: 0.686506
Train - Epoch 1, Batch: 70, Loss: 0.684449
Train - Epoch 1, Batch: 80, Loss: 0.687056
Train - Epoch 1, Batch: 90, Loss: 0.686118
Train - Epoch 1, Batch: 100, Loss: 0.685848
Train - Epoch 1, Batch: 110, Loss: 0.686426
Train - Epoch 1, Batch: 120, Loss: 0.686666
Train - Epoch 1, Batch: 130, Loss: 0.686271
Train - Epoch 1, Batch: 140, Loss: 0.687044
Train - Epoch 1, Batch: 150, Loss: 0.686008
Train - Epoch 1, Batch: 160, Loss: 0.686988
Train - Epoch 1, Batch: 170, Loss: 0.687404
Train - Epoch 1, Batch: 180, Loss: 0.685218
Train - Epoch 1, Batch: 190, Loss: 0.685471
Train - Epoch 1, Batch: 200, Loss: 0.685863
Train - Epoch 1, Batch: 210, Loss: 0.685558
Train - Epoch 1, Batch: 220, Loss: 0.686535
Train - Epoch 1, Batch: 230, Loss: 0.685841
Train - Epoch 1, Batch: 240, Loss: 0.685350
Train - Epoch 1, Batch: 250, Loss: 0.686965
Train - Epoch 1, Batch: 260, Loss: 0.685367
Train - Epoch 1, Batch: 270, Loss: 0.685254
Train - Epoch 1, Batch: 280, Loss: 0.684992
Train - Epoch 1, Batch: 290, Loss: 0.686321
Train - Epoch 1, Batch: 300, Loss: 0.685881
Train - Epoch 1, Batch: 310, Loss: 0.686472
Train - Epoch 1, Batch: 320, Loss: 0.686801
Train - Epoch 1, Batch: 330, Loss: 0.683998
Train - Epoch 1, Batch: 340, Loss: 0.685871
Train - Epoch 1, Batch: 350, Loss: 0.687205
Train - Epoch 1, Batch: 360, Loss: 0.687702
Train - Epoch 1, Batch: 370, Loss: 0.685402
Train - Epoch 1, Batch: 380, Loss: 0.686367
Train - Epoch 1, Batch: 390, Loss: 0.684429
Train - Epoch 1, Batch: 400, Loss: 0.684929
Train - Epoch 1, Batch: 410, Loss: 0.684784
Train - Epoch 1, Batch: 420, Loss: 0.685520
Train - Epoch 1, Batch: 430, Loss: 0.686190
Train - Epoch 1, Batch: 440, Loss: 0.686120
Train - Epoch 1, Batch: 450, Loss: 0.685874
Train - Epoch 1, Batch: 460, Loss: 0.685480
Train - Epoch 1, Batch: 470, Loss: 0.686275
Train - Epoch 1, Batch: 480, Loss: 0.685604
Train - Epoch 1, Batch: 490, Loss: 0.684980
Train - Epoch 1, Batch: 500, Loss: 0.687016
Train - Epoch 1, Batch: 510, Loss: 0.685754
Train - Epoch 1, Batch: 520, Loss: 0.685778
Train - Epoch 1, Batch: 530, Loss: 0.686198
Train - Epoch 1, Batch: 540, Loss: 0.685450
Train - Epoch 1, Batch: 550, Loss: 0.686574
Train - Epoch 1, Batch: 560, Loss: 0.685454
Train - Epoch 1, Batch: 570, Loss: 0.685320
Train - Epoch 1, Batch: 580, Loss: 0.685437
Train - Epoch 1, Batch: 590, Loss: 0.686353
Train - Epoch 1, Batch: 600, Loss: 0.685716
Train - Epoch 1, Batch: 610, Loss: 0.684541
Train - Epoch 1, Batch: 620, Loss: 0.685027
Train - Epoch 1, Batch: 630, Loss: 0.686005
Train - Epoch 1, Batch: 640, Loss: 0.684658
Train - Epoch 2, Batch: 0, Loss: 0.685129
Train - Epoch 2, Batch: 10, Loss: 0.686192
Train - Epoch 2, Batch: 20, Loss: 0.685512
Train - Epoch 2, Batch: 30, Loss: 0.684789
Train - Epoch 2, Batch: 40, Loss: 0.684386
Train - Epoch 2, Batch: 50, Loss: 0.685083
Train - Epoch 2, Batch: 60, Loss: 0.685721
Train - Epoch 2, Batch: 70, Loss: 0.685263
Train - Epoch 2, Batch: 80, Loss: 0.684689
Train - Epoch 2, Batch: 90, Loss: 0.685435
Train - Epoch 2, Batch: 100, Loss: 0.684760
Train - Epoch 2, Batch: 110, Loss: 0.685583
Train - Epoch 2, Batch: 120, Loss: 0.684886
Train - Epoch 2, Batch: 130, Loss: 0.685512
Train - Epoch 2, Batch: 140, Loss: 0.685599
Train - Epoch 2, Batch: 150, Loss: 0.684437
Train - Epoch 2, Batch: 160, Loss: 0.685775
Train - Epoch 2, Batch: 170, Loss: 0.684846
Train - Epoch 2, Batch: 180, Loss: 0.685074
Train - Epoch 2, Batch: 190, Loss: 0.686106
Train - Epoch 2, Batch: 200, Loss: 0.684633
Train - Epoch 2, Batch: 210, Loss: 0.685025
Train - Epoch 2, Batch: 220, Loss: 0.684909
Train - Epoch 2, Batch: 230, Loss: 0.684912
Train - Epoch 2, Batch: 240, Loss: 0.685504
Train - Epoch 2, Batch: 250, Loss: 0.683629
Train - Epoch 2, Batch: 260, Loss: 0.685014
Train - Epoch 2, Batch: 270, Loss: 0.685382
Train - Epoch 2, Batch: 280, Loss: 0.684430
Train - Epoch 2, Batch: 290, Loss: 0.683621
Train - Epoch 2, Batch: 300, Loss: 0.684268
Train - Epoch 2, Batch: 310, Loss: 0.685372
Train - Epoch 2, Batch: 320, Loss: 0.686157
Train - Epoch 2, Batch: 330, Loss: 0.686450
Train - Epoch 2, Batch: 340, Loss: 0.684550
Train - Epoch 2, Batch: 350, Loss: 0.684098
Train - Epoch 2, Batch: 360, Loss: 0.684381
Train - Epoch 2, Batch: 370, Loss: 0.683630
Train - Epoch 2, Batch: 380, Loss: 0.684586
Train - Epoch 2, Batch: 390, Loss: 0.683579
Train - Epoch 2, Batch: 400, Loss: 0.684828
Train - Epoch 2, Batch: 410, Loss: 0.684124
Train - Epoch 2, Batch: 420, Loss: 0.685637
Train - Epoch 2, Batch: 430, Loss: 0.685440
Train - Epoch 2, Batch: 440, Loss: 0.685278
Train - Epoch 2, Batch: 450, Loss: 0.684470
Train - Epoch 2, Batch: 460, Loss: 0.684419
Train - Epoch 2, Batch: 470, Loss: 0.684561
Train - Epoch 2, Batch: 480, Loss: 0.685364
Train - Epoch 2, Batch: 490, Loss: 0.684411
Train - Epoch 2, Batch: 500, Loss: 0.683803
Train - Epoch 2, Batch: 510, Loss: 0.684493
Train - Epoch 2, Batch: 520, Loss: 0.683598
Train - Epoch 2, Batch: 530, Loss: 0.685086
Train - Epoch 2, Batch: 540, Loss: 0.685622
Train - Epoch 2, Batch: 550, Loss: 0.683570/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 2, Batch: 560, Loss: 0.685034
Train - Epoch 2, Batch: 570, Loss: 0.684562
Train - Epoch 2, Batch: 580, Loss: 0.684535
Train - Epoch 2, Batch: 590, Loss: 0.685501
Train - Epoch 2, Batch: 600, Loss: 0.684711
Train - Epoch 2, Batch: 610, Loss: 0.685231
Train - Epoch 2, Batch: 620, Loss: 0.685776
Train - Epoch 2, Batch: 630, Loss: 0.683557
Train - Epoch 2, Batch: 640, Loss: 0.684552
Train - Epoch 3, Batch: 0, Loss: 0.685044
Train - Epoch 3, Batch: 10, Loss: 0.684780
Train - Epoch 3, Batch: 20, Loss: 0.683922
Train - Epoch 3, Batch: 30, Loss: 0.683732
Train - Epoch 3, Batch: 40, Loss: 0.684056
Train - Epoch 3, Batch: 50, Loss: 0.684074
Train - Epoch 3, Batch: 60, Loss: 0.685089
Train - Epoch 3, Batch: 70, Loss: 0.684002
Train - Epoch 3, Batch: 80, Loss: 0.684281
Train - Epoch 3, Batch: 90, Loss: 0.682905
Train - Epoch 3, Batch: 100, Loss: 0.683777
Train - Epoch 3, Batch: 110, Loss: 0.684662
Train - Epoch 3, Batch: 120, Loss: 0.686374
Train - Epoch 3, Batch: 130, Loss: 0.684442
Train - Epoch 3, Batch: 140, Loss: 0.684502
Train - Epoch 3, Batch: 150, Loss: 0.684569
Train - Epoch 3, Batch: 160, Loss: 0.684780
Train - Epoch 3, Batch: 170, Loss: 0.685093
Train - Epoch 3, Batch: 180, Loss: 0.684408
Train - Epoch 3, Batch: 190, Loss: 0.684867
Train - Epoch 3, Batch: 200, Loss: 0.684131
Train - Epoch 3, Batch: 210, Loss: 0.682662
Train - Epoch 3, Batch: 220, Loss: 0.683590
Train - Epoch 3, Batch: 230, Loss: 0.684694
Train - Epoch 3, Batch: 240, Loss: 0.683808
Train - Epoch 3, Batch: 250, Loss: 0.684092
Train - Epoch 3, Batch: 260, Loss: 0.684215
Train - Epoch 3, Batch: 270, Loss: 0.685009
Train - Epoch 3, Batch: 280, Loss: 0.684004
Train - Epoch 3, Batch: 290, Loss: 0.683452
Train - Epoch 3, Batch: 300, Loss: 0.684201
Train - Epoch 3, Batch: 310, Loss: 0.685116
Train - Epoch 3, Batch: 320, Loss: 0.683441
Train - Epoch 3, Batch: 330, Loss: 0.683454
Train - Epoch 3, Batch: 340, Loss: 0.683607
Train - Epoch 3, Batch: 350, Loss: 0.684124
Train - Epoch 3, Batch: 360, Loss: 0.684085
Train - Epoch 3, Batch: 370, Loss: 0.685452
Train - Epoch 3, Batch: 380, Loss: 0.684221
Train - Epoch 3, Batch: 390, Loss: 0.683744
Train - Epoch 3, Batch: 400, Loss: 0.683916
Train - Epoch 3, Batch: 410, Loss: 0.683070
Train - Epoch 3, Batch: 420, Loss: 0.684364
Train - Epoch 3, Batch: 430, Loss: 0.683295
Train - Epoch 3, Batch: 440, Loss: 0.683434
Train - Epoch 3, Batch: 450, Loss: 0.683407
Train - Epoch 3, Batch: 460, Loss: 0.683865
Train - Epoch 3, Batch: 470, Loss: 0.683725
Train - Epoch 3, Batch: 480, Loss: 0.683147
Train - Epoch 3, Batch: 490, Loss: 0.683406
Train - Epoch 3, Batch: 500, Loss: 0.683834
Train - Epoch 3, Batch: 510, Loss: 0.683585
Train - Epoch 3, Batch: 520, Loss: 0.683293
Train - Epoch 3, Batch: 530, Loss: 0.684802
Train - Epoch 3, Batch: 540, Loss: 0.684465
Train - Epoch 3, Batch: 550, Loss: 0.684123
Train - Epoch 3, Batch: 560, Loss: 0.683233
Train - Epoch 3, Batch: 570, Loss: 0.683307
Train - Epoch 3, Batch: 580, Loss: 0.683807
Train - Epoch 3, Batch: 590, Loss: 0.682979
Train - Epoch 3, Batch: 600, Loss: 0.684136
Train - Epoch 3, Batch: 610, Loss: 0.684605
Train - Epoch 3, Batch: 620, Loss: 0.683934
Train - Epoch 3, Batch: 630, Loss: 0.683641
Train - Epoch 3, Batch: 640, Loss: 0.684957
Test Avg. Loss: 0.000042, Accuracy: 0.554846
training_time:: 15.523679733276367
training time full:: 15.523746967315674
provenance prepare time:: 6.9141387939453125e-06
Test Avg. Loss: 0.000042, Accuracy: 0.554846
baseline::
python3 incremental_updates_base_line_lr_multi.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Benchmark_experiments/benchmark_exp.py:1298: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  curr_delta_id = torch.tensor(delta_id)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 100
max_epoch:: 4
batch_size:: 16384
Num of deletion:: 0, running time baseline::4.385046
Num of deletion:: 10, running time baseline::51.279229
Num of deletion:: 20, running time baseline::97.006768
Num of deletion:: 30, running time baseline::141.784617
Num of deletion:: 40, running time baseline::189.218894
Num of deletion:: 50, running time baseline::236.190166
Num of deletion:: 60, running time baseline::287.169957
Num of deletion:: 70, running time baseline::337.370136
Num of deletion:: 80, running time baseline::385.650214
Num of deletion:: 90, running time baseline::435.607897
training time is 478.88364696502686
overhead:: 0
overhead2:: 478.88148736953735
overhead3:: 8.613593339920044
time_baseline:: 478.88468647003174
curr_diff: 0 tensor(2.2593e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.2593e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554846
period:: 3
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance5_lr_multi.py 300 3 4 0.000009524 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 100
max_epoch:: 4
Num of deletion:: 0, running time provenance::4.027742
Num of deletion:: 10, running time provenance::37.381257
Num of deletion:: 20, running time provenance::68.687225
Num of deletion:: 30, running time provenance::98.752382
Num of deletion:: 40, running time provenance::131.863837
Num of deletion:: 50, running time provenance::165.614198
Num of deletion:: 60, running time provenance::194.685826
Num of deletion:: 70, running time provenance::225.639052
Num of deletion:: 80, running time provenance::255.761270
Num of deletion:: 90, running time provenance::285.499202
overhead:: 0
overhead2:: 0
overhead3:: 314.06060004234314
overhead4:: 0
overhead5:: 9.765996217727661
memory usage:: 5989236736
time_provenance:: 314.06407475471497
curr_diff: 0 tensor(1.8275e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.8275e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(2.1923e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1923e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554848
repetition 5
python3 benchmark_exp_lr.py 0.001 16384 4 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression higgs 5 0.005 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 0.729106
Train - Epoch 0, Batch: 10, Loss: 0.692546
Train - Epoch 0, Batch: 20, Loss: 0.692844
Train - Epoch 0, Batch: 30, Loss: 0.691067
Train - Epoch 0, Batch: 40, Loss: 0.690117
Train - Epoch 0, Batch: 50, Loss: 0.692191
Train - Epoch 0, Batch: 60, Loss: 0.691238
Train - Epoch 0, Batch: 70, Loss: 0.690591
Train - Epoch 0, Batch: 80, Loss: 0.691489
Train - Epoch 0, Batch: 90, Loss: 0.690358
Train - Epoch 0, Batch: 100, Loss: 0.691333
Train - Epoch 0, Batch: 110, Loss: 0.689787
Train - Epoch 0, Batch: 120, Loss: 0.689222
Train - Epoch 0, Batch: 130, Loss: 0.688381
Train - Epoch 0, Batch: 140, Loss: 0.689430
Train - Epoch 0, Batch: 150, Loss: 0.690133
Train - Epoch 0, Batch: 160, Loss: 0.689878
Train - Epoch 0, Batch: 170, Loss: 0.689846
Train - Epoch 0, Batch: 180, Loss: 0.690649
Train - Epoch 0, Batch: 190, Loss: 0.690071
Train - Epoch 0, Batch: 200, Loss: 0.688850
Train - Epoch 0, Batch: 210, Loss: 0.689369
Train - Epoch 0, Batch: 220, Loss: 0.687969
Train - Epoch 0, Batch: 230, Loss: 0.688116
Train - Epoch 0, Batch: 240, Loss: 0.687694
Train - Epoch 0, Batch: 250, Loss: 0.687743
Train - Epoch 0, Batch: 260, Loss: 0.686970
Train - Epoch 0, Batch: 270, Loss: 0.688892
Train - Epoch 0, Batch: 280, Loss: 0.687594
Train - Epoch 0, Batch: 290, Loss: 0.687647
Train - Epoch 0, Batch: 300, Loss: 0.689041
Train - Epoch 0, Batch: 310, Loss: 0.687776
Train - Epoch 0, Batch: 320, Loss: 0.689258
Train - Epoch 0, Batch: 330, Loss: 0.687536
Train - Epoch 0, Batch: 340, Loss: 0.688398
Train - Epoch 0, Batch: 350, Loss: 0.687220
Train - Epoch 0, Batch: 360, Loss: 0.687038
Train - Epoch 0, Batch: 370, Loss: 0.687449
Train - Epoch 0, Batch: 380, Loss: 0.688527
Train - Epoch 0, Batch: 390, Loss: 0.688251
Train - Epoch 0, Batch: 400, Loss: 0.687753
Train - Epoch 0, Batch: 410, Loss: 0.686703
Train - Epoch 0, Batch: 420, Loss: 0.687667
Train - Epoch 0, Batch: 430, Loss: 0.687127
Train - Epoch 0, Batch: 440, Loss: 0.688697
Train - Epoch 0, Batch: 450, Loss: 0.688487
Train - Epoch 0, Batch: 460, Loss: 0.688014
Train - Epoch 0, Batch: 470, Loss: 0.687916
Train - Epoch 0, Batch: 480, Loss: 0.686027
Train - Epoch 0, Batch: 490, Loss: 0.687498
Train - Epoch 0, Batch: 500, Loss: 0.686917
Train - Epoch 0, Batch: 510, Loss: 0.687379
Train - Epoch 0, Batch: 520, Loss: 0.686659
Train - Epoch 0, Batch: 530, Loss: 0.685920
Train - Epoch 0, Batch: 540, Loss: 0.686784
Train - Epoch 0, Batch: 550, Loss: 0.686140
Train - Epoch 0, Batch: 560, Loss: 0.686755
Train - Epoch 0, Batch: 570, Loss: 0.686949
Train - Epoch 0, Batch: 580, Loss: 0.687182
Train - Epoch 0, Batch: 590, Loss: 0.686306
Train - Epoch 0, Batch: 600, Loss: 0.687017
Train - Epoch 0, Batch: 610, Loss: 0.688051
Train - Epoch 0, Batch: 620, Loss: 0.687502
Train - Epoch 0, Batch: 630, Loss: 0.686774
Train - Epoch 0, Batch: 640, Loss: 0.686096
Train - Epoch 1, Batch: 0, Loss: 0.685565
Train - Epoch 1, Batch: 10, Loss: 0.686297
Train - Epoch 1, Batch: 20, Loss: 0.687384
Train - Epoch 1, Batch: 30, Loss: 0.686767
Train - Epoch 1, Batch: 40, Loss: 0.684964
Train - Epoch 1, Batch: 50, Loss: 0.686788
Train - Epoch 1, Batch: 60, Loss: 0.686865
Train - Epoch 1, Batch: 70, Loss: 0.686466
Train - Epoch 1, Batch: 80, Loss: 0.686375
Train - Epoch 1, Batch: 90, Loss: 0.686735
Train - Epoch 1, Batch: 100, Loss: 0.686219
Train - Epoch 1, Batch: 110, Loss: 0.686349
Train - Epoch 1, Batch: 120, Loss: 0.685637
Train - Epoch 1, Batch: 130, Loss: 0.685644
Train - Epoch 1, Batch: 140, Loss: 0.686552
Train - Epoch 1, Batch: 150, Loss: 0.685416
Train - Epoch 1, Batch: 160, Loss: 0.686119
Train - Epoch 1, Batch: 170, Loss: 0.685282
Train - Epoch 1, Batch: 180, Loss: 0.686331
Train - Epoch 1, Batch: 190, Loss: 0.686694
Train - Epoch 1, Batch: 200, Loss: 0.686606
Train - Epoch 1, Batch: 210, Loss: 0.686864
Train - Epoch 1, Batch: 220, Loss: 0.685304
Train - Epoch 1, Batch: 230, Loss: 0.686795
Train - Epoch 1, Batch: 240, Loss: 0.685871
Train - Epoch 1, Batch: 250, Loss: 0.686510
Train - Epoch 1, Batch: 260, Loss: 0.686500
Train - Epoch 1, Batch: 270, Loss: 0.686317
Train - Epoch 1, Batch: 280, Loss: 0.686968
Train - Epoch 1, Batch: 290, Loss: 0.685429
Train - Epoch 1, Batch: 300, Loss: 0.686145
Train - Epoch 1, Batch: 310, Loss: 0.687530
Train - Epoch 1, Batch: 320, Loss: 0.687033
Train - Epoch 1, Batch: 330, Loss: 0.686355
Train - Epoch 1, Batch: 340, Loss: 0.686192
Train - Epoch 1, Batch: 350, Loss: 0.685457
Train - Epoch 1, Batch: 360, Loss: 0.686389
Train - Epoch 1, Batch: 370, Loss: 0.685480
Train - Epoch 1, Batch: 380, Loss: 0.686867
Train - Epoch 1, Batch: 390, Loss: 0.687220
Train - Epoch 1, Batch: 400, Loss: 0.684924
Train - Epoch 1, Batch: 410, Loss: 0.685765
Train - Epoch 1, Batch: 420, Loss: 0.685748
Train - Epoch 1, Batch: 430, Loss: 0.685675
Train - Epoch 1, Batch: 440, Loss: 0.687412
Train - Epoch 1, Batch: 450, Loss: 0.685923
Train - Epoch 1, Batch: 460, Loss: 0.685685
Train - Epoch 1, Batch: 470, Loss: 0.686527
Train - Epoch 1, Batch: 480, Loss: 0.686826
Train - Epoch 1, Batch: 490, Loss: 0.687262
Train - Epoch 1, Batch: 500, Loss: 0.685548
Train - Epoch 1, Batch: 510, Loss: 0.685310
Train - Epoch 1, Batch: 520, Loss: 0.685400
Train - Epoch 1, Batch: 530, Loss: 0.684052
Train - Epoch 1, Batch: 540, Loss: 0.684830
Train - Epoch 1, Batch: 550, Loss: 0.686587
Train - Epoch 1, Batch: 560, Loss: 0.685047
Train - Epoch 1, Batch: 570, Loss: 0.685003
Train - Epoch 1, Batch: 580, Loss: 0.685421
Train - Epoch 1, Batch: 590, Loss: 0.684731
Train - Epoch 1, Batch: 600, Loss: 0.685349
Train - Epoch 1, Batch: 610, Loss: 0.685507
Train - Epoch 1, Batch: 620, Loss: 0.685604
Train - Epoch 1, Batch: 630, Loss: 0.684851
Train - Epoch 1, Batch: 640, Loss: 0.685306
Train - Epoch 2, Batch: 0, Loss: 0.684820
Train - Epoch 2, Batch: 10, Loss: 0.685320
Train - Epoch 2, Batch: 20, Loss: 0.685907
Train - Epoch 2, Batch: 30, Loss: 0.685398
Train - Epoch 2, Batch: 40, Loss: 0.685612
Train - Epoch 2, Batch: 50, Loss: 0.684919
Train - Epoch 2, Batch: 60, Loss: 0.684993
Train - Epoch 2, Batch: 70, Loss: 0.685374
Train - Epoch 2, Batch: 80, Loss: 0.685112
Train - Epoch 2, Batch: 90, Loss: 0.684424
Train - Epoch 2, Batch: 100, Loss: 0.684549
Train - Epoch 2, Batch: 110, Loss: 0.686757
Train - Epoch 2, Batch: 120, Loss: 0.685254
Train - Epoch 2, Batch: 130, Loss: 0.683914
Train - Epoch 2, Batch: 140, Loss: 0.684342
Train - Epoch 2, Batch: 150, Loss: 0.685975
Train - Epoch 2, Batch: 160, Loss: 0.684784
Train - Epoch 2, Batch: 170, Loss: 0.684403
Train - Epoch 2, Batch: 180, Loss: 0.685147
Train - Epoch 2, Batch: 190, Loss: 0.685295
Train - Epoch 2, Batch: 200, Loss: 0.685322
Train - Epoch 2, Batch: 210, Loss: 0.686240
Train - Epoch 2, Batch: 220, Loss: 0.684172
Train - Epoch 2, Batch: 230, Loss: 0.685311
Train - Epoch 2, Batch: 240, Loss: 0.685133
Train - Epoch 2, Batch: 250, Loss: 0.683692
Train - Epoch 2, Batch: 260, Loss: 0.685410
Train - Epoch 2, Batch: 270, Loss: 0.685592
Train - Epoch 2, Batch: 280, Loss: 0.683210
Train - Epoch 2, Batch: 290, Loss: 0.684615
Train - Epoch 2, Batch: 300, Loss: 0.684090
Train - Epoch 2, Batch: 310, Loss: 0.685416
Train - Epoch 2, Batch: 320, Loss: 0.685486
Train - Epoch 2, Batch: 330, Loss: 0.685505
Train - Epoch 2, Batch: 340, Loss: 0.684830
Train - Epoch 2, Batch: 350, Loss: 0.683890
Train - Epoch 2, Batch: 360, Loss: 0.683436
Train - Epoch 2, Batch: 370, Loss: 0.684534
Train - Epoch 2, Batch: 380, Loss: 0.685154
Train - Epoch 2, Batch: 390, Loss: 0.684290
Train - Epoch 2, Batch: 400, Loss: 0.685148
Train - Epoch 2, Batch: 410, Loss: 0.684657
Train - Epoch 2, Batch: 420, Loss: 0.684338
Train - Epoch 2, Batch: 430, Loss: 0.684615
Train - Epoch 2, Batch: 440, Loss: 0.685559
Train - Epoch 2, Batch: 450, Loss: 0.685536
Train - Epoch 2, Batch: 460, Loss: 0.685021
Train - Epoch 2, Batch: 470, Loss: 0.683680
Train - Epoch 2, Batch: 480, Loss: 0.685362
Train - Epoch 2, Batch: 490, Loss: 0.684928
Train - Epoch 2, Batch: 500, Loss: 0.684477
Train - Epoch 2, Batch: 510, Loss: 0.684288
Train - Epoch 2, Batch: 520, Loss: 0.684684
Train - Epoch 2, Batch: 530, Loss: 0.684624
Train - Epoch 2, Batch: 540, Loss: 0.685001
Train - Epoch 2, Batch: 550, Loss: 0.684705/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 2, Batch: 560, Loss: 0.684868
Train - Epoch 2, Batch: 570, Loss: 0.683825
Train - Epoch 2, Batch: 580, Loss: 0.683607
Train - Epoch 2, Batch: 590, Loss: 0.683687
Train - Epoch 2, Batch: 600, Loss: 0.685734
Train - Epoch 2, Batch: 610, Loss: 0.684675
Train - Epoch 2, Batch: 620, Loss: 0.684253
Train - Epoch 2, Batch: 630, Loss: 0.683427
Train - Epoch 2, Batch: 640, Loss: 0.684208
Train - Epoch 3, Batch: 0, Loss: 0.684413
Train - Epoch 3, Batch: 10, Loss: 0.684435
Train - Epoch 3, Batch: 20, Loss: 0.683482
Train - Epoch 3, Batch: 30, Loss: 0.684996
Train - Epoch 3, Batch: 40, Loss: 0.684058
Train - Epoch 3, Batch: 50, Loss: 0.685567
Train - Epoch 3, Batch: 60, Loss: 0.685939
Train - Epoch 3, Batch: 70, Loss: 0.684751
Train - Epoch 3, Batch: 80, Loss: 0.684256
Train - Epoch 3, Batch: 90, Loss: 0.683561
Train - Epoch 3, Batch: 100, Loss: 0.684414
Train - Epoch 3, Batch: 110, Loss: 0.684549
Train - Epoch 3, Batch: 120, Loss: 0.682960
Train - Epoch 3, Batch: 130, Loss: 0.684177
Train - Epoch 3, Batch: 140, Loss: 0.684045
Train - Epoch 3, Batch: 150, Loss: 0.684289
Train - Epoch 3, Batch: 160, Loss: 0.684116
Train - Epoch 3, Batch: 170, Loss: 0.683894
Train - Epoch 3, Batch: 180, Loss: 0.683851
Train - Epoch 3, Batch: 190, Loss: 0.685193
Train - Epoch 3, Batch: 200, Loss: 0.684427
Train - Epoch 3, Batch: 210, Loss: 0.684129
Train - Epoch 3, Batch: 220, Loss: 0.683289
Train - Epoch 3, Batch: 230, Loss: 0.684232
Train - Epoch 3, Batch: 240, Loss: 0.684068
Train - Epoch 3, Batch: 250, Loss: 0.684022
Train - Epoch 3, Batch: 260, Loss: 0.684277
Train - Epoch 3, Batch: 270, Loss: 0.684486
Train - Epoch 3, Batch: 280, Loss: 0.685219
Train - Epoch 3, Batch: 290, Loss: 0.684894
Train - Epoch 3, Batch: 300, Loss: 0.684546
Train - Epoch 3, Batch: 310, Loss: 0.684941
Train - Epoch 3, Batch: 320, Loss: 0.684154
Train - Epoch 3, Batch: 330, Loss: 0.684400
Train - Epoch 3, Batch: 340, Loss: 0.683988
Train - Epoch 3, Batch: 350, Loss: 0.682663
Train - Epoch 3, Batch: 360, Loss: 0.683935
Train - Epoch 3, Batch: 370, Loss: 0.684077
Train - Epoch 3, Batch: 380, Loss: 0.683879
Train - Epoch 3, Batch: 390, Loss: 0.684390
Train - Epoch 3, Batch: 400, Loss: 0.682787
Train - Epoch 3, Batch: 410, Loss: 0.682921
Train - Epoch 3, Batch: 420, Loss: 0.682167
Train - Epoch 3, Batch: 430, Loss: 0.683959
Train - Epoch 3, Batch: 440, Loss: 0.683768
Train - Epoch 3, Batch: 450, Loss: 0.683260
Train - Epoch 3, Batch: 460, Loss: 0.685071
Train - Epoch 3, Batch: 470, Loss: 0.683890
Train - Epoch 3, Batch: 480, Loss: 0.683148
Train - Epoch 3, Batch: 490, Loss: 0.683883
Train - Epoch 3, Batch: 500, Loss: 0.683759
Train - Epoch 3, Batch: 510, Loss: 0.684462
Train - Epoch 3, Batch: 520, Loss: 0.684832
Train - Epoch 3, Batch: 530, Loss: 0.683967
Train - Epoch 3, Batch: 540, Loss: 0.683451
Train - Epoch 3, Batch: 550, Loss: 0.683955
Train - Epoch 3, Batch: 560, Loss: 0.682819
Train - Epoch 3, Batch: 570, Loss: 0.684063
Train - Epoch 3, Batch: 580, Loss: 0.684239
Train - Epoch 3, Batch: 590, Loss: 0.683183
Train - Epoch 3, Batch: 600, Loss: 0.683768
Train - Epoch 3, Batch: 610, Loss: 0.684428
Train - Epoch 3, Batch: 620, Loss: 0.684102
Train - Epoch 3, Batch: 630, Loss: 0.683450
Train - Epoch 3, Batch: 640, Loss: 0.683580
Test Avg. Loss: 0.000042, Accuracy: 0.553252
training_time:: 15.99617314338684
training time full:: 15.99623966217041
provenance prepare time:: 6.9141387939453125e-06
Test Avg. Loss: 0.000042, Accuracy: 0.553252
baseline::
python3 incremental_updates_base_line_lr_multi.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Benchmark_experiments/benchmark_exp.py:1298: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  curr_delta_id = torch.tensor(delta_id)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 100
max_epoch:: 4
batch_size:: 16384
Num of deletion:: 0, running time baseline::4.864364
Num of deletion:: 10, running time baseline::53.068407
Num of deletion:: 20, running time baseline::101.805003
Num of deletion:: 30, running time baseline::148.045855
Num of deletion:: 40, running time baseline::196.896712
Num of deletion:: 50, running time baseline::245.312901
Num of deletion:: 60, running time baseline::295.828426
Num of deletion:: 70, running time baseline::344.613548
Num of deletion:: 80, running time baseline::393.159543
Num of deletion:: 90, running time baseline::443.467315
training time is 485.8962712287903
overhead:: 0
overhead2:: 485.8941090106964
overhead3:: 8.734938621520996
time_baseline:: 485.8972613811493
curr_diff: 0 tensor(2.6451e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.6451e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553248
period:: 3
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance5_lr_multi.py 300 3 5 0.000009524 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 100
max_epoch:: 4
Num of deletion:: 0, running time provenance::3.097651
Num of deletion:: 10, running time provenance::33.899828
Num of deletion:: 20, running time provenance::66.445385
Num of deletion:: 30, running time provenance::97.238157
Num of deletion:: 40, running time provenance::129.960280
Num of deletion:: 50, running time provenance::160.116833
Num of deletion:: 60, running time provenance::193.837348
Num of deletion:: 70, running time provenance::226.115321
Num of deletion:: 80, running time provenance::261.500882
Num of deletion:: 90, running time provenance::296.524423
overhead:: 0
overhead2:: 0
overhead3:: 326.1104474067688
overhead4:: 0
overhead5:: 9.95967149734497
memory usage:: 5989773312
time_provenance:: 326.1138391494751
curr_diff: 0 tensor(2.2068e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.2068e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(2.5586e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.5586e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.553258
repetition 6
python3 benchmark_exp_lr.py 0.001 16384 4 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression higgs 6 0.005 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 0.718631
Train - Epoch 0, Batch: 10, Loss: 0.694115
Train - Epoch 0, Batch: 20, Loss: 0.691721
Train - Epoch 0, Batch: 30, Loss: 0.692130
Train - Epoch 0, Batch: 40, Loss: 0.691742
Train - Epoch 0, Batch: 50, Loss: 0.691957
Train - Epoch 0, Batch: 60, Loss: 0.691287
Train - Epoch 0, Batch: 70, Loss: 0.690646
Train - Epoch 0, Batch: 80, Loss: 0.690412
Train - Epoch 0, Batch: 90, Loss: 0.689561
Train - Epoch 0, Batch: 100, Loss: 0.689564
Train - Epoch 0, Batch: 110, Loss: 0.688431
Train - Epoch 0, Batch: 120, Loss: 0.690137
Train - Epoch 0, Batch: 130, Loss: 0.689999
Train - Epoch 0, Batch: 140, Loss: 0.688239
Train - Epoch 0, Batch: 150, Loss: 0.689029
Train - Epoch 0, Batch: 160, Loss: 0.688372
Train - Epoch 0, Batch: 170, Loss: 0.688439
Train - Epoch 0, Batch: 180, Loss: 0.689295
Train - Epoch 0, Batch: 190, Loss: 0.687726
Train - Epoch 0, Batch: 200, Loss: 0.688605
Train - Epoch 0, Batch: 210, Loss: 0.688907
Train - Epoch 0, Batch: 220, Loss: 0.688398
Train - Epoch 0, Batch: 230, Loss: 0.688128
Train - Epoch 0, Batch: 240, Loss: 0.689107
Train - Epoch 0, Batch: 250, Loss: 0.688037
Train - Epoch 0, Batch: 260, Loss: 0.686866
Train - Epoch 0, Batch: 270, Loss: 0.688612
Train - Epoch 0, Batch: 280, Loss: 0.687441
Train - Epoch 0, Batch: 290, Loss: 0.687799
Train - Epoch 0, Batch: 300, Loss: 0.687707
Train - Epoch 0, Batch: 310, Loss: 0.687082
Train - Epoch 0, Batch: 320, Loss: 0.687572
Train - Epoch 0, Batch: 330, Loss: 0.686776
Train - Epoch 0, Batch: 340, Loss: 0.688580
Train - Epoch 0, Batch: 350, Loss: 0.687018
Train - Epoch 0, Batch: 360, Loss: 0.686319
Train - Epoch 0, Batch: 370, Loss: 0.687679
Train - Epoch 0, Batch: 380, Loss: 0.687421
Train - Epoch 0, Batch: 390, Loss: 0.687160
Train - Epoch 0, Batch: 400, Loss: 0.686715
Train - Epoch 0, Batch: 410, Loss: 0.687476
Train - Epoch 0, Batch: 420, Loss: 0.686257
Train - Epoch 0, Batch: 430, Loss: 0.686495
Train - Epoch 0, Batch: 440, Loss: 0.688655
Train - Epoch 0, Batch: 450, Loss: 0.687151
Train - Epoch 0, Batch: 460, Loss: 0.686899
Train - Epoch 0, Batch: 470, Loss: 0.686703
Train - Epoch 0, Batch: 480, Loss: 0.686757
Train - Epoch 0, Batch: 490, Loss: 0.686481
Train - Epoch 0, Batch: 500, Loss: 0.686856
Train - Epoch 0, Batch: 510, Loss: 0.686028
Train - Epoch 0, Batch: 520, Loss: 0.687321
Train - Epoch 0, Batch: 530, Loss: 0.686412
Train - Epoch 0, Batch: 540, Loss: 0.685661
Train - Epoch 0, Batch: 550, Loss: 0.687022
Train - Epoch 0, Batch: 560, Loss: 0.687326
Train - Epoch 0, Batch: 570, Loss: 0.686147
Train - Epoch 0, Batch: 580, Loss: 0.687197
Train - Epoch 0, Batch: 590, Loss: 0.685519
Train - Epoch 0, Batch: 600, Loss: 0.686335
Train - Epoch 0, Batch: 610, Loss: 0.686671
Train - Epoch 0, Batch: 620, Loss: 0.685965
Train - Epoch 0, Batch: 630, Loss: 0.686439
Train - Epoch 0, Batch: 640, Loss: 0.686054
Train - Epoch 1, Batch: 0, Loss: 0.686142
Train - Epoch 1, Batch: 10, Loss: 0.686222
Train - Epoch 1, Batch: 20, Loss: 0.685162
Train - Epoch 1, Batch: 30, Loss: 0.686524
Train - Epoch 1, Batch: 40, Loss: 0.685844
Train - Epoch 1, Batch: 50, Loss: 0.686984
Train - Epoch 1, Batch: 60, Loss: 0.685283
Train - Epoch 1, Batch: 70, Loss: 0.685562
Train - Epoch 1, Batch: 80, Loss: 0.686732
Train - Epoch 1, Batch: 90, Loss: 0.685845
Train - Epoch 1, Batch: 100, Loss: 0.684999
Train - Epoch 1, Batch: 110, Loss: 0.685033
Train - Epoch 1, Batch: 120, Loss: 0.684616
Train - Epoch 1, Batch: 130, Loss: 0.685658
Train - Epoch 1, Batch: 140, Loss: 0.685796
Train - Epoch 1, Batch: 150, Loss: 0.685308
Train - Epoch 1, Batch: 160, Loss: 0.685585
Train - Epoch 1, Batch: 170, Loss: 0.686133
Train - Epoch 1, Batch: 180, Loss: 0.687054
Train - Epoch 1, Batch: 190, Loss: 0.685462
Train - Epoch 1, Batch: 200, Loss: 0.686401
Train - Epoch 1, Batch: 210, Loss: 0.685554
Train - Epoch 1, Batch: 220, Loss: 0.685670
Train - Epoch 1, Batch: 230, Loss: 0.686500
Train - Epoch 1, Batch: 240, Loss: 0.685462
Train - Epoch 1, Batch: 250, Loss: 0.684306
Train - Epoch 1, Batch: 260, Loss: 0.685540
Train - Epoch 1, Batch: 270, Loss: 0.685273
Train - Epoch 1, Batch: 280, Loss: 0.684893
Train - Epoch 1, Batch: 290, Loss: 0.685577
Train - Epoch 1, Batch: 300, Loss: 0.685645
Train - Epoch 1, Batch: 310, Loss: 0.685036
Train - Epoch 1, Batch: 320, Loss: 0.684869
Train - Epoch 1, Batch: 330, Loss: 0.684934
Train - Epoch 1, Batch: 340, Loss: 0.684056
Train - Epoch 1, Batch: 350, Loss: 0.685600
Train - Epoch 1, Batch: 360, Loss: 0.684540
Train - Epoch 1, Batch: 370, Loss: 0.684877
Train - Epoch 1, Batch: 380, Loss: 0.685898
Train - Epoch 1, Batch: 390, Loss: 0.685464
Train - Epoch 1, Batch: 400, Loss: 0.685627
Train - Epoch 1, Batch: 410, Loss: 0.684062
Train - Epoch 1, Batch: 420, Loss: 0.685171
Train - Epoch 1, Batch: 430, Loss: 0.684800
Train - Epoch 1, Batch: 440, Loss: 0.685489
Train - Epoch 1, Batch: 450, Loss: 0.686141
Train - Epoch 1, Batch: 460, Loss: 0.684715
Train - Epoch 1, Batch: 470, Loss: 0.685752
Train - Epoch 1, Batch: 480, Loss: 0.686288
Train - Epoch 1, Batch: 490, Loss: 0.686009
Train - Epoch 1, Batch: 500, Loss: 0.686399
Train - Epoch 1, Batch: 510, Loss: 0.684537
Train - Epoch 1, Batch: 520, Loss: 0.685276
Train - Epoch 1, Batch: 530, Loss: 0.684873
Train - Epoch 1, Batch: 540, Loss: 0.685124
Train - Epoch 1, Batch: 550, Loss: 0.685006
Train - Epoch 1, Batch: 560, Loss: 0.685588
Train - Epoch 1, Batch: 570, Loss: 0.685045
Train - Epoch 1, Batch: 580, Loss: 0.685681
Train - Epoch 1, Batch: 590, Loss: 0.685402
Train - Epoch 1, Batch: 600, Loss: 0.685134
Train - Epoch 1, Batch: 610, Loss: 0.686086
Train - Epoch 1, Batch: 620, Loss: 0.684895
Train - Epoch 1, Batch: 630, Loss: 0.684611
Train - Epoch 1, Batch: 640, Loss: 0.685566
Train - Epoch 2, Batch: 0, Loss: 0.685574
Train - Epoch 2, Batch: 10, Loss: 0.685166
Train - Epoch 2, Batch: 20, Loss: 0.685513
Train - Epoch 2, Batch: 30, Loss: 0.684809
Train - Epoch 2, Batch: 40, Loss: 0.684790
Train - Epoch 2, Batch: 50, Loss: 0.686117
Train - Epoch 2, Batch: 60, Loss: 0.685873
Train - Epoch 2, Batch: 70, Loss: 0.685421
Train - Epoch 2, Batch: 80, Loss: 0.686066
Train - Epoch 2, Batch: 90, Loss: 0.684001
Train - Epoch 2, Batch: 100, Loss: 0.685263
Train - Epoch 2, Batch: 110, Loss: 0.685484
Train - Epoch 2, Batch: 120, Loss: 0.684428
Train - Epoch 2, Batch: 130, Loss: 0.684904
Train - Epoch 2, Batch: 140, Loss: 0.683746
Train - Epoch 2, Batch: 150, Loss: 0.685343
Train - Epoch 2, Batch: 160, Loss: 0.686308
Train - Epoch 2, Batch: 170, Loss: 0.684704
Train - Epoch 2, Batch: 180, Loss: 0.683962
Train - Epoch 2, Batch: 190, Loss: 0.685844
Train - Epoch 2, Batch: 200, Loss: 0.685808
Train - Epoch 2, Batch: 210, Loss: 0.684771
Train - Epoch 2, Batch: 220, Loss: 0.684107
Train - Epoch 2, Batch: 230, Loss: 0.683993
Train - Epoch 2, Batch: 240, Loss: 0.684588
Train - Epoch 2, Batch: 250, Loss: 0.685293
Train - Epoch 2, Batch: 260, Loss: 0.684764
Train - Epoch 2, Batch: 270, Loss: 0.684876
Train - Epoch 2, Batch: 280, Loss: 0.686002
Train - Epoch 2, Batch: 290, Loss: 0.684263
Train - Epoch 2, Batch: 300, Loss: 0.684612
Train - Epoch 2, Batch: 310, Loss: 0.684775
Train - Epoch 2, Batch: 320, Loss: 0.685147
Train - Epoch 2, Batch: 330, Loss: 0.684447
Train - Epoch 2, Batch: 340, Loss: 0.684406
Train - Epoch 2, Batch: 350, Loss: 0.684509
Train - Epoch 2, Batch: 360, Loss: 0.685289
Train - Epoch 2, Batch: 370, Loss: 0.684156
Train - Epoch 2, Batch: 380, Loss: 0.683968
Train - Epoch 2, Batch: 390, Loss: 0.684030
Train - Epoch 2, Batch: 400, Loss: 0.684430
Train - Epoch 2, Batch: 410, Loss: 0.685635
Train - Epoch 2, Batch: 420, Loss: 0.684587
Train - Epoch 2, Batch: 430, Loss: 0.683977
Train - Epoch 2, Batch: 440, Loss: 0.684871
Train - Epoch 2, Batch: 450, Loss: 0.685606
Train - Epoch 2, Batch: 460, Loss: 0.684691
Train - Epoch 2, Batch: 470, Loss: 0.684467
Train - Epoch 2, Batch: 480, Loss: 0.685579
Train - Epoch 2, Batch: 490, Loss: 0.684744
Train - Epoch 2, Batch: 500, Loss: 0.684618
Train - Epoch 2, Batch: 510, Loss: 0.683773
Train - Epoch 2, Batch: 520, Loss: 0.684151
Train - Epoch 2, Batch: 530, Loss: 0.685531
Train - Epoch 2, Batch: 540, Loss: 0.684515
Train - Epoch 2, Batch: 550, Loss: 0.683684/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 2, Batch: 560, Loss: 0.683766
Train - Epoch 2, Batch: 570, Loss: 0.683991
Train - Epoch 2, Batch: 580, Loss: 0.684367
Train - Epoch 2, Batch: 590, Loss: 0.683652
Train - Epoch 2, Batch: 600, Loss: 0.684338
Train - Epoch 2, Batch: 610, Loss: 0.683913
Train - Epoch 2, Batch: 620, Loss: 0.684025
Train - Epoch 2, Batch: 630, Loss: 0.684018
Train - Epoch 2, Batch: 640, Loss: 0.684668
Train - Epoch 3, Batch: 0, Loss: 0.683174
Train - Epoch 3, Batch: 10, Loss: 0.684543
Train - Epoch 3, Batch: 20, Loss: 0.684490
Train - Epoch 3, Batch: 30, Loss: 0.684814
Train - Epoch 3, Batch: 40, Loss: 0.684108
Train - Epoch 3, Batch: 50, Loss: 0.683048
Train - Epoch 3, Batch: 60, Loss: 0.685530
Train - Epoch 3, Batch: 70, Loss: 0.685482
Train - Epoch 3, Batch: 80, Loss: 0.684000
Train - Epoch 3, Batch: 90, Loss: 0.685276
Train - Epoch 3, Batch: 100, Loss: 0.684112
Train - Epoch 3, Batch: 110, Loss: 0.683922
Train - Epoch 3, Batch: 120, Loss: 0.684144
Train - Epoch 3, Batch: 130, Loss: 0.683969
Train - Epoch 3, Batch: 140, Loss: 0.684939
Train - Epoch 3, Batch: 150, Loss: 0.683238
Train - Epoch 3, Batch: 160, Loss: 0.684095
Train - Epoch 3, Batch: 170, Loss: 0.684756
Train - Epoch 3, Batch: 180, Loss: 0.684266
Train - Epoch 3, Batch: 190, Loss: 0.685129
Train - Epoch 3, Batch: 200, Loss: 0.685847
Train - Epoch 3, Batch: 210, Loss: 0.684301
Train - Epoch 3, Batch: 220, Loss: 0.683711
Train - Epoch 3, Batch: 230, Loss: 0.682777
Train - Epoch 3, Batch: 240, Loss: 0.683395
Train - Epoch 3, Batch: 250, Loss: 0.683362
Train - Epoch 3, Batch: 260, Loss: 0.683556
Train - Epoch 3, Batch: 270, Loss: 0.682733
Train - Epoch 3, Batch: 280, Loss: 0.683900
Train - Epoch 3, Batch: 290, Loss: 0.683429
Train - Epoch 3, Batch: 300, Loss: 0.684298
Train - Epoch 3, Batch: 310, Loss: 0.684800
Train - Epoch 3, Batch: 320, Loss: 0.684614
Train - Epoch 3, Batch: 330, Loss: 0.684520
Train - Epoch 3, Batch: 340, Loss: 0.682825
Train - Epoch 3, Batch: 350, Loss: 0.683706
Train - Epoch 3, Batch: 360, Loss: 0.684046
Train - Epoch 3, Batch: 370, Loss: 0.684850
Train - Epoch 3, Batch: 380, Loss: 0.685042
Train - Epoch 3, Batch: 390, Loss: 0.682671
Train - Epoch 3, Batch: 400, Loss: 0.682721
Train - Epoch 3, Batch: 410, Loss: 0.684704
Train - Epoch 3, Batch: 420, Loss: 0.683149
Train - Epoch 3, Batch: 430, Loss: 0.684626
Train - Epoch 3, Batch: 440, Loss: 0.684010
Train - Epoch 3, Batch: 450, Loss: 0.683425
Train - Epoch 3, Batch: 460, Loss: 0.682724
Train - Epoch 3, Batch: 470, Loss: 0.683678
Train - Epoch 3, Batch: 480, Loss: 0.683705
Train - Epoch 3, Batch: 490, Loss: 0.684096
Train - Epoch 3, Batch: 500, Loss: 0.683580
Train - Epoch 3, Batch: 510, Loss: 0.684799
Train - Epoch 3, Batch: 520, Loss: 0.684076
Train - Epoch 3, Batch: 530, Loss: 0.684082
Train - Epoch 3, Batch: 540, Loss: 0.684252
Train - Epoch 3, Batch: 550, Loss: 0.683766
Train - Epoch 3, Batch: 560, Loss: 0.683763
Train - Epoch 3, Batch: 570, Loss: 0.685550
Train - Epoch 3, Batch: 580, Loss: 0.684606
Train - Epoch 3, Batch: 590, Loss: 0.685186
Train - Epoch 3, Batch: 600, Loss: 0.682410
Train - Epoch 3, Batch: 610, Loss: 0.684369
Train - Epoch 3, Batch: 620, Loss: 0.683426
Train - Epoch 3, Batch: 630, Loss: 0.683376
Train - Epoch 3, Batch: 640, Loss: 0.683916
Test Avg. Loss: 0.000042, Accuracy: 0.554158
training_time:: 15.616236686706543
training time full:: 15.616300821304321
provenance prepare time:: 7.152557373046875e-06
Test Avg. Loss: 0.000042, Accuracy: 0.554158
baseline::
python3 incremental_updates_base_line_lr_multi.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Benchmark_experiments/benchmark_exp.py:1298: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  curr_delta_id = torch.tensor(delta_id)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 100
max_epoch:: 4
batch_size:: 16384
Num of deletion:: 0, running time baseline::5.029333
Num of deletion:: 10, running time baseline::54.882599
Num of deletion:: 20, running time baseline::102.406189
Num of deletion:: 30, running time baseline::150.178113
Num of deletion:: 40, running time baseline::199.439325
Num of deletion:: 50, running time baseline::248.599635
Num of deletion:: 60, running time baseline::297.657210
Num of deletion:: 70, running time baseline::346.582205
Num of deletion:: 80, running time baseline::395.146147
Num of deletion:: 90, running time baseline::444.562370
training time is 487.9319112300873
overhead:: 0
overhead2:: 487.92970967292786
overhead3:: 8.759794235229492
time_baseline:: 487.93285179138184
curr_diff: 0 tensor(1.9764e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.9764e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554158
period:: 3
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance5_lr_multi.py 300 3 6 0.000009524 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 100
max_epoch:: 4
Num of deletion:: 0, running time provenance::3.351493
Num of deletion:: 10, running time provenance::35.408591
Num of deletion:: 20, running time provenance::64.620277
Num of deletion:: 30, running time provenance::98.254289
Num of deletion:: 40, running time provenance::128.921471
Num of deletion:: 50, running time provenance::159.725928
Num of deletion:: 60, running time provenance::190.119654
Num of deletion:: 70, running time provenance::221.700342
Num of deletion:: 80, running time provenance::253.888456
Num of deletion:: 90, running time provenance::285.682720
overhead:: 0
overhead2:: 0
overhead3:: 316.22388315200806
overhead4:: 0
overhead5:: 9.975773334503174
memory usage:: 5987942400
time_provenance:: 316.22733449935913
curr_diff: 0 tensor(1.4015e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4015e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(1.9563e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.9563e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554160
repetition 7
python3 benchmark_exp_lr.py 0.001 16384 4 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression higgs 7 0.005 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 0.714717
Train - Epoch 0, Batch: 10, Loss: 0.694693
Train - Epoch 0, Batch: 20, Loss: 0.693395
Train - Epoch 0, Batch: 30, Loss: 0.693539
Train - Epoch 0, Batch: 40, Loss: 0.692078
Train - Epoch 0, Batch: 50, Loss: 0.691522
Train - Epoch 0, Batch: 60, Loss: 0.692243
Train - Epoch 0, Batch: 70, Loss: 0.690597
Train - Epoch 0, Batch: 80, Loss: 0.691638
Train - Epoch 0, Batch: 90, Loss: 0.691177
Train - Epoch 0, Batch: 100, Loss: 0.690708
Train - Epoch 0, Batch: 110, Loss: 0.690706
Train - Epoch 0, Batch: 120, Loss: 0.691218
Train - Epoch 0, Batch: 130, Loss: 0.690366
Train - Epoch 0, Batch: 140, Loss: 0.689923
Train - Epoch 0, Batch: 150, Loss: 0.689810
Train - Epoch 0, Batch: 160, Loss: 0.689144
Train - Epoch 0, Batch: 170, Loss: 0.690253
Train - Epoch 0, Batch: 180, Loss: 0.690736
Train - Epoch 0, Batch: 190, Loss: 0.689080
Train - Epoch 0, Batch: 200, Loss: 0.689515
Train - Epoch 0, Batch: 210, Loss: 0.689594
Train - Epoch 0, Batch: 220, Loss: 0.689617
Train - Epoch 0, Batch: 230, Loss: 0.688671
Train - Epoch 0, Batch: 240, Loss: 0.688164
Train - Epoch 0, Batch: 250, Loss: 0.688305
Train - Epoch 0, Batch: 260, Loss: 0.688851
Train - Epoch 0, Batch: 270, Loss: 0.689284
Train - Epoch 0, Batch: 280, Loss: 0.688708
Train - Epoch 0, Batch: 290, Loss: 0.688082
Train - Epoch 0, Batch: 300, Loss: 0.688459
Train - Epoch 0, Batch: 310, Loss: 0.688174
Train - Epoch 0, Batch: 320, Loss: 0.688880
Train - Epoch 0, Batch: 330, Loss: 0.687487
Train - Epoch 0, Batch: 340, Loss: 0.689330
Train - Epoch 0, Batch: 350, Loss: 0.688656
Train - Epoch 0, Batch: 360, Loss: 0.687656
Train - Epoch 0, Batch: 370, Loss: 0.688179
Train - Epoch 0, Batch: 380, Loss: 0.687947
Train - Epoch 0, Batch: 390, Loss: 0.688425
Train - Epoch 0, Batch: 400, Loss: 0.688103
Train - Epoch 0, Batch: 410, Loss: 0.688769
Train - Epoch 0, Batch: 420, Loss: 0.688012
Train - Epoch 0, Batch: 430, Loss: 0.687885
Train - Epoch 0, Batch: 440, Loss: 0.686971
Train - Epoch 0, Batch: 450, Loss: 0.687839
Train - Epoch 0, Batch: 460, Loss: 0.686737
Train - Epoch 0, Batch: 470, Loss: 0.687026
Train - Epoch 0, Batch: 480, Loss: 0.688032
Train - Epoch 0, Batch: 490, Loss: 0.687706
Train - Epoch 0, Batch: 500, Loss: 0.687312
Train - Epoch 0, Batch: 510, Loss: 0.688383
Train - Epoch 0, Batch: 520, Loss: 0.687465
Train - Epoch 0, Batch: 530, Loss: 0.687518
Train - Epoch 0, Batch: 540, Loss: 0.688531
Train - Epoch 0, Batch: 550, Loss: 0.687386
Train - Epoch 0, Batch: 560, Loss: 0.687099
Train - Epoch 0, Batch: 570, Loss: 0.687021
Train - Epoch 0, Batch: 580, Loss: 0.687497
Train - Epoch 0, Batch: 590, Loss: 0.685767
Train - Epoch 0, Batch: 600, Loss: 0.687899
Train - Epoch 0, Batch: 610, Loss: 0.687237
Train - Epoch 0, Batch: 620, Loss: 0.687520
Train - Epoch 0, Batch: 630, Loss: 0.687479
Train - Epoch 0, Batch: 640, Loss: 0.687087
Train - Epoch 1, Batch: 0, Loss: 0.688125
Train - Epoch 1, Batch: 10, Loss: 0.688058
Train - Epoch 1, Batch: 20, Loss: 0.685665
Train - Epoch 1, Batch: 30, Loss: 0.687335
Train - Epoch 1, Batch: 40, Loss: 0.686941
Train - Epoch 1, Batch: 50, Loss: 0.685917
Train - Epoch 1, Batch: 60, Loss: 0.688005
Train - Epoch 1, Batch: 70, Loss: 0.687096
Train - Epoch 1, Batch: 80, Loss: 0.685225
Train - Epoch 1, Batch: 90, Loss: 0.686192
Train - Epoch 1, Batch: 100, Loss: 0.686101
Train - Epoch 1, Batch: 110, Loss: 0.686228
Train - Epoch 1, Batch: 120, Loss: 0.687111
Train - Epoch 1, Batch: 130, Loss: 0.688441
Train - Epoch 1, Batch: 140, Loss: 0.686241
Train - Epoch 1, Batch: 150, Loss: 0.687707
Train - Epoch 1, Batch: 160, Loss: 0.686577
Train - Epoch 1, Batch: 170, Loss: 0.685696
Train - Epoch 1, Batch: 180, Loss: 0.686856
Train - Epoch 1, Batch: 190, Loss: 0.686464
Train - Epoch 1, Batch: 200, Loss: 0.686884
Train - Epoch 1, Batch: 210, Loss: 0.686149
Train - Epoch 1, Batch: 220, Loss: 0.685180
Train - Epoch 1, Batch: 230, Loss: 0.686491
Train - Epoch 1, Batch: 240, Loss: 0.687002
Train - Epoch 1, Batch: 250, Loss: 0.686541
Train - Epoch 1, Batch: 260, Loss: 0.686098
Train - Epoch 1, Batch: 270, Loss: 0.684933
Train - Epoch 1, Batch: 280, Loss: 0.684525
Train - Epoch 1, Batch: 290, Loss: 0.685720
Train - Epoch 1, Batch: 300, Loss: 0.686412
Train - Epoch 1, Batch: 310, Loss: 0.686538
Train - Epoch 1, Batch: 320, Loss: 0.685211
Train - Epoch 1, Batch: 330, Loss: 0.686234
Train - Epoch 1, Batch: 340, Loss: 0.686452
Train - Epoch 1, Batch: 350, Loss: 0.686841
Train - Epoch 1, Batch: 360, Loss: 0.686156
Train - Epoch 1, Batch: 370, Loss: 0.687586
Train - Epoch 1, Batch: 380, Loss: 0.686384
Train - Epoch 1, Batch: 390, Loss: 0.685132
Train - Epoch 1, Batch: 400, Loss: 0.686459
Train - Epoch 1, Batch: 410, Loss: 0.685666
Train - Epoch 1, Batch: 420, Loss: 0.685668
Train - Epoch 1, Batch: 430, Loss: 0.686286
Train - Epoch 1, Batch: 440, Loss: 0.684319
Train - Epoch 1, Batch: 450, Loss: 0.686435
Train - Epoch 1, Batch: 460, Loss: 0.685715
Train - Epoch 1, Batch: 470, Loss: 0.685286
Train - Epoch 1, Batch: 480, Loss: 0.686339
Train - Epoch 1, Batch: 490, Loss: 0.685220
Train - Epoch 1, Batch: 500, Loss: 0.685341
Train - Epoch 1, Batch: 510, Loss: 0.685043
Train - Epoch 1, Batch: 520, Loss: 0.685573
Train - Epoch 1, Batch: 530, Loss: 0.686641
Train - Epoch 1, Batch: 540, Loss: 0.686655
Train - Epoch 1, Batch: 550, Loss: 0.685088
Train - Epoch 1, Batch: 560, Loss: 0.685319
Train - Epoch 1, Batch: 570, Loss: 0.685579
Train - Epoch 1, Batch: 580, Loss: 0.686152
Train - Epoch 1, Batch: 590, Loss: 0.685239
Train - Epoch 1, Batch: 600, Loss: 0.684890
Train - Epoch 1, Batch: 610, Loss: 0.686596
Train - Epoch 1, Batch: 620, Loss: 0.685922
Train - Epoch 1, Batch: 630, Loss: 0.684396
Train - Epoch 1, Batch: 640, Loss: 0.685343
Train - Epoch 2, Batch: 0, Loss: 0.684816
Train - Epoch 2, Batch: 10, Loss: 0.685212
Train - Epoch 2, Batch: 20, Loss: 0.685736
Train - Epoch 2, Batch: 30, Loss: 0.686229
Train - Epoch 2, Batch: 40, Loss: 0.685112
Train - Epoch 2, Batch: 50, Loss: 0.685550
Train - Epoch 2, Batch: 60, Loss: 0.686161
Train - Epoch 2, Batch: 70, Loss: 0.686331
Train - Epoch 2, Batch: 80, Loss: 0.685150
Train - Epoch 2, Batch: 90, Loss: 0.685331
Train - Epoch 2, Batch: 100, Loss: 0.684975
Train - Epoch 2, Batch: 110, Loss: 0.685786
Train - Epoch 2, Batch: 120, Loss: 0.684888
Train - Epoch 2, Batch: 130, Loss: 0.684500
Train - Epoch 2, Batch: 140, Loss: 0.685488
Train - Epoch 2, Batch: 150, Loss: 0.685078
Train - Epoch 2, Batch: 160, Loss: 0.685662
Train - Epoch 2, Batch: 170, Loss: 0.685986
Train - Epoch 2, Batch: 180, Loss: 0.685214
Train - Epoch 2, Batch: 190, Loss: 0.686155
Train - Epoch 2, Batch: 200, Loss: 0.685344
Train - Epoch 2, Batch: 210, Loss: 0.685824
Train - Epoch 2, Batch: 220, Loss: 0.684731
Train - Epoch 2, Batch: 230, Loss: 0.686523
Train - Epoch 2, Batch: 240, Loss: 0.686309
Train - Epoch 2, Batch: 250, Loss: 0.684354
Train - Epoch 2, Batch: 260, Loss: 0.685728
Train - Epoch 2, Batch: 270, Loss: 0.685840
Train - Epoch 2, Batch: 280, Loss: 0.685840
Train - Epoch 2, Batch: 290, Loss: 0.685184
Train - Epoch 2, Batch: 300, Loss: 0.685181
Train - Epoch 2, Batch: 310, Loss: 0.685636
Train - Epoch 2, Batch: 320, Loss: 0.685419
Train - Epoch 2, Batch: 330, Loss: 0.685905
Train - Epoch 2, Batch: 340, Loss: 0.686386
Train - Epoch 2, Batch: 350, Loss: 0.684874
Train - Epoch 2, Batch: 360, Loss: 0.685101
Train - Epoch 2, Batch: 370, Loss: 0.684158
Train - Epoch 2, Batch: 380, Loss: 0.686049
Train - Epoch 2, Batch: 390, Loss: 0.684627
Train - Epoch 2, Batch: 400, Loss: 0.684189
Train - Epoch 2, Batch: 410, Loss: 0.686141
Train - Epoch 2, Batch: 420, Loss: 0.685135
Train - Epoch 2, Batch: 430, Loss: 0.683336
Train - Epoch 2, Batch: 440, Loss: 0.684576
Train - Epoch 2, Batch: 450, Loss: 0.684017
Train - Epoch 2, Batch: 460, Loss: 0.685843
Train - Epoch 2, Batch: 470, Loss: 0.685045
Train - Epoch 2, Batch: 480, Loss: 0.685930
Train - Epoch 2, Batch: 490, Loss: 0.684414
Train - Epoch 2, Batch: 500, Loss: 0.683791
Train - Epoch 2, Batch: 510, Loss: 0.685046
Train - Epoch 2, Batch: 520, Loss: 0.684924
Train - Epoch 2, Batch: 530, Loss: 0.683619
Train - Epoch 2, Batch: 540, Loss: 0.684710
Train - Epoch 2, Batch: 550, Loss: 0.685072/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 2, Batch: 560, Loss: 0.685270
Train - Epoch 2, Batch: 570, Loss: 0.684996
Train - Epoch 2, Batch: 580, Loss: 0.684905
Train - Epoch 2, Batch: 590, Loss: 0.685011
Train - Epoch 2, Batch: 600, Loss: 0.684753
Train - Epoch 2, Batch: 610, Loss: 0.684623
Train - Epoch 2, Batch: 620, Loss: 0.683374
Train - Epoch 2, Batch: 630, Loss: 0.684589
Train - Epoch 2, Batch: 640, Loss: 0.686229
Train - Epoch 3, Batch: 0, Loss: 0.684028
Train - Epoch 3, Batch: 10, Loss: 0.685502
Train - Epoch 3, Batch: 20, Loss: 0.685602
Train - Epoch 3, Batch: 30, Loss: 0.684319
Train - Epoch 3, Batch: 40, Loss: 0.684428
Train - Epoch 3, Batch: 50, Loss: 0.684178
Train - Epoch 3, Batch: 60, Loss: 0.683602
Train - Epoch 3, Batch: 70, Loss: 0.684991
Train - Epoch 3, Batch: 80, Loss: 0.683134
Train - Epoch 3, Batch: 90, Loss: 0.683844
Train - Epoch 3, Batch: 100, Loss: 0.684848
Train - Epoch 3, Batch: 110, Loss: 0.684057
Train - Epoch 3, Batch: 120, Loss: 0.684154
Train - Epoch 3, Batch: 130, Loss: 0.685232
Train - Epoch 3, Batch: 140, Loss: 0.684664
Train - Epoch 3, Batch: 150, Loss: 0.684577
Train - Epoch 3, Batch: 160, Loss: 0.684660
Train - Epoch 3, Batch: 170, Loss: 0.685222
Train - Epoch 3, Batch: 180, Loss: 0.685553
Train - Epoch 3, Batch: 190, Loss: 0.685200
Train - Epoch 3, Batch: 200, Loss: 0.683825
Train - Epoch 3, Batch: 210, Loss: 0.684498
Train - Epoch 3, Batch: 220, Loss: 0.684989
Train - Epoch 3, Batch: 230, Loss: 0.684960
Train - Epoch 3, Batch: 240, Loss: 0.684198
Train - Epoch 3, Batch: 250, Loss: 0.685101
Train - Epoch 3, Batch: 260, Loss: 0.683605
Train - Epoch 3, Batch: 270, Loss: 0.685899
Train - Epoch 3, Batch: 280, Loss: 0.684397
Train - Epoch 3, Batch: 290, Loss: 0.682498
Train - Epoch 3, Batch: 300, Loss: 0.683571
Train - Epoch 3, Batch: 310, Loss: 0.684003
Train - Epoch 3, Batch: 320, Loss: 0.682689
Train - Epoch 3, Batch: 330, Loss: 0.684139
Train - Epoch 3, Batch: 340, Loss: 0.685634
Train - Epoch 3, Batch: 350, Loss: 0.684374
Train - Epoch 3, Batch: 360, Loss: 0.683305
Train - Epoch 3, Batch: 370, Loss: 0.684060
Train - Epoch 3, Batch: 380, Loss: 0.684480
Train - Epoch 3, Batch: 390, Loss: 0.684869
Train - Epoch 3, Batch: 400, Loss: 0.684105
Train - Epoch 3, Batch: 410, Loss: 0.683421
Train - Epoch 3, Batch: 420, Loss: 0.684659
Train - Epoch 3, Batch: 430, Loss: 0.684170
Train - Epoch 3, Batch: 440, Loss: 0.684495
Train - Epoch 3, Batch: 450, Loss: 0.684318
Train - Epoch 3, Batch: 460, Loss: 0.683987
Train - Epoch 3, Batch: 470, Loss: 0.684498
Train - Epoch 3, Batch: 480, Loss: 0.682471
Train - Epoch 3, Batch: 490, Loss: 0.682184
Train - Epoch 3, Batch: 500, Loss: 0.684859
Train - Epoch 3, Batch: 510, Loss: 0.684651
Train - Epoch 3, Batch: 520, Loss: 0.684377
Train - Epoch 3, Batch: 530, Loss: 0.683457
Train - Epoch 3, Batch: 540, Loss: 0.684288
Train - Epoch 3, Batch: 550, Loss: 0.682758
Train - Epoch 3, Batch: 560, Loss: 0.684042
Train - Epoch 3, Batch: 570, Loss: 0.684619
Train - Epoch 3, Batch: 580, Loss: 0.684043
Train - Epoch 3, Batch: 590, Loss: 0.684512
Train - Epoch 3, Batch: 600, Loss: 0.683261
Train - Epoch 3, Batch: 610, Loss: 0.684520
Train - Epoch 3, Batch: 620, Loss: 0.683624
Train - Epoch 3, Batch: 630, Loss: 0.683062
Train - Epoch 3, Batch: 640, Loss: 0.685299
Test Avg. Loss: 0.000042, Accuracy: 0.554824
training_time:: 16.47522521018982
training time full:: 16.475299835205078
provenance prepare time:: 7.3909759521484375e-06
Test Avg. Loss: 0.000042, Accuracy: 0.554824
baseline::
python3 incremental_updates_base_line_lr_multi.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Benchmark_experiments/benchmark_exp.py:1298: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  curr_delta_id = torch.tensor(delta_id)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 100
max_epoch:: 4
batch_size:: 16384
Num of deletion:: 0, running time baseline::5.378245
Num of deletion:: 10, running time baseline::56.378065
Num of deletion:: 20, running time baseline::108.224188
Num of deletion:: 30, running time baseline::156.453425
Num of deletion:: 40, running time baseline::204.842014
Num of deletion:: 50, running time baseline::251.455452
Num of deletion:: 60, running time baseline::302.087616
Num of deletion:: 70, running time baseline::350.472126
Num of deletion:: 80, running time baseline::401.397212
Num of deletion:: 90, running time baseline::451.329143
training time is 494.2935655117035
overhead:: 0
overhead2:: 494.2913906574249
overhead3:: 8.789767503738403
time_baseline:: 494.2944416999817
curr_diff: 0 tensor(1.8150e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.8150e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554828
period:: 3
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance5_lr_multi.py 300 3 7 0.000009524 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 100
max_epoch:: 4
Num of deletion:: 0, running time provenance::3.364609
Num of deletion:: 10, running time provenance::34.878978
Num of deletion:: 20, running time provenance::67.985672
Num of deletion:: 30, running time provenance::100.508699
Num of deletion:: 40, running time provenance::130.534297
Num of deletion:: 50, running time provenance::162.120724
Num of deletion:: 60, running time provenance::192.777204
Num of deletion:: 70, running time provenance::226.199592
Num of deletion:: 80, running time provenance::256.186146
Num of deletion:: 90, running time provenance::285.412102
overhead:: 0
overhead2:: 0
overhead3:: 314.9579930305481
overhead4:: 0
overhead5:: 9.894410371780396
memory usage:: 5988253696
time_provenance:: 314.96144437789917
curr_diff: 0 tensor(1.0591e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0591e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(1.7617e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.7617e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554828
repetition 8
python3 benchmark_exp_lr.py 0.001 16384 4 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression higgs 8 0.005 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 0.695375
Train - Epoch 0, Batch: 10, Loss: 0.694542
Train - Epoch 0, Batch: 20, Loss: 0.693777
Train - Epoch 0, Batch: 30, Loss: 0.692574
Train - Epoch 0, Batch: 40, Loss: 0.692347
Train - Epoch 0, Batch: 50, Loss: 0.691748
Train - Epoch 0, Batch: 60, Loss: 0.692785
Train - Epoch 0, Batch: 70, Loss: 0.691427
Train - Epoch 0, Batch: 80, Loss: 0.691276
Train - Epoch 0, Batch: 90, Loss: 0.691515
Train - Epoch 0, Batch: 100, Loss: 0.691852
Train - Epoch 0, Batch: 110, Loss: 0.690375
Train - Epoch 0, Batch: 120, Loss: 0.690526
Train - Epoch 0, Batch: 130, Loss: 0.689603
Train - Epoch 0, Batch: 140, Loss: 0.689931
Train - Epoch 0, Batch: 150, Loss: 0.689362
Train - Epoch 0, Batch: 160, Loss: 0.689589
Train - Epoch 0, Batch: 170, Loss: 0.690315
Train - Epoch 0, Batch: 180, Loss: 0.689560
Train - Epoch 0, Batch: 190, Loss: 0.689606
Train - Epoch 0, Batch: 200, Loss: 0.689854
Train - Epoch 0, Batch: 210, Loss: 0.689766
Train - Epoch 0, Batch: 220, Loss: 0.690216
Train - Epoch 0, Batch: 230, Loss: 0.688990
Train - Epoch 0, Batch: 240, Loss: 0.689608
Train - Epoch 0, Batch: 250, Loss: 0.688869
Train - Epoch 0, Batch: 260, Loss: 0.690284
Train - Epoch 0, Batch: 270, Loss: 0.688431
Train - Epoch 0, Batch: 280, Loss: 0.689302
Train - Epoch 0, Batch: 290, Loss: 0.689064
Train - Epoch 0, Batch: 300, Loss: 0.689023
Train - Epoch 0, Batch: 310, Loss: 0.689456
Train - Epoch 0, Batch: 320, Loss: 0.688680
Train - Epoch 0, Batch: 330, Loss: 0.688813
Train - Epoch 0, Batch: 340, Loss: 0.688310
Train - Epoch 0, Batch: 350, Loss: 0.687594
Train - Epoch 0, Batch: 360, Loss: 0.688610
Train - Epoch 0, Batch: 370, Loss: 0.688080
Train - Epoch 0, Batch: 380, Loss: 0.688613
Train - Epoch 0, Batch: 390, Loss: 0.689111
Train - Epoch 0, Batch: 400, Loss: 0.688643
Train - Epoch 0, Batch: 410, Loss: 0.688494
Train - Epoch 0, Batch: 420, Loss: 0.688646
Train - Epoch 0, Batch: 430, Loss: 0.687574
Train - Epoch 0, Batch: 440, Loss: 0.687960
Train - Epoch 0, Batch: 450, Loss: 0.687916
Train - Epoch 0, Batch: 460, Loss: 0.688544
Train - Epoch 0, Batch: 470, Loss: 0.686784
Train - Epoch 0, Batch: 480, Loss: 0.687520
Train - Epoch 0, Batch: 490, Loss: 0.686998
Train - Epoch 0, Batch: 500, Loss: 0.688176
Train - Epoch 0, Batch: 510, Loss: 0.687465
Train - Epoch 0, Batch: 520, Loss: 0.688084
Train - Epoch 0, Batch: 530, Loss: 0.686584
Train - Epoch 0, Batch: 540, Loss: 0.687705
Train - Epoch 0, Batch: 550, Loss: 0.688329
Train - Epoch 0, Batch: 560, Loss: 0.687867
Train - Epoch 0, Batch: 570, Loss: 0.687099
Train - Epoch 0, Batch: 580, Loss: 0.688246
Train - Epoch 0, Batch: 590, Loss: 0.686950
Train - Epoch 0, Batch: 600, Loss: 0.686402
Train - Epoch 0, Batch: 610, Loss: 0.687096
Train - Epoch 0, Batch: 620, Loss: 0.686759
Train - Epoch 0, Batch: 630, Loss: 0.688151
Train - Epoch 0, Batch: 640, Loss: 0.687797
Train - Epoch 1, Batch: 0, Loss: 0.687987
Train - Epoch 1, Batch: 10, Loss: 0.687202
Train - Epoch 1, Batch: 20, Loss: 0.687068
Train - Epoch 1, Batch: 30, Loss: 0.686654
Train - Epoch 1, Batch: 40, Loss: 0.686115
Train - Epoch 1, Batch: 50, Loss: 0.685491
Train - Epoch 1, Batch: 60, Loss: 0.686948
Train - Epoch 1, Batch: 70, Loss: 0.685632
Train - Epoch 1, Batch: 80, Loss: 0.686444
Train - Epoch 1, Batch: 90, Loss: 0.686268
Train - Epoch 1, Batch: 100, Loss: 0.686287
Train - Epoch 1, Batch: 110, Loss: 0.686968
Train - Epoch 1, Batch: 120, Loss: 0.688058
Train - Epoch 1, Batch: 130, Loss: 0.687908
Train - Epoch 1, Batch: 140, Loss: 0.686870
Train - Epoch 1, Batch: 150, Loss: 0.686879
Train - Epoch 1, Batch: 160, Loss: 0.686981
Train - Epoch 1, Batch: 170, Loss: 0.686300
Train - Epoch 1, Batch: 180, Loss: 0.687181
Train - Epoch 1, Batch: 190, Loss: 0.687335
Train - Epoch 1, Batch: 200, Loss: 0.686147
Train - Epoch 1, Batch: 210, Loss: 0.686309
Train - Epoch 1, Batch: 220, Loss: 0.685983
Train - Epoch 1, Batch: 230, Loss: 0.686213
Train - Epoch 1, Batch: 240, Loss: 0.685114
Train - Epoch 1, Batch: 250, Loss: 0.686605
Train - Epoch 1, Batch: 260, Loss: 0.686239
Train - Epoch 1, Batch: 270, Loss: 0.686073
Train - Epoch 1, Batch: 280, Loss: 0.686398
Train - Epoch 1, Batch: 290, Loss: 0.686227
Train - Epoch 1, Batch: 300, Loss: 0.686198
Train - Epoch 1, Batch: 310, Loss: 0.685775
Train - Epoch 1, Batch: 320, Loss: 0.685827
Train - Epoch 1, Batch: 330, Loss: 0.686289
Train - Epoch 1, Batch: 340, Loss: 0.686621
Train - Epoch 1, Batch: 350, Loss: 0.686137
Train - Epoch 1, Batch: 360, Loss: 0.685805
Train - Epoch 1, Batch: 370, Loss: 0.686003
Train - Epoch 1, Batch: 380, Loss: 0.686757
Train - Epoch 1, Batch: 390, Loss: 0.685494
Train - Epoch 1, Batch: 400, Loss: 0.686951
Train - Epoch 1, Batch: 410, Loss: 0.685261
Train - Epoch 1, Batch: 420, Loss: 0.686523
Train - Epoch 1, Batch: 430, Loss: 0.685517
Train - Epoch 1, Batch: 440, Loss: 0.687106
Train - Epoch 1, Batch: 450, Loss: 0.684910
Train - Epoch 1, Batch: 460, Loss: 0.685591
Train - Epoch 1, Batch: 470, Loss: 0.685997
Train - Epoch 1, Batch: 480, Loss: 0.685025
Train - Epoch 1, Batch: 490, Loss: 0.685608
Train - Epoch 1, Batch: 500, Loss: 0.686221
Train - Epoch 1, Batch: 510, Loss: 0.684911
Train - Epoch 1, Batch: 520, Loss: 0.685472
Train - Epoch 1, Batch: 530, Loss: 0.685809
Train - Epoch 1, Batch: 540, Loss: 0.686225
Train - Epoch 1, Batch: 550, Loss: 0.685738
Train - Epoch 1, Batch: 560, Loss: 0.685511
Train - Epoch 1, Batch: 570, Loss: 0.685654
Train - Epoch 1, Batch: 580, Loss: 0.686715
Train - Epoch 1, Batch: 590, Loss: 0.685834
Train - Epoch 1, Batch: 600, Loss: 0.685433
Train - Epoch 1, Batch: 610, Loss: 0.685579
Train - Epoch 1, Batch: 620, Loss: 0.684902
Train - Epoch 1, Batch: 630, Loss: 0.685792
Train - Epoch 1, Batch: 640, Loss: 0.684759
Train - Epoch 2, Batch: 0, Loss: 0.684728
Train - Epoch 2, Batch: 10, Loss: 0.687536
Train - Epoch 2, Batch: 20, Loss: 0.686585
Train - Epoch 2, Batch: 30, Loss: 0.685214
Train - Epoch 2, Batch: 40, Loss: 0.683294
Train - Epoch 2, Batch: 50, Loss: 0.685792
Train - Epoch 2, Batch: 60, Loss: 0.685648
Train - Epoch 2, Batch: 70, Loss: 0.686200
Train - Epoch 2, Batch: 80, Loss: 0.685006
Train - Epoch 2, Batch: 90, Loss: 0.685713
Train - Epoch 2, Batch: 100, Loss: 0.685166
Train - Epoch 2, Batch: 110, Loss: 0.685253
Train - Epoch 2, Batch: 120, Loss: 0.684657
Train - Epoch 2, Batch: 130, Loss: 0.686348
Train - Epoch 2, Batch: 140, Loss: 0.685351
Train - Epoch 2, Batch: 150, Loss: 0.684906
Train - Epoch 2, Batch: 160, Loss: 0.684340
Train - Epoch 2, Batch: 170, Loss: 0.685912
Train - Epoch 2, Batch: 180, Loss: 0.685468
Train - Epoch 2, Batch: 190, Loss: 0.684687
Train - Epoch 2, Batch: 200, Loss: 0.686348
Train - Epoch 2, Batch: 210, Loss: 0.685715
Train - Epoch 2, Batch: 220, Loss: 0.685216
Train - Epoch 2, Batch: 230, Loss: 0.686074
Train - Epoch 2, Batch: 240, Loss: 0.685668
Train - Epoch 2, Batch: 250, Loss: 0.683492
Train - Epoch 2, Batch: 260, Loss: 0.684865
Train - Epoch 2, Batch: 270, Loss: 0.684933
Train - Epoch 2, Batch: 280, Loss: 0.683894
Train - Epoch 2, Batch: 290, Loss: 0.685385
Train - Epoch 2, Batch: 300, Loss: 0.683469
Train - Epoch 2, Batch: 310, Loss: 0.685711
Train - Epoch 2, Batch: 320, Loss: 0.684809
Train - Epoch 2, Batch: 330, Loss: 0.684829
Train - Epoch 2, Batch: 340, Loss: 0.685832
Train - Epoch 2, Batch: 350, Loss: 0.685695
Train - Epoch 2, Batch: 360, Loss: 0.684743
Train - Epoch 2, Batch: 370, Loss: 0.684901
Train - Epoch 2, Batch: 380, Loss: 0.685073
Train - Epoch 2, Batch: 390, Loss: 0.685135
Train - Epoch 2, Batch: 400, Loss: 0.685089
Train - Epoch 2, Batch: 410, Loss: 0.683526
Train - Epoch 2, Batch: 420, Loss: 0.685915
Train - Epoch 2, Batch: 430, Loss: 0.684844
Train - Epoch 2, Batch: 440, Loss: 0.685330
Train - Epoch 2, Batch: 450, Loss: 0.683977
Train - Epoch 2, Batch: 460, Loss: 0.683874
Train - Epoch 2, Batch: 470, Loss: 0.684397
Train - Epoch 2, Batch: 480, Loss: 0.684464
Train - Epoch 2, Batch: 490, Loss: 0.685195
Train - Epoch 2, Batch: 500, Loss: 0.686108
Train - Epoch 2, Batch: 510, Loss: 0.683646
Train - Epoch 2, Batch: 520, Loss: 0.684149
Train - Epoch 2, Batch: 530, Loss: 0.683134
Train - Epoch 2, Batch: 540, Loss: 0.685854
Train - Epoch 2, Batch: 550, Loss: 0.684043/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 2, Batch: 560, Loss: 0.683143
Train - Epoch 2, Batch: 570, Loss: 0.684500
Train - Epoch 2, Batch: 580, Loss: 0.685431
Train - Epoch 2, Batch: 590, Loss: 0.684308
Train - Epoch 2, Batch: 600, Loss: 0.684479
Train - Epoch 2, Batch: 610, Loss: 0.683471
Train - Epoch 2, Batch: 620, Loss: 0.683465
Train - Epoch 2, Batch: 630, Loss: 0.684000
Train - Epoch 2, Batch: 640, Loss: 0.684471
Train - Epoch 3, Batch: 0, Loss: 0.684922
Train - Epoch 3, Batch: 10, Loss: 0.685217
Train - Epoch 3, Batch: 20, Loss: 0.683813
Train - Epoch 3, Batch: 30, Loss: 0.684219
Train - Epoch 3, Batch: 40, Loss: 0.685443
Train - Epoch 3, Batch: 50, Loss: 0.683198
Train - Epoch 3, Batch: 60, Loss: 0.684503
Train - Epoch 3, Batch: 70, Loss: 0.683920
Train - Epoch 3, Batch: 80, Loss: 0.683695
Train - Epoch 3, Batch: 90, Loss: 0.683984
Train - Epoch 3, Batch: 100, Loss: 0.686297
Train - Epoch 3, Batch: 110, Loss: 0.685341
Train - Epoch 3, Batch: 120, Loss: 0.684729
Train - Epoch 3, Batch: 130, Loss: 0.684172
Train - Epoch 3, Batch: 140, Loss: 0.685011
Train - Epoch 3, Batch: 150, Loss: 0.683745
Train - Epoch 3, Batch: 160, Loss: 0.684146
Train - Epoch 3, Batch: 170, Loss: 0.684443
Train - Epoch 3, Batch: 180, Loss: 0.683009
Train - Epoch 3, Batch: 190, Loss: 0.684705
Train - Epoch 3, Batch: 200, Loss: 0.684327
Train - Epoch 3, Batch: 210, Loss: 0.683873
Train - Epoch 3, Batch: 220, Loss: 0.684166
Train - Epoch 3, Batch: 230, Loss: 0.684808
Train - Epoch 3, Batch: 240, Loss: 0.683637
Train - Epoch 3, Batch: 250, Loss: 0.684586
Train - Epoch 3, Batch: 260, Loss: 0.684272
Train - Epoch 3, Batch: 270, Loss: 0.684091
Train - Epoch 3, Batch: 280, Loss: 0.683202
Train - Epoch 3, Batch: 290, Loss: 0.684476
Train - Epoch 3, Batch: 300, Loss: 0.685238
Train - Epoch 3, Batch: 310, Loss: 0.683448
Train - Epoch 3, Batch: 320, Loss: 0.684263
Train - Epoch 3, Batch: 330, Loss: 0.684030
Train - Epoch 3, Batch: 340, Loss: 0.684754
Train - Epoch 3, Batch: 350, Loss: 0.683968
Train - Epoch 3, Batch: 360, Loss: 0.684275
Train - Epoch 3, Batch: 370, Loss: 0.684362
Train - Epoch 3, Batch: 380, Loss: 0.683303
Train - Epoch 3, Batch: 390, Loss: 0.684018
Train - Epoch 3, Batch: 400, Loss: 0.683796
Train - Epoch 3, Batch: 410, Loss: 0.684603
Train - Epoch 3, Batch: 420, Loss: 0.683426
Train - Epoch 3, Batch: 430, Loss: 0.682519
Train - Epoch 3, Batch: 440, Loss: 0.683384
Train - Epoch 3, Batch: 450, Loss: 0.683738
Train - Epoch 3, Batch: 460, Loss: 0.684913
Train - Epoch 3, Batch: 470, Loss: 0.683716
Train - Epoch 3, Batch: 480, Loss: 0.682906
Train - Epoch 3, Batch: 490, Loss: 0.684265
Train - Epoch 3, Batch: 500, Loss: 0.684056
Train - Epoch 3, Batch: 510, Loss: 0.684804
Train - Epoch 3, Batch: 520, Loss: 0.683668
Train - Epoch 3, Batch: 530, Loss: 0.684127
Train - Epoch 3, Batch: 540, Loss: 0.684412
Train - Epoch 3, Batch: 550, Loss: 0.682580
Train - Epoch 3, Batch: 560, Loss: 0.682861
Train - Epoch 3, Batch: 570, Loss: 0.684295
Train - Epoch 3, Batch: 580, Loss: 0.683467
Train - Epoch 3, Batch: 590, Loss: 0.684847
Train - Epoch 3, Batch: 600, Loss: 0.685212
Train - Epoch 3, Batch: 610, Loss: 0.682991
Train - Epoch 3, Batch: 620, Loss: 0.685152
Train - Epoch 3, Batch: 630, Loss: 0.684129
Train - Epoch 3, Batch: 640, Loss: 0.683962
Test Avg. Loss: 0.000042, Accuracy: 0.552260
training_time:: 15.55273962020874
training time full:: 15.552807092666626
provenance prepare time:: 7.62939453125e-06
Test Avg. Loss: 0.000042, Accuracy: 0.552260
baseline::
python3 incremental_updates_base_line_lr_multi.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Benchmark_experiments/benchmark_exp.py:1298: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  curr_delta_id = torch.tensor(delta_id)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 100
max_epoch:: 4
batch_size:: 16384
Num of deletion:: 0, running time baseline::4.416630
Num of deletion:: 10, running time baseline::52.409193
Num of deletion:: 20, running time baseline::97.625042
Num of deletion:: 30, running time baseline::148.142439
Num of deletion:: 40, running time baseline::196.399245
Num of deletion:: 50, running time baseline::247.174446
Num of deletion:: 60, running time baseline::296.077732
Num of deletion:: 70, running time baseline::347.610124
Num of deletion:: 80, running time baseline::395.525242
Num of deletion:: 90, running time baseline::442.569201
training time is 486.62490916252136
overhead:: 0
overhead2:: 486.62257075309753
overhead3:: 9.122822523117065
time_baseline:: 486.6258420944214
curr_diff: 0 tensor(2.2089e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.2089e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.552270
period:: 3
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance5_lr_multi.py 300 3 8 0.000009524 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 100
max_epoch:: 4
Num of deletion:: 0, running time provenance::2.902807
Num of deletion:: 10, running time provenance::33.232745
Num of deletion:: 20, running time provenance::65.609325
Num of deletion:: 30, running time provenance::96.451859
Num of deletion:: 40, running time provenance::127.051744
Num of deletion:: 50, running time provenance::159.040039
Num of deletion:: 60, running time provenance::190.276895
Num of deletion:: 70, running time provenance::222.441869
Num of deletion:: 80, running time provenance::253.046770
Num of deletion:: 90, running time provenance::284.518924
overhead:: 0
overhead2:: 0
overhead3:: 314.0349576473236
overhead4:: 0
overhead5:: 10.098288297653198
memory usage:: 5987569664
time_provenance:: 314.0395143032074
curr_diff: 0 tensor(1.0926e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0926e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(2.1891e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1891e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.552268
repetition 9
python3 benchmark_exp_lr.py 0.001 16384 4 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression higgs 9 0.005 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 0.696502
Train - Epoch 0, Batch: 10, Loss: 0.695202
Train - Epoch 0, Batch: 20, Loss: 0.694002
Train - Epoch 0, Batch: 30, Loss: 0.693198
Train - Epoch 0, Batch: 40, Loss: 0.692040
Train - Epoch 0, Batch: 50, Loss: 0.691870
Train - Epoch 0, Batch: 60, Loss: 0.691228
Train - Epoch 0, Batch: 70, Loss: 0.691223
Train - Epoch 0, Batch: 80, Loss: 0.691196
Train - Epoch 0, Batch: 90, Loss: 0.690292
Train - Epoch 0, Batch: 100, Loss: 0.690697
Train - Epoch 0, Batch: 110, Loss: 0.690434
Train - Epoch 0, Batch: 120, Loss: 0.689905
Train - Epoch 0, Batch: 130, Loss: 0.689818
Train - Epoch 0, Batch: 140, Loss: 0.689024
Train - Epoch 0, Batch: 150, Loss: 0.690187
Train - Epoch 0, Batch: 160, Loss: 0.689297
Train - Epoch 0, Batch: 170, Loss: 0.688906
Train - Epoch 0, Batch: 180, Loss: 0.690414
Train - Epoch 0, Batch: 190, Loss: 0.689354
Train - Epoch 0, Batch: 200, Loss: 0.690325
Train - Epoch 0, Batch: 210, Loss: 0.689223
Train - Epoch 0, Batch: 220, Loss: 0.689609
Train - Epoch 0, Batch: 230, Loss: 0.688037
Train - Epoch 0, Batch: 240, Loss: 0.687612
Train - Epoch 0, Batch: 250, Loss: 0.688335
Train - Epoch 0, Batch: 260, Loss: 0.688761
Train - Epoch 0, Batch: 270, Loss: 0.688253
Train - Epoch 0, Batch: 280, Loss: 0.687516
Train - Epoch 0, Batch: 290, Loss: 0.688267
Train - Epoch 0, Batch: 300, Loss: 0.688235
Train - Epoch 0, Batch: 310, Loss: 0.689268
Train - Epoch 0, Batch: 320, Loss: 0.688610
Train - Epoch 0, Batch: 330, Loss: 0.687955
Train - Epoch 0, Batch: 340, Loss: 0.687439
Train - Epoch 0, Batch: 350, Loss: 0.685408
Train - Epoch 0, Batch: 360, Loss: 0.687255
Train - Epoch 0, Batch: 370, Loss: 0.687639
Train - Epoch 0, Batch: 380, Loss: 0.686934
Train - Epoch 0, Batch: 390, Loss: 0.687441
Train - Epoch 0, Batch: 400, Loss: 0.686279
Train - Epoch 0, Batch: 410, Loss: 0.689128
Train - Epoch 0, Batch: 420, Loss: 0.687178
Train - Epoch 0, Batch: 430, Loss: 0.686857
Train - Epoch 0, Batch: 440, Loss: 0.687020
Train - Epoch 0, Batch: 450, Loss: 0.686599
Train - Epoch 0, Batch: 460, Loss: 0.687391
Train - Epoch 0, Batch: 470, Loss: 0.686050
Train - Epoch 0, Batch: 480, Loss: 0.687192
Train - Epoch 0, Batch: 490, Loss: 0.687624
Train - Epoch 0, Batch: 500, Loss: 0.687209
Train - Epoch 0, Batch: 510, Loss: 0.687135
Train - Epoch 0, Batch: 520, Loss: 0.687489
Train - Epoch 0, Batch: 530, Loss: 0.685971
Train - Epoch 0, Batch: 540, Loss: 0.687949
Train - Epoch 0, Batch: 550, Loss: 0.686563
Train - Epoch 0, Batch: 560, Loss: 0.686664
Train - Epoch 0, Batch: 570, Loss: 0.687555
Train - Epoch 0, Batch: 580, Loss: 0.686633
Train - Epoch 0, Batch: 590, Loss: 0.686640
Train - Epoch 0, Batch: 600, Loss: 0.686822
Train - Epoch 0, Batch: 610, Loss: 0.685713
Train - Epoch 0, Batch: 620, Loss: 0.686996
Train - Epoch 0, Batch: 630, Loss: 0.687402
Train - Epoch 0, Batch: 640, Loss: 0.687725
Train - Epoch 1, Batch: 0, Loss: 0.686848
Train - Epoch 1, Batch: 10, Loss: 0.685208
Train - Epoch 1, Batch: 20, Loss: 0.686003
Train - Epoch 1, Batch: 30, Loss: 0.685908
Train - Epoch 1, Batch: 40, Loss: 0.686087
Train - Epoch 1, Batch: 50, Loss: 0.686404
Train - Epoch 1, Batch: 60, Loss: 0.686221
Train - Epoch 1, Batch: 70, Loss: 0.685989
Train - Epoch 1, Batch: 80, Loss: 0.685063
Train - Epoch 1, Batch: 90, Loss: 0.685454
Train - Epoch 1, Batch: 100, Loss: 0.685379
Train - Epoch 1, Batch: 110, Loss: 0.685858
Train - Epoch 1, Batch: 120, Loss: 0.685802
Train - Epoch 1, Batch: 130, Loss: 0.685848
Train - Epoch 1, Batch: 140, Loss: 0.685754
Train - Epoch 1, Batch: 150, Loss: 0.687210
Train - Epoch 1, Batch: 160, Loss: 0.686186
Train - Epoch 1, Batch: 170, Loss: 0.686413
Train - Epoch 1, Batch: 180, Loss: 0.685507
Train - Epoch 1, Batch: 190, Loss: 0.686086
Train - Epoch 1, Batch: 200, Loss: 0.685853
Train - Epoch 1, Batch: 210, Loss: 0.684530
Train - Epoch 1, Batch: 220, Loss: 0.686713
Train - Epoch 1, Batch: 230, Loss: 0.685889
Train - Epoch 1, Batch: 240, Loss: 0.685049
Train - Epoch 1, Batch: 250, Loss: 0.685742
Train - Epoch 1, Batch: 260, Loss: 0.685639
Train - Epoch 1, Batch: 270, Loss: 0.686455
Train - Epoch 1, Batch: 280, Loss: 0.685274
Train - Epoch 1, Batch: 290, Loss: 0.686175
Train - Epoch 1, Batch: 300, Loss: 0.686535
Train - Epoch 1, Batch: 310, Loss: 0.684651
Train - Epoch 1, Batch: 320, Loss: 0.685617
Train - Epoch 1, Batch: 330, Loss: 0.684850
Train - Epoch 1, Batch: 340, Loss: 0.685599
Train - Epoch 1, Batch: 350, Loss: 0.685403
Train - Epoch 1, Batch: 360, Loss: 0.684815
Train - Epoch 1, Batch: 370, Loss: 0.685928
Train - Epoch 1, Batch: 380, Loss: 0.686013
Train - Epoch 1, Batch: 390, Loss: 0.686255
Train - Epoch 1, Batch: 400, Loss: 0.684570
Train - Epoch 1, Batch: 410, Loss: 0.684768
Train - Epoch 1, Batch: 420, Loss: 0.684420
Train - Epoch 1, Batch: 430, Loss: 0.685019
Train - Epoch 1, Batch: 440, Loss: 0.685119
Train - Epoch 1, Batch: 450, Loss: 0.685112
Train - Epoch 1, Batch: 460, Loss: 0.685725
Train - Epoch 1, Batch: 470, Loss: 0.684644
Train - Epoch 1, Batch: 480, Loss: 0.684551
Train - Epoch 1, Batch: 490, Loss: 0.685491
Train - Epoch 1, Batch: 500, Loss: 0.685783
Train - Epoch 1, Batch: 510, Loss: 0.685349
Train - Epoch 1, Batch: 520, Loss: 0.685037
Train - Epoch 1, Batch: 530, Loss: 0.685378
Train - Epoch 1, Batch: 540, Loss: 0.684183
Train - Epoch 1, Batch: 550, Loss: 0.685964
Train - Epoch 1, Batch: 560, Loss: 0.685642
Train - Epoch 1, Batch: 570, Loss: 0.684729
Train - Epoch 1, Batch: 580, Loss: 0.685586
Train - Epoch 1, Batch: 590, Loss: 0.686101
Train - Epoch 1, Batch: 600, Loss: 0.684265
Train - Epoch 1, Batch: 610, Loss: 0.686212
Train - Epoch 1, Batch: 620, Loss: 0.685526
Train - Epoch 1, Batch: 630, Loss: 0.684261
Train - Epoch 1, Batch: 640, Loss: 0.685332
Train - Epoch 2, Batch: 0, Loss: 0.684354
Train - Epoch 2, Batch: 10, Loss: 0.684827
Train - Epoch 2, Batch: 20, Loss: 0.683255
Train - Epoch 2, Batch: 30, Loss: 0.685336
Train - Epoch 2, Batch: 40, Loss: 0.684616
Train - Epoch 2, Batch: 50, Loss: 0.685621
Train - Epoch 2, Batch: 60, Loss: 0.685407
Train - Epoch 2, Batch: 70, Loss: 0.683943
Train - Epoch 2, Batch: 80, Loss: 0.684117
Train - Epoch 2, Batch: 90, Loss: 0.685453
Train - Epoch 2, Batch: 100, Loss: 0.684234
Train - Epoch 2, Batch: 110, Loss: 0.685038
Train - Epoch 2, Batch: 120, Loss: 0.684379
Train - Epoch 2, Batch: 130, Loss: 0.685089
Train - Epoch 2, Batch: 140, Loss: 0.684629
Train - Epoch 2, Batch: 150, Loss: 0.685001
Train - Epoch 2, Batch: 160, Loss: 0.683816
Train - Epoch 2, Batch: 170, Loss: 0.684251
Train - Epoch 2, Batch: 180, Loss: 0.685348
Train - Epoch 2, Batch: 190, Loss: 0.684408
Train - Epoch 2, Batch: 200, Loss: 0.682983
Train - Epoch 2, Batch: 210, Loss: 0.685608
Train - Epoch 2, Batch: 220, Loss: 0.685446
Train - Epoch 2, Batch: 230, Loss: 0.685784
Train - Epoch 2, Batch: 240, Loss: 0.684875
Train - Epoch 2, Batch: 250, Loss: 0.685617
Train - Epoch 2, Batch: 260, Loss: 0.685757
Train - Epoch 2, Batch: 270, Loss: 0.684000
Train - Epoch 2, Batch: 280, Loss: 0.684586
Train - Epoch 2, Batch: 290, Loss: 0.684192
Train - Epoch 2, Batch: 300, Loss: 0.684379
Train - Epoch 2, Batch: 310, Loss: 0.686583
Train - Epoch 2, Batch: 320, Loss: 0.684851
Train - Epoch 2, Batch: 330, Loss: 0.684713
Train - Epoch 2, Batch: 340, Loss: 0.685288
Train - Epoch 2, Batch: 350, Loss: 0.684908
Train - Epoch 2, Batch: 360, Loss: 0.684864
Train - Epoch 2, Batch: 370, Loss: 0.684423
Train - Epoch 2, Batch: 380, Loss: 0.685898
Train - Epoch 2, Batch: 390, Loss: 0.685202
Train - Epoch 2, Batch: 400, Loss: 0.684865
Train - Epoch 2, Batch: 410, Loss: 0.684869
Train - Epoch 2, Batch: 420, Loss: 0.683841
Train - Epoch 2, Batch: 430, Loss: 0.683501
Train - Epoch 2, Batch: 440, Loss: 0.685032
Train - Epoch 2, Batch: 450, Loss: 0.684616
Train - Epoch 2, Batch: 460, Loss: 0.683918
Train - Epoch 2, Batch: 470, Loss: 0.684305
Train - Epoch 2, Batch: 480, Loss: 0.683829
Train - Epoch 2, Batch: 490, Loss: 0.683457
Train - Epoch 2, Batch: 500, Loss: 0.684145
Train - Epoch 2, Batch: 510, Loss: 0.684242
Train - Epoch 2, Batch: 520, Loss: 0.686110
Train - Epoch 2, Batch: 530, Loss: 0.684928
Train - Epoch 2, Batch: 540, Loss: 0.685313
Train - Epoch 2, Batch: 550, Loss: 0.683969/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 2, Batch: 560, Loss: 0.683813
Train - Epoch 2, Batch: 570, Loss: 0.683703
Train - Epoch 2, Batch: 580, Loss: 0.684750
Train - Epoch 2, Batch: 590, Loss: 0.684215
Train - Epoch 2, Batch: 600, Loss: 0.686083
Train - Epoch 2, Batch: 610, Loss: 0.684252
Train - Epoch 2, Batch: 620, Loss: 0.685617
Train - Epoch 2, Batch: 630, Loss: 0.684819
Train - Epoch 2, Batch: 640, Loss: 0.683867
Train - Epoch 3, Batch: 0, Loss: 0.682552
Train - Epoch 3, Batch: 10, Loss: 0.684598
Train - Epoch 3, Batch: 20, Loss: 0.685122
Train - Epoch 3, Batch: 30, Loss: 0.685392
Train - Epoch 3, Batch: 40, Loss: 0.683961
Train - Epoch 3, Batch: 50, Loss: 0.684633
Train - Epoch 3, Batch: 60, Loss: 0.684429
Train - Epoch 3, Batch: 70, Loss: 0.683621
Train - Epoch 3, Batch: 80, Loss: 0.683692
Train - Epoch 3, Batch: 90, Loss: 0.683322
Train - Epoch 3, Batch: 100, Loss: 0.684829
Train - Epoch 3, Batch: 110, Loss: 0.684441
Train - Epoch 3, Batch: 120, Loss: 0.685063
Train - Epoch 3, Batch: 130, Loss: 0.684574
Train - Epoch 3, Batch: 140, Loss: 0.684353
Train - Epoch 3, Batch: 150, Loss: 0.683518
Train - Epoch 3, Batch: 160, Loss: 0.683990
Train - Epoch 3, Batch: 170, Loss: 0.683886
Train - Epoch 3, Batch: 180, Loss: 0.684353
Train - Epoch 3, Batch: 190, Loss: 0.684379
Train - Epoch 3, Batch: 200, Loss: 0.684532
Train - Epoch 3, Batch: 210, Loss: 0.684613
Train - Epoch 3, Batch: 220, Loss: 0.683608
Train - Epoch 3, Batch: 230, Loss: 0.683985
Train - Epoch 3, Batch: 240, Loss: 0.683589
Train - Epoch 3, Batch: 250, Loss: 0.684513
Train - Epoch 3, Batch: 260, Loss: 0.685792
Train - Epoch 3, Batch: 270, Loss: 0.683720
Train - Epoch 3, Batch: 280, Loss: 0.683334
Train - Epoch 3, Batch: 290, Loss: 0.684246
Train - Epoch 3, Batch: 300, Loss: 0.684051
Train - Epoch 3, Batch: 310, Loss: 0.684690
Train - Epoch 3, Batch: 320, Loss: 0.684085
Train - Epoch 3, Batch: 330, Loss: 0.684047
Train - Epoch 3, Batch: 340, Loss: 0.684400
Train - Epoch 3, Batch: 350, Loss: 0.684462
Train - Epoch 3, Batch: 360, Loss: 0.683980
Train - Epoch 3, Batch: 370, Loss: 0.684646
Train - Epoch 3, Batch: 380, Loss: 0.684249
Train - Epoch 3, Batch: 390, Loss: 0.684165
Train - Epoch 3, Batch: 400, Loss: 0.685143
Train - Epoch 3, Batch: 410, Loss: 0.684293
Train - Epoch 3, Batch: 420, Loss: 0.683016
Train - Epoch 3, Batch: 430, Loss: 0.683919
Train - Epoch 3, Batch: 440, Loss: 0.683859
Train - Epoch 3, Batch: 450, Loss: 0.684013
Train - Epoch 3, Batch: 460, Loss: 0.682968
Train - Epoch 3, Batch: 470, Loss: 0.684181
Train - Epoch 3, Batch: 480, Loss: 0.684622
Train - Epoch 3, Batch: 490, Loss: 0.683988
Train - Epoch 3, Batch: 500, Loss: 0.683901
Train - Epoch 3, Batch: 510, Loss: 0.684011
Train - Epoch 3, Batch: 520, Loss: 0.683420
Train - Epoch 3, Batch: 530, Loss: 0.683661
Train - Epoch 3, Batch: 540, Loss: 0.682996
Train - Epoch 3, Batch: 550, Loss: 0.682165
Train - Epoch 3, Batch: 560, Loss: 0.683949
Train - Epoch 3, Batch: 570, Loss: 0.683959
Train - Epoch 3, Batch: 580, Loss: 0.683373
Train - Epoch 3, Batch: 590, Loss: 0.682943
Train - Epoch 3, Batch: 600, Loss: 0.683655
Train - Epoch 3, Batch: 610, Loss: 0.683512
Train - Epoch 3, Batch: 620, Loss: 0.683345
Train - Epoch 3, Batch: 630, Loss: 0.683883
Train - Epoch 3, Batch: 640, Loss: 0.682537
Test Avg. Loss: 0.000042, Accuracy: 0.554540
training_time:: 15.799099683761597
training time full:: 15.799165487289429
provenance prepare time:: 7.152557373046875e-06
Test Avg. Loss: 0.000042, Accuracy: 0.554540
baseline::
python3 incremental_updates_base_line_lr_multi.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Benchmark_experiments/benchmark_exp.py:1298: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  curr_delta_id = torch.tensor(delta_id)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 100
max_epoch:: 4
batch_size:: 16384
Num of deletion:: 0, running time baseline::4.948678
Num of deletion:: 10, running time baseline::53.338431
Num of deletion:: 20, running time baseline::100.949282
Num of deletion:: 30, running time baseline::148.298308
Num of deletion:: 40, running time baseline::196.671063
Num of deletion:: 50, running time baseline::244.201392
Num of deletion:: 60, running time baseline::292.533631
Num of deletion:: 70, running time baseline::340.192120
Num of deletion:: 80, running time baseline::386.976569
Num of deletion:: 90, running time baseline::432.262570
training time is 476.2973880767822
overhead:: 0
overhead2:: 476.2952923774719
overhead3:: 8.654680967330933
time_baseline:: 476.29827880859375
curr_diff: 0 tensor(2.0335e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.0335e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554540
period:: 3
init_iters:: 300
incremental updates::
python3 incremental_updates_provenance5_lr_multi.py 300 3 9 0.000009524 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 4
delta_size:: 100
max_epoch:: 4
Num of deletion:: 0, running time provenance::2.675659
Num of deletion:: 10, running time provenance::33.747801
Num of deletion:: 20, running time provenance::64.236074
Num of deletion:: 30, running time provenance::94.159102
Num of deletion:: 40, running time provenance::124.288557
Num of deletion:: 50, running time provenance::156.145399
Num of deletion:: 60, running time provenance::189.515076
Num of deletion:: 70, running time provenance::221.011214
Num of deletion:: 80, running time provenance::254.554165
Num of deletion:: 90, running time provenance::283.207572
overhead:: 0
overhead2:: 0
overhead3:: 309.0757474899292
overhead4:: 0
overhead5:: 9.602567195892334
memory usage:: 5988155392
time_provenance:: 309.0791766643524
curr_diff: 0 tensor(1.4847e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4847e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(1.9831e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.9831e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000042, Accuracy: 0.554538
