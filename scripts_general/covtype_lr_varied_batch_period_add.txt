period::
init_iters::
varied deletion rate::
varied number of samples::
python3 generate_dataset_train_test.py Logistic_regression covtype 16384 120 5
start loading data...
normalization start!!
deletion rate:: 0.00001
python3 generate_rand_ids 0.00001  covtype 1
start loading data...
normalization start!!
tensor([391428,  96645, 390617, 270357, 373849])
batch size:: 16384
repetition 0
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 0 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 1.895522
Train - Epoch 0, Batch: 10, Loss: 1.377142
Train - Epoch 0, Batch: 20, Loss: 1.232211
Train - Epoch 0, Batch: 30, Loss: 1.192960
Train - Epoch 1, Batch: 0, Loss: 1.194794
Train - Epoch 1, Batch: 10, Loss: 1.168576
Train - Epoch 1, Batch: 20, Loss: 1.155323
Train - Epoch 1, Batch: 30, Loss: 1.129382
Train - Epoch 2, Batch: 0, Loss: 1.131795
Train - Epoch 2, Batch: 10, Loss: 1.122499
Train - Epoch 2, Batch: 20, Loss: 1.130601
Train - Epoch 2, Batch: 30, Loss: 1.105301
Train - Epoch 3, Batch: 0, Loss: 1.100761
Train - Epoch 3, Batch: 10, Loss: 1.091308
Train - Epoch 3, Batch: 20, Loss: 1.083935
Train - Epoch 3, Batch: 30, Loss: 1.077877
Train - Epoch 4, Batch: 0, Loss: 1.071863
Train - Epoch 4, Batch: 10, Loss: 1.079062
Train - Epoch 4, Batch: 20, Loss: 1.061374
Train - Epoch 4, Batch: 30, Loss: 1.067203
Train - Epoch 5, Batch: 0, Loss: 1.053795
Train - Epoch 5, Batch: 10, Loss: 1.043385
Train - Epoch 5, Batch: 20, Loss: 1.052081
Train - Epoch 5, Batch: 30, Loss: 1.053229
Train - Epoch 6, Batch: 0, Loss: 1.048869
Train - Epoch 6, Batch: 10, Loss: 1.035917
Train - Epoch 6, Batch: 20, Loss: 1.028992
Train - Epoch 6, Batch: 30, Loss: 1.024054
Train - Epoch 7, Batch: 0, Loss: 1.029957
Train - Epoch 7, Batch: 10, Loss: 1.016328
Train - Epoch 7, Batch: 20, Loss: 1.022326
Train - Epoch 7, Batch: 30, Loss: 1.009438
Train - Epoch 8, Batch: 0, Loss: 1.006232
Train - Epoch 8, Batch: 10, Loss: 1.002182
Train - Epoch 8, Batch: 20, Loss: 0.998218
Train - Epoch 8, Batch: 30, Loss: 0.998537
Train - Epoch 9, Batch: 0, Loss: 1.002499
Train - Epoch 9, Batch: 10, Loss: 0.993330
Train - Epoch 9, Batch: 20, Loss: 0.999649
Train - Epoch 9, Batch: 30, Loss: 0.988240
Train - Epoch 10, Batch: 0, Loss: 0.984628
Train - Epoch 10, Batch: 10, Loss: 0.995500
Train - Epoch 10, Batch: 20, Loss: 0.993718
Train - Epoch 10, Batch: 30, Loss: 0.981916
Train - Epoch 11, Batch: 0, Loss: 0.978390
Train - Epoch 11, Batch: 10, Loss: 0.994471
Train - Epoch 11, Batch: 20, Loss: 0.972838
Train - Epoch 11, Batch: 30, Loss: 0.969949
Train - Epoch 12, Batch: 0, Loss: 0.982695
Train - Epoch 12, Batch: 10, Loss: 0.975871
Train - Epoch 12, Batch: 20, Loss: 0.966318
Train - Epoch 12, Batch: 30, Loss: 0.968031
Train - Epoch 13, Batch: 0, Loss: 0.961197
Train - Epoch 13, Batch: 10, Loss: 0.968367
Train - Epoch 13, Batch: 20, Loss: 0.967336
Train - Epoch 13, Batch: 30, Loss: 0.956780
Train - Epoch 14, Batch: 0, Loss: 0.964220
Train - Epoch 14, Batch: 10, Loss: 0.958445
Train - Epoch 14, Batch: 20, Loss: 0.954077
Train - Epoch 14, Batch: 30, Loss: 0.957823
Train - Epoch 15, Batch: 0, Loss: 0.955946
Train - Epoch 15, Batch: 10, Loss: 0.958309
Train - Epoch 15, Batch: 20, Loss: 0.948901
Train - Epoch 15, Batch: 30, Loss: 0.944505
Train - Epoch 16, Batch: 0, Loss: 0.953299
Train - Epoch 16, Batch: 10, Loss: 0.955195
Train - Epoch 16, Batch: 20, Loss: 0.950231
Train - Epoch 16, Batch: 30, Loss: 0.947873
Train - Epoch 17, Batch: 0, Loss: 0.948215
Train - Epoch 17, Batch: 10, Loss: 0.951619
Train - Epoch 17, Batch: 20, Loss: 0.934735
Train - Epoch 17, Batch: 30, Loss: 0.949279
Train - Epoch 18, Batch: 0, Loss: 0.936967
Train - Epoch 18, Batch: 10, Loss: 0.936893
Train - Epoch 18, Batch: 20, Loss: 0.935644
Train - Epoch 18, Batch: 30, Loss: 0.932559
Train - Epoch 19, Batch: 0, Loss: 0.933547
Train - Epoch 19, Batch: 10, Loss: 0.940850
Train - Epoch 19, Batch: 20, Loss: 0.926223
Train - Epoch 19, Batch: 30, Loss: 0.929725
Train - Epoch 20, Batch: 0, Loss: 0.941060
Train - Epoch 20, Batch: 10, Loss: 0.926925
Train - Epoch 20, Batch: 20, Loss: 0.916192
Train - Epoch 20, Batch: 30, Loss: 0.919445
Train - Epoch 21, Batch: 0, Loss: 0.922647
Train - Epoch 21, Batch: 10, Loss: 0.925221
Train - Epoch 21, Batch: 20, Loss: 0.923150
Train - Epoch 21, Batch: 30, Loss: 0.928247
Train - Epoch 22, Batch: 0, Loss: 0.920340
Train - Epoch 22, Batch: 10, Loss: 0.913534
Train - Epoch 22, Batch: 20, Loss: 0.916221
Train - Epoch 22, Batch: 30, Loss: 0.910783
Train - Epoch 23, Batch: 0, Loss: 0.908162
Train - Epoch 23, Batch: 10, Loss: 0.916399
Train - Epoch 23, Batch: 20, Loss: 0.920383
Train - Epoch 23, Batch: 30, Loss: 0.930483
Train - Epoch 24, Batch: 0, Loss: 0.920975
Train - Epoch 24, Batch: 10, Loss: 0.922392
Train - Epoch 24, Batch: 20, Loss: 0.917797
Train - Epoch 24, Batch: 30, Loss: 0.915206
Train - Epoch 25, Batch: 0, Loss: 0.922254
Train - Epoch 25, Batch: 10, Loss: 0.906214
Train - Epoch 25, Batch: 20, Loss: 0.913610
Train - Epoch 25, Batch: 30, Loss: 0.914297
Train - Epoch 26, Batch: 0, Loss: 0.918230
Train - Epoch 26, Batch: 10, Loss: 0.907588
Train - Epoch 26, Batch: 20, Loss: 0.911079
Train - Epoch 26, Batch: 30, Loss: 0.906968
Train - Epoch 27, Batch: 0, Loss: 0.910698
Train - Epoch 27, Batch: 10, Loss: 0.909031
Train - Epoch 27, Batch: 20, Loss: 0.908346
Train - Epoch 27, Batch: 30, Loss: 0.901694
Train - Epoch 28, Batch: 0, Loss: 0.909623
Train - Epoch 28, Batch: 10, Loss: 0.904868
Train - Epoch 28, Batch: 20, Loss: 0.901493
Train - Epoch 28, Batch: 30, Loss: 0.896671
Train - Epoch 29, Batch: 0, Loss: 0.900324
Train - Epoch 29, Batch: 10, Loss: 0.890219
Train - Epoch 29, Batch: 20, Loss: 0.900449
Train - Epoch 29, Batch: 30, Loss: 0.903537
Train - Epoch 30, Batch: 0, Loss: 0.901260
Train - Epoch 30, Batch: 10, Loss: 0.893417
Train - Epoch 30, Batch: 20, Loss: 0.903848
Train - Epoch 30, Batch: 30, Loss: 0.892806
Train - Epoch 31, Batch: 0, Loss: 0.899039
Train - Epoch 31, Batch: 10, Loss: 0.892353
Train - Epoch 31, Batch: 20, Loss: 0.902516
Train - Epoch 31, Batch: 30, Loss: 0.900664
training_time:: 3.8520166873931885
training time full:: 3.8520607948303223
provenance prepare time:: 1.1920928955078125e-06
Test Avg. Loss: 0.000070, Accuracy: 0.629959
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 5
training time is 2.3550684452056885
overhead:: 0
overhead2:: 0
time_baseline:: 2.3567891120910645
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629908
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 0 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.0019559860229492188
overhead3:: 0.03701615333557129
overhead4:: 0.09779024124145508
overhead5:: 0
time_provenance:: 0.5153393745422363
curr_diff: 0 tensor(2.1449e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1449e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629925
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 0 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.003158092498779297
overhead3:: 0.04372572898864746
overhead4:: 0.16165590286254883
overhead5:: 0
time_provenance:: 0.6084191799163818
curr_diff: 0 tensor(1.5873e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5873e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629908
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 0 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.002886533737182617
overhead3:: 0.038660287857055664
overhead4:: 0.1452023983001709
overhead5:: 0
time_provenance:: 0.5552992820739746
curr_diff: 0 tensor(2.2009e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.2009e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629925
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 0 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.004070281982421875
overhead3:: 0.03824901580810547
overhead4:: 0.19521260261535645
overhead5:: 0
time_provenance:: 0.5839979648590088
curr_diff: 0 tensor(1.5912e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5912e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629908
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 0 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.005252838134765625
overhead3:: 0.07818412780761719
overhead4:: 0.28229331970214844
overhead5:: 0
time_provenance:: 0.9149839878082275
curr_diff: 0 tensor(1.0605e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0605e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629908
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 0 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.004144191741943359
overhead3:: 0.053900718688964844
overhead4:: 0.25705552101135254
overhead5:: 0
time_provenance:: 0.737839937210083
curr_diff: 0 tensor(1.0734e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0734e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629908
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 0 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.0046694278717041016
overhead3:: 0.06080818176269531
overhead4:: 0.2592639923095703
overhead5:: 0
time_provenance:: 0.7076766490936279
curr_diff: 0 tensor(1.0723e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0723e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629908
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 0 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.0052258968353271484
overhead3:: 0.061527252197265625
overhead4:: 0.2767019271850586
overhead5:: 0
time_provenance:: 0.7608637809753418
curr_diff: 0 tensor(1.0664e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0664e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629908
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 0 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.009440183639526367
overhead3:: 0.11019539833068848
overhead4:: 0.41163015365600586
overhead5:: 0
time_provenance:: 0.9807388782501221
curr_diff: 0 tensor(5.0855e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.0855e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629908
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 0 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.00969839096069336
overhead3:: 0.1000823974609375
overhead4:: 0.4402952194213867
overhead5:: 0
time_provenance:: 1.0157365798950195
curr_diff: 0 tensor(5.2005e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.2005e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629908
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 0 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.012746334075927734
overhead3:: 0.14293241500854492
overhead4:: 0.47852301597595215
overhead5:: 0
time_provenance:: 1.076340675354004
curr_diff: 0 tensor(5.2356e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.2356e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629908
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 0 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.014460325241088867
overhead3:: 0.12557435035705566
overhead4:: 0.494922399520874
overhead5:: 0
time_provenance:: 1.2135932445526123
curr_diff: 0 tensor(5.2425e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.2425e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629908
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 0 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.018640756607055664
overhead3:: 0.23554277420043945
overhead4:: 0.9658048152923584
overhead5:: 0
time_provenance:: 1.7720744609832764
curr_diff: 0 tensor(1.0210e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0210e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629908
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 0 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.018957138061523438
overhead3:: 0.2044370174407959
overhead4:: 0.9883954524993896
overhead5:: 0
time_provenance:: 1.759885311126709
curr_diff: 0 tensor(1.0423e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0423e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629908
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 0 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.020296096801757812
overhead3:: 0.21404480934143066
overhead4:: 0.9887619018554688
overhead5:: 0
time_provenance:: 1.7655608654022217
curr_diff: 0 tensor(1.0515e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0515e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629908
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 0 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.02864694595336914
overhead3:: 0.2784860134124756
overhead4:: 1.1594007015228271
overhead5:: 0
time_provenance:: 2.18817400932312
curr_diff: 0 tensor(1.0530e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0530e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629908
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 0 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.04960155487060547
overhead3:: 0.5118904113769531
overhead4:: 1.6127161979675293
overhead5:: 0
time_provenance:: 2.3838067054748535
curr_diff: 0 tensor(7.9906e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.9906e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629908
repetition 1
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 1 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 1.950253
Train - Epoch 0, Batch: 10, Loss: 1.384564
Train - Epoch 0, Batch: 20, Loss: 1.244401
Train - Epoch 0, Batch: 30, Loss: 1.192660
Train - Epoch 1, Batch: 0, Loss: 1.192899
Train - Epoch 1, Batch: 10, Loss: 1.149418
Train - Epoch 1, Batch: 20, Loss: 1.154087
Train - Epoch 1, Batch: 30, Loss: 1.129409
Train - Epoch 2, Batch: 0, Loss: 1.138234
Train - Epoch 2, Batch: 10, Loss: 1.124729
Train - Epoch 2, Batch: 20, Loss: 1.107211
Train - Epoch 2, Batch: 30, Loss: 1.102367
Train - Epoch 3, Batch: 0, Loss: 1.091498
Train - Epoch 3, Batch: 10, Loss: 1.088510
Train - Epoch 3, Batch: 20, Loss: 1.085445
Train - Epoch 3, Batch: 30, Loss: 1.069764
Train - Epoch 4, Batch: 0, Loss: 1.075066
Train - Epoch 4, Batch: 10, Loss: 1.066838
Train - Epoch 4, Batch: 20, Loss: 1.059426
Train - Epoch 4, Batch: 30, Loss: 1.061419
Train - Epoch 5, Batch: 0, Loss: 1.055447
Train - Epoch 5, Batch: 10, Loss: 1.051331
Train - Epoch 5, Batch: 20, Loss: 1.037014
Train - Epoch 5, Batch: 30, Loss: 1.030761
Train - Epoch 6, Batch: 0, Loss: 1.035485
Train - Epoch 6, Batch: 10, Loss: 1.031049
Train - Epoch 6, Batch: 20, Loss: 1.020128
Train - Epoch 6, Batch: 30, Loss: 1.022993
Train - Epoch 7, Batch: 0, Loss: 1.021673
Train - Epoch 7, Batch: 10, Loss: 1.028001
Train - Epoch 7, Batch: 20, Loss: 1.010519
Train - Epoch 7, Batch: 30, Loss: 1.012739
Train - Epoch 8, Batch: 0, Loss: 1.012884
Train - Epoch 8, Batch: 10, Loss: 1.011417
Train - Epoch 8, Batch: 20, Loss: 1.005996
Train - Epoch 8, Batch: 30, Loss: 0.995676
Train - Epoch 9, Batch: 0, Loss: 0.995501
Train - Epoch 9, Batch: 10, Loss: 0.987292
Train - Epoch 9, Batch: 20, Loss: 1.004336
Train - Epoch 9, Batch: 30, Loss: 0.978284
Train - Epoch 10, Batch: 0, Loss: 0.995993
Train - Epoch 10, Batch: 10, Loss: 0.976723
Train - Epoch 10, Batch: 20, Loss: 0.976529
Train - Epoch 10, Batch: 30, Loss: 0.978431
Train - Epoch 11, Batch: 0, Loss: 0.975773
Train - Epoch 11, Batch: 10, Loss: 0.975561
Train - Epoch 11, Batch: 20, Loss: 0.981053
Train - Epoch 11, Batch: 30, Loss: 0.975234
Train - Epoch 12, Batch: 0, Loss: 0.967371
Train - Epoch 12, Batch: 10, Loss: 0.971495
Train - Epoch 12, Batch: 20, Loss: 0.968547
Train - Epoch 12, Batch: 30, Loss: 0.964394
Train - Epoch 13, Batch: 0, Loss: 0.958001
Train - Epoch 13, Batch: 10, Loss: 0.965150
Train - Epoch 13, Batch: 20, Loss: 0.956651
Train - Epoch 13, Batch: 30, Loss: 0.955492
Train - Epoch 14, Batch: 0, Loss: 0.962718
Train - Epoch 14, Batch: 10, Loss: 0.958146
Train - Epoch 14, Batch: 20, Loss: 0.952019
Train - Epoch 14, Batch: 30, Loss: 0.951933
Train - Epoch 15, Batch: 0, Loss: 0.956271
Train - Epoch 15, Batch: 10, Loss: 0.946753
Train - Epoch 15, Batch: 20, Loss: 0.944791
Train - Epoch 15, Batch: 30, Loss: 0.937869
Train - Epoch 16, Batch: 0, Loss: 0.942348
Train - Epoch 16, Batch: 10, Loss: 0.942759
Train - Epoch 16, Batch: 20, Loss: 0.948460
Train - Epoch 16, Batch: 30, Loss: 0.946762
Train - Epoch 17, Batch: 0, Loss: 0.941626
Train - Epoch 17, Batch: 10, Loss: 0.931206
Train - Epoch 17, Batch: 20, Loss: 0.937529
Train - Epoch 17, Batch: 30, Loss: 0.931895
Train - Epoch 18, Batch: 0, Loss: 0.943338
Train - Epoch 18, Batch: 10, Loss: 0.921118
Train - Epoch 18, Batch: 20, Loss: 0.938115
Train - Epoch 18, Batch: 30, Loss: 0.934021
Train - Epoch 19, Batch: 0, Loss: 0.936277
Train - Epoch 19, Batch: 10, Loss: 0.940917
Train - Epoch 19, Batch: 20, Loss: 0.930776
Train - Epoch 19, Batch: 30, Loss: 0.926956
Train - Epoch 20, Batch: 0, Loss: 0.927617
Train - Epoch 20, Batch: 10, Loss: 0.925718
Train - Epoch 20, Batch: 20, Loss: 0.932863
Train - Epoch 20, Batch: 30, Loss: 0.927585
Train - Epoch 21, Batch: 0, Loss: 0.921287
Train - Epoch 21, Batch: 10, Loss: 0.922149
Train - Epoch 21, Batch: 20, Loss: 0.919883
Train - Epoch 21, Batch: 30, Loss: 0.923372
Train - Epoch 22, Batch: 0, Loss: 0.917973
Train - Epoch 22, Batch: 10, Loss: 0.921852
Train - Epoch 22, Batch: 20, Loss: 0.912054
Train - Epoch 22, Batch: 30, Loss: 0.918476
Train - Epoch 23, Batch: 0, Loss: 0.911896
Train - Epoch 23, Batch: 10, Loss: 0.921308
Train - Epoch 23, Batch: 20, Loss: 0.915952
Train - Epoch 23, Batch: 30, Loss: 0.914073
Train - Epoch 24, Batch: 0, Loss: 0.912819
Train - Epoch 24, Batch: 10, Loss: 0.904231
Train - Epoch 24, Batch: 20, Loss: 0.904184
Train - Epoch 24, Batch: 30, Loss: 0.917481
Train - Epoch 25, Batch: 0, Loss: 0.910401
Train - Epoch 25, Batch: 10, Loss: 0.909146
Train - Epoch 25, Batch: 20, Loss: 0.913307
Train - Epoch 25, Batch: 30, Loss: 0.906355
Train - Epoch 26, Batch: 0, Loss: 0.906416
Train - Epoch 26, Batch: 10, Loss: 0.903128
Train - Epoch 26, Batch: 20, Loss: 0.895174
Train - Epoch 26, Batch: 30, Loss: 0.907595
Train - Epoch 27, Batch: 0, Loss: 0.901715
Train - Epoch 27, Batch: 10, Loss: 0.909365
Train - Epoch 27, Batch: 20, Loss: 0.891276
Train - Epoch 27, Batch: 30, Loss: 0.898115
Train - Epoch 28, Batch: 0, Loss: 0.898189
Train - Epoch 28, Batch: 10, Loss: 0.904645
Train - Epoch 28, Batch: 20, Loss: 0.902641
Train - Epoch 28, Batch: 30, Loss: 0.902114
Train - Epoch 29, Batch: 0, Loss: 0.897893
Train - Epoch 29, Batch: 10, Loss: 0.901855
Train - Epoch 29, Batch: 20, Loss: 0.889832
Train - Epoch 29, Batch: 30, Loss: 0.891316
Train - Epoch 30, Batch: 0, Loss: 0.891672
Train - Epoch 30, Batch: 10, Loss: 0.904216
Train - Epoch 30, Batch: 20, Loss: 0.898687
Train - Epoch 30, Batch: 30, Loss: 0.895426
Train - Epoch 31, Batch: 0, Loss: 0.887678
Train - Epoch 31, Batch: 10, Loss: 0.897015
Train - Epoch 31, Batch: 20, Loss: 0.891990
Train - Epoch 31, Batch: 30, Loss: 0.896050
training_time:: 3.8284201622009277
training time full:: 3.828479528427124
provenance prepare time:: 7.152557373046875e-07
Test Avg. Loss: 0.000070, Accuracy: 0.630854
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 5
training time is 2.020063877105713
overhead:: 0
overhead2:: 0
time_baseline:: 2.0216009616851807
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630854
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 1 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.003175020217895508
overhead3:: 0.03887653350830078
overhead4:: 0.12572360038757324
overhead5:: 0
time_provenance:: 0.6271228790283203
curr_diff: 0 tensor(1.5765e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5765e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630854
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 1 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.0030145645141601562
overhead3:: 0.04540228843688965
overhead4:: 0.14503240585327148
overhead5:: 0
time_provenance:: 0.5631210803985596
curr_diff: 0 tensor(1.7245e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.7245e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630854
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 1 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.0038251876831054688
overhead3:: 0.040282487869262695
overhead4:: 0.15448212623596191
overhead5:: 0
time_provenance:: 0.5792891979217529
curr_diff: 0 tensor(1.5888e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5888e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630854
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 1 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.003913164138793945
overhead3:: 0.059418678283691406
overhead4:: 0.1612710952758789
overhead5:: 0
time_provenance:: 0.5977296829223633
curr_diff: 0 tensor(1.7028e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.7028e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630854
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 1 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.004163980484008789
overhead3:: 0.04943037033081055
overhead4:: 0.23354029655456543
overhead5:: 0
time_provenance:: 0.681708812713623
curr_diff: 0 tensor(1.0613e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0613e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630854
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 1 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.004114866256713867
overhead3:: 0.0532376766204834
overhead4:: 0.23530125617980957
overhead5:: 0
time_provenance:: 0.6756935119628906
curr_diff: 0 tensor(1.0604e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0604e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630854
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 1 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.005160808563232422
overhead3:: 0.053067684173583984
overhead4:: 0.2500605583190918
overhead5:: 0
time_provenance:: 0.6731691360473633
curr_diff: 0 tensor(1.0613e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0613e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630854
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 1 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.005197048187255859
overhead3:: 0.06615042686462402
overhead4:: 0.2663564682006836
overhead5:: 0
time_provenance:: 0.7472732067108154
curr_diff: 0 tensor(1.0627e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0627e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630854
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 1 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.006434917449951172
overhead3:: 0.08862495422363281
overhead4:: 0.4187033176422119
overhead5:: 0
time_provenance:: 0.9651279449462891
curr_diff: 0 tensor(4.5883e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.5883e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630854
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 1 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.006482362747192383
overhead3:: 0.10455060005187988
overhead4:: 0.423799991607666
overhead5:: 0
time_provenance:: 0.968759298324585
curr_diff: 0 tensor(4.5908e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.5908e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630854
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 1 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.006925821304321289
overhead3:: 0.09543514251708984
overhead4:: 0.4102935791015625
overhead5:: 0
time_provenance:: 0.9590775966644287
curr_diff: 0 tensor(4.6137e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.6137e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630854
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 1 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.009629964828491211
overhead3:: 0.16179108619689941
overhead4:: 0.5334675312042236
overhead5:: 0
time_provenance:: 1.2756354808807373
curr_diff: 0 tensor(4.6165e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.6165e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630854
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 1 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.019999027252197266
overhead3:: 0.20280861854553223
overhead4:: 0.8913962841033936
overhead5:: 0
time_provenance:: 1.6461269855499268
curr_diff: 0 tensor(1.4365e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4365e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630854
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 1 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.01940178871154785
overhead3:: 0.19659829139709473
overhead4:: 0.869642972946167
overhead5:: 0
time_provenance:: 1.6141712665557861
curr_diff: 0 tensor(1.4368e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4368e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630854
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 1 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.019946813583374023
overhead3:: 0.20702409744262695
overhead4:: 1.0021629333496094
overhead5:: 0
time_provenance:: 1.7610979080200195
curr_diff: 0 tensor(1.4677e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4677e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630854
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 1 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.02643871307373047
overhead3:: 0.26442408561706543
overhead4:: 1.2150194644927979
overhead5:: 0
time_provenance:: 2.2206900119781494
curr_diff: 0 tensor(1.4682e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4682e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630854
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 1 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.045017242431640625
overhead3:: 0.4224550724029541
overhead4:: 1.5498113632202148
overhead5:: 0
time_provenance:: 2.2264344692230225
curr_diff: 0 tensor(1.2499e-15, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2499e-15, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630854
repetition 2
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.023276
Train - Epoch 0, Batch: 10, Loss: 1.393858
Train - Epoch 0, Batch: 20, Loss: 1.251087
Train - Epoch 0, Batch: 30, Loss: 1.197289
Train - Epoch 1, Batch: 0, Loss: 1.181655
Train - Epoch 1, Batch: 10, Loss: 1.163860
Train - Epoch 1, Batch: 20, Loss: 1.138286
Train - Epoch 1, Batch: 30, Loss: 1.122667
Train - Epoch 2, Batch: 0, Loss: 1.131766
Train - Epoch 2, Batch: 10, Loss: 1.116635
Train - Epoch 2, Batch: 20, Loss: 1.116029
Train - Epoch 2, Batch: 30, Loss: 1.090012
Train - Epoch 3, Batch: 0, Loss: 1.088132
Train - Epoch 3, Batch: 10, Loss: 1.083134
Train - Epoch 3, Batch: 20, Loss: 1.078939
Train - Epoch 3, Batch: 30, Loss: 1.070337
Train - Epoch 4, Batch: 0, Loss: 1.065386
Train - Epoch 4, Batch: 10, Loss: 1.067061
Train - Epoch 4, Batch: 20, Loss: 1.051128
Train - Epoch 4, Batch: 30, Loss: 1.050512
Train - Epoch 5, Batch: 0, Loss: 1.045462
Train - Epoch 5, Batch: 10, Loss: 1.047415
Train - Epoch 5, Batch: 20, Loss: 1.038166
Train - Epoch 5, Batch: 30, Loss: 1.034388
Train - Epoch 6, Batch: 0, Loss: 1.033642
Train - Epoch 6, Batch: 10, Loss: 1.023236
Train - Epoch 6, Batch: 20, Loss: 1.020571
Train - Epoch 6, Batch: 30, Loss: 1.031035
Train - Epoch 7, Batch: 0, Loss: 1.017385
Train - Epoch 7, Batch: 10, Loss: 1.020559
Train - Epoch 7, Batch: 20, Loss: 1.015881
Train - Epoch 7, Batch: 30, Loss: 1.008529
Train - Epoch 8, Batch: 0, Loss: 1.004033
Train - Epoch 8, Batch: 10, Loss: 1.002827
Train - Epoch 8, Batch: 20, Loss: 1.002972
Train - Epoch 8, Batch: 30, Loss: 0.996655
Train - Epoch 9, Batch: 0, Loss: 1.011940
Train - Epoch 9, Batch: 10, Loss: 0.986962
Train - Epoch 9, Batch: 20, Loss: 0.995642
Train - Epoch 9, Batch: 30, Loss: 0.989956
Train - Epoch 10, Batch: 0, Loss: 0.997981
Train - Epoch 10, Batch: 10, Loss: 0.979898
Train - Epoch 10, Batch: 20, Loss: 0.979290
Train - Epoch 10, Batch: 30, Loss: 0.977695
Train - Epoch 11, Batch: 0, Loss: 0.979227
Train - Epoch 11, Batch: 10, Loss: 0.980440
Train - Epoch 11, Batch: 20, Loss: 0.971404
Train - Epoch 11, Batch: 30, Loss: 0.962818
Train - Epoch 12, Batch: 0, Loss: 0.972516
Train - Epoch 12, Batch: 10, Loss: 0.967989
Train - Epoch 12, Batch: 20, Loss: 0.963648
Train - Epoch 12, Batch: 30, Loss: 0.972954
Train - Epoch 13, Batch: 0, Loss: 0.965644
Train - Epoch 13, Batch: 10, Loss: 0.967398
Train - Epoch 13, Batch: 20, Loss: 0.956272
Train - Epoch 13, Batch: 30, Loss: 0.957764
Train - Epoch 14, Batch: 0, Loss: 0.960430
Train - Epoch 14, Batch: 10, Loss: 0.960489
Train - Epoch 14, Batch: 20, Loss: 0.948455
Train - Epoch 14, Batch: 30, Loss: 0.943874
Train - Epoch 15, Batch: 0, Loss: 0.942830
Train - Epoch 15, Batch: 10, Loss: 0.944656
Train - Epoch 15, Batch: 20, Loss: 0.947277
Train - Epoch 15, Batch: 30, Loss: 0.935755
Train - Epoch 16, Batch: 0, Loss: 0.947733
Train - Epoch 16, Batch: 10, Loss: 0.941769
Train - Epoch 16, Batch: 20, Loss: 0.941643
Train - Epoch 16, Batch: 30, Loss: 0.949458
Train - Epoch 17, Batch: 0, Loss: 0.945690
Train - Epoch 17, Batch: 10, Loss: 0.928238
Train - Epoch 17, Batch: 20, Loss: 0.938840
Train - Epoch 17, Batch: 30, Loss: 0.932003
Train - Epoch 18, Batch: 0, Loss: 0.937008
Train - Epoch 18, Batch: 10, Loss: 0.929981
Train - Epoch 18, Batch: 20, Loss: 0.934142
Train - Epoch 18, Batch: 30, Loss: 0.931924
Train - Epoch 19, Batch: 0, Loss: 0.931317
Train - Epoch 19, Batch: 10, Loss: 0.940070
Train - Epoch 19, Batch: 20, Loss: 0.927578
Train - Epoch 19, Batch: 30, Loss: 0.934945
Train - Epoch 20, Batch: 0, Loss: 0.926407
Train - Epoch 20, Batch: 10, Loss: 0.922117
Train - Epoch 20, Batch: 20, Loss: 0.923059
Train - Epoch 20, Batch: 30, Loss: 0.925508
Train - Epoch 21, Batch: 0, Loss: 0.925949
Train - Epoch 21, Batch: 10, Loss: 0.935359
Train - Epoch 21, Batch: 20, Loss: 0.917510
Train - Epoch 21, Batch: 30, Loss: 0.916165
Train - Epoch 22, Batch: 0, Loss: 0.923689
Train - Epoch 22, Batch: 10, Loss: 0.921118
Train - Epoch 22, Batch: 20, Loss: 0.932233
Train - Epoch 22, Batch: 30, Loss: 0.915736
Train - Epoch 23, Batch: 0, Loss: 0.913246
Train - Epoch 23, Batch: 10, Loss: 0.910904
Train - Epoch 23, Batch: 20, Loss: 0.923369
Train - Epoch 23, Batch: 30, Loss: 0.911371
Train - Epoch 24, Batch: 0, Loss: 0.917746
Train - Epoch 24, Batch: 10, Loss: 0.910325
Train - Epoch 24, Batch: 20, Loss: 0.912434
Train - Epoch 24, Batch: 30, Loss: 0.912175
Train - Epoch 25, Batch: 0, Loss: 0.911978
Train - Epoch 25, Batch: 10, Loss: 0.921831
Train - Epoch 25, Batch: 20, Loss: 0.902338
Train - Epoch 25, Batch: 30, Loss: 0.916604
Train - Epoch 26, Batch: 0, Loss: 0.897711
Train - Epoch 26, Batch: 10, Loss: 0.906434
Train - Epoch 26, Batch: 20, Loss: 0.908258
Train - Epoch 26, Batch: 30, Loss: 0.910319
Train - Epoch 27, Batch: 0, Loss: 0.905954
Train - Epoch 27, Batch: 10, Loss: 0.897677
Train - Epoch 27, Batch: 20, Loss: 0.910633
Train - Epoch 27, Batch: 30, Loss: 0.899259
Train - Epoch 28, Batch: 0, Loss: 0.903523
Train - Epoch 28, Batch: 10, Loss: 0.902396
Train - Epoch 28, Batch: 20, Loss: 0.898767
Train - Epoch 28, Batch: 30, Loss: 0.898657
Train - Epoch 29, Batch: 0, Loss: 0.891703
Train - Epoch 29, Batch: 10, Loss: 0.898189
Train - Epoch 29, Batch: 20, Loss: 0.897985
Train - Epoch 29, Batch: 30, Loss: 0.899880
Train - Epoch 30, Batch: 0, Loss: 0.898944
Train - Epoch 30, Batch: 10, Loss: 0.888652
Train - Epoch 30, Batch: 20, Loss: 0.897749
Train - Epoch 30, Batch: 30, Loss: 0.901075
Train - Epoch 31, Batch: 0, Loss: 0.887385
Train - Epoch 31, Batch: 10, Loss: 0.897396
Train - Epoch 31, Batch: 20, Loss: 0.894875
Train - Epoch 31, Batch: 30, Loss: 0.894682
training_time:: 3.8969545364379883
training time full:: 3.896998643875122
provenance prepare time:: 2.384185791015625e-07
Test Avg. Loss: 0.000070, Accuracy: 0.629271
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 5
training time is 2.2880876064300537
overhead:: 0
overhead2:: 0
time_baseline:: 2.2897024154663086
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629271
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 2 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.002973318099975586
overhead3:: 0.028629064559936523
overhead4:: 0.11807894706726074
overhead5:: 0
time_provenance:: 0.5020246505737305
curr_diff: 0 tensor(1.4345e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4345e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629271
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 2 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.0028913021087646484
overhead3:: 0.0379796028137207
overhead4:: 0.1503891944885254
overhead5:: 0
time_provenance:: 0.5717523097991943
curr_diff: 0 tensor(1.4634e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4634e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629271
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 2 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.003301858901977539
overhead3:: 0.04975438117980957
overhead4:: 0.16255927085876465
overhead5:: 0
time_provenance:: 0.5897312164306641
curr_diff: 0 tensor(1.5858e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5858e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629271
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 2 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.004172563552856445
overhead3:: 0.06710958480834961
overhead4:: 0.16257977485656738
overhead5:: 0
time_provenance:: 0.6256146430969238
curr_diff: 0 tensor(1.4214e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4214e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629271
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 2 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.004564046859741211
overhead3:: 0.05104970932006836
overhead4:: 0.23176097869873047
overhead5:: 0
time_provenance:: 0.6877546310424805
curr_diff: 0 tensor(9.1438e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.1438e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629271
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 2 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.0044574737548828125
overhead3:: 0.053617238998413086
overhead4:: 0.25980710983276367
overhead5:: 0
time_provenance:: 0.7190382480621338
curr_diff: 0 tensor(9.6639e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.6639e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629271
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 2 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.0059812068939208984
overhead3:: 0.07589197158813477
overhead4:: 0.29732298851013184
overhead5:: 0
time_provenance:: 0.938248872756958
curr_diff: 0 tensor(9.6091e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.6091e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629271
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 2 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.006455659866333008
overhead3:: 0.07036232948303223
overhead4:: 0.3145287036895752
overhead5:: 0
time_provenance:: 0.9342310428619385
curr_diff: 0 tensor(9.5141e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.5141e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629271
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 2 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.007938861846923828
overhead3:: 0.0919187068939209
overhead4:: 0.4368908405303955
overhead5:: 0
time_provenance:: 0.9854724407196045
curr_diff: 0 tensor(4.3416e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.3416e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629271
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 2 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.008144855499267578
overhead3:: 0.09649324417114258
overhead4:: 0.4463953971862793
overhead5:: 0
time_provenance:: 0.9912242889404297
curr_diff: 0 tensor(4.5591e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.5591e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629271
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 2 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.008623123168945312
overhead3:: 0.09816265106201172
overhead4:: 0.44613003730773926
overhead5:: 0
time_provenance:: 1.0062181949615479
curr_diff: 0 tensor(4.5751e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.5751e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629271
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 2 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.008287429809570312
overhead3:: 0.09667062759399414
overhead4:: 0.42936062812805176
overhead5:: 0
time_provenance:: 0.9766416549682617
curr_diff: 0 tensor(4.5346e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.5346e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629271
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 2 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.02192521095275879
overhead3:: 0.21208429336547852
overhead4:: 0.9667868614196777
overhead5:: 0
time_provenance:: 1.7443556785583496
curr_diff: 0 tensor(1.1922e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1922e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629271
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 2 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.023075580596923828
overhead3:: 0.22843003273010254
overhead4:: 0.9678449630737305
overhead5:: 0
time_provenance:: 1.756617784500122
curr_diff: 0 tensor(1.2086e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2086e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629271
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 2 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.02326178550720215
overhead3:: 0.22312378883361816
overhead4:: 0.9349093437194824
overhead5:: 0
time_provenance:: 1.7193002700805664
curr_diff: 0 tensor(1.2126e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2126e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629271
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 2 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.022644519805908203
overhead3:: 0.2162327766418457
overhead4:: 0.9654245376586914
overhead5:: 0
time_provenance:: 1.7444052696228027
curr_diff: 0 tensor(1.2132e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2132e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629271
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 2 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.04873919486999512
overhead3:: 0.44878458976745605
overhead4:: 1.6215705871582031
overhead5:: 0
time_provenance:: 2.33528208732605
curr_diff: 0 tensor(1.0869e-15, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0869e-15, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629271
repetition 3
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 3 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 1.919936
Train - Epoch 0, Batch: 10, Loss: 1.381694
Train - Epoch 0, Batch: 20, Loss: 1.243615
Train - Epoch 0, Batch: 30, Loss: 1.198054
Train - Epoch 1, Batch: 0, Loss: 1.197877
Train - Epoch 1, Batch: 10, Loss: 1.162441
Train - Epoch 1, Batch: 20, Loss: 1.151946
Train - Epoch 1, Batch: 30, Loss: 1.138123
Train - Epoch 2, Batch: 0, Loss: 1.141322
Train - Epoch 2, Batch: 10, Loss: 1.123327
Train - Epoch 2, Batch: 20, Loss: 1.120325
Train - Epoch 2, Batch: 30, Loss: 1.099364
Train - Epoch 3, Batch: 0, Loss: 1.101505
Train - Epoch 3, Batch: 10, Loss: 1.093374
Train - Epoch 3, Batch: 20, Loss: 1.088144
Train - Epoch 3, Batch: 30, Loss: 1.079767
Train - Epoch 4, Batch: 0, Loss: 1.081373
Train - Epoch 4, Batch: 10, Loss: 1.081857
Train - Epoch 4, Batch: 20, Loss: 1.061545
Train - Epoch 4, Batch: 30, Loss: 1.060192
Train - Epoch 5, Batch: 0, Loss: 1.046337
Train - Epoch 5, Batch: 10, Loss: 1.049222
Train - Epoch 5, Batch: 20, Loss: 1.027817
Train - Epoch 5, Batch: 30, Loss: 1.046374
Train - Epoch 6, Batch: 0, Loss: 1.043708
Train - Epoch 6, Batch: 10, Loss: 1.030365
Train - Epoch 6, Batch: 20, Loss: 1.029941
Train - Epoch 6, Batch: 30, Loss: 1.028184
Train - Epoch 7, Batch: 0, Loss: 1.017664
Train - Epoch 7, Batch: 10, Loss: 1.029095
Train - Epoch 7, Batch: 20, Loss: 1.010733
Train - Epoch 7, Batch: 30, Loss: 1.011103
Train - Epoch 8, Batch: 0, Loss: 1.006029
Train - Epoch 8, Batch: 10, Loss: 1.009379
Train - Epoch 8, Batch: 20, Loss: 1.007664
Train - Epoch 8, Batch: 30, Loss: 0.993817
Train - Epoch 9, Batch: 0, Loss: 1.001602
Train - Epoch 9, Batch: 10, Loss: 0.993396
Train - Epoch 9, Batch: 20, Loss: 0.989827
Train - Epoch 9, Batch: 30, Loss: 0.986719
Train - Epoch 10, Batch: 0, Loss: 0.997575
Train - Epoch 10, Batch: 10, Loss: 0.980129
Train - Epoch 10, Batch: 20, Loss: 0.985089
Train - Epoch 10, Batch: 30, Loss: 0.973611
Train - Epoch 11, Batch: 0, Loss: 0.982079
Train - Epoch 11, Batch: 10, Loss: 0.988115
Train - Epoch 11, Batch: 20, Loss: 0.981825
Train - Epoch 11, Batch: 30, Loss: 0.975458
Train - Epoch 12, Batch: 0, Loss: 0.977727
Train - Epoch 12, Batch: 10, Loss: 0.967377
Train - Epoch 12, Batch: 20, Loss: 0.972182
Train - Epoch 12, Batch: 30, Loss: 0.962698
Train - Epoch 13, Batch: 0, Loss: 0.959815
Train - Epoch 13, Batch: 10, Loss: 0.962549
Train - Epoch 13, Batch: 20, Loss: 0.966984
Train - Epoch 13, Batch: 30, Loss: 0.961272
Train - Epoch 14, Batch: 0, Loss: 0.968789
Train - Epoch 14, Batch: 10, Loss: 0.959299
Train - Epoch 14, Batch: 20, Loss: 0.955649
Train - Epoch 14, Batch: 30, Loss: 0.950488
Train - Epoch 15, Batch: 0, Loss: 0.954106
Train - Epoch 15, Batch: 10, Loss: 0.942962
Train - Epoch 15, Batch: 20, Loss: 0.946543
Train - Epoch 15, Batch: 30, Loss: 0.937982
Train - Epoch 16, Batch: 0, Loss: 0.942593
Train - Epoch 16, Batch: 10, Loss: 0.941920
Train - Epoch 16, Batch: 20, Loss: 0.939493
Train - Epoch 16, Batch: 30, Loss: 0.946678
Train - Epoch 17, Batch: 0, Loss: 0.939665
Train - Epoch 17, Batch: 10, Loss: 0.940552
Train - Epoch 17, Batch: 20, Loss: 0.942339
Train - Epoch 17, Batch: 30, Loss: 0.923598
Train - Epoch 18, Batch: 0, Loss: 0.933437
Train - Epoch 18, Batch: 10, Loss: 0.933413
Train - Epoch 18, Batch: 20, Loss: 0.938322
Train - Epoch 18, Batch: 30, Loss: 0.931711
Train - Epoch 19, Batch: 0, Loss: 0.935462
Train - Epoch 19, Batch: 10, Loss: 0.927348
Train - Epoch 19, Batch: 20, Loss: 0.921202
Train - Epoch 19, Batch: 30, Loss: 0.928763
Train - Epoch 20, Batch: 0, Loss: 0.919378
Train - Epoch 20, Batch: 10, Loss: 0.935850
Train - Epoch 20, Batch: 20, Loss: 0.924538
Train - Epoch 20, Batch: 30, Loss: 0.925570
Train - Epoch 21, Batch: 0, Loss: 0.925533
Train - Epoch 21, Batch: 10, Loss: 0.931720
Train - Epoch 21, Batch: 20, Loss: 0.925436
Train - Epoch 21, Batch: 30, Loss: 0.917011
Train - Epoch 22, Batch: 0, Loss: 0.934532
Train - Epoch 22, Batch: 10, Loss: 0.924974
Train - Epoch 22, Batch: 20, Loss: 0.913249
Train - Epoch 22, Batch: 30, Loss: 0.908234
Train - Epoch 23, Batch: 0, Loss: 0.910057
Train - Epoch 23, Batch: 10, Loss: 0.921883
Train - Epoch 23, Batch: 20, Loss: 0.924609
Train - Epoch 23, Batch: 30, Loss: 0.911134
Train - Epoch 24, Batch: 0, Loss: 0.911227
Train - Epoch 24, Batch: 10, Loss: 0.923535
Train - Epoch 24, Batch: 20, Loss: 0.910635
Train - Epoch 24, Batch: 30, Loss: 0.923913
Train - Epoch 25, Batch: 0, Loss: 0.900817
Train - Epoch 25, Batch: 10, Loss: 0.902659
Train - Epoch 25, Batch: 20, Loss: 0.906114
Train - Epoch 25, Batch: 30, Loss: 0.907009
Train - Epoch 26, Batch: 0, Loss: 0.906396
Train - Epoch 26, Batch: 10, Loss: 0.903303
Train - Epoch 26, Batch: 20, Loss: 0.907042
Train - Epoch 26, Batch: 30, Loss: 0.900411
Train - Epoch 27, Batch: 0, Loss: 0.907324
Train - Epoch 27, Batch: 10, Loss: 0.902285
Train - Epoch 27, Batch: 20, Loss: 0.903900
Train - Epoch 27, Batch: 30, Loss: 0.903470
Train - Epoch 28, Batch: 0, Loss: 0.896364
Train - Epoch 28, Batch: 10, Loss: 0.906297
Train - Epoch 28, Batch: 20, Loss: 0.903409
Train - Epoch 28, Batch: 30, Loss: 0.904009
Train - Epoch 29, Batch: 0, Loss: 0.900817
Train - Epoch 29, Batch: 10, Loss: 0.892627
Train - Epoch 29, Batch: 20, Loss: 0.899759
Train - Epoch 29, Batch: 30, Loss: 0.890427
Train - Epoch 30, Batch: 0, Loss: 0.893436
Train - Epoch 30, Batch: 10, Loss: 0.897356
Train - Epoch 30, Batch: 20, Loss: 0.908688
Train - Epoch 30, Batch: 30, Loss: 0.896260
Train - Epoch 31, Batch: 0, Loss: 0.897824
Train - Epoch 31, Batch: 10, Loss: 0.897563
Train - Epoch 31, Batch: 20, Loss: 0.890794
Train - Epoch 31, Batch: 30, Loss: 0.896846
training_time:: 3.386619806289673
training time full:: 3.386664628982544
provenance prepare time:: 4.76837158203125e-07
Test Avg. Loss: 0.000070, Accuracy: 0.630235
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 5
training time is 2.223926067352295
overhead:: 0
overhead2:: 0
time_baseline:: 2.2253918647766113
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630235
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 3 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.0027022361755371094
overhead3:: 0.030187368392944336
overhead4:: 0.14452075958251953
overhead5:: 0
time_provenance:: 0.5635585784912109
curr_diff: 0 tensor(1.5609e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5609e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630235
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 3 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.0028150081634521484
overhead3:: 0.032227277755737305
overhead4:: 0.13591647148132324
overhead5:: 0
time_provenance:: 0.5607845783233643
curr_diff: 0 tensor(1.8382e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.8382e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630235
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 3 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.0030345916748046875
overhead3:: 0.06021547317504883
overhead4:: 0.15073585510253906
overhead5:: 0
time_provenance:: 0.5826001167297363
curr_diff: 0 tensor(1.5972e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5972e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630235
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 3 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.004144191741943359
overhead3:: 0.04962038993835449
overhead4:: 0.1565241813659668
overhead5:: 0
time_provenance:: 0.5895960330963135
curr_diff: 0 tensor(1.8871e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.8871e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630235
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 3 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.00461125373840332
overhead3:: 0.047943830490112305
overhead4:: 0.201765775680542
overhead5:: 0
time_provenance:: 0.6322345733642578
curr_diff: 0 tensor(1.0445e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0445e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630235
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 3 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.004578113555908203
overhead3:: 0.05292701721191406
overhead4:: 0.24249720573425293
overhead5:: 0
time_provenance:: 0.6959438323974609
curr_diff: 0 tensor(1.0532e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0532e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630235
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 3 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.0055887699127197266
overhead3:: 0.055529117584228516
overhead4:: 0.23329567909240723
overhead5:: 0
time_provenance:: 0.67486572265625
curr_diff: 0 tensor(1.0588e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0588e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630235
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 3 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.0067501068115234375
overhead3:: 0.07875823974609375
overhead4:: 0.271557092666626
overhead5:: 0
time_provenance:: 0.7686495780944824
curr_diff: 0 tensor(1.0530e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0530e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630235
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 3 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.009113311767578125
overhead3:: 0.08510446548461914
overhead4:: 0.41784024238586426
overhead5:: 0
time_provenance:: 0.9628732204437256
curr_diff: 0 tensor(4.3321e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.3321e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630235
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 3 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.008661508560180664
overhead3:: 0.08740425109863281
overhead4:: 0.3933217525482178
overhead5:: 0
time_provenance:: 0.897162914276123
curr_diff: 0 tensor(4.3486e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.3486e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630235
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 3 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.011299848556518555
overhead3:: 0.1152191162109375
overhead4:: 0.5034134387969971
overhead5:: 0
time_provenance:: 1.1524667739868164
curr_diff: 0 tensor(4.3515e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.3515e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630235
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 3 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.010103225708007812
overhead3:: 0.09920263290405273
overhead4:: 0.42721986770629883
overhead5:: 0
time_provenance:: 0.9622573852539062
curr_diff: 0 tensor(4.4492e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.4492e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630235
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 3 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.02169513702392578
overhead3:: 0.20621204376220703
overhead4:: 0.9500269889831543
overhead5:: 0
time_provenance:: 1.7212448120117188
curr_diff: 0 tensor(1.6562e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6562e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630235
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 3 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.02429485321044922
overhead3:: 0.24581003189086914
overhead4:: 0.9044637680053711
overhead5:: 0
time_provenance:: 1.7045488357543945
curr_diff: 0 tensor(1.6576e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6576e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630235
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 3 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.02240777015686035
overhead3:: 0.21675562858581543
overhead4:: 0.9940719604492188
overhead5:: 0
time_provenance:: 1.7652430534362793
curr_diff: 0 tensor(1.6603e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6603e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630235
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 3 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.03137922286987305
overhead3:: 0.2816619873046875
overhead4:: 1.1585798263549805
overhead5:: 0
time_provenance:: 2.186541795730591
curr_diff: 0 tensor(1.6505e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6505e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630235
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 3 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.048037052154541016
overhead3:: 0.45607709884643555
overhead4:: 1.5842058658599854
overhead5:: 0
time_provenance:: 2.299689769744873
curr_diff: 0 tensor(1.3641e-15, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3641e-15, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630235
repetition 4
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 4 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 1.758927
Train - Epoch 0, Batch: 10, Loss: 1.330901
Train - Epoch 0, Batch: 20, Loss: 1.221287
Train - Epoch 0, Batch: 30, Loss: 1.173060
Train - Epoch 1, Batch: 0, Loss: 1.166909
Train - Epoch 1, Batch: 10, Loss: 1.144275
Train - Epoch 1, Batch: 20, Loss: 1.125130
Train - Epoch 1, Batch: 30, Loss: 1.120505
Train - Epoch 2, Batch: 0, Loss: 1.106426
Train - Epoch 2, Batch: 10, Loss: 1.106661
Train - Epoch 2, Batch: 20, Loss: 1.106030
Train - Epoch 2, Batch: 30, Loss: 1.101462
Train - Epoch 3, Batch: 0, Loss: 1.092815
Train - Epoch 3, Batch: 10, Loss: 1.085056
Train - Epoch 3, Batch: 20, Loss: 1.069409
Train - Epoch 3, Batch: 30, Loss: 1.071680
Train - Epoch 4, Batch: 0, Loss: 1.070104
Train - Epoch 4, Batch: 10, Loss: 1.066408
Train - Epoch 4, Batch: 20, Loss: 1.057835
Train - Epoch 4, Batch: 30, Loss: 1.047287
Train - Epoch 5, Batch: 0, Loss: 1.053459
Train - Epoch 5, Batch: 10, Loss: 1.037031
Train - Epoch 5, Batch: 20, Loss: 1.037746
Train - Epoch 5, Batch: 30, Loss: 1.030003
Train - Epoch 6, Batch: 0, Loss: 1.028775
Train - Epoch 6, Batch: 10, Loss: 1.030795
Train - Epoch 6, Batch: 20, Loss: 1.015738
Train - Epoch 6, Batch: 30, Loss: 1.015060
Train - Epoch 7, Batch: 0, Loss: 1.020290
Train - Epoch 7, Batch: 10, Loss: 1.009319
Train - Epoch 7, Batch: 20, Loss: 1.013716
Train - Epoch 7, Batch: 30, Loss: 1.016814
Train - Epoch 8, Batch: 0, Loss: 0.997948
Train - Epoch 8, Batch: 10, Loss: 1.004187
Train - Epoch 8, Batch: 20, Loss: 0.993968
Train - Epoch 8, Batch: 30, Loss: 0.991287
Train - Epoch 9, Batch: 0, Loss: 1.000914
Train - Epoch 9, Batch: 10, Loss: 0.992459
Train - Epoch 9, Batch: 20, Loss: 0.983931
Train - Epoch 9, Batch: 30, Loss: 0.987289
Train - Epoch 10, Batch: 0, Loss: 0.989912
Train - Epoch 10, Batch: 10, Loss: 0.984882
Train - Epoch 10, Batch: 20, Loss: 0.990065
Train - Epoch 10, Batch: 30, Loss: 0.988679
Train - Epoch 11, Batch: 0, Loss: 0.979050
Train - Epoch 11, Batch: 10, Loss: 0.971778
Train - Epoch 11, Batch: 20, Loss: 0.967665
Train - Epoch 11, Batch: 30, Loss: 0.968839
Train - Epoch 12, Batch: 0, Loss: 0.970847
Train - Epoch 12, Batch: 10, Loss: 0.971243
Train - Epoch 12, Batch: 20, Loss: 0.973548
Train - Epoch 12, Batch: 30, Loss: 0.967362
Train - Epoch 13, Batch: 0, Loss: 0.959931
Train - Epoch 13, Batch: 10, Loss: 0.967626
Train - Epoch 13, Batch: 20, Loss: 0.960360
Train - Epoch 13, Batch: 30, Loss: 0.959581
Train - Epoch 14, Batch: 0, Loss: 0.951089
Train - Epoch 14, Batch: 10, Loss: 0.956868
Train - Epoch 14, Batch: 20, Loss: 0.951465
Train - Epoch 14, Batch: 30, Loss: 0.966814
Train - Epoch 15, Batch: 0, Loss: 0.955369
Train - Epoch 15, Batch: 10, Loss: 0.941660
Train - Epoch 15, Batch: 20, Loss: 0.946462
Train - Epoch 15, Batch: 30, Loss: 0.950046
Train - Epoch 16, Batch: 0, Loss: 0.949833
Train - Epoch 16, Batch: 10, Loss: 0.942675
Train - Epoch 16, Batch: 20, Loss: 0.929786
Train - Epoch 16, Batch: 30, Loss: 0.941770
Train - Epoch 17, Batch: 0, Loss: 0.944237
Train - Epoch 17, Batch: 10, Loss: 0.933195
Train - Epoch 17, Batch: 20, Loss: 0.938579
Train - Epoch 17, Batch: 30, Loss: 0.932496
Train - Epoch 18, Batch: 0, Loss: 0.931082
Train - Epoch 18, Batch: 10, Loss: 0.933321
Train - Epoch 18, Batch: 20, Loss: 0.936347
Train - Epoch 18, Batch: 30, Loss: 0.929135
Train - Epoch 19, Batch: 0, Loss: 0.928739
Train - Epoch 19, Batch: 10, Loss: 0.933585
Train - Epoch 19, Batch: 20, Loss: 0.917585
Train - Epoch 19, Batch: 30, Loss: 0.917930
Train - Epoch 20, Batch: 0, Loss: 0.931207
Train - Epoch 20, Batch: 10, Loss: 0.922709
Train - Epoch 20, Batch: 20, Loss: 0.932684
Train - Epoch 20, Batch: 30, Loss: 0.932235
Train - Epoch 21, Batch: 0, Loss: 0.919415
Train - Epoch 21, Batch: 10, Loss: 0.922261
Train - Epoch 21, Batch: 20, Loss: 0.927391
Train - Epoch 21, Batch: 30, Loss: 0.927882
Train - Epoch 22, Batch: 0, Loss: 0.914286
Train - Epoch 22, Batch: 10, Loss: 0.914539
Train - Epoch 22, Batch: 20, Loss: 0.918463
Train - Epoch 22, Batch: 30, Loss: 0.915229
Train - Epoch 23, Batch: 0, Loss: 0.916470
Train - Epoch 23, Batch: 10, Loss: 0.918822
Train - Epoch 23, Batch: 20, Loss: 0.914262
Train - Epoch 23, Batch: 30, Loss: 0.916945
Train - Epoch 24, Batch: 0, Loss: 0.913255
Train - Epoch 24, Batch: 10, Loss: 0.913072
Train - Epoch 24, Batch: 20, Loss: 0.909260
Train - Epoch 24, Batch: 30, Loss: 0.906838
Train - Epoch 25, Batch: 0, Loss: 0.907947
Train - Epoch 25, Batch: 10, Loss: 0.909117
Train - Epoch 25, Batch: 20, Loss: 0.918197
Train - Epoch 25, Batch: 30, Loss: 0.897961
Train - Epoch 26, Batch: 0, Loss: 0.905033
Train - Epoch 26, Batch: 10, Loss: 0.910825
Train - Epoch 26, Batch: 20, Loss: 0.903609
Train - Epoch 26, Batch: 30, Loss: 0.916508
Train - Epoch 27, Batch: 0, Loss: 0.911944
Train - Epoch 27, Batch: 10, Loss: 0.897644
Train - Epoch 27, Batch: 20, Loss: 0.906888
Train - Epoch 27, Batch: 30, Loss: 0.903785
Train - Epoch 28, Batch: 0, Loss: 0.908505
Train - Epoch 28, Batch: 10, Loss: 0.900090
Train - Epoch 28, Batch: 20, Loss: 0.895302
Train - Epoch 28, Batch: 30, Loss: 0.893042
Train - Epoch 29, Batch: 0, Loss: 0.893619
Train - Epoch 29, Batch: 10, Loss: 0.903805
Train - Epoch 29, Batch: 20, Loss: 0.890973
Train - Epoch 29, Batch: 30, Loss: 0.895902
Train - Epoch 30, Batch: 0, Loss: 0.894564
Train - Epoch 30, Batch: 10, Loss: 0.888637
Train - Epoch 30, Batch: 20, Loss: 0.893875
Train - Epoch 30, Batch: 30, Loss: 0.903049
Train - Epoch 31, Batch: 0, Loss: 0.899549
Train - Epoch 31, Batch: 10, Loss: 0.900666
Train - Epoch 31, Batch: 20, Loss: 0.910323
Train - Epoch 31, Batch: 30, Loss: 0.899841
training_time:: 4.029339075088501
training time full:: 4.029383420944214
provenance prepare time:: 2.384185791015625e-07
Test Avg. Loss: 0.000070, Accuracy: 0.630648
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 5
training time is 2.0231006145477295
overhead:: 0
overhead2:: 0
time_baseline:: 2.024750232696533
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630648
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 4 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.002911806106567383
overhead3:: 0.027965545654296875
overhead4:: 0.1052556037902832
overhead5:: 0
time_provenance:: 0.4872713088989258
curr_diff: 0 tensor(1.4959e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4959e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630648
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 4 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.003936290740966797
overhead3:: 0.03694939613342285
overhead4:: 0.13919687271118164
overhead5:: 0
time_provenance:: 0.5655138492584229
curr_diff: 0 tensor(2.3823e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.3823e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630648
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 4 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.003613710403442383
overhead3:: 0.05016613006591797
overhead4:: 0.16306781768798828
overhead5:: 0
time_provenance:: 0.5860981941223145
curr_diff: 0 tensor(1.6816e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6816e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630648
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 4 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.006136417388916016
overhead3:: 0.04542398452758789
overhead4:: 0.18122124671936035
overhead5:: 0
time_provenance:: 0.6188557147979736
curr_diff: 0 tensor(2.4236e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.4236e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630648
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 4 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.0063250064849853516
overhead3:: 0.05451774597167969
overhead4:: 0.22905421257019043
overhead5:: 0
time_provenance:: 0.6895780563354492
curr_diff: 0 tensor(9.2215e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.2215e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630648
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 4 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.005594491958618164
overhead3:: 0.061469316482543945
overhead4:: 0.2510972023010254
overhead5:: 0
time_provenance:: 0.7145113945007324
curr_diff: 0 tensor(9.4155e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.4155e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630648
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 4 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.007662296295166016
overhead3:: 0.06962299346923828
overhead4:: 0.2970004081726074
overhead5:: 0
time_provenance:: 0.8969099521636963
curr_diff: 0 tensor(9.4747e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.4747e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630648
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 4 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.006991863250732422
overhead3:: 0.06180691719055176
overhead4:: 0.27431702613830566
overhead5:: 0
time_provenance:: 0.7434370517730713
curr_diff: 0 tensor(9.5111e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.5111e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630648
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 4 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.008400440216064453
overhead3:: 0.08777093887329102
overhead4:: 0.4157865047454834
overhead5:: 0
time_provenance:: 0.956906795501709
curr_diff: 0 tensor(4.5620e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.5620e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630648
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 4 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.008672475814819336
overhead3:: 0.09739899635314941
overhead4:: 0.4015359878540039
overhead5:: 0
time_provenance:: 0.9202916622161865
curr_diff: 0 tensor(4.6267e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.6267e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630648
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 4 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.010081052780151367
overhead3:: 0.09475040435791016
overhead4:: 0.39893150329589844
overhead5:: 0
time_provenance:: 0.9215991497039795
curr_diff: 0 tensor(4.6458e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.6458e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630648
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 4 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.013342142105102539
overhead3:: 0.12708377838134766
overhead4:: 0.4989199638366699
overhead5:: 0
time_provenance:: 1.2146620750427246
curr_diff: 0 tensor(4.6501e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.6501e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630648
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 4 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.02353835105895996
overhead3:: 0.2115027904510498
overhead4:: 0.9847304821014404
overhead5:: 0
time_provenance:: 1.7605910301208496
curr_diff: 0 tensor(7.8013e-07, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.8013e-07, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630648
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 4 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.023101091384887695
overhead3:: 0.20917105674743652
overhead4:: 0.9154717922210693
overhead5:: 0
time_provenance:: 1.6731317043304443
curr_diff: 0 tensor(7.8542e-07, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.8542e-07, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630648
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 4 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.023695707321166992
overhead3:: 0.2099590301513672
overhead4:: 0.9196064472198486
overhead5:: 0
time_provenance:: 1.664050579071045
curr_diff: 0 tensor(8.2323e-07, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.2323e-07, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630648
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 4 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.02771925926208496
overhead3:: 0.23321819305419922
overhead4:: 1.039921760559082
overhead5:: 0
time_provenance:: 1.8976762294769287
curr_diff: 0 tensor(8.1841e-07, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.1841e-07, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630648
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 4 0.00001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5
max_epoch:: 32
overhead:: 0
overhead2:: 0.04380321502685547
overhead3:: 0.39535999298095703
overhead4:: 1.5166985988616943
overhead5:: 0
time_provenance:: 2.162431478500366
curr_diff: 0 tensor(1.1733e-15, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1733e-15, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630648
deletion rate:: 0.00002
python3 generate_rand_ids 0.00002  covtype 0
tensor([389411, 391428,  96645, 127591, 255018, 390617, 270357,  42901, 373849,
        425947])
batch size:: 16384
repetition 0
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 0 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 1.923219
Train - Epoch 0, Batch: 10, Loss: 1.365768
Train - Epoch 0, Batch: 20, Loss: 1.243107
Train - Epoch 0, Batch: 30, Loss: 1.176824
Train - Epoch 1, Batch: 0, Loss: 1.170792
Train - Epoch 1, Batch: 10, Loss: 1.149536
Train - Epoch 1, Batch: 20, Loss: 1.136356
Train - Epoch 1, Batch: 30, Loss: 1.115689
Train - Epoch 2, Batch: 0, Loss: 1.123944
Train - Epoch 2, Batch: 10, Loss: 1.105897
Train - Epoch 2, Batch: 20, Loss: 1.102409
Train - Epoch 2, Batch: 30, Loss: 1.078598
Train - Epoch 3, Batch: 0, Loss: 1.094469
Train - Epoch 3, Batch: 10, Loss: 1.078794
Train - Epoch 3, Batch: 20, Loss: 1.072779
Train - Epoch 3, Batch: 30, Loss: 1.070732
Train - Epoch 4, Batch: 0, Loss: 1.067422
Train - Epoch 4, Batch: 10, Loss: 1.054424
Train - Epoch 4, Batch: 20, Loss: 1.059733
Train - Epoch 4, Batch: 30, Loss: 1.053984
Train - Epoch 5, Batch: 0, Loss: 1.048959
Train - Epoch 5, Batch: 10, Loss: 1.049428
Train - Epoch 5, Batch: 20, Loss: 1.033427
Train - Epoch 5, Batch: 30, Loss: 1.041178
Train - Epoch 6, Batch: 0, Loss: 1.025331
Train - Epoch 6, Batch: 10, Loss: 1.029299
Train - Epoch 6, Batch: 20, Loss: 1.014879
Train - Epoch 6, Batch: 30, Loss: 1.014938
Train - Epoch 7, Batch: 0, Loss: 1.028738
Train - Epoch 7, Batch: 10, Loss: 1.001314
Train - Epoch 7, Batch: 20, Loss: 1.003645
Train - Epoch 7, Batch: 30, Loss: 1.008365
Train - Epoch 8, Batch: 0, Loss: 1.012189
Train - Epoch 8, Batch: 10, Loss: 0.985539
Train - Epoch 8, Batch: 20, Loss: 0.999688
Train - Epoch 8, Batch: 30, Loss: 0.992006
Train - Epoch 9, Batch: 0, Loss: 1.004785
Train - Epoch 9, Batch: 10, Loss: 0.982619
Train - Epoch 9, Batch: 20, Loss: 0.987677
Train - Epoch 9, Batch: 30, Loss: 0.982230
Train - Epoch 10, Batch: 0, Loss: 0.983816
Train - Epoch 10, Batch: 10, Loss: 0.986035
Train - Epoch 10, Batch: 20, Loss: 0.984183
Train - Epoch 10, Batch: 30, Loss: 0.973295
Train - Epoch 11, Batch: 0, Loss: 0.978439
Train - Epoch 11, Batch: 10, Loss: 0.981031
Train - Epoch 11, Batch: 20, Loss: 0.978946
Train - Epoch 11, Batch: 30, Loss: 0.972380
Train - Epoch 12, Batch: 0, Loss: 0.974702
Train - Epoch 12, Batch: 10, Loss: 0.956861
Train - Epoch 12, Batch: 20, Loss: 0.958069
Train - Epoch 12, Batch: 30, Loss: 0.959743
Train - Epoch 13, Batch: 0, Loss: 0.949563
Train - Epoch 13, Batch: 10, Loss: 0.956748
Train - Epoch 13, Batch: 20, Loss: 0.959913
Train - Epoch 13, Batch: 30, Loss: 0.960393
Train - Epoch 14, Batch: 0, Loss: 0.952733
Train - Epoch 14, Batch: 10, Loss: 0.961936
Train - Epoch 14, Batch: 20, Loss: 0.946402
Train - Epoch 14, Batch: 30, Loss: 0.949044
Train - Epoch 15, Batch: 0, Loss: 0.945461
Train - Epoch 15, Batch: 10, Loss: 0.950528
Train - Epoch 15, Batch: 20, Loss: 0.942757
Train - Epoch 15, Batch: 30, Loss: 0.942465
Train - Epoch 16, Batch: 0, Loss: 0.935614
Train - Epoch 16, Batch: 10, Loss: 0.937400
Train - Epoch 16, Batch: 20, Loss: 0.934551
Train - Epoch 16, Batch: 30, Loss: 0.940510
Train - Epoch 17, Batch: 0, Loss: 0.932790
Train - Epoch 17, Batch: 10, Loss: 0.942966
Train - Epoch 17, Batch: 20, Loss: 0.927186
Train - Epoch 17, Batch: 30, Loss: 0.940188
Train - Epoch 18, Batch: 0, Loss: 0.949414
Train - Epoch 18, Batch: 10, Loss: 0.928045
Train - Epoch 18, Batch: 20, Loss: 0.935154
Train - Epoch 18, Batch: 30, Loss: 0.920570
Train - Epoch 19, Batch: 0, Loss: 0.923618
Train - Epoch 19, Batch: 10, Loss: 0.923179
Train - Epoch 19, Batch: 20, Loss: 0.925781
Train - Epoch 19, Batch: 30, Loss: 0.920999
Train - Epoch 20, Batch: 0, Loss: 0.923627
Train - Epoch 20, Batch: 10, Loss: 0.933202
Train - Epoch 20, Batch: 20, Loss: 0.924556
Train - Epoch 20, Batch: 30, Loss: 0.923640
Train - Epoch 21, Batch: 0, Loss: 0.926127
Train - Epoch 21, Batch: 10, Loss: 0.912050
Train - Epoch 21, Batch: 20, Loss: 0.916939
Train - Epoch 21, Batch: 30, Loss: 0.916408
Train - Epoch 22, Batch: 0, Loss: 0.919349
Train - Epoch 22, Batch: 10, Loss: 0.917740
Train - Epoch 22, Batch: 20, Loss: 0.915413
Train - Epoch 22, Batch: 30, Loss: 0.924454
Train - Epoch 23, Batch: 0, Loss: 0.914293
Train - Epoch 23, Batch: 10, Loss: 0.916681
Train - Epoch 23, Batch: 20, Loss: 0.910832
Train - Epoch 23, Batch: 30, Loss: 0.919571
Train - Epoch 24, Batch: 0, Loss: 0.916138
Train - Epoch 24, Batch: 10, Loss: 0.915913
Train - Epoch 24, Batch: 20, Loss: 0.914766
Train - Epoch 24, Batch: 30, Loss: 0.904778
Train - Epoch 25, Batch: 0, Loss: 0.910885
Train - Epoch 25, Batch: 10, Loss: 0.899371
Train - Epoch 25, Batch: 20, Loss: 0.912393
Train - Epoch 25, Batch: 30, Loss: 0.906571
Train - Epoch 26, Batch: 0, Loss: 0.907093
Train - Epoch 26, Batch: 10, Loss: 0.899765
Train - Epoch 26, Batch: 20, Loss: 0.907867
Train - Epoch 26, Batch: 30, Loss: 0.905794
Train - Epoch 27, Batch: 0, Loss: 0.900855
Train - Epoch 27, Batch: 10, Loss: 0.900167
Train - Epoch 27, Batch: 20, Loss: 0.897085
Train - Epoch 27, Batch: 30, Loss: 0.901378
Train - Epoch 28, Batch: 0, Loss: 0.907066
Train - Epoch 28, Batch: 10, Loss: 0.901794
Train - Epoch 28, Batch: 20, Loss: 0.895956
Train - Epoch 28, Batch: 30, Loss: 0.905987
Train - Epoch 29, Batch: 0, Loss: 0.891189
Train - Epoch 29, Batch: 10, Loss: 0.911154
Train - Epoch 29, Batch: 20, Loss: 0.893772
Train - Epoch 29, Batch: 30, Loss: 0.890674
Train - Epoch 30, Batch: 0, Loss: 0.891462
Train - Epoch 30, Batch: 10, Loss: 0.905921
Train - Epoch 30, Batch: 20, Loss: 0.900426
Train - Epoch 30, Batch: 30, Loss: 0.896743
Train - Epoch 31, Batch: 0, Loss: 0.896523
Train - Epoch 31, Batch: 10, Loss: 0.889263
Train - Epoch 31, Batch: 20, Loss: 0.888618
Train - Epoch 31, Batch: 30, Loss: 0.888425
training_time:: 3.320338010787964
training time full:: 3.320378541946411
provenance prepare time:: 4.76837158203125e-07
Test Avg. Loss: 0.000070, Accuracy: 0.630493
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 10
training time is 2.247523069381714
overhead:: 0
overhead2:: 0
time_baseline:: 2.2492756843566895
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630493
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.0042531490325927734
overhead3:: 0.03890061378479004
overhead4:: 0.1330583095550537
overhead5:: 0
time_provenance:: 0.6152534484863281
curr_diff: 0 tensor(2.3481e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.3481e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630493
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.004677295684814453
overhead3:: 0.03984355926513672
overhead4:: 0.16198110580444336
overhead5:: 0
time_provenance:: 0.6332442760467529
curr_diff: 0 tensor(2.4872e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.4872e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630493
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.005476713180541992
overhead3:: 0.05407404899597168
overhead4:: 0.13214969635009766
overhead5:: 0
time_provenance:: 0.6048352718353271
curr_diff: 0 tensor(2.3872e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.3872e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630493
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.006371974945068359
overhead3:: 0.04091691970825195
overhead4:: 0.15674304962158203
overhead5:: 0
time_provenance:: 0.6647624969482422
curr_diff: 0 tensor(2.9482e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.9482e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630493
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.006848573684692383
overhead3:: 0.046975135803222656
overhead4:: 0.2340095043182373
overhead5:: 0
time_provenance:: 0.7359626293182373
curr_diff: 0 tensor(1.0496e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0496e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630493
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.007044076919555664
overhead3:: 0.04862475395202637
overhead4:: 0.21756649017333984
overhead5:: 0
time_provenance:: 0.7124621868133545
curr_diff: 0 tensor(1.0793e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0793e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630493
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.010614633560180664
overhead3:: 0.08142805099487305
overhead4:: 0.27939820289611816
overhead5:: 0
time_provenance:: 0.8531742095947266
curr_diff: 0 tensor(1.1089e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1089e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630493
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.009168148040771484
overhead3:: 0.06369256973266602
overhead4:: 0.2853100299835205
overhead5:: 0
time_provenance:: 0.8069806098937988
curr_diff: 0 tensor(1.1070e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1070e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630493
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.01407480239868164
overhead3:: 0.09017658233642578
overhead4:: 0.41019511222839355
overhead5:: 0
time_provenance:: 1.0171184539794922
curr_diff: 0 tensor(4.1485e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.1485e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630493
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.013392210006713867
overhead3:: 0.1031653881072998
overhead4:: 0.412445068359375
overhead5:: 0
time_provenance:: 1.0056629180908203
curr_diff: 0 tensor(4.2114e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.2114e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630493
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.015333175659179688
overhead3:: 0.09589958190917969
overhead4:: 0.4060027599334717
overhead5:: 0
time_provenance:: 1.0140726566314697
curr_diff: 0 tensor(4.2733e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.2733e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630493
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.019295692443847656
overhead3:: 0.11775016784667969
overhead4:: 0.5058622360229492
overhead5:: 0
time_provenance:: 1.2564539909362793
curr_diff: 0 tensor(4.2627e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.2627e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630493
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.055419921875
overhead3:: 0.26477956771850586
overhead4:: 1.1622967720031738
overhead5:: 0
time_provenance:: 2.250108242034912
curr_diff: 0 tensor(1.2548e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2548e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630493
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.04274106025695801
overhead3:: 0.21982479095458984
overhead4:: 0.9856357574462891
overhead5:: 0
time_provenance:: 1.801074504852295
curr_diff: 0 tensor(1.2553e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2553e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630493
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.04475045204162598
overhead3:: 0.21270370483398438
overhead4:: 0.973015308380127
overhead5:: 0
time_provenance:: 1.7874975204467773
curr_diff: 0 tensor(1.2667e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2667e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630493
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.055983781814575195
overhead3:: 0.2818763256072998
overhead4:: 1.140808343887329
overhead5:: 0
time_provenance:: 2.1752705574035645
curr_diff: 0 tensor(1.2678e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2678e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630493
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.09380531311035156
overhead3:: 0.5373742580413818
overhead4:: 1.6843304634094238
overhead5:: 0
time_provenance:: 2.5360262393951416
curr_diff: 0 tensor(1.0578e-15, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0578e-15, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630493
repetition 1
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 1 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.010373
Train - Epoch 0, Batch: 10, Loss: 1.398579
Train - Epoch 0, Batch: 20, Loss: 1.255179
Train - Epoch 0, Batch: 30, Loss: 1.195620
Train - Epoch 1, Batch: 0, Loss: 1.191843
Train - Epoch 1, Batch: 10, Loss: 1.165769
Train - Epoch 1, Batch: 20, Loss: 1.158231
Train - Epoch 1, Batch: 30, Loss: 1.141918
Train - Epoch 2, Batch: 0, Loss: 1.143382
Train - Epoch 2, Batch: 10, Loss: 1.123571
Train - Epoch 2, Batch: 20, Loss: 1.115676
Train - Epoch 2, Batch: 30, Loss: 1.107150
Train - Epoch 3, Batch: 0, Loss: 1.123732
Train - Epoch 3, Batch: 10, Loss: 1.095029
Train - Epoch 3, Batch: 20, Loss: 1.082396
Train - Epoch 3, Batch: 30, Loss: 1.076065
Train - Epoch 4, Batch: 0, Loss: 1.078306
Train - Epoch 4, Batch: 10, Loss: 1.066671
Train - Epoch 4, Batch: 20, Loss: 1.065521
Train - Epoch 4, Batch: 30, Loss: 1.053571
Train - Epoch 5, Batch: 0, Loss: 1.054909
Train - Epoch 5, Batch: 10, Loss: 1.056496
Train - Epoch 5, Batch: 20, Loss: 1.059170
Train - Epoch 5, Batch: 30, Loss: 1.041539
Train - Epoch 6, Batch: 0, Loss: 1.031474
Train - Epoch 6, Batch: 10, Loss: 1.036261
Train - Epoch 6, Batch: 20, Loss: 1.017304
Train - Epoch 6, Batch: 30, Loss: 1.020109
Train - Epoch 7, Batch: 0, Loss: 1.019493
Train - Epoch 7, Batch: 10, Loss: 1.019628
Train - Epoch 7, Batch: 20, Loss: 1.009831
Train - Epoch 7, Batch: 30, Loss: 1.011742
Train - Epoch 8, Batch: 0, Loss: 1.009955
Train - Epoch 8, Batch: 10, Loss: 1.010057
Train - Epoch 8, Batch: 20, Loss: 0.990831
Train - Epoch 8, Batch: 30, Loss: 1.006066
Train - Epoch 9, Batch: 0, Loss: 0.997310
Train - Epoch 9, Batch: 10, Loss: 0.997408
Train - Epoch 9, Batch: 20, Loss: 0.994344
Train - Epoch 9, Batch: 30, Loss: 0.986844
Train - Epoch 10, Batch: 0, Loss: 0.995739
Train - Epoch 10, Batch: 10, Loss: 0.987462
Train - Epoch 10, Batch: 20, Loss: 0.982280
Train - Epoch 10, Batch: 30, Loss: 0.980591
Train - Epoch 11, Batch: 0, Loss: 0.983406
Train - Epoch 11, Batch: 10, Loss: 0.966635
Train - Epoch 11, Batch: 20, Loss: 0.976226
Train - Epoch 11, Batch: 30, Loss: 0.987630
Train - Epoch 12, Batch: 0, Loss: 0.967790
Train - Epoch 12, Batch: 10, Loss: 0.961673
Train - Epoch 12, Batch: 20, Loss: 0.969015
Train - Epoch 12, Batch: 30, Loss: 0.973413
Train - Epoch 13, Batch: 0, Loss: 0.966221
Train - Epoch 13, Batch: 10, Loss: 0.966578
Train - Epoch 13, Batch: 20, Loss: 0.958675
Train - Epoch 13, Batch: 30, Loss: 0.957003
Train - Epoch 14, Batch: 0, Loss: 0.956959
Train - Epoch 14, Batch: 10, Loss: 0.951843
Train - Epoch 14, Batch: 20, Loss: 0.950259
Train - Epoch 14, Batch: 30, Loss: 0.953328
Train - Epoch 15, Batch: 0, Loss: 0.942262
Train - Epoch 15, Batch: 10, Loss: 0.946238
Train - Epoch 15, Batch: 20, Loss: 0.945628
Train - Epoch 15, Batch: 30, Loss: 0.951787
Train - Epoch 16, Batch: 0, Loss: 0.947924
Train - Epoch 16, Batch: 10, Loss: 0.958718
Train - Epoch 16, Batch: 20, Loss: 0.946053
Train - Epoch 16, Batch: 30, Loss: 0.936828
Train - Epoch 17, Batch: 0, Loss: 0.932027
Train - Epoch 17, Batch: 10, Loss: 0.935641
Train - Epoch 17, Batch: 20, Loss: 0.942216
Train - Epoch 17, Batch: 30, Loss: 0.937053
Train - Epoch 18, Batch: 0, Loss: 0.939942
Train - Epoch 18, Batch: 10, Loss: 0.935429
Train - Epoch 18, Batch: 20, Loss: 0.932154
Train - Epoch 18, Batch: 30, Loss: 0.935034
Train - Epoch 19, Batch: 0, Loss: 0.941671
Train - Epoch 19, Batch: 10, Loss: 0.926798
Train - Epoch 19, Batch: 20, Loss: 0.935524
Train - Epoch 19, Batch: 30, Loss: 0.932460
Train - Epoch 20, Batch: 0, Loss: 0.934329
Train - Epoch 20, Batch: 10, Loss: 0.929660
Train - Epoch 20, Batch: 20, Loss: 0.923295
Train - Epoch 20, Batch: 30, Loss: 0.919298
Train - Epoch 21, Batch: 0, Loss: 0.917825
Train - Epoch 21, Batch: 10, Loss: 0.928880
Train - Epoch 21, Batch: 20, Loss: 0.925402
Train - Epoch 21, Batch: 30, Loss: 0.918811
Train - Epoch 22, Batch: 0, Loss: 0.915084
Train - Epoch 22, Batch: 10, Loss: 0.918992
Train - Epoch 22, Batch: 20, Loss: 0.919076
Train - Epoch 22, Batch: 30, Loss: 0.913393
Train - Epoch 23, Batch: 0, Loss: 0.921421
Train - Epoch 23, Batch: 10, Loss: 0.916690
Train - Epoch 23, Batch: 20, Loss: 0.919836
Train - Epoch 23, Batch: 30, Loss: 0.914644
Train - Epoch 24, Batch: 0, Loss: 0.912234
Train - Epoch 24, Batch: 10, Loss: 0.911199
Train - Epoch 24, Batch: 20, Loss: 0.907052
Train - Epoch 24, Batch: 30, Loss: 0.908709
Train - Epoch 25, Batch: 0, Loss: 0.919616
Train - Epoch 25, Batch: 10, Loss: 0.911008
Train - Epoch 25, Batch: 20, Loss: 0.908149
Train - Epoch 25, Batch: 30, Loss: 0.912743
Train - Epoch 26, Batch: 0, Loss: 0.903171
Train - Epoch 26, Batch: 10, Loss: 0.913869
Train - Epoch 26, Batch: 20, Loss: 0.911680
Train - Epoch 26, Batch: 30, Loss: 0.907987
Train - Epoch 27, Batch: 0, Loss: 0.904380
Train - Epoch 27, Batch: 10, Loss: 0.910414
Train - Epoch 27, Batch: 20, Loss: 0.902828
Train - Epoch 27, Batch: 30, Loss: 0.900754
Train - Epoch 28, Batch: 0, Loss: 0.896820
Train - Epoch 28, Batch: 10, Loss: 0.905047
Train - Epoch 28, Batch: 20, Loss: 0.906139
Train - Epoch 28, Batch: 30, Loss: 0.907812
Train - Epoch 29, Batch: 0, Loss: 0.897038
Train - Epoch 29, Batch: 10, Loss: 0.901055
Train - Epoch 29, Batch: 20, Loss: 0.906208
Train - Epoch 29, Batch: 30, Loss: 0.897418
Train - Epoch 30, Batch: 0, Loss: 0.896797
Train - Epoch 30, Batch: 10, Loss: 0.898793
Train - Epoch 30, Batch: 20, Loss: 0.897072
Train - Epoch 30, Batch: 30, Loss: 0.902333
Train - Epoch 31, Batch: 0, Loss: 0.895165
Train - Epoch 31, Batch: 10, Loss: 0.898276
Train - Epoch 31, Batch: 20, Loss: 0.891393
Train - Epoch 31, Batch: 30, Loss: 0.891971
training_time:: 3.3937625885009766
training time full:: 3.3938040733337402
provenance prepare time:: 4.76837158203125e-07
Test Avg. Loss: 0.000070, Accuracy: 0.629064
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 10
training time is 2.2679443359375
overhead:: 0
overhead2:: 0
time_baseline:: 2.2695205211639404
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629064
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.005670070648193359
overhead3:: 0.03796839714050293
overhead4:: 0.13317441940307617
overhead5:: 0
time_provenance:: 0.6018545627593994
curr_diff: 0 tensor(3.0326e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.0326e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629064
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.006473541259765625
overhead3:: 0.0409083366394043
overhead4:: 0.14220523834228516
overhead5:: 0
time_provenance:: 0.6124157905578613
curr_diff: 0 tensor(2.7984e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.7984e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629064
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.0076634883880615234
overhead3:: 0.04397249221801758
overhead4:: 0.14934206008911133
overhead5:: 0
time_provenance:: 0.6057980060577393
curr_diff: 0 tensor(3.2889e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.2889e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629064
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.008202791213989258
overhead3:: 0.039522409439086914
overhead4:: 0.1485917568206787
overhead5:: 0
time_provenance:: 0.6426656246185303
curr_diff: 0 tensor(3.0962e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.0962e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629064
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.01025843620300293
overhead3:: 0.05042219161987305
overhead4:: 0.24003243446350098
overhead5:: 0
time_provenance:: 0.7494280338287354
curr_diff: 0 tensor(1.3436e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3436e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629064
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.010738372802734375
overhead3:: 0.05208706855773926
overhead4:: 0.27025723457336426
overhead5:: 0
time_provenance:: 0.7771000862121582
curr_diff: 0 tensor(1.3564e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3564e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629064
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.011388778686523438
overhead3:: 0.052878618240356445
overhead4:: 0.23238396644592285
overhead5:: 0
time_provenance:: 0.7290980815887451
curr_diff: 0 tensor(1.4450e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4450e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629064
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.012361526489257812
overhead3:: 0.058011770248413086
overhead4:: 0.2673947811126709
overhead5:: 0
time_provenance:: 0.7870476245880127
curr_diff: 0 tensor(1.4781e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4781e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629064
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.018934011459350586
overhead3:: 0.09181451797485352
overhead4:: 0.4292874336242676
overhead5:: 0
time_provenance:: 1.0311777591705322
curr_diff: 0 tensor(5.3211e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.3211e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629064
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.020597457885742188
overhead3:: 0.09020256996154785
overhead4:: 0.4293367862701416
overhead5:: 0
time_provenance:: 1.020533561706543
curr_diff: 0 tensor(5.4256e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.4256e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629064
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.020562171936035156
overhead3:: 0.1024627685546875
overhead4:: 0.4446985721588135
overhead5:: 0
time_provenance:: 1.0438973903656006
curr_diff: 0 tensor(5.7547e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.7547e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629064
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.02103424072265625
overhead3:: 0.12033486366271973
overhead4:: 0.4177532196044922
overhead5:: 0
time_provenance:: 1.0353796482086182
curr_diff: 0 tensor(5.7807e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.7807e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629064
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.04130268096923828
overhead3:: 0.20734763145446777
overhead4:: 0.8980717658996582
overhead5:: 0
time_provenance:: 1.7018370628356934
curr_diff: 0 tensor(1.3937e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3937e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629064
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.04295849800109863
overhead3:: 0.2256464958190918
overhead4:: 0.980391263961792
overhead5:: 0
time_provenance:: 1.8045365810394287
curr_diff: 0 tensor(1.4031e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4031e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629064
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.056137800216674805
overhead3:: 0.2765629291534424
overhead4:: 1.1818580627441406
overhead5:: 0
time_provenance:: 2.272099018096924
curr_diff: 0 tensor(1.4220e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4220e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629064
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.04059910774230957
overhead3:: 0.22028517723083496
overhead4:: 0.9858713150024414
overhead5:: 0
time_provenance:: 1.7985270023345947
curr_diff: 0 tensor(1.4385e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4385e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629064
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.08460187911987305
overhead3:: 0.4388141632080078
overhead4:: 1.5614445209503174
overhead5:: 0
time_provenance:: 2.3114545345306396
curr_diff: 0 tensor(1.4336e-15, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4336e-15, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629064
repetition 2
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 1.865510
Train - Epoch 0, Batch: 10, Loss: 1.377976
Train - Epoch 0, Batch: 20, Loss: 1.244082
Train - Epoch 0, Batch: 30, Loss: 1.185947
Train - Epoch 1, Batch: 0, Loss: 1.192959
Train - Epoch 1, Batch: 10, Loss: 1.157603
Train - Epoch 1, Batch: 20, Loss: 1.149525
Train - Epoch 1, Batch: 30, Loss: 1.129304
Train - Epoch 2, Batch: 0, Loss: 1.138439
Train - Epoch 2, Batch: 10, Loss: 1.128848
Train - Epoch 2, Batch: 20, Loss: 1.108077
Train - Epoch 2, Batch: 30, Loss: 1.099880
Train - Epoch 3, Batch: 0, Loss: 1.110852
Train - Epoch 3, Batch: 10, Loss: 1.096670
Train - Epoch 3, Batch: 20, Loss: 1.089458
Train - Epoch 3, Batch: 30, Loss: 1.074840
Train - Epoch 4, Batch: 0, Loss: 1.084403
Train - Epoch 4, Batch: 10, Loss: 1.074768
Train - Epoch 4, Batch: 20, Loss: 1.069957
Train - Epoch 4, Batch: 30, Loss: 1.059455
Train - Epoch 5, Batch: 0, Loss: 1.058157
Train - Epoch 5, Batch: 10, Loss: 1.054468
Train - Epoch 5, Batch: 20, Loss: 1.043982
Train - Epoch 5, Batch: 30, Loss: 1.050839
Train - Epoch 6, Batch: 0, Loss: 1.039654
Train - Epoch 6, Batch: 10, Loss: 1.041959
Train - Epoch 6, Batch: 20, Loss: 1.030225
Train - Epoch 6, Batch: 30, Loss: 1.028758
Train - Epoch 7, Batch: 0, Loss: 1.031224
Train - Epoch 7, Batch: 10, Loss: 1.016468
Train - Epoch 7, Batch: 20, Loss: 1.020521
Train - Epoch 7, Batch: 30, Loss: 1.015451
Train - Epoch 8, Batch: 0, Loss: 1.005448
Train - Epoch 8, Batch: 10, Loss: 1.017792
Train - Epoch 8, Batch: 20, Loss: 1.002241
Train - Epoch 8, Batch: 30, Loss: 1.015527
Train - Epoch 9, Batch: 0, Loss: 1.009990
Train - Epoch 9, Batch: 10, Loss: 0.998804
Train - Epoch 9, Batch: 20, Loss: 0.995986
Train - Epoch 9, Batch: 30, Loss: 0.989360
Train - Epoch 10, Batch: 0, Loss: 0.994641
Train - Epoch 10, Batch: 10, Loss: 0.983066
Train - Epoch 10, Batch: 20, Loss: 0.987082
Train - Epoch 10, Batch: 30, Loss: 0.980591
Train - Epoch 11, Batch: 0, Loss: 0.984096
Train - Epoch 11, Batch: 10, Loss: 0.983639
Train - Epoch 11, Batch: 20, Loss: 0.985280
Train - Epoch 11, Batch: 30, Loss: 0.974721
Train - Epoch 12, Batch: 0, Loss: 0.981719
Train - Epoch 12, Batch: 10, Loss: 0.968894
Train - Epoch 12, Batch: 20, Loss: 0.967571
Train - Epoch 12, Batch: 30, Loss: 0.958522
Train - Epoch 13, Batch: 0, Loss: 0.975146
Train - Epoch 13, Batch: 10, Loss: 0.967183
Train - Epoch 13, Batch: 20, Loss: 0.967372
Train - Epoch 13, Batch: 30, Loss: 0.963287
Train - Epoch 14, Batch: 0, Loss: 0.953508
Train - Epoch 14, Batch: 10, Loss: 0.967094
Train - Epoch 14, Batch: 20, Loss: 0.948856
Train - Epoch 14, Batch: 30, Loss: 0.957931
Train - Epoch 15, Batch: 0, Loss: 0.951911
Train - Epoch 15, Batch: 10, Loss: 0.956394
Train - Epoch 15, Batch: 20, Loss: 0.964110
Train - Epoch 15, Batch: 30, Loss: 0.949244
Train - Epoch 16, Batch: 0, Loss: 0.943732
Train - Epoch 16, Batch: 10, Loss: 0.955900
Train - Epoch 16, Batch: 20, Loss: 0.948839
Train - Epoch 16, Batch: 30, Loss: 0.944448
Train - Epoch 17, Batch: 0, Loss: 0.945229
Train - Epoch 17, Batch: 10, Loss: 0.943558
Train - Epoch 17, Batch: 20, Loss: 0.947697
Train - Epoch 17, Batch: 30, Loss: 0.948433
Train - Epoch 18, Batch: 0, Loss: 0.942235
Train - Epoch 18, Batch: 10, Loss: 0.947604
Train - Epoch 18, Batch: 20, Loss: 0.939802
Train - Epoch 18, Batch: 30, Loss: 0.933105
Train - Epoch 19, Batch: 0, Loss: 0.933496
Train - Epoch 19, Batch: 10, Loss: 0.929908
Train - Epoch 19, Batch: 20, Loss: 0.925825
Train - Epoch 19, Batch: 30, Loss: 0.930794
Train - Epoch 20, Batch: 0, Loss: 0.935354
Train - Epoch 20, Batch: 10, Loss: 0.924456
Train - Epoch 20, Batch: 20, Loss: 0.931943
Train - Epoch 20, Batch: 30, Loss: 0.933657
Train - Epoch 21, Batch: 0, Loss: 0.924508
Train - Epoch 21, Batch: 10, Loss: 0.928300
Train - Epoch 21, Batch: 20, Loss: 0.925865
Train - Epoch 21, Batch: 30, Loss: 0.914468
Train - Epoch 22, Batch: 0, Loss: 0.916202
Train - Epoch 22, Batch: 10, Loss: 0.921441
Train - Epoch 22, Batch: 20, Loss: 0.925204
Train - Epoch 22, Batch: 30, Loss: 0.908119
Train - Epoch 23, Batch: 0, Loss: 0.923159
Train - Epoch 23, Batch: 10, Loss: 0.919530
Train - Epoch 23, Batch: 20, Loss: 0.925631
Train - Epoch 23, Batch: 30, Loss: 0.902454
Train - Epoch 24, Batch: 0, Loss: 0.916427
Train - Epoch 24, Batch: 10, Loss: 0.900103
Train - Epoch 24, Batch: 20, Loss: 0.916063
Train - Epoch 24, Batch: 30, Loss: 0.903978
Train - Epoch 25, Batch: 0, Loss: 0.913564
Train - Epoch 25, Batch: 10, Loss: 0.902671
Train - Epoch 25, Batch: 20, Loss: 0.909420
Train - Epoch 25, Batch: 30, Loss: 0.904123
Train - Epoch 26, Batch: 0, Loss: 0.904538
Train - Epoch 26, Batch: 10, Loss: 0.902273
Train - Epoch 26, Batch: 20, Loss: 0.903828
Train - Epoch 26, Batch: 30, Loss: 0.909945
Train - Epoch 27, Batch: 0, Loss: 0.894659
Train - Epoch 27, Batch: 10, Loss: 0.906562
Train - Epoch 27, Batch: 20, Loss: 0.909594
Train - Epoch 27, Batch: 30, Loss: 0.904597
Train - Epoch 28, Batch: 0, Loss: 0.903425
Train - Epoch 28, Batch: 10, Loss: 0.903471
Train - Epoch 28, Batch: 20, Loss: 0.901715
Train - Epoch 28, Batch: 30, Loss: 0.898813
Train - Epoch 29, Batch: 0, Loss: 0.901573
Train - Epoch 29, Batch: 10, Loss: 0.900678
Train - Epoch 29, Batch: 20, Loss: 0.906694
Train - Epoch 29, Batch: 30, Loss: 0.896525
Train - Epoch 30, Batch: 0, Loss: 0.909569
Train - Epoch 30, Batch: 10, Loss: 0.904933
Train - Epoch 30, Batch: 20, Loss: 0.900070
Train - Epoch 30, Batch: 30, Loss: 0.893533
Train - Epoch 31, Batch: 0, Loss: 0.899945
Train - Epoch 31, Batch: 10, Loss: 0.898645
Train - Epoch 31, Batch: 20, Loss: 0.899634
Train - Epoch 31, Batch: 30, Loss: 0.894420
training_time:: 3.7411201000213623
training time full:: 3.741161584854126
provenance prepare time:: 1.1920928955078125e-06
Test Avg. Loss: 0.000070, Accuracy: 0.629426
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 10
training time is 2.393179416656494
overhead:: 0
overhead2:: 0
time_baseline:: 2.3950002193450928
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629408
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.004860877990722656
overhead3:: 0.02884387969970703
overhead4:: 0.13635921478271484
overhead5:: 0
time_provenance:: 0.6034598350524902
curr_diff: 0 tensor(2.7343e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.7343e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629408
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.006970643997192383
overhead3:: 0.0420069694519043
overhead4:: 0.1314857006072998
overhead5:: 0
time_provenance:: 0.5988311767578125
curr_diff: 0 tensor(2.6646e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.6646e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629408
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.006541728973388672
overhead3:: 0.05366349220275879
overhead4:: 0.15862202644348145
overhead5:: 0
time_provenance:: 0.6545255184173584
curr_diff: 0 tensor(2.7914e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.7914e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629408
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.007634639739990234
overhead3:: 0.04944109916687012
overhead4:: 0.1651620864868164
overhead5:: 0
time_provenance:: 0.6589508056640625
curr_diff: 0 tensor(2.6011e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.6011e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629408
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.009734153747558594
overhead3:: 0.04990196228027344
overhead4:: 0.24954819679260254
overhead5:: 0
time_provenance:: 0.7628846168518066
curr_diff: 0 tensor(1.1630e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1630e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629408
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.01012730598449707
overhead3:: 0.051717281341552734
overhead4:: 0.23085761070251465
overhead5:: 0
time_provenance:: 0.7186267375946045
curr_diff: 0 tensor(1.1813e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1813e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629408
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.012833833694458008
overhead3:: 0.06743669509887695
overhead4:: 0.30596160888671875
overhead5:: 0
time_provenance:: 0.9851970672607422
curr_diff: 0 tensor(1.1806e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1806e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629408
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.010757923126220703
overhead3:: 0.06268763542175293
overhead4:: 0.27325868606567383
overhead5:: 0
time_provenance:: 0.7696807384490967
curr_diff: 0 tensor(1.1900e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1900e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629408
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.016898632049560547
overhead3:: 0.11068177223205566
overhead4:: 0.4131617546081543
overhead5:: 0
time_provenance:: 1.0334882736206055
curr_diff: 0 tensor(4.9700e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.9700e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629408
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.01681041717529297
overhead3:: 0.09116506576538086
overhead4:: 0.43323683738708496
overhead5:: 0
time_provenance:: 1.0278642177581787
curr_diff: 0 tensor(5.0502e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.0502e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629408
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.017492055892944336
overhead3:: 0.10759305953979492
overhead4:: 0.4460892677307129
overhead5:: 0
time_provenance:: 1.0702354907989502
curr_diff: 0 tensor(5.0544e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.0544e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629408
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.017786741256713867
overhead3:: 0.1121835708618164
overhead4:: 0.4429328441619873
overhead5:: 0
time_provenance:: 1.0686016082763672
curr_diff: 0 tensor(5.0826e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.0826e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629408
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.040042877197265625
overhead3:: 0.21404385566711426
overhead4:: 0.975792407989502
overhead5:: 0
time_provenance:: 1.7978715896606445
curr_diff: 0 tensor(1.4251e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4251e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629408
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.04104351997375488
overhead3:: 0.2167520523071289
overhead4:: 0.9163718223571777
overhead5:: 0
time_provenance:: 1.7279181480407715
curr_diff: 0 tensor(1.4411e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4411e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629408
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.03924155235290527
overhead3:: 0.2071056365966797
overhead4:: 0.9360148906707764
overhead5:: 0
time_provenance:: 1.742645263671875
curr_diff: 0 tensor(1.4414e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4414e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629408
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.059047698974609375
overhead3:: 0.28498196601867676
overhead4:: 1.1905274391174316
overhead5:: 0
time_provenance:: 2.2729315757751465
curr_diff: 0 tensor(1.4462e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4462e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629408
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.0847470760345459
overhead3:: 0.4448385238647461
overhead4:: 1.5890028476715088
overhead5:: 0
time_provenance:: 2.3485164642333984
curr_diff: 0 tensor(1.1291e-15, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1291e-15, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629408
repetition 3
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 3 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.006772
Train - Epoch 0, Batch: 10, Loss: 1.405289
Train - Epoch 0, Batch: 20, Loss: 1.251329
Train - Epoch 0, Batch: 30, Loss: 1.205563
Train - Epoch 1, Batch: 0, Loss: 1.198085
Train - Epoch 1, Batch: 10, Loss: 1.163840
Train - Epoch 1, Batch: 20, Loss: 1.147870
Train - Epoch 1, Batch: 30, Loss: 1.142316
Train - Epoch 2, Batch: 0, Loss: 1.128342
Train - Epoch 2, Batch: 10, Loss: 1.129657
Train - Epoch 2, Batch: 20, Loss: 1.120893
Train - Epoch 2, Batch: 30, Loss: 1.105274
Train - Epoch 3, Batch: 0, Loss: 1.100538
Train - Epoch 3, Batch: 10, Loss: 1.089531
Train - Epoch 3, Batch: 20, Loss: 1.081192
Train - Epoch 3, Batch: 30, Loss: 1.078154
Train - Epoch 4, Batch: 0, Loss: 1.078274
Train - Epoch 4, Batch: 10, Loss: 1.078248
Train - Epoch 4, Batch: 20, Loss: 1.073747
Train - Epoch 4, Batch: 30, Loss: 1.060494
Train - Epoch 5, Batch: 0, Loss: 1.061979
Train - Epoch 5, Batch: 10, Loss: 1.054036
Train - Epoch 5, Batch: 20, Loss: 1.040642
Train - Epoch 5, Batch: 30, Loss: 1.045923
Train - Epoch 6, Batch: 0, Loss: 1.038789
Train - Epoch 6, Batch: 10, Loss: 1.041087
Train - Epoch 6, Batch: 20, Loss: 1.034140
Train - Epoch 6, Batch: 30, Loss: 1.020770
Train - Epoch 7, Batch: 0, Loss: 1.021893
Train - Epoch 7, Batch: 10, Loss: 1.018669
Train - Epoch 7, Batch: 20, Loss: 1.021681
Train - Epoch 7, Batch: 30, Loss: 1.007793
Train - Epoch 8, Batch: 0, Loss: 1.013725
Train - Epoch 8, Batch: 10, Loss: 1.009892
Train - Epoch 8, Batch: 20, Loss: 0.996268
Train - Epoch 8, Batch: 30, Loss: 0.997696
Train - Epoch 9, Batch: 0, Loss: 0.987853
Train - Epoch 9, Batch: 10, Loss: 1.004500
Train - Epoch 9, Batch: 20, Loss: 0.986587
Train - Epoch 9, Batch: 30, Loss: 0.985747
Train - Epoch 10, Batch: 0, Loss: 0.987334
Train - Epoch 10, Batch: 10, Loss: 0.980947
Train - Epoch 10, Batch: 20, Loss: 0.986396
Train - Epoch 10, Batch: 30, Loss: 0.982129
Train - Epoch 11, Batch: 0, Loss: 0.979902
Train - Epoch 11, Batch: 10, Loss: 0.988433
Train - Epoch 11, Batch: 20, Loss: 0.977665
Train - Epoch 11, Batch: 30, Loss: 0.979186
Train - Epoch 12, Batch: 0, Loss: 0.988034
Train - Epoch 12, Batch: 10, Loss: 0.957970
Train - Epoch 12, Batch: 20, Loss: 0.962919
Train - Epoch 12, Batch: 30, Loss: 0.976210
Train - Epoch 13, Batch: 0, Loss: 0.961517
Train - Epoch 13, Batch: 10, Loss: 0.961080
Train - Epoch 13, Batch: 20, Loss: 0.955274
Train - Epoch 13, Batch: 30, Loss: 0.948192
Train - Epoch 14, Batch: 0, Loss: 0.963457
Train - Epoch 14, Batch: 10, Loss: 0.960615
Train - Epoch 14, Batch: 20, Loss: 0.948793
Train - Epoch 14, Batch: 30, Loss: 0.947180
Train - Epoch 15, Batch: 0, Loss: 0.952560
Train - Epoch 15, Batch: 10, Loss: 0.955398
Train - Epoch 15, Batch: 20, Loss: 0.957650
Train - Epoch 15, Batch: 30, Loss: 0.955214
Train - Epoch 16, Batch: 0, Loss: 0.947345
Train - Epoch 16, Batch: 10, Loss: 0.939352
Train - Epoch 16, Batch: 20, Loss: 0.939995
Train - Epoch 16, Batch: 30, Loss: 0.942665
Train - Epoch 17, Batch: 0, Loss: 0.935474
Train - Epoch 17, Batch: 10, Loss: 0.943947
Train - Epoch 17, Batch: 20, Loss: 0.939954
Train - Epoch 17, Batch: 30, Loss: 0.952787
Train - Epoch 18, Batch: 0, Loss: 0.938466
Train - Epoch 18, Batch: 10, Loss: 0.939680
Train - Epoch 18, Batch: 20, Loss: 0.934988
Train - Epoch 18, Batch: 30, Loss: 0.932557
Train - Epoch 19, Batch: 0, Loss: 0.931631
Train - Epoch 19, Batch: 10, Loss: 0.932382
Train - Epoch 19, Batch: 20, Loss: 0.934997
Train - Epoch 19, Batch: 30, Loss: 0.929034
Train - Epoch 20, Batch: 0, Loss: 0.929932
Train - Epoch 20, Batch: 10, Loss: 0.923015
Train - Epoch 20, Batch: 20, Loss: 0.932110
Train - Epoch 20, Batch: 30, Loss: 0.923878
Train - Epoch 21, Batch: 0, Loss: 0.917360
Train - Epoch 21, Batch: 10, Loss: 0.927897
Train - Epoch 21, Batch: 20, Loss: 0.922242
Train - Epoch 21, Batch: 30, Loss: 0.924208
Train - Epoch 22, Batch: 0, Loss: 0.917873
Train - Epoch 22, Batch: 10, Loss: 0.924227
Train - Epoch 22, Batch: 20, Loss: 0.919823
Train - Epoch 22, Batch: 30, Loss: 0.923393
Train - Epoch 23, Batch: 0, Loss: 0.932258
Train - Epoch 23, Batch: 10, Loss: 0.920086
Train - Epoch 23, Batch: 20, Loss: 0.921418
Train - Epoch 23, Batch: 30, Loss: 0.913388
Train - Epoch 24, Batch: 0, Loss: 0.918504
Train - Epoch 24, Batch: 10, Loss: 0.920053
Train - Epoch 24, Batch: 20, Loss: 0.910623
Train - Epoch 24, Batch: 30, Loss: 0.910512
Train - Epoch 25, Batch: 0, Loss: 0.912744
Train - Epoch 25, Batch: 10, Loss: 0.904453
Train - Epoch 25, Batch: 20, Loss: 0.920333
Train - Epoch 25, Batch: 30, Loss: 0.908982
Train - Epoch 26, Batch: 0, Loss: 0.909061
Train - Epoch 26, Batch: 10, Loss: 0.914521
Train - Epoch 26, Batch: 20, Loss: 0.901487
Train - Epoch 26, Batch: 30, Loss: 0.900150
Train - Epoch 27, Batch: 0, Loss: 0.910022
Train - Epoch 27, Batch: 10, Loss: 0.907158
Train - Epoch 27, Batch: 20, Loss: 0.902453
Train - Epoch 27, Batch: 30, Loss: 0.898088
Train - Epoch 28, Batch: 0, Loss: 0.901811
Train - Epoch 28, Batch: 10, Loss: 0.904465
Train - Epoch 28, Batch: 20, Loss: 0.896234
Train - Epoch 28, Batch: 30, Loss: 0.903479
Train - Epoch 29, Batch: 0, Loss: 0.910767
Train - Epoch 29, Batch: 10, Loss: 0.905849
Train - Epoch 29, Batch: 20, Loss: 0.899045
Train - Epoch 29, Batch: 30, Loss: 0.898390
Train - Epoch 30, Batch: 0, Loss: 0.894019
Train - Epoch 30, Batch: 10, Loss: 0.897389
Train - Epoch 30, Batch: 20, Loss: 0.901318
Train - Epoch 30, Batch: 30, Loss: 0.894826
Train - Epoch 31, Batch: 0, Loss: 0.900364
Train - Epoch 31, Batch: 10, Loss: 0.887982
Train - Epoch 31, Batch: 20, Loss: 0.894695
Train - Epoch 31, Batch: 30, Loss: 0.881910
training_time:: 3.9822449684143066
training time full:: 3.982287883758545
provenance prepare time:: 4.76837158203125e-07
Test Avg. Loss: 0.000070, Accuracy: 0.629512
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 10
training time is 2.4762301445007324
overhead:: 0
overhead2:: 0
time_baseline:: 2.478027820587158
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629546
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.011868000030517578
overhead3:: 0.03499150276184082
overhead4:: 0.11203622817993164
overhead5:: 0
time_provenance:: 0.5725023746490479
curr_diff: 0 tensor(2.9420e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.9420e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629529
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.006330013275146484
overhead3:: 0.03543424606323242
overhead4:: 0.1442701816558838
overhead5:: 0
time_provenance:: 0.6100978851318359
curr_diff: 0 tensor(3.1769e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.1769e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629529
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.006351470947265625
overhead3:: 0.05346369743347168
overhead4:: 0.15032458305358887
overhead5:: 0
time_provenance:: 0.7174699306488037
curr_diff: 0 tensor(2.9253e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.9253e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629529
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.007532358169555664
overhead3:: 0.03867030143737793
overhead4:: 0.16125202178955078
overhead5:: 0
time_provenance:: 0.6161270141601562
curr_diff: 0 tensor(3.2341e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.2341e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629529
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.008419036865234375
overhead3:: 0.05009889602661133
overhead4:: 0.2333824634552002
overhead5:: 0
time_provenance:: 0.7325954437255859
curr_diff: 0 tensor(1.4198e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4198e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629546
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.008542537689208984
overhead3:: 0.05181694030761719
overhead4:: 0.2441098690032959
overhead5:: 0
time_provenance:: 0.7307121753692627
curr_diff: 0 tensor(1.4337e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4337e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629546
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.012644052505493164
overhead3:: 0.06699347496032715
overhead4:: 0.2659740447998047
overhead5:: 0
time_provenance:: 0.8577067852020264
curr_diff: 0 tensor(1.4223e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4223e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629546
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.010573863983154297
overhead3:: 0.060686349868774414
overhead4:: 0.27580952644348145
overhead5:: 0
time_provenance:: 0.797187328338623
curr_diff: 0 tensor(1.4132e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4132e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629546
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.014037847518920898
overhead3:: 0.08771061897277832
overhead4:: 0.41341710090637207
overhead5:: 0
time_provenance:: 1.0041332244873047
curr_diff: 0 tensor(5.7209e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.7209e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629546
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.014744758605957031
overhead3:: 0.09104084968566895
overhead4:: 0.419126033782959
overhead5:: 0
time_provenance:: 1.000549077987671
curr_diff: 0 tensor(5.7322e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.7322e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629546
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.01785421371459961
overhead3:: 0.0947568416595459
overhead4:: 0.4011383056640625
overhead5:: 0
time_provenance:: 0.9698371887207031
curr_diff: 0 tensor(5.7772e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.7772e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629546
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.016910552978515625
overhead3:: 0.09757828712463379
overhead4:: 0.45038342475891113
overhead5:: 0
time_provenance:: 1.0609221458435059
curr_diff: 0 tensor(5.7788e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.7788e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629546
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.040775299072265625
overhead3:: 0.21538400650024414
overhead4:: 0.9422769546508789
overhead5:: 0
time_provenance:: 1.7614610195159912
curr_diff: 0 tensor(1.1340e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1340e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629546
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.0395965576171875
overhead3:: 0.2054755687713623
overhead4:: 0.9899730682373047
overhead5:: 0
time_provenance:: 1.8053224086761475
curr_diff: 0 tensor(1.1446e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1446e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629546
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.04029130935668945
overhead3:: 0.19822072982788086
overhead4:: 0.8606574535369873
overhead5:: 0
time_provenance:: 1.6424570083618164
curr_diff: 0 tensor(1.1592e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1592e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629546
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.04185199737548828
overhead3:: 0.2115163803100586
overhead4:: 0.9457499980926514
overhead5:: 0
time_provenance:: 1.7490408420562744
curr_diff: 0 tensor(1.1473e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1473e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629546
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.10327982902526855
overhead3:: 0.612236499786377
overhead4:: 1.7532563209533691
overhead5:: 0
time_provenance:: 2.685434341430664
curr_diff: 0 tensor(1.0423e-15, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0423e-15, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629546
repetition 4
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 4 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 1.982298
Train - Epoch 0, Batch: 10, Loss: 1.391361
Train - Epoch 0, Batch: 20, Loss: 1.249624
Train - Epoch 0, Batch: 30, Loss: 1.190334
Train - Epoch 1, Batch: 0, Loss: 1.192977
Train - Epoch 1, Batch: 10, Loss: 1.152967
Train - Epoch 1, Batch: 20, Loss: 1.152049
Train - Epoch 1, Batch: 30, Loss: 1.129573
Train - Epoch 2, Batch: 0, Loss: 1.131994
Train - Epoch 2, Batch: 10, Loss: 1.115269
Train - Epoch 2, Batch: 20, Loss: 1.119358
Train - Epoch 2, Batch: 30, Loss: 1.097188
Train - Epoch 3, Batch: 0, Loss: 1.094278
Train - Epoch 3, Batch: 10, Loss: 1.099757
Train - Epoch 3, Batch: 20, Loss: 1.086051
Train - Epoch 3, Batch: 30, Loss: 1.065868
Train - Epoch 4, Batch: 0, Loss: 1.071943
Train - Epoch 4, Batch: 10, Loss: 1.074059
Train - Epoch 4, Batch: 20, Loss: 1.055047
Train - Epoch 4, Batch: 30, Loss: 1.049100
Train - Epoch 5, Batch: 0, Loss: 1.052010
Train - Epoch 5, Batch: 10, Loss: 1.042549
Train - Epoch 5, Batch: 20, Loss: 1.045496
Train - Epoch 5, Batch: 30, Loss: 1.034498
Train - Epoch 6, Batch: 0, Loss: 1.047513
Train - Epoch 6, Batch: 10, Loss: 1.040811
Train - Epoch 6, Batch: 20, Loss: 1.024051
Train - Epoch 6, Batch: 30, Loss: 1.023442
Train - Epoch 7, Batch: 0, Loss: 1.012620
Train - Epoch 7, Batch: 10, Loss: 1.018032
Train - Epoch 7, Batch: 20, Loss: 1.008180
Train - Epoch 7, Batch: 30, Loss: 1.006831
Train - Epoch 8, Batch: 0, Loss: 1.001120
Train - Epoch 8, Batch: 10, Loss: 1.000952
Train - Epoch 8, Batch: 20, Loss: 1.001908
Train - Epoch 8, Batch: 30, Loss: 0.999821
Train - Epoch 9, Batch: 0, Loss: 0.998358
Train - Epoch 9, Batch: 10, Loss: 1.003285
Train - Epoch 9, Batch: 20, Loss: 0.986889
Train - Epoch 9, Batch: 30, Loss: 0.980080
Train - Epoch 10, Batch: 0, Loss: 0.979400
Train - Epoch 10, Batch: 10, Loss: 0.982145
Train - Epoch 10, Batch: 20, Loss: 0.973318
Train - Epoch 10, Batch: 30, Loss: 0.982018
Train - Epoch 11, Batch: 0, Loss: 0.967793
Train - Epoch 11, Batch: 10, Loss: 0.962311
Train - Epoch 11, Batch: 20, Loss: 0.971908
Train - Epoch 11, Batch: 30, Loss: 0.968056
Train - Epoch 12, Batch: 0, Loss: 0.966350
Train - Epoch 12, Batch: 10, Loss: 0.966063
Train - Epoch 12, Batch: 20, Loss: 0.960793
Train - Epoch 12, Batch: 30, Loss: 0.972331
Train - Epoch 13, Batch: 0, Loss: 0.961828
Train - Epoch 13, Batch: 10, Loss: 0.961834
Train - Epoch 13, Batch: 20, Loss: 0.957035
Train - Epoch 13, Batch: 30, Loss: 0.957081
Train - Epoch 14, Batch: 0, Loss: 0.947217
Train - Epoch 14, Batch: 10, Loss: 0.958046
Train - Epoch 14, Batch: 20, Loss: 0.957614
Train - Epoch 14, Batch: 30, Loss: 0.949707
Train - Epoch 15, Batch: 0, Loss: 0.950598
Train - Epoch 15, Batch: 10, Loss: 0.949987
Train - Epoch 15, Batch: 20, Loss: 0.941472
Train - Epoch 15, Batch: 30, Loss: 0.940829
Train - Epoch 16, Batch: 0, Loss: 0.952473
Train - Epoch 16, Batch: 10, Loss: 0.942750
Train - Epoch 16, Batch: 20, Loss: 0.942022
Train - Epoch 16, Batch: 30, Loss: 0.936720
Train - Epoch 17, Batch: 0, Loss: 0.945826
Train - Epoch 17, Batch: 10, Loss: 0.938212
Train - Epoch 17, Batch: 20, Loss: 0.928833
Train - Epoch 17, Batch: 30, Loss: 0.935715
Train - Epoch 18, Batch: 0, Loss: 0.933023
Train - Epoch 18, Batch: 10, Loss: 0.928177
Train - Epoch 18, Batch: 20, Loss: 0.929023
Train - Epoch 18, Batch: 30, Loss: 0.928670
Train - Epoch 19, Batch: 0, Loss: 0.931446
Train - Epoch 19, Batch: 10, Loss: 0.924918
Train - Epoch 19, Batch: 20, Loss: 0.920497
Train - Epoch 19, Batch: 30, Loss: 0.923297
Train - Epoch 20, Batch: 0, Loss: 0.929424
Train - Epoch 20, Batch: 10, Loss: 0.921910
Train - Epoch 20, Batch: 20, Loss: 0.921810
Train - Epoch 20, Batch: 30, Loss: 0.913110
Train - Epoch 21, Batch: 0, Loss: 0.912259
Train - Epoch 21, Batch: 10, Loss: 0.921459
Train - Epoch 21, Batch: 20, Loss: 0.926564
Train - Epoch 21, Batch: 30, Loss: 0.917089
Train - Epoch 22, Batch: 0, Loss: 0.920798
Train - Epoch 22, Batch: 10, Loss: 0.904499
Train - Epoch 22, Batch: 20, Loss: 0.919319
Train - Epoch 22, Batch: 30, Loss: 0.913213
Train - Epoch 23, Batch: 0, Loss: 0.907337
Train - Epoch 23, Batch: 10, Loss: 0.916285
Train - Epoch 23, Batch: 20, Loss: 0.911820
Train - Epoch 23, Batch: 30, Loss: 0.915350
Train - Epoch 24, Batch: 0, Loss: 0.906755
Train - Epoch 24, Batch: 10, Loss: 0.911013
Train - Epoch 24, Batch: 20, Loss: 0.911246
Train - Epoch 24, Batch: 30, Loss: 0.904789
Train - Epoch 25, Batch: 0, Loss: 0.904095
Train - Epoch 25, Batch: 10, Loss: 0.905965
Train - Epoch 25, Batch: 20, Loss: 0.911624
Train - Epoch 25, Batch: 30, Loss: 0.905826
Train - Epoch 26, Batch: 0, Loss: 0.903782
Train - Epoch 26, Batch: 10, Loss: 0.902851
Train - Epoch 26, Batch: 20, Loss: 0.906869
Train - Epoch 26, Batch: 30, Loss: 0.898156
Train - Epoch 27, Batch: 0, Loss: 0.896076
Train - Epoch 27, Batch: 10, Loss: 0.902133
Train - Epoch 27, Batch: 20, Loss: 0.898219
Train - Epoch 27, Batch: 30, Loss: 0.893553
Train - Epoch 28, Batch: 0, Loss: 0.902851
Train - Epoch 28, Batch: 10, Loss: 0.902610
Train - Epoch 28, Batch: 20, Loss: 0.908781
Train - Epoch 28, Batch: 30, Loss: 0.900590
Train - Epoch 29, Batch: 0, Loss: 0.893346
Train - Epoch 29, Batch: 10, Loss: 0.890324
Train - Epoch 29, Batch: 20, Loss: 0.895143
Train - Epoch 29, Batch: 30, Loss: 0.899017
Train - Epoch 30, Batch: 0, Loss: 0.899513
Train - Epoch 30, Batch: 10, Loss: 0.883627
Train - Epoch 30, Batch: 20, Loss: 0.888515
Train - Epoch 30, Batch: 30, Loss: 0.905783
Train - Epoch 31, Batch: 0, Loss: 0.899502
Train - Epoch 31, Batch: 10, Loss: 0.884144
Train - Epoch 31, Batch: 20, Loss: 0.891134
Train - Epoch 31, Batch: 30, Loss: 0.893876
training_time:: 3.7640199661254883
training time full:: 3.764066457748413
provenance prepare time:: 2.384185791015625e-07
Test Avg. Loss: 0.000070, Accuracy: 0.631061
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 10
training time is 2.0371623039245605
overhead:: 0
overhead2:: 0
time_baseline:: 2.038677215576172
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631061
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.004016876220703125
overhead3:: 0.0311739444732666
overhead4:: 0.1252434253692627
overhead5:: 0
time_provenance:: 0.5723598003387451
curr_diff: 0 tensor(2.8208e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.8208e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631061
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.008053064346313477
overhead3:: 0.046915531158447266
overhead4:: 0.13854146003723145
overhead5:: 0
time_provenance:: 0.605945348739624
curr_diff: 0 tensor(1.8502e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.8502e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631061
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.005677223205566406
overhead3:: 0.04112887382507324
overhead4:: 0.165388822555542
overhead5:: 0
time_provenance:: 0.6394214630126953
curr_diff: 0 tensor(2.9208e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.9208e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631061
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.008710384368896484
overhead3:: 0.037369489669799805
overhead4:: 0.16761159896850586
overhead5:: 0
time_provenance:: 0.6483461856842041
curr_diff: 0 tensor(2.4526e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.4526e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631061
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.012146472930908203
overhead3:: 0.05995488166809082
overhead4:: 0.28037214279174805
overhead5:: 0
time_provenance:: 0.9274230003356934
curr_diff: 0 tensor(1.4162e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4162e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631061
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.011346101760864258
overhead3:: 0.0520167350769043
overhead4:: 0.23278141021728516
overhead5:: 0
time_provenance:: 0.7245800495147705
curr_diff: 0 tensor(1.4558e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4558e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631061
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.010751008987426758
overhead3:: 0.07355165481567383
overhead4:: 0.24886775016784668
overhead5:: 0
time_provenance:: 0.766852855682373
curr_diff: 0 tensor(1.5005e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5005e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631061
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.013654470443725586
overhead3:: 0.0718231201171875
overhead4:: 0.3018655776977539
overhead5:: 0
time_provenance:: 0.9491088390350342
curr_diff: 0 tensor(1.5083e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5083e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631061
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.017438411712646484
overhead3:: 0.10029458999633789
overhead4:: 0.4355604648590088
overhead5:: 0
time_provenance:: 1.0528407096862793
curr_diff: 0 tensor(4.6754e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.6754e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631061
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.016559362411499023
overhead3:: 0.08818984031677246
overhead4:: 0.394451379776001
overhead5:: 0
time_provenance:: 0.9570310115814209
curr_diff: 0 tensor(4.8423e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.8423e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631061
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.021901607513427734
overhead3:: 0.1179649829864502
overhead4:: 0.5289802551269531
overhead5:: 0
time_provenance:: 1.311880111694336
curr_diff: 0 tensor(5.0706e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.0706e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631061
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.022179603576660156
overhead3:: 0.12659764289855957
overhead4:: 0.5258541107177734
overhead5:: 0
time_provenance:: 1.3221070766448975
curr_diff: 0 tensor(5.0873e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.0873e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631061
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.042873382568359375
overhead3:: 0.21325206756591797
overhead4:: 0.9571542739868164
overhead5:: 0
time_provenance:: 1.7730143070220947
curr_diff: 0 tensor(1.2205e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2205e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631061
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.04214286804199219
overhead3:: 0.2023029327392578
overhead4:: 0.9634239673614502
overhead5:: 0
time_provenance:: 1.766392469406128
curr_diff: 0 tensor(1.2395e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2395e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631061
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.043413639068603516
overhead3:: 0.20633935928344727
overhead4:: 0.9647676944732666
overhead5:: 0
time_provenance:: 1.776533603668213
curr_diff: 0 tensor(1.2867e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2867e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631061
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.043725013732910156
overhead3:: 0.21207189559936523
overhead4:: 0.9689419269561768
overhead5:: 0
time_provenance:: 1.7857475280761719
curr_diff: 0 tensor(1.2706e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2706e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631061
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.08634591102600098
overhead3:: 0.420971155166626
overhead4:: 1.5477712154388428
overhead5:: 0
time_provenance:: 2.2920050621032715
curr_diff: 0 tensor(1.0524e-15, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0524e-15, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631061
deletion rate:: 0.00005
python3 generate_rand_ids 0.00005  covtype 0
tensor([277122,  96643, 391428,  96645,   5378, 273797, 480582, 183688, 315473,
        270357,  42901, 373849, 390617, 425947, 461721, 394714, 229340,  71902,
        389411, 158950, 127591, 518951, 255018,  64940, 489463, 159484])
batch size:: 16384
repetition 0
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 0 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.031643
Train - Epoch 0, Batch: 10, Loss: 1.422014
Train - Epoch 0, Batch: 20, Loss: 1.264181
Train - Epoch 0, Batch: 30, Loss: 1.202984
Train - Epoch 1, Batch: 0, Loss: 1.197248
Train - Epoch 1, Batch: 10, Loss: 1.176793
Train - Epoch 1, Batch: 20, Loss: 1.154234
Train - Epoch 1, Batch: 30, Loss: 1.149898
Train - Epoch 2, Batch: 0, Loss: 1.141578
Train - Epoch 2, Batch: 10, Loss: 1.127165
Train - Epoch 2, Batch: 20, Loss: 1.112500
Train - Epoch 2, Batch: 30, Loss: 1.105073
Train - Epoch 3, Batch: 0, Loss: 1.107130
Train - Epoch 3, Batch: 10, Loss: 1.092611
Train - Epoch 3, Batch: 20, Loss: 1.082618
Train - Epoch 3, Batch: 30, Loss: 1.080177
Train - Epoch 4, Batch: 0, Loss: 1.070384
Train - Epoch 4, Batch: 10, Loss: 1.070703
Train - Epoch 4, Batch: 20, Loss: 1.050979
Train - Epoch 4, Batch: 30, Loss: 1.048123
Train - Epoch 5, Batch: 0, Loss: 1.053776
Train - Epoch 5, Batch: 10, Loss: 1.043281
Train - Epoch 5, Batch: 20, Loss: 1.044817
Train - Epoch 5, Batch: 30, Loss: 1.054580
Train - Epoch 6, Batch: 0, Loss: 1.032891
Train - Epoch 6, Batch: 10, Loss: 1.034986
Train - Epoch 6, Batch: 20, Loss: 1.026009
Train - Epoch 6, Batch: 30, Loss: 1.022314
Train - Epoch 7, Batch: 0, Loss: 1.023953
Train - Epoch 7, Batch: 10, Loss: 1.026834
Train - Epoch 7, Batch: 20, Loss: 1.014730
Train - Epoch 7, Batch: 30, Loss: 1.017382
Train - Epoch 8, Batch: 0, Loss: 1.011811
Train - Epoch 8, Batch: 10, Loss: 1.003004
Train - Epoch 8, Batch: 20, Loss: 1.000820
Train - Epoch 8, Batch: 30, Loss: 0.995278
Train - Epoch 9, Batch: 0, Loss: 1.002514
Train - Epoch 9, Batch: 10, Loss: 0.997076
Train - Epoch 9, Batch: 20, Loss: 0.980465
Train - Epoch 9, Batch: 30, Loss: 0.979766
Train - Epoch 10, Batch: 0, Loss: 0.993069
Train - Epoch 10, Batch: 10, Loss: 0.989490
Train - Epoch 10, Batch: 20, Loss: 0.980020
Train - Epoch 10, Batch: 30, Loss: 0.985006
Train - Epoch 11, Batch: 0, Loss: 0.973782
Train - Epoch 11, Batch: 10, Loss: 0.977783
Train - Epoch 11, Batch: 20, Loss: 0.967639
Train - Epoch 11, Batch: 30, Loss: 0.980795
Train - Epoch 12, Batch: 0, Loss: 0.975924
Train - Epoch 12, Batch: 10, Loss: 0.965545
Train - Epoch 12, Batch: 20, Loss: 0.960615
Train - Epoch 12, Batch: 30, Loss: 0.976979
Train - Epoch 13, Batch: 0, Loss: 0.954067
Train - Epoch 13, Batch: 10, Loss: 0.967987
Train - Epoch 13, Batch: 20, Loss: 0.953641
Train - Epoch 13, Batch: 30, Loss: 0.956130
Train - Epoch 14, Batch: 0, Loss: 0.953352
Train - Epoch 14, Batch: 10, Loss: 0.956972
Train - Epoch 14, Batch: 20, Loss: 0.953014
Train - Epoch 14, Batch: 30, Loss: 0.955366
Train - Epoch 15, Batch: 0, Loss: 0.945036
Train - Epoch 15, Batch: 10, Loss: 0.935318
Train - Epoch 15, Batch: 20, Loss: 0.937949
Train - Epoch 15, Batch: 30, Loss: 0.949120
Train - Epoch 16, Batch: 0, Loss: 0.946513
Train - Epoch 16, Batch: 10, Loss: 0.933853
Train - Epoch 16, Batch: 20, Loss: 0.936753
Train - Epoch 16, Batch: 30, Loss: 0.939531
Train - Epoch 17, Batch: 0, Loss: 0.942571
Train - Epoch 17, Batch: 10, Loss: 0.934326
Train - Epoch 17, Batch: 20, Loss: 0.941262
Train - Epoch 17, Batch: 30, Loss: 0.938937
Train - Epoch 18, Batch: 0, Loss: 0.932364
Train - Epoch 18, Batch: 10, Loss: 0.930995
Train - Epoch 18, Batch: 20, Loss: 0.932060
Train - Epoch 18, Batch: 30, Loss: 0.938362
Train - Epoch 19, Batch: 0, Loss: 0.919762
Train - Epoch 19, Batch: 10, Loss: 0.937380
Train - Epoch 19, Batch: 20, Loss: 0.919092
Train - Epoch 19, Batch: 30, Loss: 0.920414
Train - Epoch 20, Batch: 0, Loss: 0.923203
Train - Epoch 20, Batch: 10, Loss: 0.928646
Train - Epoch 20, Batch: 20, Loss: 0.914927
Train - Epoch 20, Batch: 30, Loss: 0.927602
Train - Epoch 21, Batch: 0, Loss: 0.921505
Train - Epoch 21, Batch: 10, Loss: 0.928826
Train - Epoch 21, Batch: 20, Loss: 0.929326
Train - Epoch 21, Batch: 30, Loss: 0.906179
Train - Epoch 22, Batch: 0, Loss: 0.926074
Train - Epoch 22, Batch: 10, Loss: 0.912602
Train - Epoch 22, Batch: 20, Loss: 0.918429
Train - Epoch 22, Batch: 30, Loss: 0.924095
Train - Epoch 23, Batch: 0, Loss: 0.913666
Train - Epoch 23, Batch: 10, Loss: 0.912986
Train - Epoch 23, Batch: 20, Loss: 0.920443
Train - Epoch 23, Batch: 30, Loss: 0.911707
Train - Epoch 24, Batch: 0, Loss: 0.914557
Train - Epoch 24, Batch: 10, Loss: 0.903582
Train - Epoch 24, Batch: 20, Loss: 0.901688
Train - Epoch 24, Batch: 30, Loss: 0.915726
Train - Epoch 25, Batch: 0, Loss: 0.914079
Train - Epoch 25, Batch: 10, Loss: 0.905528
Train - Epoch 25, Batch: 20, Loss: 0.910511
Train - Epoch 25, Batch: 30, Loss: 0.898348
Train - Epoch 26, Batch: 0, Loss: 0.910645
Train - Epoch 26, Batch: 10, Loss: 0.901323
Train - Epoch 26, Batch: 20, Loss: 0.912450
Train - Epoch 26, Batch: 30, Loss: 0.902049
Train - Epoch 27, Batch: 0, Loss: 0.911170
Train - Epoch 27, Batch: 10, Loss: 0.902430
Train - Epoch 27, Batch: 20, Loss: 0.905492
Train - Epoch 27, Batch: 30, Loss: 0.903865
Train - Epoch 28, Batch: 0, Loss: 0.902723
Train - Epoch 28, Batch: 10, Loss: 0.907451
Train - Epoch 28, Batch: 20, Loss: 0.895964
Train - Epoch 28, Batch: 30, Loss: 0.898387
Train - Epoch 29, Batch: 0, Loss: 0.895693
Train - Epoch 29, Batch: 10, Loss: 0.903573
Train - Epoch 29, Batch: 20, Loss: 0.891448
Train - Epoch 29, Batch: 30, Loss: 0.888680
Train - Epoch 30, Batch: 0, Loss: 0.891948
Train - Epoch 30, Batch: 10, Loss: 0.896064
Train - Epoch 30, Batch: 20, Loss: 0.900303
Train - Epoch 30, Batch: 30, Loss: 0.899665
Train - Epoch 31, Batch: 0, Loss: 0.893116
Train - Epoch 31, Batch: 10, Loss: 0.891292
Train - Epoch 31, Batch: 20, Loss: 0.898003
Train - Epoch 31, Batch: 30, Loss: 0.896250
training_time:: 3.854052782058716
training time full:: 3.8541011810302734
provenance prepare time:: 4.76837158203125e-07
Test Avg. Loss: 0.000070, Accuracy: 0.630097
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 26
training time is 1.958940029144287
overhead:: 0
overhead2:: 0
time_baseline:: 1.960218906402588
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630097
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.009189844131469727
overhead3:: 0.028204917907714844
overhead4:: 0.10437750816345215
overhead5:: 0
time_provenance:: 0.6622309684753418
curr_diff: 0 tensor(4.3425e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.3425e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630097
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.012558698654174805
overhead3:: 0.031069517135620117
overhead4:: 0.11571359634399414
overhead5:: 0
time_provenance:: 0.7088029384613037
curr_diff: 0 tensor(3.9991e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.9991e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630114
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.0124053955078125
overhead3:: 0.045244455337524414
overhead4:: 0.15455031394958496
overhead5:: 0
time_provenance:: 0.7707805633544922
curr_diff: 0 tensor(4.4540e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.4540e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630097
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.015733957290649414
overhead3:: 0.053240299224853516
overhead4:: 0.15791535377502441
overhead5:: 0
time_provenance:: 0.80501389503479
curr_diff: 0 tensor(4.1330e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.1330e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630114
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.018822193145751953
overhead3:: 0.04662442207336426
overhead4:: 0.22249388694763184
overhead5:: 0
time_provenance:: 0.8545873165130615
curr_diff: 0 tensor(2.0269e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.0269e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630097
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.020636796951293945
overhead3:: 0.05340886116027832
overhead4:: 0.23290181159973145
overhead5:: 0
time_provenance:: 0.876518964767456
curr_diff: 0 tensor(2.0761e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.0761e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630097
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.022609472274780273
overhead3:: 0.059797048568725586
overhead4:: 0.27838754653930664
overhead5:: 0
time_provenance:: 0.9605269432067871
curr_diff: 0 tensor(2.0522e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.0522e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630097
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.025213003158569336
overhead3:: 0.06329989433288574
overhead4:: 0.28296351432800293
overhead5:: 0
time_provenance:: 1.0118107795715332
curr_diff: 0 tensor(2.1170e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1170e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630097
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.04494905471801758
overhead3:: 0.11063003540039062
overhead4:: 0.4890007972717285
overhead5:: 0
time_provenance:: 1.4338302612304688
curr_diff: 0 tensor(9.2212e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.2212e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630097
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.03426003456115723
overhead3:: 0.0888512134552002
overhead4:: 0.43526482582092285
overhead5:: 0
time_provenance:: 1.1456012725830078
curr_diff: 0 tensor(9.4220e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.4220e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630097
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.03767228126525879
overhead3:: 0.09254956245422363
overhead4:: 0.37160754203796387
overhead5:: 0
time_provenance:: 1.07719087600708
curr_diff: 0 tensor(9.4270e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.4270e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630097
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.05360984802246094
overhead3:: 0.12554621696472168
overhead4:: 0.5019927024841309
overhead5:: 0
time_provenance:: 1.4873433113098145
curr_diff: 0 tensor(9.6097e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.6097e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630097
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.0755763053894043
overhead3:: 0.21608495712280273
overhead4:: 0.9719855785369873
overhead5:: 0
time_provenance:: 1.8970608711242676
curr_diff: 0 tensor(2.1390e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1390e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630097
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.0821218490600586
overhead3:: 0.21247076988220215
overhead4:: 0.9936280250549316
overhead5:: 0
time_provenance:: 1.9227590560913086
curr_diff: 0 tensor(2.1970e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1970e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630097
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.08429956436157227
overhead3:: 0.21860480308532715
overhead4:: 0.9839949607849121
overhead5:: 0
time_provenance:: 1.9431679248809814
curr_diff: 0 tensor(2.2139e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.2139e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630097
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.11698460578918457
overhead3:: 0.278369665145874
overhead4:: 1.1555728912353516
overhead5:: 0
time_provenance:: 2.397571325302124
curr_diff: 0 tensor(2.3082e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.3082e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630097
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.1697218418121338
overhead3:: 0.4026768207550049
overhead4:: 1.5341107845306396
overhead5:: 0
time_provenance:: 2.378584861755371
curr_diff: 0 tensor(1.1199e-15, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1199e-15, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630097
repetition 1
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 1 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 1.998897
Train - Epoch 0, Batch: 10, Loss: 1.381601
Train - Epoch 0, Batch: 20, Loss: 1.236889
Train - Epoch 0, Batch: 30, Loss: 1.167744
Train - Epoch 1, Batch: 0, Loss: 1.175461
Train - Epoch 1, Batch: 10, Loss: 1.146104
Train - Epoch 1, Batch: 20, Loss: 1.122468
Train - Epoch 1, Batch: 30, Loss: 1.118583
Train - Epoch 2, Batch: 0, Loss: 1.115047
Train - Epoch 2, Batch: 10, Loss: 1.105290
Train - Epoch 2, Batch: 20, Loss: 1.094684
Train - Epoch 2, Batch: 30, Loss: 1.090798
Train - Epoch 3, Batch: 0, Loss: 1.100064
Train - Epoch 3, Batch: 10, Loss: 1.077618
Train - Epoch 3, Batch: 20, Loss: 1.070153
Train - Epoch 3, Batch: 30, Loss: 1.065350
Train - Epoch 4, Batch: 0, Loss: 1.062895
Train - Epoch 4, Batch: 10, Loss: 1.064386
Train - Epoch 4, Batch: 20, Loss: 1.047519
Train - Epoch 4, Batch: 30, Loss: 1.057815
Train - Epoch 5, Batch: 0, Loss: 1.045514
Train - Epoch 5, Batch: 10, Loss: 1.040095
Train - Epoch 5, Batch: 20, Loss: 1.041177
Train - Epoch 5, Batch: 30, Loss: 1.034110
Train - Epoch 6, Batch: 0, Loss: 1.038243
Train - Epoch 6, Batch: 10, Loss: 1.019237
Train - Epoch 6, Batch: 20, Loss: 1.025949
Train - Epoch 6, Batch: 30, Loss: 1.020843
Train - Epoch 7, Batch: 0, Loss: 1.010004
Train - Epoch 7, Batch: 10, Loss: 1.017279
Train - Epoch 7, Batch: 20, Loss: 1.010304
Train - Epoch 7, Batch: 30, Loss: 1.007465
Train - Epoch 8, Batch: 0, Loss: 1.007991
Train - Epoch 8, Batch: 10, Loss: 1.016582
Train - Epoch 8, Batch: 20, Loss: 0.994009
Train - Epoch 8, Batch: 30, Loss: 0.996306
Train - Epoch 9, Batch: 0, Loss: 0.987663
Train - Epoch 9, Batch: 10, Loss: 0.996556
Train - Epoch 9, Batch: 20, Loss: 0.983752
Train - Epoch 9, Batch: 30, Loss: 0.989890
Train - Epoch 10, Batch: 0, Loss: 0.985690
Train - Epoch 10, Batch: 10, Loss: 0.983498
Train - Epoch 10, Batch: 20, Loss: 0.971618
Train - Epoch 10, Batch: 30, Loss: 0.972253
Train - Epoch 11, Batch: 0, Loss: 0.979131
Train - Epoch 11, Batch: 10, Loss: 0.970052
Train - Epoch 11, Batch: 20, Loss: 0.974716
Train - Epoch 11, Batch: 30, Loss: 0.954970
Train - Epoch 12, Batch: 0, Loss: 0.966851
Train - Epoch 12, Batch: 10, Loss: 0.966168
Train - Epoch 12, Batch: 20, Loss: 0.946730
Train - Epoch 12, Batch: 30, Loss: 0.964257
Train - Epoch 13, Batch: 0, Loss: 0.959932
Train - Epoch 13, Batch: 10, Loss: 0.956474
Train - Epoch 13, Batch: 20, Loss: 0.960188
Train - Epoch 13, Batch: 30, Loss: 0.959324
Train - Epoch 14, Batch: 0, Loss: 0.954495
Train - Epoch 14, Batch: 10, Loss: 0.950343
Train - Epoch 14, Batch: 20, Loss: 0.945883
Train - Epoch 14, Batch: 30, Loss: 0.948597
Train - Epoch 15, Batch: 0, Loss: 0.960521
Train - Epoch 15, Batch: 10, Loss: 0.949720
Train - Epoch 15, Batch: 20, Loss: 0.942658
Train - Epoch 15, Batch: 30, Loss: 0.943060
Train - Epoch 16, Batch: 0, Loss: 0.948154
Train - Epoch 16, Batch: 10, Loss: 0.936386
Train - Epoch 16, Batch: 20, Loss: 0.945736
Train - Epoch 16, Batch: 30, Loss: 0.943318
Train - Epoch 17, Batch: 0, Loss: 0.942264
Train - Epoch 17, Batch: 10, Loss: 0.938356
Train - Epoch 17, Batch: 20, Loss: 0.949562
Train - Epoch 17, Batch: 30, Loss: 0.932548
Train - Epoch 18, Batch: 0, Loss: 0.945781
Train - Epoch 18, Batch: 10, Loss: 0.935338
Train - Epoch 18, Batch: 20, Loss: 0.930318
Train - Epoch 18, Batch: 30, Loss: 0.928045
Train - Epoch 19, Batch: 0, Loss: 0.929342
Train - Epoch 19, Batch: 10, Loss: 0.922278
Train - Epoch 19, Batch: 20, Loss: 0.935132
Train - Epoch 19, Batch: 30, Loss: 0.926422
Train - Epoch 20, Batch: 0, Loss: 0.930152
Train - Epoch 20, Batch: 10, Loss: 0.926069
Train - Epoch 20, Batch: 20, Loss: 0.917218
Train - Epoch 20, Batch: 30, Loss: 0.917842
Train - Epoch 21, Batch: 0, Loss: 0.922807
Train - Epoch 21, Batch: 10, Loss: 0.917673
Train - Epoch 21, Batch: 20, Loss: 0.913684
Train - Epoch 21, Batch: 30, Loss: 0.924782
Train - Epoch 22, Batch: 0, Loss: 0.912760
Train - Epoch 22, Batch: 10, Loss: 0.918741
Train - Epoch 22, Batch: 20, Loss: 0.917024
Train - Epoch 22, Batch: 30, Loss: 0.926498
Train - Epoch 23, Batch: 0, Loss: 0.921676
Train - Epoch 23, Batch: 10, Loss: 0.910114
Train - Epoch 23, Batch: 20, Loss: 0.911834
Train - Epoch 23, Batch: 30, Loss: 0.913799
Train - Epoch 24, Batch: 0, Loss: 0.916247
Train - Epoch 24, Batch: 10, Loss: 0.921102
Train - Epoch 24, Batch: 20, Loss: 0.909197
Train - Epoch 24, Batch: 30, Loss: 0.911548
Train - Epoch 25, Batch: 0, Loss: 0.913715
Train - Epoch 25, Batch: 10, Loss: 0.916583
Train - Epoch 25, Batch: 20, Loss: 0.911078
Train - Epoch 25, Batch: 30, Loss: 0.900705
Train - Epoch 26, Batch: 0, Loss: 0.920163
Train - Epoch 26, Batch: 10, Loss: 0.911817
Train - Epoch 26, Batch: 20, Loss: 0.916172
Train - Epoch 26, Batch: 30, Loss: 0.889917
Train - Epoch 27, Batch: 0, Loss: 0.906100
Train - Epoch 27, Batch: 10, Loss: 0.908038
Train - Epoch 27, Batch: 20, Loss: 0.905445
Train - Epoch 27, Batch: 30, Loss: 0.893795
Train - Epoch 28, Batch: 0, Loss: 0.901342
Train - Epoch 28, Batch: 10, Loss: 0.900893
Train - Epoch 28, Batch: 20, Loss: 0.898091
Train - Epoch 28, Batch: 30, Loss: 0.902022
Train - Epoch 29, Batch: 0, Loss: 0.898987
Train - Epoch 29, Batch: 10, Loss: 0.895018
Train - Epoch 29, Batch: 20, Loss: 0.899016
Train - Epoch 29, Batch: 30, Loss: 0.909261
Train - Epoch 30, Batch: 0, Loss: 0.893342
Train - Epoch 30, Batch: 10, Loss: 0.892188
Train - Epoch 30, Batch: 20, Loss: 0.904849
Train - Epoch 30, Batch: 30, Loss: 0.881702
Train - Epoch 31, Batch: 0, Loss: 0.903545
Train - Epoch 31, Batch: 10, Loss: 0.895892
Train - Epoch 31, Batch: 20, Loss: 0.893506
Train - Epoch 31, Batch: 30, Loss: 0.891537
training_time:: 3.8297390937805176
training time full:: 3.829784393310547
provenance prepare time:: 4.76837158203125e-07
Test Avg. Loss: 0.000070, Accuracy: 0.629667
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 26
training time is 2.210771083831787
overhead:: 0
overhead2:: 0
time_baseline:: 2.2124578952789307
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629632
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.01139378547668457
overhead3:: 0.028718233108520508
overhead4:: 0.10812783241271973
overhead5:: 0
time_provenance:: 0.6891767978668213
curr_diff: 0 tensor(4.3990e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.3990e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629632
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.011956453323364258
overhead3:: 0.03305768966674805
overhead4:: 0.12098813056945801
overhead5:: 0
time_provenance:: 0.7016682624816895
curr_diff: 0 tensor(4.6405e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.6405e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629615
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.013983011245727539
overhead3:: 0.04677844047546387
overhead4:: 0.12931346893310547
overhead5:: 0
time_provenance:: 0.7038047313690186
curr_diff: 0 tensor(4.4809e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.4809e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629632
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.016990184783935547
overhead3:: 0.0511014461517334
overhead4:: 0.17971205711364746
overhead5:: 0
time_provenance:: 0.8134288787841797
curr_diff: 0 tensor(4.8182e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.8182e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629615
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.018601179122924805
overhead3:: 0.04891633987426758
overhead4:: 0.2215135097503662
overhead5:: 0
time_provenance:: 0.8442692756652832
curr_diff: 0 tensor(2.3402e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.3402e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629615
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.020982742309570312
overhead3:: 0.05343174934387207
overhead4:: 0.25808167457580566
overhead5:: 0
time_provenance:: 0.8939592838287354
curr_diff: 0 tensor(2.3549e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.3549e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629615
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.02298712730407715
overhead3:: 0.06315493583679199
overhead4:: 0.23764252662658691
overhead5:: 0
time_provenance:: 0.8815441131591797
curr_diff: 0 tensor(2.3769e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.3769e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629615
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.03059101104736328
overhead3:: 0.07772040367126465
overhead4:: 0.2836017608642578
overhead5:: 0
time_provenance:: 1.0767502784729004
curr_diff: 0 tensor(2.5023e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.5023e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629615
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.03558063507080078
overhead3:: 0.09285211563110352
overhead4:: 0.42900896072387695
overhead5:: 0
time_provenance:: 1.2090675830841064
curr_diff: 0 tensor(9.2401e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.2401e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629615
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.03451085090637207
overhead3:: 0.09156298637390137
overhead4:: 0.433093786239624
overhead5:: 0
time_provenance:: 1.142549991607666
curr_diff: 0 tensor(9.3432e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.3432e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629615
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.05006003379821777
overhead3:: 0.11998271942138672
overhead4:: 0.48998379707336426
overhead5:: 0
time_provenance:: 1.4597387313842773
curr_diff: 0 tensor(9.3011e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.3011e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629615
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.03837132453918457
overhead3:: 0.12160062789916992
overhead4:: 0.4307682514190674
overhead5:: 0
time_provenance:: 1.185715675354004
curr_diff: 0 tensor(9.7247e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.7247e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629615
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.08142304420471191
overhead3:: 0.2015221118927002
overhead4:: 0.9696762561798096
overhead5:: 0
time_provenance:: 1.8658978939056396
curr_diff: 0 tensor(1.9847e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.9847e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629632
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.0938878059387207
overhead3:: 0.23512554168701172
overhead4:: 0.9540629386901855
overhead5:: 0
time_provenance:: 1.909815788269043
curr_diff: 0 tensor(1.9894e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.9894e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629632
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.08531498908996582
overhead3:: 0.20937418937683105
overhead4:: 0.961575984954834
overhead5:: 0
time_provenance:: 1.8711729049682617
curr_diff: 0 tensor(2.0588e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.0588e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629632
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.09125232696533203
overhead3:: 0.22933650016784668
overhead4:: 1.0281050205230713
overhead5:: 0
time_provenance:: 1.9715425968170166
curr_diff: 0 tensor(2.1629e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1629e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629632
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.16873836517333984
overhead3:: 0.4022345542907715
overhead4:: 1.5556108951568604
overhead5:: 0
time_provenance:: 2.395791530609131
curr_diff: 0 tensor(1.1010e-15, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1010e-15, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629632
repetition 2
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 1.941445
Train - Epoch 0, Batch: 10, Loss: 1.375142
Train - Epoch 0, Batch: 20, Loss: 1.246588
Train - Epoch 0, Batch: 30, Loss: 1.192547
Train - Epoch 1, Batch: 0, Loss: 1.188416
Train - Epoch 1, Batch: 10, Loss: 1.154636
Train - Epoch 1, Batch: 20, Loss: 1.146537
Train - Epoch 1, Batch: 30, Loss: 1.137467
Train - Epoch 2, Batch: 0, Loss: 1.124055
Train - Epoch 2, Batch: 10, Loss: 1.114142
Train - Epoch 2, Batch: 20, Loss: 1.108799
Train - Epoch 2, Batch: 30, Loss: 1.096560
Train - Epoch 3, Batch: 0, Loss: 1.093005
Train - Epoch 3, Batch: 10, Loss: 1.099484
Train - Epoch 3, Batch: 20, Loss: 1.078537
Train - Epoch 3, Batch: 30, Loss: 1.074708
Train - Epoch 4, Batch: 0, Loss: 1.069765
Train - Epoch 4, Batch: 10, Loss: 1.061858
Train - Epoch 4, Batch: 20, Loss: 1.057477
Train - Epoch 4, Batch: 30, Loss: 1.065026
Train - Epoch 5, Batch: 0, Loss: 1.046525
Train - Epoch 5, Batch: 10, Loss: 1.032593
Train - Epoch 5, Batch: 20, Loss: 1.046348
Train - Epoch 5, Batch: 30, Loss: 1.034305
Train - Epoch 6, Batch: 0, Loss: 1.028351
Train - Epoch 6, Batch: 10, Loss: 1.033049
Train - Epoch 6, Batch: 20, Loss: 1.023257
Train - Epoch 6, Batch: 30, Loss: 1.021503
Train - Epoch 7, Batch: 0, Loss: 1.026674
Train - Epoch 7, Batch: 10, Loss: 1.017509
Train - Epoch 7, Batch: 20, Loss: 1.007018
Train - Epoch 7, Batch: 30, Loss: 1.003141
Train - Epoch 8, Batch: 0, Loss: 1.008394
Train - Epoch 8, Batch: 10, Loss: 1.005460
Train - Epoch 8, Batch: 20, Loss: 1.001011
Train - Epoch 8, Batch: 30, Loss: 0.995283
Train - Epoch 9, Batch: 0, Loss: 1.007569
Train - Epoch 9, Batch: 10, Loss: 0.995778
Train - Epoch 9, Batch: 20, Loss: 0.987896
Train - Epoch 9, Batch: 30, Loss: 0.991195
Train - Epoch 10, Batch: 0, Loss: 0.986892
Train - Epoch 10, Batch: 10, Loss: 0.986883
Train - Epoch 10, Batch: 20, Loss: 0.980143
Train - Epoch 10, Batch: 30, Loss: 0.983471
Train - Epoch 11, Batch: 0, Loss: 0.976821
Train - Epoch 11, Batch: 10, Loss: 0.983047
Train - Epoch 11, Batch: 20, Loss: 0.980805
Train - Epoch 11, Batch: 30, Loss: 0.963209
Train - Epoch 12, Batch: 0, Loss: 0.964325
Train - Epoch 12, Batch: 10, Loss: 0.960675
Train - Epoch 12, Batch: 20, Loss: 0.965401
Train - Epoch 12, Batch: 30, Loss: 0.960685
Train - Epoch 13, Batch: 0, Loss: 0.955490
Train - Epoch 13, Batch: 10, Loss: 0.963171
Train - Epoch 13, Batch: 20, Loss: 0.962300
Train - Epoch 13, Batch: 30, Loss: 0.957426
Train - Epoch 14, Batch: 0, Loss: 0.948534
Train - Epoch 14, Batch: 10, Loss: 0.954557
Train - Epoch 14, Batch: 20, Loss: 0.952774
Train - Epoch 14, Batch: 30, Loss: 0.940555
Train - Epoch 15, Batch: 0, Loss: 0.955479
Train - Epoch 15, Batch: 10, Loss: 0.941995
Train - Epoch 15, Batch: 20, Loss: 0.941569
Train - Epoch 15, Batch: 30, Loss: 0.945932
Train - Epoch 16, Batch: 0, Loss: 0.944228
Train - Epoch 16, Batch: 10, Loss: 0.949690
Train - Epoch 16, Batch: 20, Loss: 0.939183
Train - Epoch 16, Batch: 30, Loss: 0.940828
Train - Epoch 17, Batch: 0, Loss: 0.943857
Train - Epoch 17, Batch: 10, Loss: 0.944873
Train - Epoch 17, Batch: 20, Loss: 0.938524
Train - Epoch 17, Batch: 30, Loss: 0.923926
Train - Epoch 18, Batch: 0, Loss: 0.934641
Train - Epoch 18, Batch: 10, Loss: 0.946784
Train - Epoch 18, Batch: 20, Loss: 0.934152
Train - Epoch 18, Batch: 30, Loss: 0.932174
Train - Epoch 19, Batch: 0, Loss: 0.929130
Train - Epoch 19, Batch: 10, Loss: 0.924289
Train - Epoch 19, Batch: 20, Loss: 0.932472
Train - Epoch 19, Batch: 30, Loss: 0.924935
Train - Epoch 20, Batch: 0, Loss: 0.923210
Train - Epoch 20, Batch: 10, Loss: 0.927486
Train - Epoch 20, Batch: 20, Loss: 0.914770
Train - Epoch 20, Batch: 30, Loss: 0.920582
Train - Epoch 21, Batch: 0, Loss: 0.908127
Train - Epoch 21, Batch: 10, Loss: 0.912276
Train - Epoch 21, Batch: 20, Loss: 0.920224
Train - Epoch 21, Batch: 30, Loss: 0.926708
Train - Epoch 22, Batch: 0, Loss: 0.906709
Train - Epoch 22, Batch: 10, Loss: 0.927910
Train - Epoch 22, Batch: 20, Loss: 0.913265
Train - Epoch 22, Batch: 30, Loss: 0.913143
Train - Epoch 23, Batch: 0, Loss: 0.923240
Train - Epoch 23, Batch: 10, Loss: 0.925572
Train - Epoch 23, Batch: 20, Loss: 0.906835
Train - Epoch 23, Batch: 30, Loss: 0.920304
Train - Epoch 24, Batch: 0, Loss: 0.921382
Train - Epoch 24, Batch: 10, Loss: 0.907507
Train - Epoch 24, Batch: 20, Loss: 0.903339
Train - Epoch 24, Batch: 30, Loss: 0.911869
Train - Epoch 25, Batch: 0, Loss: 0.908686
Train - Epoch 25, Batch: 10, Loss: 0.911371
Train - Epoch 25, Batch: 20, Loss: 0.904520
Train - Epoch 25, Batch: 30, Loss: 0.922119
Train - Epoch 26, Batch: 0, Loss: 0.909515
Train - Epoch 26, Batch: 10, Loss: 0.901314
Train - Epoch 26, Batch: 20, Loss: 0.888451
Train - Epoch 26, Batch: 30, Loss: 0.909489
Train - Epoch 27, Batch: 0, Loss: 0.892296
Train - Epoch 27, Batch: 10, Loss: 0.898011
Train - Epoch 27, Batch: 20, Loss: 0.904780
Train - Epoch 27, Batch: 30, Loss: 0.904170
Train - Epoch 28, Batch: 0, Loss: 0.899865
Train - Epoch 28, Batch: 10, Loss: 0.898935
Train - Epoch 28, Batch: 20, Loss: 0.892994
Train - Epoch 28, Batch: 30, Loss: 0.896452
Train - Epoch 29, Batch: 0, Loss: 0.900479
Train - Epoch 29, Batch: 10, Loss: 0.896006
Train - Epoch 29, Batch: 20, Loss: 0.898253
Train - Epoch 29, Batch: 30, Loss: 0.906535
Train - Epoch 30, Batch: 0, Loss: 0.894027
Train - Epoch 30, Batch: 10, Loss: 0.891214
Train - Epoch 30, Batch: 20, Loss: 0.895673
Train - Epoch 30, Batch: 30, Loss: 0.888793
Train - Epoch 31, Batch: 0, Loss: 0.890835
Train - Epoch 31, Batch: 10, Loss: 0.892253
Train - Epoch 31, Batch: 20, Loss: 0.887540
Train - Epoch 31, Batch: 30, Loss: 0.893455
training_time:: 3.8000435829162598
training time full:: 3.800086736679077
provenance prepare time:: 4.76837158203125e-07
Test Avg. Loss: 0.000070, Accuracy: 0.629512
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 26
training time is 2.521561861038208
overhead:: 0
overhead2:: 0
time_baseline:: 2.522952079772949
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629529
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.01000523567199707
overhead3:: 0.026863574981689453
overhead4:: 0.1078338623046875
overhead5:: 0
time_provenance:: 0.6652882099151611
curr_diff: 0 tensor(3.4582e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.4582e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629529
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.013622283935546875
overhead3:: 0.04424595832824707
overhead4:: 0.11622738838195801
overhead5:: 0
time_provenance:: 0.7249324321746826
curr_diff: 0 tensor(3.7026e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.7026e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629529
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.013705730438232422
overhead3:: 0.03466606140136719
overhead4:: 0.132490873336792
overhead5:: 0
time_provenance:: 0.7293224334716797
curr_diff: 0 tensor(3.5941e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.5941e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629529
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.015880346298217773
overhead3:: 0.048683881759643555
overhead4:: 0.1499471664428711
overhead5:: 0
time_provenance:: 0.7218468189239502
curr_diff: 0 tensor(3.8987e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.8987e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629529
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.01949310302734375
overhead3:: 0.05977439880371094
overhead4:: 0.24562978744506836
overhead5:: 0
time_provenance:: 0.888530969619751
curr_diff: 0 tensor(2.4238e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.4238e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629529
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.02037501335144043
overhead3:: 0.0521390438079834
overhead4:: 0.23941898345947266
overhead5:: 0
time_provenance:: 0.8739807605743408
curr_diff: 0 tensor(2.4651e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.4651e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629529
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.021172285079956055
overhead3:: 0.05311012268066406
overhead4:: 0.24145913124084473
overhead5:: 0
time_provenance:: 0.8866653442382812
curr_diff: 0 tensor(2.5387e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.5387e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629529
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.023266077041625977
overhead3:: 0.05816531181335449
overhead4:: 0.25055527687072754
overhead5:: 0
time_provenance:: 0.9085164070129395
curr_diff: 0 tensor(2.5340e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.5340e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629529
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.032475948333740234
overhead3:: 0.0864555835723877
overhead4:: 0.4059433937072754
overhead5:: 0
time_provenance:: 1.1113991737365723
curr_diff: 0 tensor(9.0259e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.0259e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629529
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.03381204605102539
overhead3:: 0.08761119842529297
overhead4:: 0.41616129875183105
overhead5:: 0
time_provenance:: 1.1074721813201904
curr_diff: 0 tensor(9.0890e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.0890e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629529
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.03934836387634277
overhead3:: 0.0985567569732666
overhead4:: 0.4484670162200928
overhead5:: 0
time_provenance:: 1.240842580795288
curr_diff: 0 tensor(9.5307e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.5307e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629529
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.03682661056518555
overhead3:: 0.09638285636901855
overhead4:: 0.42514801025390625
overhead5:: 0
time_provenance:: 1.1545987129211426
curr_diff: 0 tensor(9.5343e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.5343e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629529
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.08336186408996582
overhead3:: 0.2090613842010498
overhead4:: 0.9669568538665771
overhead5:: 0
time_provenance:: 1.880648136138916
curr_diff: 0 tensor(1.9864e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.9864e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629529
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.08556151390075684
overhead3:: 0.22365903854370117
overhead4:: 0.9781506061553955
overhead5:: 0
time_provenance:: 1.8966717720031738
curr_diff: 0 tensor(2.0853e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.0853e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629529
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.08740997314453125
overhead3:: 0.21180105209350586
overhead4:: 0.9535186290740967
overhead5:: 0
time_provenance:: 1.8638646602630615
curr_diff: 0 tensor(2.1191e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1191e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629529
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.0982656478881836
overhead3:: 0.24187922477722168
overhead4:: 0.9608862400054932
overhead5:: 0
time_provenance:: 1.965040683746338
curr_diff: 0 tensor(2.1266e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1266e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629529
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.20720553398132324
overhead3:: 0.5818982124328613
overhead4:: 1.7273802757263184
overhead5:: 0
time_provenance:: 2.759383201599121
curr_diff: 0 tensor(1.2668e-15, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2668e-15, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629529
repetition 3
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 3 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.054706
Train - Epoch 0, Batch: 10, Loss: 1.395891
Train - Epoch 0, Batch: 20, Loss: 1.254654
Train - Epoch 0, Batch: 30, Loss: 1.194828
Train - Epoch 1, Batch: 0, Loss: 1.183718
Train - Epoch 1, Batch: 10, Loss: 1.165139
Train - Epoch 1, Batch: 20, Loss: 1.134552
Train - Epoch 1, Batch: 30, Loss: 1.138156
Train - Epoch 2, Batch: 0, Loss: 1.123419
Train - Epoch 2, Batch: 10, Loss: 1.113131
Train - Epoch 2, Batch: 20, Loss: 1.102554
Train - Epoch 2, Batch: 30, Loss: 1.092771
Train - Epoch 3, Batch: 0, Loss: 1.094241
Train - Epoch 3, Batch: 10, Loss: 1.085934
Train - Epoch 3, Batch: 20, Loss: 1.089650
Train - Epoch 3, Batch: 30, Loss: 1.076671
Train - Epoch 4, Batch: 0, Loss: 1.070472
Train - Epoch 4, Batch: 10, Loss: 1.065583
Train - Epoch 4, Batch: 20, Loss: 1.053933
Train - Epoch 4, Batch: 30, Loss: 1.050416
Train - Epoch 5, Batch: 0, Loss: 1.032018
Train - Epoch 5, Batch: 10, Loss: 1.051794
Train - Epoch 5, Batch: 20, Loss: 1.033813
Train - Epoch 5, Batch: 30, Loss: 1.027034
Train - Epoch 6, Batch: 0, Loss: 1.038120
Train - Epoch 6, Batch: 10, Loss: 1.026950
Train - Epoch 6, Batch: 20, Loss: 1.026497
Train - Epoch 6, Batch: 30, Loss: 1.026501
Train - Epoch 7, Batch: 0, Loss: 1.019187
Train - Epoch 7, Batch: 10, Loss: 1.015459
Train - Epoch 7, Batch: 20, Loss: 1.010916
Train - Epoch 7, Batch: 30, Loss: 1.013369
Train - Epoch 8, Batch: 0, Loss: 1.015904
Train - Epoch 8, Batch: 10, Loss: 0.991265
Train - Epoch 8, Batch: 20, Loss: 0.994096
Train - Epoch 8, Batch: 30, Loss: 0.995279
Train - Epoch 9, Batch: 0, Loss: 0.990947
Train - Epoch 9, Batch: 10, Loss: 0.998124
Train - Epoch 9, Batch: 20, Loss: 0.996069
Train - Epoch 9, Batch: 30, Loss: 0.974957
Train - Epoch 10, Batch: 0, Loss: 0.988498
Train - Epoch 10, Batch: 10, Loss: 0.981870
Train - Epoch 10, Batch: 20, Loss: 0.970909
Train - Epoch 10, Batch: 30, Loss: 0.976584
Train - Epoch 11, Batch: 0, Loss: 0.984057
Train - Epoch 11, Batch: 10, Loss: 0.982402
Train - Epoch 11, Batch: 20, Loss: 0.969860
Train - Epoch 11, Batch: 30, Loss: 0.966530
Train - Epoch 12, Batch: 0, Loss: 0.963836
Train - Epoch 12, Batch: 10, Loss: 0.965383
Train - Epoch 12, Batch: 20, Loss: 0.969830
Train - Epoch 12, Batch: 30, Loss: 0.969172
Train - Epoch 13, Batch: 0, Loss: 0.951590
Train - Epoch 13, Batch: 10, Loss: 0.959591
Train - Epoch 13, Batch: 20, Loss: 0.949529
Train - Epoch 13, Batch: 30, Loss: 0.954975
Train - Epoch 14, Batch: 0, Loss: 0.955830
Train - Epoch 14, Batch: 10, Loss: 0.972640
Train - Epoch 14, Batch: 20, Loss: 0.946700
Train - Epoch 14, Batch: 30, Loss: 0.952100
Train - Epoch 15, Batch: 0, Loss: 0.948606
Train - Epoch 15, Batch: 10, Loss: 0.950925
Train - Epoch 15, Batch: 20, Loss: 0.950546
Train - Epoch 15, Batch: 30, Loss: 0.935221
Train - Epoch 16, Batch: 0, Loss: 0.934884
Train - Epoch 16, Batch: 10, Loss: 0.951648
Train - Epoch 16, Batch: 20, Loss: 0.943407
Train - Epoch 16, Batch: 30, Loss: 0.942647
Train - Epoch 17, Batch: 0, Loss: 0.935654
Train - Epoch 17, Batch: 10, Loss: 0.938083
Train - Epoch 17, Batch: 20, Loss: 0.942825
Train - Epoch 17, Batch: 30, Loss: 0.937311
Train - Epoch 18, Batch: 0, Loss: 0.934488
Train - Epoch 18, Batch: 10, Loss: 0.934840
Train - Epoch 18, Batch: 20, Loss: 0.933510
Train - Epoch 18, Batch: 30, Loss: 0.945237
Train - Epoch 19, Batch: 0, Loss: 0.934847
Train - Epoch 19, Batch: 10, Loss: 0.928439
Train - Epoch 19, Batch: 20, Loss: 0.931249
Train - Epoch 19, Batch: 30, Loss: 0.919904
Train - Epoch 20, Batch: 0, Loss: 0.924386
Train - Epoch 20, Batch: 10, Loss: 0.927261
Train - Epoch 20, Batch: 20, Loss: 0.925172
Train - Epoch 20, Batch: 30, Loss: 0.927766
Train - Epoch 21, Batch: 0, Loss: 0.921892
Train - Epoch 21, Batch: 10, Loss: 0.923793
Train - Epoch 21, Batch: 20, Loss: 0.932790
Train - Epoch 21, Batch: 30, Loss: 0.917697
Train - Epoch 22, Batch: 0, Loss: 0.926452
Train - Epoch 22, Batch: 10, Loss: 0.928171
Train - Epoch 22, Batch: 20, Loss: 0.915490
Train - Epoch 22, Batch: 30, Loss: 0.925042
Train - Epoch 23, Batch: 0, Loss: 0.905073
Train - Epoch 23, Batch: 10, Loss: 0.916212
Train - Epoch 23, Batch: 20, Loss: 0.914351
Train - Epoch 23, Batch: 30, Loss: 0.920758
Train - Epoch 24, Batch: 0, Loss: 0.916374
Train - Epoch 24, Batch: 10, Loss: 0.914495
Train - Epoch 24, Batch: 20, Loss: 0.919340
Train - Epoch 24, Batch: 30, Loss: 0.910934
Train - Epoch 25, Batch: 0, Loss: 0.913700
Train - Epoch 25, Batch: 10, Loss: 0.909121
Train - Epoch 25, Batch: 20, Loss: 0.910516
Train - Epoch 25, Batch: 30, Loss: 0.910143
Train - Epoch 26, Batch: 0, Loss: 0.908527
Train - Epoch 26, Batch: 10, Loss: 0.906735
Train - Epoch 26, Batch: 20, Loss: 0.907520
Train - Epoch 26, Batch: 30, Loss: 0.898597
Train - Epoch 27, Batch: 0, Loss: 0.914079
Train - Epoch 27, Batch: 10, Loss: 0.907530
Train - Epoch 27, Batch: 20, Loss: 0.905715
Train - Epoch 27, Batch: 30, Loss: 0.904513
Train - Epoch 28, Batch: 0, Loss: 0.905902
Train - Epoch 28, Batch: 10, Loss: 0.902456
Train - Epoch 28, Batch: 20, Loss: 0.900947
Train - Epoch 28, Batch: 30, Loss: 0.897621
Train - Epoch 29, Batch: 0, Loss: 0.894700
Train - Epoch 29, Batch: 10, Loss: 0.895972
Train - Epoch 29, Batch: 20, Loss: 0.895022
Train - Epoch 29, Batch: 30, Loss: 0.897408
Train - Epoch 30, Batch: 0, Loss: 0.910456
Train - Epoch 30, Batch: 10, Loss: 0.894657
Train - Epoch 30, Batch: 20, Loss: 0.902735
Train - Epoch 30, Batch: 30, Loss: 0.891490
Train - Epoch 31, Batch: 0, Loss: 0.889183
Train - Epoch 31, Batch: 10, Loss: 0.893332
Train - Epoch 31, Batch: 20, Loss: 0.898351
Train - Epoch 31, Batch: 30, Loss: 0.888933
training_time:: 3.8275997638702393
training time full:: 3.8276445865631104
provenance prepare time:: 9.5367431640625e-07
Test Avg. Loss: 0.000070, Accuracy: 0.630544
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 26
training time is 2.0969176292419434
overhead:: 0
overhead2:: 0
time_baseline:: 2.0983693599700928
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630544
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.012243270874023438
overhead3:: 0.031102657318115234
overhead4:: 0.12360811233520508
overhead5:: 0
time_provenance:: 0.735985517501831
curr_diff: 0 tensor(4.8786e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.8786e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630562
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.012629985809326172
overhead3:: 0.03395843505859375
overhead4:: 0.11828327178955078
overhead5:: 0
time_provenance:: 0.738987922668457
curr_diff: 0 tensor(5.3517e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.3517e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630562
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.014958620071411133
overhead3:: 0.037206172943115234
overhead4:: 0.13420891761779785
overhead5:: 0
time_provenance:: 0.776564359664917
curr_diff: 0 tensor(5.6894e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.6894e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630562
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.015045881271362305
overhead3:: 0.03902888298034668
overhead4:: 0.15348124504089355
overhead5:: 0
time_provenance:: 0.7474720478057861
curr_diff: 0 tensor(5.4180e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.4180e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630562
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.019034147262573242
overhead3:: 0.0482177734375
overhead4:: 0.23456978797912598
overhead5:: 0
time_provenance:: 0.8511888980865479
curr_diff: 0 tensor(1.9207e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.9207e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630544
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.02008819580078125
overhead3:: 0.05257844924926758
overhead4:: 0.2371692657470703
overhead5:: 0
time_provenance:: 0.8708128929138184
curr_diff: 0 tensor(2.0190e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.0190e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630544
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.022648096084594727
overhead3:: 0.055574655532836914
overhead4:: 0.23655104637145996
overhead5:: 0
time_provenance:: 0.8953487873077393
curr_diff: 0 tensor(2.0293e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.0293e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630544
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.022180795669555664
overhead3:: 0.05865359306335449
overhead4:: 0.2776498794555664
overhead5:: 0
time_provenance:: 0.9292840957641602
curr_diff: 0 tensor(2.0581e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.0581e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630544
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.036695241928100586
overhead3:: 0.08797669410705566
overhead4:: 0.41491222381591797
overhead5:: 0
time_provenance:: 1.1301095485687256
curr_diff: 0 tensor(9.5287e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.5287e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630544
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.04761481285095215
overhead3:: 0.12541937828063965
overhead4:: 0.49510955810546875
overhead5:: 0
time_provenance:: 1.4660437107086182
curr_diff: 0 tensor(9.8976e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.8976e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630544
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.04111194610595703
overhead3:: 0.10165238380432129
overhead4:: 0.43714141845703125
overhead5:: 0
time_provenance:: 1.1697518825531006
curr_diff: 0 tensor(9.9235e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.9235e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630544
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.038375139236450195
overhead3:: 0.09548068046569824
overhead4:: 0.4273805618286133
overhead5:: 0
time_provenance:: 1.152360200881958
curr_diff: 0 tensor(1.0027e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0027e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630544
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.08907604217529297
overhead3:: 0.2120046615600586
overhead4:: 0.9659476280212402
overhead5:: 0
time_provenance:: 1.9000942707061768
curr_diff: 0 tensor(1.9960e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.9960e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630544
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.08413052558898926
overhead3:: 0.21500182151794434
overhead4:: 0.9744577407836914
overhead5:: 0
time_provenance:: 1.8919742107391357
curr_diff: 0 tensor(2.0540e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.0540e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630544
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.08414936065673828
overhead3:: 0.2078227996826172
overhead4:: 0.9885179996490479
overhead5:: 0
time_provenance:: 1.8929600715637207
curr_diff: 0 tensor(2.0407e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.0407e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630544
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.12492966651916504
overhead3:: 0.281888484954834
overhead4:: 1.1650207042694092
overhead5:: 0
time_provenance:: 2.4199752807617188
curr_diff: 0 tensor(2.0588e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.0588e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630544
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.19429850578308105
overhead3:: 0.5269827842712402
overhead4:: 1.6709461212158203
overhead5:: 0
time_provenance:: 2.641578435897827
curr_diff: 0 tensor(1.2498e-15, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2498e-15, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630544
repetition 4
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 4 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.074465
Train - Epoch 0, Batch: 10, Loss: 1.416320
Train - Epoch 0, Batch: 20, Loss: 1.252619
Train - Epoch 0, Batch: 30, Loss: 1.193963
Train - Epoch 1, Batch: 0, Loss: 1.194866
Train - Epoch 1, Batch: 10, Loss: 1.172096
Train - Epoch 1, Batch: 20, Loss: 1.152373
Train - Epoch 1, Batch: 30, Loss: 1.128529
Train - Epoch 2, Batch: 0, Loss: 1.130498
Train - Epoch 2, Batch: 10, Loss: 1.118214
Train - Epoch 2, Batch: 20, Loss: 1.097619
Train - Epoch 2, Batch: 30, Loss: 1.104105
Train - Epoch 3, Batch: 0, Loss: 1.097441
Train - Epoch 3, Batch: 10, Loss: 1.087271
Train - Epoch 3, Batch: 20, Loss: 1.085148
Train - Epoch 3, Batch: 30, Loss: 1.076621
Train - Epoch 4, Batch: 0, Loss: 1.060967
Train - Epoch 4, Batch: 10, Loss: 1.063481
Train - Epoch 4, Batch: 20, Loss: 1.062850
Train - Epoch 4, Batch: 30, Loss: 1.060085
Train - Epoch 5, Batch: 0, Loss: 1.052296
Train - Epoch 5, Batch: 10, Loss: 1.039261
Train - Epoch 5, Batch: 20, Loss: 1.037022
Train - Epoch 5, Batch: 30, Loss: 1.041962
Train - Epoch 6, Batch: 0, Loss: 1.035590
Train - Epoch 6, Batch: 10, Loss: 1.028101
Train - Epoch 6, Batch: 20, Loss: 1.018030
Train - Epoch 6, Batch: 30, Loss: 1.026812
Train - Epoch 7, Batch: 0, Loss: 1.013357
Train - Epoch 7, Batch: 10, Loss: 1.010925
Train - Epoch 7, Batch: 20, Loss: 1.016957
Train - Epoch 7, Batch: 30, Loss: 1.018368
Train - Epoch 8, Batch: 0, Loss: 1.012954
Train - Epoch 8, Batch: 10, Loss: 1.003349
Train - Epoch 8, Batch: 20, Loss: 0.997931
Train - Epoch 8, Batch: 30, Loss: 1.001847
Train - Epoch 9, Batch: 0, Loss: 0.996496
Train - Epoch 9, Batch: 10, Loss: 0.991989
Train - Epoch 9, Batch: 20, Loss: 0.996637
Train - Epoch 9, Batch: 30, Loss: 0.996022
Train - Epoch 10, Batch: 0, Loss: 0.990338
Train - Epoch 10, Batch: 10, Loss: 0.986204
Train - Epoch 10, Batch: 20, Loss: 0.981627
Train - Epoch 10, Batch: 30, Loss: 0.977095
Train - Epoch 11, Batch: 0, Loss: 0.983640
Train - Epoch 11, Batch: 10, Loss: 0.974442
Train - Epoch 11, Batch: 20, Loss: 0.972141
Train - Epoch 11, Batch: 30, Loss: 0.975358
Train - Epoch 12, Batch: 0, Loss: 0.961405
Train - Epoch 12, Batch: 10, Loss: 0.966153
Train - Epoch 12, Batch: 20, Loss: 0.970063
Train - Epoch 12, Batch: 30, Loss: 0.969710
Train - Epoch 13, Batch: 0, Loss: 0.957966
Train - Epoch 13, Batch: 10, Loss: 0.960156
Train - Epoch 13, Batch: 20, Loss: 0.952176
Train - Epoch 13, Batch: 30, Loss: 0.963960
Train - Epoch 14, Batch: 0, Loss: 0.948381
Train - Epoch 14, Batch: 10, Loss: 0.944268
Train - Epoch 14, Batch: 20, Loss: 0.955953
Train - Epoch 14, Batch: 30, Loss: 0.956550
Train - Epoch 15, Batch: 0, Loss: 0.949963
Train - Epoch 15, Batch: 10, Loss: 0.943584
Train - Epoch 15, Batch: 20, Loss: 0.950216
Train - Epoch 15, Batch: 30, Loss: 0.941591
Train - Epoch 16, Batch: 0, Loss: 0.944782
Train - Epoch 16, Batch: 10, Loss: 0.945676
Train - Epoch 16, Batch: 20, Loss: 0.945623
Train - Epoch 16, Batch: 30, Loss: 0.953056
Train - Epoch 17, Batch: 0, Loss: 0.943861
Train - Epoch 17, Batch: 10, Loss: 0.940694
Train - Epoch 17, Batch: 20, Loss: 0.936037
Train - Epoch 17, Batch: 30, Loss: 0.948934
Train - Epoch 18, Batch: 0, Loss: 0.939645
Train - Epoch 18, Batch: 10, Loss: 0.931234
Train - Epoch 18, Batch: 20, Loss: 0.931483
Train - Epoch 18, Batch: 30, Loss: 0.936535
Train - Epoch 19, Batch: 0, Loss: 0.939778
Train - Epoch 19, Batch: 10, Loss: 0.926735
Train - Epoch 19, Batch: 20, Loss: 0.931239
Train - Epoch 19, Batch: 30, Loss: 0.931935
Train - Epoch 20, Batch: 0, Loss: 0.922506
Train - Epoch 20, Batch: 10, Loss: 0.930825
Train - Epoch 20, Batch: 20, Loss: 0.929537
Train - Epoch 20, Batch: 30, Loss: 0.919938
Train - Epoch 21, Batch: 0, Loss: 0.940266
Train - Epoch 21, Batch: 10, Loss: 0.938912
Train - Epoch 21, Batch: 20, Loss: 0.918166
Train - Epoch 21, Batch: 30, Loss: 0.919370
Train - Epoch 22, Batch: 0, Loss: 0.928474
Train - Epoch 22, Batch: 10, Loss: 0.917185
Train - Epoch 22, Batch: 20, Loss: 0.921409
Train - Epoch 22, Batch: 30, Loss: 0.918656
Train - Epoch 23, Batch: 0, Loss: 0.927401
Train - Epoch 23, Batch: 10, Loss: 0.922239
Train - Epoch 23, Batch: 20, Loss: 0.922225
Train - Epoch 23, Batch: 30, Loss: 0.919549
Train - Epoch 24, Batch: 0, Loss: 0.916952
Train - Epoch 24, Batch: 10, Loss: 0.905203
Train - Epoch 24, Batch: 20, Loss: 0.914672
Train - Epoch 24, Batch: 30, Loss: 0.910990
Train - Epoch 25, Batch: 0, Loss: 0.910606
Train - Epoch 25, Batch: 10, Loss: 0.907190
Train - Epoch 25, Batch: 20, Loss: 0.914064
Train - Epoch 25, Batch: 30, Loss: 0.915482
Train - Epoch 26, Batch: 0, Loss: 0.903683
Train - Epoch 26, Batch: 10, Loss: 0.908046
Train - Epoch 26, Batch: 20, Loss: 0.903644
Train - Epoch 26, Batch: 30, Loss: 0.907838
Train - Epoch 27, Batch: 0, Loss: 0.915050
Train - Epoch 27, Batch: 10, Loss: 0.907270
Train - Epoch 27, Batch: 20, Loss: 0.897072
Train - Epoch 27, Batch: 30, Loss: 0.903046
Train - Epoch 28, Batch: 0, Loss: 0.895826
Train - Epoch 28, Batch: 10, Loss: 0.896427
Train - Epoch 28, Batch: 20, Loss: 0.893211
Train - Epoch 28, Batch: 30, Loss: 0.902373
Train - Epoch 29, Batch: 0, Loss: 0.905908
Train - Epoch 29, Batch: 10, Loss: 0.904201
Train - Epoch 29, Batch: 20, Loss: 0.897738
Train - Epoch 29, Batch: 30, Loss: 0.895313
Train - Epoch 30, Batch: 0, Loss: 0.895863
Train - Epoch 30, Batch: 10, Loss: 0.901087
Train - Epoch 30, Batch: 20, Loss: 0.903995
Train - Epoch 30, Batch: 30, Loss: 0.903214
Train - Epoch 31, Batch: 0, Loss: 0.904609
Train - Epoch 31, Batch: 10, Loss: 0.894587
Train - Epoch 31, Batch: 20, Loss: 0.886773
Train - Epoch 31, Batch: 30, Loss: 0.895590
training_time:: 3.749983072280884
training time full:: 3.7500240802764893
provenance prepare time:: 4.76837158203125e-07
Test Avg. Loss: 0.000070, Accuracy: 0.630080
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 26
training time is 2.1141934394836426
overhead:: 0
overhead2:: 0
time_baseline:: 2.115898847579956
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630062
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.011634349822998047
overhead3:: 0.029078006744384766
overhead4:: 0.11577773094177246
overhead5:: 0
time_provenance:: 0.7192907333374023
curr_diff: 0 tensor(4.7596e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.7596e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630080
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.012819051742553711
overhead3:: 0.033866167068481445
overhead4:: 0.13474583625793457
overhead5:: 0
time_provenance:: 0.781731128692627
curr_diff: 0 tensor(4.5206e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.5206e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630062
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.014397144317626953
overhead3:: 0.03614974021911621
overhead4:: 0.13261938095092773
overhead5:: 0
time_provenance:: 0.7403628826141357
curr_diff: 0 tensor(4.8494e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.8494e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630080
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.016246318817138672
overhead3:: 0.048210859298706055
overhead4:: 0.15123510360717773
overhead5:: 0
time_provenance:: 0.7312402725219727
curr_diff: 0 tensor(4.5095e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.5095e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630062
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.021128177642822266
overhead3:: 0.05409502983093262
overhead4:: 0.23279333114624023
overhead5:: 0
time_provenance:: 0.88179612159729
curr_diff: 0 tensor(2.0531e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.0531e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630062
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.020519495010375977
overhead3:: 0.05082345008850098
overhead4:: 0.2401571273803711
overhead5:: 0
time_provenance:: 0.8841183185577393
curr_diff: 0 tensor(2.1046e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1046e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630062
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.0225677490234375
overhead3:: 0.06755924224853516
overhead4:: 0.25179409980773926
overhead5:: 0
time_provenance:: 0.9017488956451416
curr_diff: 0 tensor(2.1168e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1168e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630062
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.026524782180786133
overhead3:: 0.0639655590057373
overhead4:: 0.2786431312561035
overhead5:: 0
time_provenance:: 0.9339182376861572
curr_diff: 0 tensor(2.1193e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1193e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630062
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.036371707916259766
overhead3:: 0.10050559043884277
overhead4:: 0.4084892272949219
overhead5:: 0
time_provenance:: 1.158858060836792
curr_diff: 0 tensor(8.6043e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.6043e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630062
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.035424232482910156
overhead3:: 0.08926177024841309
overhead4:: 0.43737077713012695
overhead5:: 0
time_provenance:: 1.1518704891204834
curr_diff: 0 tensor(8.8912e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.8912e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630062
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.03807950019836426
overhead3:: 0.09626364707946777
overhead4:: 0.41202569007873535
overhead5:: 0
time_provenance:: 1.1418652534484863
curr_diff: 0 tensor(8.9565e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.9565e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630062
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.04127168655395508
overhead3:: 0.10307431221008301
overhead4:: 0.4620683193206787
overhead5:: 0
time_provenance:: 1.2495040893554688
curr_diff: 0 tensor(8.9846e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.9846e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630062
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.09306955337524414
overhead3:: 0.2333509922027588
overhead4:: 1.0514402389526367
overhead5:: 0
time_provenance:: 2.115391731262207
curr_diff: 0 tensor(2.3029e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.3029e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630062
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.08186197280883789
overhead3:: 0.20979857444763184
overhead4:: 0.9680988788604736
overhead5:: 0
time_provenance:: 1.8977487087249756
curr_diff: 0 tensor(2.3165e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.3165e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630062
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.08211231231689453
overhead3:: 0.20974349975585938
overhead4:: 0.9556586742401123
overhead5:: 0
time_provenance:: 1.8763842582702637
curr_diff: 0 tensor(2.3388e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.3388e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630062
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.08433985710144043
overhead3:: 0.21190977096557617
overhead4:: 0.9968075752258301
overhead5:: 0
time_provenance:: 1.919764757156372
curr_diff: 0 tensor(2.3430e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.3430e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630062
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.19901394844055176
overhead3:: 0.5299115180969238
overhead4:: 1.674759864807129
overhead5:: 0
time_provenance:: 2.6627726554870605
curr_diff: 0 tensor(8.7404e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.7404e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630062
deletion rate:: 0.0001
python3 generate_rand_ids 0.0001  covtype 0
tensor([  5378, 277122,  96643, 391428,  96645, 273797, 183688, 417795, 507524,
        379784, 344968, 270357,  42901,  87191, 461721,   2847, 323104, 389411,
        279846, 518951, 255018, 298026,  64940, 450347, 132015, 264375, 486973,
        342720, 480582, 170568, 352847, 315473, 485331, 314748, 390617, 394714,
        373849, 425947, 229340,  71902, 208217, 106108, 235618, 105442, 206309,
        158950, 127591, 262140, 335868, 441076, 489463, 159484])
batch size:: 16384
repetition 0
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 0 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 1.919721
Train - Epoch 0, Batch: 10, Loss: 1.370873
Train - Epoch 0, Batch: 20, Loss: 1.236298
Train - Epoch 0, Batch: 30, Loss: 1.190386
Train - Epoch 1, Batch: 0, Loss: 1.186408
Train - Epoch 1, Batch: 10, Loss: 1.154293
Train - Epoch 1, Batch: 20, Loss: 1.144045
Train - Epoch 1, Batch: 30, Loss: 1.121190
Train - Epoch 2, Batch: 0, Loss: 1.124991
Train - Epoch 2, Batch: 10, Loss: 1.113237
Train - Epoch 2, Batch: 20, Loss: 1.107676
Train - Epoch 2, Batch: 30, Loss: 1.101469
Train - Epoch 3, Batch: 0, Loss: 1.098431
Train - Epoch 3, Batch: 10, Loss: 1.082831
Train - Epoch 3, Batch: 20, Loss: 1.078686
Train - Epoch 3, Batch: 30, Loss: 1.079315
Train - Epoch 4, Batch: 0, Loss: 1.083845
Train - Epoch 4, Batch: 10, Loss: 1.060741
Train - Epoch 4, Batch: 20, Loss: 1.066425
Train - Epoch 4, Batch: 30, Loss: 1.056258
Train - Epoch 5, Batch: 0, Loss: 1.058458
Train - Epoch 5, Batch: 10, Loss: 1.043502
Train - Epoch 5, Batch: 20, Loss: 1.035602
Train - Epoch 5, Batch: 30, Loss: 1.042986
Train - Epoch 6, Batch: 0, Loss: 1.037140
Train - Epoch 6, Batch: 10, Loss: 1.037837
Train - Epoch 6, Batch: 20, Loss: 1.025196
Train - Epoch 6, Batch: 30, Loss: 1.024853
Train - Epoch 7, Batch: 0, Loss: 1.017054
Train - Epoch 7, Batch: 10, Loss: 1.015651
Train - Epoch 7, Batch: 20, Loss: 1.016098
Train - Epoch 7, Batch: 30, Loss: 1.004406
Train - Epoch 8, Batch: 0, Loss: 1.000959
Train - Epoch 8, Batch: 10, Loss: 0.999847
Train - Epoch 8, Batch: 20, Loss: 1.002893
Train - Epoch 8, Batch: 30, Loss: 1.011290
Train - Epoch 9, Batch: 0, Loss: 0.989537
Train - Epoch 9, Batch: 10, Loss: 0.997720
Train - Epoch 9, Batch: 20, Loss: 0.997745
Train - Epoch 9, Batch: 30, Loss: 0.988559
Train - Epoch 10, Batch: 0, Loss: 0.998496
Train - Epoch 10, Batch: 10, Loss: 0.994904
Train - Epoch 10, Batch: 20, Loss: 0.986054
Train - Epoch 10, Batch: 30, Loss: 0.978519
Train - Epoch 11, Batch: 0, Loss: 0.972130
Train - Epoch 11, Batch: 10, Loss: 0.979678
Train - Epoch 11, Batch: 20, Loss: 0.979549
Train - Epoch 11, Batch: 30, Loss: 0.974788
Train - Epoch 12, Batch: 0, Loss: 0.971112
Train - Epoch 12, Batch: 10, Loss: 0.964985
Train - Epoch 12, Batch: 20, Loss: 0.979648
Train - Epoch 12, Batch: 30, Loss: 0.954737
Train - Epoch 13, Batch: 0, Loss: 0.964483
Train - Epoch 13, Batch: 10, Loss: 0.960054
Train - Epoch 13, Batch: 20, Loss: 0.969568
Train - Epoch 13, Batch: 30, Loss: 0.975756
Train - Epoch 14, Batch: 0, Loss: 0.955984
Train - Epoch 14, Batch: 10, Loss: 0.956244
Train - Epoch 14, Batch: 20, Loss: 0.959468
Train - Epoch 14, Batch: 30, Loss: 0.947200
Train - Epoch 15, Batch: 0, Loss: 0.948220
Train - Epoch 15, Batch: 10, Loss: 0.954481
Train - Epoch 15, Batch: 20, Loss: 0.947642
Train - Epoch 15, Batch: 30, Loss: 0.950389
Train - Epoch 16, Batch: 0, Loss: 0.948023
Train - Epoch 16, Batch: 10, Loss: 0.945816
Train - Epoch 16, Batch: 20, Loss: 0.947254
Train - Epoch 16, Batch: 30, Loss: 0.946929
Train - Epoch 17, Batch: 0, Loss: 0.940666
Train - Epoch 17, Batch: 10, Loss: 0.940556
Train - Epoch 17, Batch: 20, Loss: 0.941728
Train - Epoch 17, Batch: 30, Loss: 0.928331
Train - Epoch 18, Batch: 0, Loss: 0.939920
Train - Epoch 18, Batch: 10, Loss: 0.934520
Train - Epoch 18, Batch: 20, Loss: 0.930867
Train - Epoch 18, Batch: 30, Loss: 0.930547
Train - Epoch 19, Batch: 0, Loss: 0.926527
Train - Epoch 19, Batch: 10, Loss: 0.927866
Train - Epoch 19, Batch: 20, Loss: 0.922580
Train - Epoch 19, Batch: 30, Loss: 0.927541
Train - Epoch 20, Batch: 0, Loss: 0.930091
Train - Epoch 20, Batch: 10, Loss: 0.930907
Train - Epoch 20, Batch: 20, Loss: 0.929935
Train - Epoch 20, Batch: 30, Loss: 0.922595
Train - Epoch 21, Batch: 0, Loss: 0.920476
Train - Epoch 21, Batch: 10, Loss: 0.921821
Train - Epoch 21, Batch: 20, Loss: 0.924318
Train - Epoch 21, Batch: 30, Loss: 0.920907
Train - Epoch 22, Batch: 0, Loss: 0.933779
Train - Epoch 22, Batch: 10, Loss: 0.925709
Train - Epoch 22, Batch: 20, Loss: 0.921602
Train - Epoch 22, Batch: 30, Loss: 0.920198
Train - Epoch 23, Batch: 0, Loss: 0.916206
Train - Epoch 23, Batch: 10, Loss: 0.913905
Train - Epoch 23, Batch: 20, Loss: 0.918206
Train - Epoch 23, Batch: 30, Loss: 0.912257
Train - Epoch 24, Batch: 0, Loss: 0.906213
Train - Epoch 24, Batch: 10, Loss: 0.911052
Train - Epoch 24, Batch: 20, Loss: 0.915898
Train - Epoch 24, Batch: 30, Loss: 0.909554
Train - Epoch 25, Batch: 0, Loss: 0.915808
Train - Epoch 25, Batch: 10, Loss: 0.918800
Train - Epoch 25, Batch: 20, Loss: 0.906697
Train - Epoch 25, Batch: 30, Loss: 0.904411
Train - Epoch 26, Batch: 0, Loss: 0.899751
Train - Epoch 26, Batch: 10, Loss: 0.903974
Train - Epoch 26, Batch: 20, Loss: 0.907968
Train - Epoch 26, Batch: 30, Loss: 0.913579
Train - Epoch 27, Batch: 0, Loss: 0.903703
Train - Epoch 27, Batch: 10, Loss: 0.909577
Train - Epoch 27, Batch: 20, Loss: 0.907596
Train - Epoch 27, Batch: 30, Loss: 0.898064
Train - Epoch 28, Batch: 0, Loss: 0.901406
Train - Epoch 28, Batch: 10, Loss: 0.912291
Train - Epoch 28, Batch: 20, Loss: 0.907016
Train - Epoch 28, Batch: 30, Loss: 0.902159
Train - Epoch 29, Batch: 0, Loss: 0.894186
Train - Epoch 29, Batch: 10, Loss: 0.900288
Train - Epoch 29, Batch: 20, Loss: 0.907581
Train - Epoch 29, Batch: 30, Loss: 0.898958
Train - Epoch 30, Batch: 0, Loss: 0.895766
Train - Epoch 30, Batch: 10, Loss: 0.892944
Train - Epoch 30, Batch: 20, Loss: 0.897396
Train - Epoch 30, Batch: 30, Loss: 0.898049
Train - Epoch 31, Batch: 0, Loss: 0.901718
Train - Epoch 31, Batch: 10, Loss: 0.895357
Train - Epoch 31, Batch: 20, Loss: 0.889411
Train - Epoch 31, Batch: 30, Loss: 0.901026
training_time:: 4.000913381576538
training time full:: 4.00095796585083
provenance prepare time:: 7.152557373046875e-07
Test Avg. Loss: 0.000070, Accuracy: 0.628651
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 52
training time is 2.2186977863311768
overhead:: 0
overhead2:: 0
time_baseline:: 2.2201085090637207
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628668
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.017189741134643555
overhead3:: 0.03158116340637207
overhead4:: 0.10638189315795898
overhead5:: 0
time_provenance:: 0.8673887252807617
curr_diff: 0 tensor(5.3238e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.3238e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628668
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.017958641052246094
overhead3:: 0.054369449615478516
overhead4:: 0.13051080703735352
overhead5:: 0
time_provenance:: 0.892444372177124
curr_diff: 0 tensor(5.9491e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.9491e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628668
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.02120232582092285
overhead3:: 0.036041259765625
overhead4:: 0.14014720916748047
overhead5:: 0
time_provenance:: 0.8579330444335938
curr_diff: 0 tensor(5.7343e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.7343e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628668
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.020410776138305664
overhead3:: 0.04492592811584473
overhead4:: 0.1657264232635498
overhead5:: 0
time_provenance:: 0.8640508651733398
curr_diff: 0 tensor(6.6059e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.6059e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628668
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.026880264282226562
overhead3:: 0.04889535903930664
overhead4:: 0.2184751033782959
overhead5:: 0
time_provenance:: 0.9605042934417725
curr_diff: 0 tensor(2.2258e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.2258e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628668
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.028286218643188477
overhead3:: 0.05178117752075195
overhead4:: 0.22662639617919922
overhead5:: 0
time_provenance:: 0.9844856262207031
curr_diff: 0 tensor(2.2601e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.2601e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628668
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.030945777893066406
overhead3:: 0.06612610816955566
overhead4:: 0.24932575225830078
overhead5:: 0
time_provenance:: 0.9856789112091064
curr_diff: 0 tensor(2.5375e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.5375e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628668
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.032053470611572266
overhead3:: 0.05759072303771973
overhead4:: 0.2534146308898926
overhead5:: 0
time_provenance:: 1.0133881568908691
curr_diff: 0 tensor(2.5718e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.5718e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628668
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.05265069007873535
overhead3:: 0.08254241943359375
overhead4:: 0.35741519927978516
overhead5:: 0
time_provenance:: 1.1536166667938232
curr_diff: 0 tensor(1.2888e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2888e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628668
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.05195021629333496
overhead3:: 0.09194636344909668
overhead4:: 0.41034388542175293
overhead5:: 0
time_provenance:: 1.2374308109283447
curr_diff: 0 tensor(1.2712e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2712e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628668
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.05274653434753418
overhead3:: 0.09391522407531738
overhead4:: 0.4567279815673828
overhead5:: 0
time_provenance:: 1.2839980125427246
curr_diff: 0 tensor(1.3365e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3365e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628668
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.05852460861206055
overhead3:: 0.10354757308959961
overhead4:: 0.3960075378417969
overhead5:: 0
time_provenance:: 1.2237112522125244
curr_diff: 0 tensor(1.3564e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3564e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628668
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.11508679389953613
overhead3:: 0.19241642951965332
overhead4:: 0.8424942493438721
overhead5:: 0
time_provenance:: 1.7756190299987793
curr_diff: 0 tensor(1.9541e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.9541e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628668
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.12262606620788574
overhead3:: 0.2199857234954834
overhead4:: 0.9731745719909668
overhead5:: 0
time_provenance:: 1.9814763069152832
curr_diff: 0 tensor(2.0318e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.0318e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628668
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.12105679512023926
overhead3:: 0.20888519287109375
overhead4:: 0.9622235298156738
overhead5:: 0
time_provenance:: 1.9464998245239258
curr_diff: 0 tensor(2.1971e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1971e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628668
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.1265125274658203
overhead3:: 0.23580002784729004
overhead4:: 0.9754452705383301
overhead5:: 0
time_provenance:: 2.001343250274658
curr_diff: 0 tensor(2.2133e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.2133e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628668
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.24612927436828613
overhead3:: 0.4166431427001953
overhead4:: 1.5377652645111084
overhead5:: 0
time_provenance:: 2.467379570007324
curr_diff: 0 tensor(1.0490e-15, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0490e-15, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628668
repetition 1
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 1 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 1.891628
Train - Epoch 0, Batch: 10, Loss: 1.376056
Train - Epoch 0, Batch: 20, Loss: 1.247176
Train - Epoch 0, Batch: 30, Loss: 1.207694
Train - Epoch 1, Batch: 0, Loss: 1.186214
Train - Epoch 1, Batch: 10, Loss: 1.161542
Train - Epoch 1, Batch: 20, Loss: 1.157671
Train - Epoch 1, Batch: 30, Loss: 1.133463
Train - Epoch 2, Batch: 0, Loss: 1.137285
Train - Epoch 2, Batch: 10, Loss: 1.119451
Train - Epoch 2, Batch: 20, Loss: 1.112462
Train - Epoch 2, Batch: 30, Loss: 1.095522
Train - Epoch 3, Batch: 0, Loss: 1.100191
Train - Epoch 3, Batch: 10, Loss: 1.093037
Train - Epoch 3, Batch: 20, Loss: 1.088154
Train - Epoch 3, Batch: 30, Loss: 1.075955
Train - Epoch 4, Batch: 0, Loss: 1.089851
Train - Epoch 4, Batch: 10, Loss: 1.081150
Train - Epoch 4, Batch: 20, Loss: 1.061629
Train - Epoch 4, Batch: 30, Loss: 1.059798
Train - Epoch 5, Batch: 0, Loss: 1.053765
Train - Epoch 5, Batch: 10, Loss: 1.047740
Train - Epoch 5, Batch: 20, Loss: 1.045058
Train - Epoch 5, Batch: 30, Loss: 1.041266
Train - Epoch 6, Batch: 0, Loss: 1.039414
Train - Epoch 6, Batch: 10, Loss: 1.041757
Train - Epoch 6, Batch: 20, Loss: 1.028869
Train - Epoch 6, Batch: 30, Loss: 1.030467
Train - Epoch 7, Batch: 0, Loss: 1.020712
Train - Epoch 7, Batch: 10, Loss: 1.023317
Train - Epoch 7, Batch: 20, Loss: 1.014629
Train - Epoch 7, Batch: 30, Loss: 1.007610
Train - Epoch 8, Batch: 0, Loss: 1.009288
Train - Epoch 8, Batch: 10, Loss: 1.001300
Train - Epoch 8, Batch: 20, Loss: 1.003850
Train - Epoch 8, Batch: 30, Loss: 0.996113
Train - Epoch 9, Batch: 0, Loss: 0.992410
Train - Epoch 9, Batch: 10, Loss: 0.996515
Train - Epoch 9, Batch: 20, Loss: 0.992492
Train - Epoch 9, Batch: 30, Loss: 0.994628
Train - Epoch 10, Batch: 0, Loss: 0.982317
Train - Epoch 10, Batch: 10, Loss: 0.992065
Train - Epoch 10, Batch: 20, Loss: 0.991517
Train - Epoch 10, Batch: 30, Loss: 0.974675
Train - Epoch 11, Batch: 0, Loss: 0.986211
Train - Epoch 11, Batch: 10, Loss: 0.972852
Train - Epoch 11, Batch: 20, Loss: 0.978495
Train - Epoch 11, Batch: 30, Loss: 0.969421
Train - Epoch 12, Batch: 0, Loss: 0.974137
Train - Epoch 12, Batch: 10, Loss: 0.974166
Train - Epoch 12, Batch: 20, Loss: 0.962373
Train - Epoch 12, Batch: 30, Loss: 0.964222
Train - Epoch 13, Batch: 0, Loss: 0.965527
Train - Epoch 13, Batch: 10, Loss: 0.960022
Train - Epoch 13, Batch: 20, Loss: 0.956226
Train - Epoch 13, Batch: 30, Loss: 0.967727
Train - Epoch 14, Batch: 0, Loss: 0.960960
Train - Epoch 14, Batch: 10, Loss: 0.958803
Train - Epoch 14, Batch: 20, Loss: 0.952937
Train - Epoch 14, Batch: 30, Loss: 0.954490
Train - Epoch 15, Batch: 0, Loss: 0.960738
Train - Epoch 15, Batch: 10, Loss: 0.943241
Train - Epoch 15, Batch: 20, Loss: 0.962913
Train - Epoch 15, Batch: 30, Loss: 0.947688
Train - Epoch 16, Batch: 0, Loss: 0.942110
Train - Epoch 16, Batch: 10, Loss: 0.947513
Train - Epoch 16, Batch: 20, Loss: 0.934522
Train - Epoch 16, Batch: 30, Loss: 0.950700
Train - Epoch 17, Batch: 0, Loss: 0.938404
Train - Epoch 17, Batch: 10, Loss: 0.941126
Train - Epoch 17, Batch: 20, Loss: 0.940195
Train - Epoch 17, Batch: 30, Loss: 0.935455
Train - Epoch 18, Batch: 0, Loss: 0.938983
Train - Epoch 18, Batch: 10, Loss: 0.940832
Train - Epoch 18, Batch: 20, Loss: 0.941110
Train - Epoch 18, Batch: 30, Loss: 0.929561
Train - Epoch 19, Batch: 0, Loss: 0.940520
Train - Epoch 19, Batch: 10, Loss: 0.943576
Train - Epoch 19, Batch: 20, Loss: 0.927109
Train - Epoch 19, Batch: 30, Loss: 0.936127
Train - Epoch 20, Batch: 0, Loss: 0.926080
Train - Epoch 20, Batch: 10, Loss: 0.927705
Train - Epoch 20, Batch: 20, Loss: 0.934574
Train - Epoch 20, Batch: 30, Loss: 0.924247
Train - Epoch 21, Batch: 0, Loss: 0.933811
Train - Epoch 21, Batch: 10, Loss: 0.927013
Train - Epoch 21, Batch: 20, Loss: 0.920061
Train - Epoch 21, Batch: 30, Loss: 0.925282
Train - Epoch 22, Batch: 0, Loss: 0.922439
Train - Epoch 22, Batch: 10, Loss: 0.918452
Train - Epoch 22, Batch: 20, Loss: 0.918898
Train - Epoch 22, Batch: 30, Loss: 0.915003
Train - Epoch 23, Batch: 0, Loss: 0.928762
Train - Epoch 23, Batch: 10, Loss: 0.914388
Train - Epoch 23, Batch: 20, Loss: 0.912385
Train - Epoch 23, Batch: 30, Loss: 0.908363
Train - Epoch 24, Batch: 0, Loss: 0.921871
Train - Epoch 24, Batch: 10, Loss: 0.915869
Train - Epoch 24, Batch: 20, Loss: 0.906985
Train - Epoch 24, Batch: 30, Loss: 0.914081
Train - Epoch 25, Batch: 0, Loss: 0.908898
Train - Epoch 25, Batch: 10, Loss: 0.911784
Train - Epoch 25, Batch: 20, Loss: 0.914238
Train - Epoch 25, Batch: 30, Loss: 0.913876
Train - Epoch 26, Batch: 0, Loss: 0.909840
Train - Epoch 26, Batch: 10, Loss: 0.907106
Train - Epoch 26, Batch: 20, Loss: 0.907086
Train - Epoch 26, Batch: 30, Loss: 0.897341
Train - Epoch 27, Batch: 0, Loss: 0.917523
Train - Epoch 27, Batch: 10, Loss: 0.906871
Train - Epoch 27, Batch: 20, Loss: 0.902017
Train - Epoch 27, Batch: 30, Loss: 0.903653
Train - Epoch 28, Batch: 0, Loss: 0.902755
Train - Epoch 28, Batch: 10, Loss: 0.905075
Train - Epoch 28, Batch: 20, Loss: 0.911172
Train - Epoch 28, Batch: 30, Loss: 0.907753
Train - Epoch 29, Batch: 0, Loss: 0.897731
Train - Epoch 29, Batch: 10, Loss: 0.902775
Train - Epoch 29, Batch: 20, Loss: 0.897541
Train - Epoch 29, Batch: 30, Loss: 0.903601
Train - Epoch 30, Batch: 0, Loss: 0.902458
Train - Epoch 30, Batch: 10, Loss: 0.900861
Train - Epoch 30, Batch: 20, Loss: 0.903317
Train - Epoch 30, Batch: 30, Loss: 0.889909
Train - Epoch 31, Batch: 0, Loss: 0.893145
Train - Epoch 31, Batch: 10, Loss: 0.902259
Train - Epoch 31, Batch: 20, Loss: 0.880790
Train - Epoch 31, Batch: 30, Loss: 0.904065
training_time:: 3.574070692062378
training time full:: 3.574113607406616
provenance prepare time:: 2.384185791015625e-07
Test Avg. Loss: 0.000070, Accuracy: 0.629116
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 52
training time is 2.585951805114746
overhead:: 0
overhead2:: 0
time_baseline:: 2.587669849395752
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629116
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.014187097549438477
overhead3:: 0.03628373146057129
overhead4:: 0.1102750301361084
overhead5:: 0
time_provenance:: 0.7808876037597656
curr_diff: 0 tensor(5.3656e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.3656e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629116
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.01668524742126465
overhead3:: 0.04984283447265625
overhead4:: 0.13536977767944336
overhead5:: 0
time_provenance:: 0.8244032859802246
curr_diff: 0 tensor(6.4779e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.4779e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629116
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.019064903259277344
overhead3:: 0.04394221305847168
overhead4:: 0.1388394832611084
overhead5:: 0
time_provenance:: 0.8307733535766602
curr_diff: 0 tensor(5.8320e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.8320e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629116
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.01992654800415039
overhead3:: 0.048029422760009766
overhead4:: 0.1625659465789795
overhead5:: 0
time_provenance:: 0.8708910942077637
curr_diff: 0 tensor(6.7020e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.7020e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629116
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.02570486068725586
overhead3:: 0.054784536361694336
overhead4:: 0.2561464309692383
overhead5:: 0
time_provenance:: 0.9860491752624512
curr_diff: 0 tensor(2.7866e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.7866e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629116
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.027772903442382812
overhead3:: 0.057205915451049805
overhead4:: 0.2570228576660156
overhead5:: 0
time_provenance:: 0.9900717735290527
curr_diff: 0 tensor(2.8641e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.8641e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629116
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.03279757499694824
overhead3:: 0.05831551551818848
overhead4:: 0.2648463249206543
overhead5:: 0
time_provenance:: 1.1003820896148682
curr_diff: 0 tensor(3.0001e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.0001e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629116
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.03211712837219238
overhead3:: 0.0582578182220459
overhead4:: 0.2279036045074463
overhead5:: 0
time_provenance:: 0.9622821807861328
curr_diff: 0 tensor(3.0454e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.0454e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629116
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.04714703559875488
overhead3:: 0.08899235725402832
overhead4:: 0.4238002300262451
overhead5:: 0
time_provenance:: 1.228424310684204
curr_diff: 0 tensor(1.1052e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1052e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629116
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.051149606704711914
overhead3:: 0.1009061336517334
overhead4:: 0.4055747985839844
overhead5:: 0
time_provenance:: 1.231482744216919
curr_diff: 0 tensor(1.1324e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1324e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629116
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.05415773391723633
overhead3:: 0.09653472900390625
overhead4:: 0.41962265968322754
overhead5:: 0
time_provenance:: 1.2758831977844238
curr_diff: 0 tensor(1.1662e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1662e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629116
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.0544886589050293
overhead3:: 0.11384963989257812
overhead4:: 0.4259002208709717
overhead5:: 0
time_provenance:: 1.2690067291259766
curr_diff: 0 tensor(1.1846e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1846e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629116
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.11657381057739258
overhead3:: 0.2052907943725586
overhead4:: 0.9701411724090576
overhead5:: 0
time_provenance:: 1.961646318435669
curr_diff: 0 tensor(2.8036e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.8036e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629116
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.1166996955871582
overhead3:: 0.2065296173095703
overhead4:: 0.954784631729126
overhead5:: 0
time_provenance:: 1.933584451675415
curr_diff: 0 tensor(2.8718e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.8718e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629116
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.1165928840637207
overhead3:: 0.2104170322418213
overhead4:: 0.9612247943878174
overhead5:: 0
time_provenance:: 1.9556643962860107
curr_diff: 0 tensor(2.8870e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.8870e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629116
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.16231226921081543
overhead3:: 0.2906348705291748
overhead4:: 1.0950279235839844
overhead5:: 0
time_provenance:: 2.436974048614502
curr_diff: 0 tensor(2.8672e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.8672e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629116
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.23323845863342285
overhead3:: 0.3838961124420166
overhead4:: 1.495980978012085
overhead5:: 0
time_provenance:: 2.373566150665283
curr_diff: 0 tensor(9.1906e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.1906e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629116
repetition 2
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 1.957953
Train - Epoch 0, Batch: 10, Loss: 1.385589
Train - Epoch 0, Batch: 20, Loss: 1.234269
Train - Epoch 0, Batch: 30, Loss: 1.195544
Train - Epoch 1, Batch: 0, Loss: 1.187460
Train - Epoch 1, Batch: 10, Loss: 1.164199
Train - Epoch 1, Batch: 20, Loss: 1.158434
Train - Epoch 1, Batch: 30, Loss: 1.137576
Train - Epoch 2, Batch: 0, Loss: 1.124759
Train - Epoch 2, Batch: 10, Loss: 1.134784
Train - Epoch 2, Batch: 20, Loss: 1.116666
Train - Epoch 2, Batch: 30, Loss: 1.099097
Train - Epoch 3, Batch: 0, Loss: 1.100149
Train - Epoch 3, Batch: 10, Loss: 1.092364
Train - Epoch 3, Batch: 20, Loss: 1.086975
Train - Epoch 3, Batch: 30, Loss: 1.080267
Train - Epoch 4, Batch: 0, Loss: 1.073500
Train - Epoch 4, Batch: 10, Loss: 1.076816
Train - Epoch 4, Batch: 20, Loss: 1.068515
Train - Epoch 4, Batch: 30, Loss: 1.055479
Train - Epoch 5, Batch: 0, Loss: 1.057151
Train - Epoch 5, Batch: 10, Loss: 1.041929
Train - Epoch 5, Batch: 20, Loss: 1.042706
Train - Epoch 5, Batch: 30, Loss: 1.037904
Train - Epoch 6, Batch: 0, Loss: 1.045539
Train - Epoch 6, Batch: 10, Loss: 1.029535
Train - Epoch 6, Batch: 20, Loss: 1.017735
Train - Epoch 6, Batch: 30, Loss: 1.023238
Train - Epoch 7, Batch: 0, Loss: 1.029193
Train - Epoch 7, Batch: 10, Loss: 1.017315
Train - Epoch 7, Batch: 20, Loss: 1.022606
Train - Epoch 7, Batch: 30, Loss: 1.000795
Train - Epoch 8, Batch: 0, Loss: 1.007031
Train - Epoch 8, Batch: 10, Loss: 1.008898
Train - Epoch 8, Batch: 20, Loss: 1.001925
Train - Epoch 8, Batch: 30, Loss: 0.996592
Train - Epoch 9, Batch: 0, Loss: 1.014517
Train - Epoch 9, Batch: 10, Loss: 0.991782
Train - Epoch 9, Batch: 20, Loss: 0.985925
Train - Epoch 9, Batch: 30, Loss: 0.994156
Train - Epoch 10, Batch: 0, Loss: 0.987525
Train - Epoch 10, Batch: 10, Loss: 0.987658
Train - Epoch 10, Batch: 20, Loss: 0.985634
Train - Epoch 10, Batch: 30, Loss: 0.980351
Train - Epoch 11, Batch: 0, Loss: 0.997959
Train - Epoch 11, Batch: 10, Loss: 0.975402
Train - Epoch 11, Batch: 20, Loss: 0.964350
Train - Epoch 11, Batch: 30, Loss: 0.975449
Train - Epoch 12, Batch: 0, Loss: 0.972446
Train - Epoch 12, Batch: 10, Loss: 0.976319
Train - Epoch 12, Batch: 20, Loss: 0.973340
Train - Epoch 12, Batch: 30, Loss: 0.957794
Train - Epoch 13, Batch: 0, Loss: 0.961279
Train - Epoch 13, Batch: 10, Loss: 0.955480
Train - Epoch 13, Batch: 20, Loss: 0.978006
Train - Epoch 13, Batch: 30, Loss: 0.955069
Train - Epoch 14, Batch: 0, Loss: 0.958259
Train - Epoch 14, Batch: 10, Loss: 0.946002
Train - Epoch 14, Batch: 20, Loss: 0.955151
Train - Epoch 14, Batch: 30, Loss: 0.948640
Train - Epoch 15, Batch: 0, Loss: 0.955772
Train - Epoch 15, Batch: 10, Loss: 0.955981
Train - Epoch 15, Batch: 20, Loss: 0.949737
Train - Epoch 15, Batch: 30, Loss: 0.949266
Train - Epoch 16, Batch: 0, Loss: 0.939866
Train - Epoch 16, Batch: 10, Loss: 0.939741
Train - Epoch 16, Batch: 20, Loss: 0.942455
Train - Epoch 16, Batch: 30, Loss: 0.947766
Train - Epoch 17, Batch: 0, Loss: 0.942477
Train - Epoch 17, Batch: 10, Loss: 0.949025
Train - Epoch 17, Batch: 20, Loss: 0.936020
Train - Epoch 17, Batch: 30, Loss: 0.930429
Train - Epoch 18, Batch: 0, Loss: 0.936105
Train - Epoch 18, Batch: 10, Loss: 0.929464
Train - Epoch 18, Batch: 20, Loss: 0.943744
Train - Epoch 18, Batch: 30, Loss: 0.930498
Train - Epoch 19, Batch: 0, Loss: 0.933399
Train - Epoch 19, Batch: 10, Loss: 0.928919
Train - Epoch 19, Batch: 20, Loss: 0.938674
Train - Epoch 19, Batch: 30, Loss: 0.927041
Train - Epoch 20, Batch: 0, Loss: 0.928303
Train - Epoch 20, Batch: 10, Loss: 0.927985
Train - Epoch 20, Batch: 20, Loss: 0.923956
Train - Epoch 20, Batch: 30, Loss: 0.929575
Train - Epoch 21, Batch: 0, Loss: 0.924968
Train - Epoch 21, Batch: 10, Loss: 0.929981
Train - Epoch 21, Batch: 20, Loss: 0.929030
Train - Epoch 21, Batch: 30, Loss: 0.919384
Train - Epoch 22, Batch: 0, Loss: 0.925131
Train - Epoch 22, Batch: 10, Loss: 0.920846
Train - Epoch 22, Batch: 20, Loss: 0.914955
Train - Epoch 22, Batch: 30, Loss: 0.908854
Train - Epoch 23, Batch: 0, Loss: 0.919423
Train - Epoch 23, Batch: 10, Loss: 0.923638
Train - Epoch 23, Batch: 20, Loss: 0.917514
Train - Epoch 23, Batch: 30, Loss: 0.923566
Train - Epoch 24, Batch: 0, Loss: 0.915684
Train - Epoch 24, Batch: 10, Loss: 0.923402
Train - Epoch 24, Batch: 20, Loss: 0.917859
Train - Epoch 24, Batch: 30, Loss: 0.910063
Train - Epoch 25, Batch: 0, Loss: 0.909751
Train - Epoch 25, Batch: 10, Loss: 0.917102
Train - Epoch 25, Batch: 20, Loss: 0.912613
Train - Epoch 25, Batch: 30, Loss: 0.906954
Train - Epoch 26, Batch: 0, Loss: 0.914176
Train - Epoch 26, Batch: 10, Loss: 0.906611
Train - Epoch 26, Batch: 20, Loss: 0.916199
Train - Epoch 26, Batch: 30, Loss: 0.899317
Train - Epoch 27, Batch: 0, Loss: 0.902647
Train - Epoch 27, Batch: 10, Loss: 0.901050
Train - Epoch 27, Batch: 20, Loss: 0.903825
Train - Epoch 27, Batch: 30, Loss: 0.913888
Train - Epoch 28, Batch: 0, Loss: 0.897693
Train - Epoch 28, Batch: 10, Loss: 0.898481
Train - Epoch 28, Batch: 20, Loss: 0.900411
Train - Epoch 28, Batch: 30, Loss: 0.900109
Train - Epoch 29, Batch: 0, Loss: 0.900131
Train - Epoch 29, Batch: 10, Loss: 0.900465
Train - Epoch 29, Batch: 20, Loss: 0.893423
Train - Epoch 29, Batch: 30, Loss: 0.901365
Train - Epoch 30, Batch: 0, Loss: 0.898440
Train - Epoch 30, Batch: 10, Loss: 0.882786
Train - Epoch 30, Batch: 20, Loss: 0.896790
Train - Epoch 30, Batch: 30, Loss: 0.898841
Train - Epoch 31, Batch: 0, Loss: 0.897761
Train - Epoch 31, Batch: 10, Loss: 0.898158
Train - Epoch 31, Batch: 20, Loss: 0.894157
Train - Epoch 31, Batch: 30, Loss: 0.896128
training_time:: 4.0652055740356445
training time full:: 4.065248966217041
provenance prepare time:: 4.76837158203125e-07
Test Avg. Loss: 0.000070, Accuracy: 0.630889
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 52
training time is 2.2970306873321533
overhead:: 0
overhead2:: 0
time_baseline:: 2.298903226852417
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630906
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.016030550003051758
overhead3:: 0.04170513153076172
overhead4:: 0.1319260597229004
overhead5:: 0
time_provenance:: 0.8834106922149658
curr_diff: 0 tensor(4.1973e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.1973e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630906
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.018341779708862305
overhead3:: 0.03452587127685547
overhead4:: 0.15001845359802246
overhead5:: 0
time_provenance:: 0.913419246673584
curr_diff: 0 tensor(4.4823e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.4823e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630906
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.01979231834411621
overhead3:: 0.03858184814453125
overhead4:: 0.14101362228393555
overhead5:: 0
time_provenance:: 0.9012353420257568
curr_diff: 0 tensor(4.4341e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.4341e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630906
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.020430803298950195
overhead3:: 0.03799796104431152
overhead4:: 0.1441664695739746
overhead5:: 0
time_provenance:: 0.8511395454406738
curr_diff: 0 tensor(5.0033e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.0033e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630906
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.025450706481933594
overhead3:: 0.04914116859436035
overhead4:: 0.23102498054504395
overhead5:: 0
time_provenance:: 0.9663634300231934
curr_diff: 0 tensor(2.8724e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.8724e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630906
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.030128955841064453
overhead3:: 0.06274676322937012
overhead4:: 0.22342514991760254
overhead5:: 0
time_provenance:: 0.976804256439209
curr_diff: 0 tensor(2.7990e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.7990e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630906
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.0328364372253418
overhead3:: 0.06719589233398438
overhead4:: 0.2496938705444336
overhead5:: 0
time_provenance:: 1.0550525188446045
curr_diff: 0 tensor(2.9282e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.9282e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630906
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.03220343589782715
overhead3:: 0.07251763343811035
overhead4:: 0.25722813606262207
overhead5:: 0
time_provenance:: 1.0246024131774902
curr_diff: 0 tensor(2.9667e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.9667e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630906
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.046616315841674805
overhead3:: 0.08764934539794922
overhead4:: 0.40679383277893066
overhead5:: 0
time_provenance:: 1.2192471027374268
curr_diff: 0 tensor(1.1922e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1922e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630906
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.04744148254394531
overhead3:: 0.0894615650177002
overhead4:: 0.43174123764038086
overhead5:: 0
time_provenance:: 1.2354059219360352
curr_diff: 0 tensor(1.2066e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2066e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630906
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.05382084846496582
overhead3:: 0.10198688507080078
overhead4:: 0.3943169116973877
overhead5:: 0
time_provenance:: 1.2153079509735107
curr_diff: 0 tensor(1.2718e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2718e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630906
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.06851863861083984
overhead3:: 0.13332700729370117
overhead4:: 0.5442736148834229
overhead5:: 0
time_provenance:: 1.6586899757385254
curr_diff: 0 tensor(1.2809e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2809e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630906
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.11254453659057617
overhead3:: 0.20087766647338867
overhead4:: 0.9393436908721924
overhead5:: 0
time_provenance:: 1.9193706512451172
curr_diff: 0 tensor(3.1684e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.1684e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630906
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.11964082717895508
overhead3:: 0.2153322696685791
overhead4:: 0.9485905170440674
overhead5:: 0
time_provenance:: 1.9394655227661133
curr_diff: 0 tensor(3.2487e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.2487e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630906
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.11470985412597656
overhead3:: 0.2047591209411621
overhead4:: 0.9538555145263672
overhead5:: 0
time_provenance:: 1.9366915225982666
curr_diff: 0 tensor(3.3698e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.3698e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630906
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.12317872047424316
overhead3:: 0.23672008514404297
overhead4:: 0.9921305179595947
overhead5:: 0
time_provenance:: 2.02024245262146
curr_diff: 0 tensor(3.4342e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.4342e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630906
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.27762413024902344
overhead3:: 0.5349595546722412
overhead4:: 1.6652660369873047
overhead5:: 0
time_provenance:: 2.7433977127075195
curr_diff: 0 tensor(8.6569e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.6569e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630906
repetition 3
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 3 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 1.930925
Train - Epoch 0, Batch: 10, Loss: 1.385768
Train - Epoch 0, Batch: 20, Loss: 1.254051
Train - Epoch 0, Batch: 30, Loss: 1.202545
Train - Epoch 1, Batch: 0, Loss: 1.218577
Train - Epoch 1, Batch: 10, Loss: 1.168304
Train - Epoch 1, Batch: 20, Loss: 1.152965
Train - Epoch 1, Batch: 30, Loss: 1.141021
Train - Epoch 2, Batch: 0, Loss: 1.126107
Train - Epoch 2, Batch: 10, Loss: 1.132578
Train - Epoch 2, Batch: 20, Loss: 1.121609
Train - Epoch 2, Batch: 30, Loss: 1.112814
Train - Epoch 3, Batch: 0, Loss: 1.107451
Train - Epoch 3, Batch: 10, Loss: 1.109478
Train - Epoch 3, Batch: 20, Loss: 1.089439
Train - Epoch 3, Batch: 30, Loss: 1.078836
Train - Epoch 4, Batch: 0, Loss: 1.090854
Train - Epoch 4, Batch: 10, Loss: 1.070729
Train - Epoch 4, Batch: 20, Loss: 1.065651
Train - Epoch 4, Batch: 30, Loss: 1.064953
Train - Epoch 5, Batch: 0, Loss: 1.061216
Train - Epoch 5, Batch: 10, Loss: 1.063088
Train - Epoch 5, Batch: 20, Loss: 1.051453
Train - Epoch 5, Batch: 30, Loss: 1.044755
Train - Epoch 6, Batch: 0, Loss: 1.040432
Train - Epoch 6, Batch: 10, Loss: 1.037250
Train - Epoch 6, Batch: 20, Loss: 1.017980
Train - Epoch 6, Batch: 30, Loss: 1.024968
Train - Epoch 7, Batch: 0, Loss: 1.021109
Train - Epoch 7, Batch: 10, Loss: 1.028131
Train - Epoch 7, Batch: 20, Loss: 1.017390
Train - Epoch 7, Batch: 30, Loss: 1.015077
Train - Epoch 8, Batch: 0, Loss: 1.002900
Train - Epoch 8, Batch: 10, Loss: 1.005702
Train - Epoch 8, Batch: 20, Loss: 1.005296
Train - Epoch 8, Batch: 30, Loss: 0.998950
Train - Epoch 9, Batch: 0, Loss: 1.000596
Train - Epoch 9, Batch: 10, Loss: 1.001866
Train - Epoch 9, Batch: 20, Loss: 1.002932
Train - Epoch 9, Batch: 30, Loss: 0.985650
Train - Epoch 10, Batch: 0, Loss: 0.992354
Train - Epoch 10, Batch: 10, Loss: 0.979288
Train - Epoch 10, Batch: 20, Loss: 0.979058
Train - Epoch 10, Batch: 30, Loss: 0.986330
Train - Epoch 11, Batch: 0, Loss: 0.981673
Train - Epoch 11, Batch: 10, Loss: 0.980101
Train - Epoch 11, Batch: 20, Loss: 0.981427
Train - Epoch 11, Batch: 30, Loss: 0.976564
Train - Epoch 12, Batch: 0, Loss: 0.977516
Train - Epoch 12, Batch: 10, Loss: 0.982112
Train - Epoch 12, Batch: 20, Loss: 0.964114
Train - Epoch 12, Batch: 30, Loss: 0.966841
Train - Epoch 13, Batch: 0, Loss: 0.973358
Train - Epoch 13, Batch: 10, Loss: 0.958227
Train - Epoch 13, Batch: 20, Loss: 0.964150
Train - Epoch 13, Batch: 30, Loss: 0.955461
Train - Epoch 14, Batch: 0, Loss: 0.961281
Train - Epoch 14, Batch: 10, Loss: 0.965328
Train - Epoch 14, Batch: 20, Loss: 0.959572
Train - Epoch 14, Batch: 30, Loss: 0.961511
Train - Epoch 15, Batch: 0, Loss: 0.954246
Train - Epoch 15, Batch: 10, Loss: 0.953278
Train - Epoch 15, Batch: 20, Loss: 0.955339
Train - Epoch 15, Batch: 30, Loss: 0.942437
Train - Epoch 16, Batch: 0, Loss: 0.953728
Train - Epoch 16, Batch: 10, Loss: 0.950183
Train - Epoch 16, Batch: 20, Loss: 0.943183
Train - Epoch 16, Batch: 30, Loss: 0.949057
Train - Epoch 17, Batch: 0, Loss: 0.940213
Train - Epoch 17, Batch: 10, Loss: 0.947266
Train - Epoch 17, Batch: 20, Loss: 0.945679
Train - Epoch 17, Batch: 30, Loss: 0.935858
Train - Epoch 18, Batch: 0, Loss: 0.941493
Train - Epoch 18, Batch: 10, Loss: 0.935900
Train - Epoch 18, Batch: 20, Loss: 0.935947
Train - Epoch 18, Batch: 30, Loss: 0.936781
Train - Epoch 19, Batch: 0, Loss: 0.928894
Train - Epoch 19, Batch: 10, Loss: 0.935998
Train - Epoch 19, Batch: 20, Loss: 0.925796
Train - Epoch 19, Batch: 30, Loss: 0.926951
Train - Epoch 20, Batch: 0, Loss: 0.938811
Train - Epoch 20, Batch: 10, Loss: 0.925238
Train - Epoch 20, Batch: 20, Loss: 0.934485
Train - Epoch 20, Batch: 30, Loss: 0.909700
Train - Epoch 21, Batch: 0, Loss: 0.925313
Train - Epoch 21, Batch: 10, Loss: 0.922178
Train - Epoch 21, Batch: 20, Loss: 0.922107
Train - Epoch 21, Batch: 30, Loss: 0.924650
Train - Epoch 22, Batch: 0, Loss: 0.920854
Train - Epoch 22, Batch: 10, Loss: 0.910192
Train - Epoch 22, Batch: 20, Loss: 0.927650
Train - Epoch 22, Batch: 30, Loss: 0.923915
Train - Epoch 23, Batch: 0, Loss: 0.921294
Train - Epoch 23, Batch: 10, Loss: 0.917596
Train - Epoch 23, Batch: 20, Loss: 0.910652
Train - Epoch 23, Batch: 30, Loss: 0.915372
Train - Epoch 24, Batch: 0, Loss: 0.922909
Train - Epoch 24, Batch: 10, Loss: 0.906549
Train - Epoch 24, Batch: 20, Loss: 0.914497
Train - Epoch 24, Batch: 30, Loss: 0.919616
Train - Epoch 25, Batch: 0, Loss: 0.914510
Train - Epoch 25, Batch: 10, Loss: 0.907139
Train - Epoch 25, Batch: 20, Loss: 0.910153
Train - Epoch 25, Batch: 30, Loss: 0.923392
Train - Epoch 26, Batch: 0, Loss: 0.916174
Train - Epoch 26, Batch: 10, Loss: 0.909517
Train - Epoch 26, Batch: 20, Loss: 0.911344
Train - Epoch 26, Batch: 30, Loss: 0.900459
Train - Epoch 27, Batch: 0, Loss: 0.905052
Train - Epoch 27, Batch: 10, Loss: 0.893082
Train - Epoch 27, Batch: 20, Loss: 0.907073
Train - Epoch 27, Batch: 30, Loss: 0.895857
Train - Epoch 28, Batch: 0, Loss: 0.907975
Train - Epoch 28, Batch: 10, Loss: 0.894002
Train - Epoch 28, Batch: 20, Loss: 0.901678
Train - Epoch 28, Batch: 30, Loss: 0.898469
Train - Epoch 29, Batch: 0, Loss: 0.896136
Train - Epoch 29, Batch: 10, Loss: 0.907401
Train - Epoch 29, Batch: 20, Loss: 0.893238
Train - Epoch 29, Batch: 30, Loss: 0.894234
Train - Epoch 30, Batch: 0, Loss: 0.902581
Train - Epoch 30, Batch: 10, Loss: 0.900197
Train - Epoch 30, Batch: 20, Loss: 0.901235
Train - Epoch 30, Batch: 30, Loss: 0.901606
Train - Epoch 31, Batch: 0, Loss: 0.890614
Train - Epoch 31, Batch: 10, Loss: 0.896720
Train - Epoch 31, Batch: 20, Loss: 0.904077
Train - Epoch 31, Batch: 30, Loss: 0.885298
training_time:: 3.8846030235290527
training time full:: 3.884647846221924
provenance prepare time:: 4.76837158203125e-07
Test Avg. Loss: 0.000070, Accuracy: 0.629391
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 52
training time is 2.0850939750671387
overhead:: 0
overhead2:: 0
time_baseline:: 2.0866711139678955
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629357
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.01381826400756836
overhead3:: 0.02688741683959961
overhead4:: 0.1138010025024414
overhead5:: 0
time_provenance:: 0.7623841762542725
curr_diff: 0 tensor(6.5933e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.5933e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629357
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.016761064529418945
overhead3:: 0.03159904479980469
overhead4:: 0.1381235122680664
overhead5:: 0
time_provenance:: 0.8158795833587646
curr_diff: 0 tensor(4.2179e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.2179e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629357
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.017832040786743164
overhead3:: 0.04596209526062012
overhead4:: 0.14795899391174316
overhead5:: 0
time_provenance:: 0.8397397994995117
curr_diff: 0 tensor(6.8453e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.8453e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629357
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.027463436126708984
overhead3:: 0.048033952713012695
overhead4:: 0.16269302368164062
overhead5:: 0
time_provenance:: 0.8653385639190674
curr_diff: 0 tensor(4.2108e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.2108e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629357
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.026511430740356445
overhead3:: 0.06031203269958496
overhead4:: 0.22962188720703125
overhead5:: 0
time_provenance:: 0.9630591869354248
curr_diff: 0 tensor(3.1319e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.1319e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629357
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.027942180633544922
overhead3:: 0.05583691596984863
overhead4:: 0.24260973930358887
overhead5:: 0
time_provenance:: 0.990126371383667
curr_diff: 0 tensor(3.2033e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.2033e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629357
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.02918076515197754
overhead3:: 0.06299161911010742
overhead4:: 0.26471567153930664
overhead5:: 0
time_provenance:: 1.003671646118164
curr_diff: 0 tensor(3.0979e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.0979e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629357
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.03250885009765625
overhead3:: 0.05861067771911621
overhead4:: 0.2751655578613281
overhead5:: 0
time_provenance:: 1.0299029350280762
curr_diff: 0 tensor(3.2998e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.2998e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629357
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.047638654708862305
overhead3:: 0.08485555648803711
overhead4:: 0.3723015785217285
overhead5:: 0
time_provenance:: 1.1648945808410645
curr_diff: 0 tensor(1.6091e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6091e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629357
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.0551295280456543
overhead3:: 0.09999227523803711
overhead4:: 0.43301987648010254
overhead5:: 0
time_provenance:: 1.2828001976013184
curr_diff: 0 tensor(1.6249e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6249e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629357
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.05260133743286133
overhead3:: 0.09278988838195801
overhead4:: 0.4300217628479004
overhead5:: 0
time_provenance:: 1.241175651550293
curr_diff: 0 tensor(1.6084e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6084e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629357
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.0765695571899414
overhead3:: 0.1240239143371582
overhead4:: 0.5050649642944336
overhead5:: 0
time_provenance:: 1.6229808330535889
curr_diff: 0 tensor(1.6824e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6824e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629357
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.11471438407897949
overhead3:: 0.20299434661865234
overhead4:: 0.9664294719696045
overhead5:: 0
time_provenance:: 1.9423816204071045
curr_diff: 0 tensor(2.7660e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.7660e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629357
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.11785006523132324
overhead3:: 0.20378518104553223
overhead4:: 0.997014045715332
overhead5:: 0
time_provenance:: 1.9754915237426758
curr_diff: 0 tensor(2.8911e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.8911e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629357
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.1575791835784912
overhead3:: 0.268141508102417
overhead4:: 1.1825191974639893
overhead5:: 0
time_provenance:: 2.5255916118621826
curr_diff: 0 tensor(2.7813e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.7813e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629357
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.17652559280395508
overhead3:: 0.28115272521972656
overhead4:: 1.1497445106506348
overhead5:: 0
time_provenance:: 2.53415846824646
curr_diff: 0 tensor(2.9846e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.9846e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629357
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.25235915184020996
overhead3:: 0.4342517852783203
overhead4:: 1.585505485534668
overhead5:: 0
time_provenance:: 2.5451650619506836
curr_diff: 0 tensor(1.1714e-15, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1714e-15, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629357
repetition 4
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 4 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 1.913687
Train - Epoch 0, Batch: 10, Loss: 1.372254
Train - Epoch 0, Batch: 20, Loss: 1.229458
Train - Epoch 0, Batch: 30, Loss: 1.192342
Train - Epoch 1, Batch: 0, Loss: 1.182963
Train - Epoch 1, Batch: 10, Loss: 1.160906
Train - Epoch 1, Batch: 20, Loss: 1.145622
Train - Epoch 1, Batch: 30, Loss: 1.131308
Train - Epoch 2, Batch: 0, Loss: 1.123891
Train - Epoch 2, Batch: 10, Loss: 1.110084
Train - Epoch 2, Batch: 20, Loss: 1.114426
Train - Epoch 2, Batch: 30, Loss: 1.095526
Train - Epoch 3, Batch: 0, Loss: 1.104143
Train - Epoch 3, Batch: 10, Loss: 1.093931
Train - Epoch 3, Batch: 20, Loss: 1.097818
Train - Epoch 3, Batch: 30, Loss: 1.070170
Train - Epoch 4, Batch: 0, Loss: 1.071449
Train - Epoch 4, Batch: 10, Loss: 1.066053
Train - Epoch 4, Batch: 20, Loss: 1.070822
Train - Epoch 4, Batch: 30, Loss: 1.052502
Train - Epoch 5, Batch: 0, Loss: 1.048606
Train - Epoch 5, Batch: 10, Loss: 1.048904
Train - Epoch 5, Batch: 20, Loss: 1.047693
Train - Epoch 5, Batch: 30, Loss: 1.051738
Train - Epoch 6, Batch: 0, Loss: 1.037143
Train - Epoch 6, Batch: 10, Loss: 1.026101
Train - Epoch 6, Batch: 20, Loss: 1.032555
Train - Epoch 6, Batch: 30, Loss: 1.019455
Train - Epoch 7, Batch: 0, Loss: 1.013991
Train - Epoch 7, Batch: 10, Loss: 1.011080
Train - Epoch 7, Batch: 20, Loss: 1.019848
Train - Epoch 7, Batch: 30, Loss: 1.013720
Train - Epoch 8, Batch: 0, Loss: 1.015261
Train - Epoch 8, Batch: 10, Loss: 1.007869
Train - Epoch 8, Batch: 20, Loss: 1.013232
Train - Epoch 8, Batch: 30, Loss: 1.006405
Train - Epoch 9, Batch: 0, Loss: 0.990049
Train - Epoch 9, Batch: 10, Loss: 0.989757
Train - Epoch 9, Batch: 20, Loss: 0.987407
Train - Epoch 9, Batch: 30, Loss: 0.982123
Train - Epoch 10, Batch: 0, Loss: 0.983650
Train - Epoch 10, Batch: 10, Loss: 0.981977
Train - Epoch 10, Batch: 20, Loss: 0.981653
Train - Epoch 10, Batch: 30, Loss: 0.976057
Train - Epoch 11, Batch: 0, Loss: 0.966978
Train - Epoch 11, Batch: 10, Loss: 0.983187
Train - Epoch 11, Batch: 20, Loss: 0.973548
Train - Epoch 11, Batch: 30, Loss: 0.974755
Train - Epoch 12, Batch: 0, Loss: 0.971099
Train - Epoch 12, Batch: 10, Loss: 0.975021
Train - Epoch 12, Batch: 20, Loss: 0.961973
Train - Epoch 12, Batch: 30, Loss: 0.964685
Train - Epoch 13, Batch: 0, Loss: 0.964001
Train - Epoch 13, Batch: 10, Loss: 0.963288
Train - Epoch 13, Batch: 20, Loss: 0.964126
Train - Epoch 13, Batch: 30, Loss: 0.967962
Train - Epoch 14, Batch: 0, Loss: 0.961432
Train - Epoch 14, Batch: 10, Loss: 0.952069
Train - Epoch 14, Batch: 20, Loss: 0.955280
Train - Epoch 14, Batch: 30, Loss: 0.956407
Train - Epoch 15, Batch: 0, Loss: 0.948704
Train - Epoch 15, Batch: 10, Loss: 0.951432
Train - Epoch 15, Batch: 20, Loss: 0.946142
Train - Epoch 15, Batch: 30, Loss: 0.946760
Train - Epoch 16, Batch: 0, Loss: 0.945313
Train - Epoch 16, Batch: 10, Loss: 0.950724
Train - Epoch 16, Batch: 20, Loss: 0.941363
Train - Epoch 16, Batch: 30, Loss: 0.936348
Train - Epoch 17, Batch: 0, Loss: 0.943549
Train - Epoch 17, Batch: 10, Loss: 0.928902
Train - Epoch 17, Batch: 20, Loss: 0.941982
Train - Epoch 17, Batch: 30, Loss: 0.936690
Train - Epoch 18, Batch: 0, Loss: 0.936223
Train - Epoch 18, Batch: 10, Loss: 0.945451
Train - Epoch 18, Batch: 20, Loss: 0.928497
Train - Epoch 18, Batch: 30, Loss: 0.928270
Train - Epoch 19, Batch: 0, Loss: 0.939204
Train - Epoch 19, Batch: 10, Loss: 0.933673
Train - Epoch 19, Batch: 20, Loss: 0.933416
Train - Epoch 19, Batch: 30, Loss: 0.928548
Train - Epoch 20, Batch: 0, Loss: 0.927485
Train - Epoch 20, Batch: 10, Loss: 0.938223
Train - Epoch 20, Batch: 20, Loss: 0.937735
Train - Epoch 20, Batch: 30, Loss: 0.921325
Train - Epoch 21, Batch: 0, Loss: 0.915723
Train - Epoch 21, Batch: 10, Loss: 0.918464
Train - Epoch 21, Batch: 20, Loss: 0.928764
Train - Epoch 21, Batch: 30, Loss: 0.915661
Train - Epoch 22, Batch: 0, Loss: 0.918417
Train - Epoch 22, Batch: 10, Loss: 0.920293
Train - Epoch 22, Batch: 20, Loss: 0.924420
Train - Epoch 22, Batch: 30, Loss: 0.907786
Train - Epoch 23, Batch: 0, Loss: 0.924239
Train - Epoch 23, Batch: 10, Loss: 0.915688
Train - Epoch 23, Batch: 20, Loss: 0.916981
Train - Epoch 23, Batch: 30, Loss: 0.919288
Train - Epoch 24, Batch: 0, Loss: 0.916136
Train - Epoch 24, Batch: 10, Loss: 0.906865
Train - Epoch 24, Batch: 20, Loss: 0.915836
Train - Epoch 24, Batch: 30, Loss: 0.899587
Train - Epoch 25, Batch: 0, Loss: 0.907836
Train - Epoch 25, Batch: 10, Loss: 0.907864
Train - Epoch 25, Batch: 20, Loss: 0.908372
Train - Epoch 25, Batch: 30, Loss: 0.892626
Train - Epoch 26, Batch: 0, Loss: 0.910429
Train - Epoch 26, Batch: 10, Loss: 0.916722
Train - Epoch 26, Batch: 20, Loss: 0.903744
Train - Epoch 26, Batch: 30, Loss: 0.902892
Train - Epoch 27, Batch: 0, Loss: 0.904014
Train - Epoch 27, Batch: 10, Loss: 0.912808
Train - Epoch 27, Batch: 20, Loss: 0.902300
Train - Epoch 27, Batch: 30, Loss: 0.900789
Train - Epoch 28, Batch: 0, Loss: 0.900441
Train - Epoch 28, Batch: 10, Loss: 0.902731
Train - Epoch 28, Batch: 20, Loss: 0.897666
Train - Epoch 28, Batch: 30, Loss: 0.910897
Train - Epoch 29, Batch: 0, Loss: 0.907747
Train - Epoch 29, Batch: 10, Loss: 0.897087
Train - Epoch 29, Batch: 20, Loss: 0.897768
Train - Epoch 29, Batch: 30, Loss: 0.897390
Train - Epoch 30, Batch: 0, Loss: 0.904781
Train - Epoch 30, Batch: 10, Loss: 0.904634
Train - Epoch 30, Batch: 20, Loss: 0.890547
Train - Epoch 30, Batch: 30, Loss: 0.894993
Train - Epoch 31, Batch: 0, Loss: 0.898746
Train - Epoch 31, Batch: 10, Loss: 0.902923
Train - Epoch 31, Batch: 20, Loss: 0.904956
Train - Epoch 31, Batch: 30, Loss: 0.885961
training_time:: 4.004011869430542
training time full:: 4.00405478477478
provenance prepare time:: 1.1920928955078125e-06
Test Avg. Loss: 0.000070, Accuracy: 0.629770
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 52
training time is 2.3868138790130615
overhead:: 0
overhead2:: 0
time_baseline:: 2.388503313064575
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629804
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.012868165969848633
overhead3:: 0.029045820236206055
overhead4:: 0.12580108642578125
overhead5:: 0
time_provenance:: 0.8246264457702637
curr_diff: 0 tensor(5.2662e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.2662e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629804
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.015395164489746094
overhead3:: 0.04532885551452637
overhead4:: 0.13494205474853516
overhead5:: 0
time_provenance:: 0.8428752422332764
curr_diff: 0 tensor(5.1657e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.1657e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629804
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.0174407958984375
overhead3:: 0.04181790351867676
overhead4:: 0.15043926239013672
overhead5:: 0
time_provenance:: 0.8585491180419922
curr_diff: 0 tensor(3.6363e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.6363e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629804
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.020829439163208008
overhead3:: 0.05620288848876953
overhead4:: 0.16839289665222168
overhead5:: 0
time_provenance:: 0.9629960060119629
curr_diff: 0 tensor(5.7294e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.7294e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629804
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.02298259735107422
overhead3:: 0.04779171943664551
overhead4:: 0.22598648071289062
overhead5:: 0
time_provenance:: 0.9450435638427734
curr_diff: 0 tensor(2.6998e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.6998e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629804
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.025066852569580078
overhead3:: 0.05259251594543457
overhead4:: 0.24119925498962402
overhead5:: 0
time_provenance:: 0.9907493591308594
curr_diff: 0 tensor(2.6875e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.6875e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629804
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.026350975036621094
overhead3:: 0.061934471130371094
overhead4:: 0.23711776733398438
overhead5:: 0
time_provenance:: 0.9428756237030029
curr_diff: 0 tensor(3.0779e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.0779e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629804
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.03050518035888672
overhead3:: 0.06239581108093262
overhead4:: 0.28917908668518066
overhead5:: 0
time_provenance:: 1.052375316619873
curr_diff: 0 tensor(3.2790e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.2790e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629804
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.04869508743286133
overhead3:: 0.08596563339233398
overhead4:: 0.37970733642578125
overhead5:: 0
time_provenance:: 1.1794288158416748
curr_diff: 0 tensor(9.4251e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.4251e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629804
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.05396223068237305
overhead3:: 0.09604859352111816
overhead4:: 0.44043397903442383
overhead5:: 0
time_provenance:: 1.319523572921753
curr_diff: 0 tensor(9.1496e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.1496e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629804
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.05235028266906738
overhead3:: 0.09842205047607422
overhead4:: 0.40169429779052734
overhead5:: 0
time_provenance:: 1.2374112606048584
curr_diff: 0 tensor(9.8331e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.8331e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629804
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.05481863021850586
overhead3:: 0.09388279914855957
overhead4:: 0.40379786491394043
overhead5:: 0
time_provenance:: 1.1879031658172607
curr_diff: 0 tensor(1.0606e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0606e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629804
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.12523508071899414
overhead3:: 0.2239389419555664
overhead4:: 0.9872279167175293
overhead5:: 0
time_provenance:: 2.045647382736206
curr_diff: 0 tensor(2.6263e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.6263e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629804
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.11386537551879883
overhead3:: 0.20831561088562012
overhead4:: 0.9652893543243408
overhead5:: 0
time_provenance:: 1.9569313526153564
curr_diff: 0 tensor(2.6722e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.6722e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629804
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.15242338180541992
overhead3:: 0.2679121494293213
overhead4:: 1.1571223735809326
overhead5:: 0
time_provenance:: 2.505521774291992
curr_diff: 0 tensor(2.7484e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.7484e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629804
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.14664626121520996
overhead3:: 0.2572197914123535
overhead4:: 1.10878586769104
overhead5:: 0
time_provenance:: 2.3733279705047607
curr_diff: 0 tensor(2.8933e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.8933e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629804
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.24536681175231934
overhead3:: 0.42464303970336914
overhead4:: 1.552973985671997
overhead5:: 0
time_provenance:: 2.49283766746521
curr_diff: 0 tensor(1.0381e-15, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0381e-15, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629804
deletion rate:: 0.0002
python3 generate_rand_ids 0.0002  covtype 0
tensor([  5378, 417795, 391428, 468230, 270357, 362011,   2847, 323104, 389411,
        198179, 279846, 518951, 255018, 298026, 450347, 455730,  52792, 486973,
        201791, 298818, 480582, 170568,  32072, 352847, 315473, 445778, 478806,
        312919, 127832, 373849, 208217,  25946,  29788, 235618, 127591, 148074,
        175469, 405870, 151921, 254322, 106108, 314748, 139132, 227713, 277122,
         96643, 507524,  96645, 273797, 445573, 183688, 379784, 344968, 289157,
        214928,  83604,  42901,  87191, 461721, 148635, 210589, 181406, 196767,
         30625,  64940, 132524, 187565, 132015, 257454, 264375, 484793,    954,
        395454, 342720, 348865, 361926, 313799,  59079,  15305, 172497, 485331,
         31444, 390617, 394714, 425947, 229340, 176603,  71902, 261088, 105442,
        292067, 206309, 158950, 195302, 487144, 262140, 335868, 288499, 441076,
        139507, 489463, 159484, 417533, 479743])
batch size:: 16384
repetition 0
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 0 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 1.955958
Train - Epoch 0, Batch: 10, Loss: 1.399202
Train - Epoch 0, Batch: 20, Loss: 1.255040
Train - Epoch 0, Batch: 30, Loss: 1.196438
Train - Epoch 1, Batch: 0, Loss: 1.191496
Train - Epoch 1, Batch: 10, Loss: 1.168692
Train - Epoch 1, Batch: 20, Loss: 1.155899
Train - Epoch 1, Batch: 30, Loss: 1.130557
Train - Epoch 2, Batch: 0, Loss: 1.135845
Train - Epoch 2, Batch: 10, Loss: 1.126119
Train - Epoch 2, Batch: 20, Loss: 1.118118
Train - Epoch 2, Batch: 30, Loss: 1.103572
Train - Epoch 3, Batch: 0, Loss: 1.107109
Train - Epoch 3, Batch: 10, Loss: 1.103458
Train - Epoch 3, Batch: 20, Loss: 1.083842
Train - Epoch 3, Batch: 30, Loss: 1.077351
Train - Epoch 4, Batch: 0, Loss: 1.080154
Train - Epoch 4, Batch: 10, Loss: 1.073022
Train - Epoch 4, Batch: 20, Loss: 1.066159
Train - Epoch 4, Batch: 30, Loss: 1.059427
Train - Epoch 5, Batch: 0, Loss: 1.064747
Train - Epoch 5, Batch: 10, Loss: 1.051458
Train - Epoch 5, Batch: 20, Loss: 1.035738
Train - Epoch 5, Batch: 30, Loss: 1.033648
Train - Epoch 6, Batch: 0, Loss: 1.057183
Train - Epoch 6, Batch: 10, Loss: 1.028881
Train - Epoch 6, Batch: 20, Loss: 1.022834
Train - Epoch 6, Batch: 30, Loss: 1.020328
Train - Epoch 7, Batch: 0, Loss: 1.026142
Train - Epoch 7, Batch: 10, Loss: 1.026535
Train - Epoch 7, Batch: 20, Loss: 1.018784
Train - Epoch 7, Batch: 30, Loss: 1.001390
Train - Epoch 8, Batch: 0, Loss: 1.008994
Train - Epoch 8, Batch: 10, Loss: 1.000901
Train - Epoch 8, Batch: 20, Loss: 1.015749
Train - Epoch 8, Batch: 30, Loss: 1.002370
Train - Epoch 9, Batch: 0, Loss: 1.006904
Train - Epoch 9, Batch: 10, Loss: 0.993539
Train - Epoch 9, Batch: 20, Loss: 1.000200
Train - Epoch 9, Batch: 30, Loss: 0.988113
Train - Epoch 10, Batch: 0, Loss: 0.984606
Train - Epoch 10, Batch: 10, Loss: 0.989093
Train - Epoch 10, Batch: 20, Loss: 0.983391
Train - Epoch 10, Batch: 30, Loss: 0.983464
Train - Epoch 11, Batch: 0, Loss: 0.983461
Train - Epoch 11, Batch: 10, Loss: 0.967388
Train - Epoch 11, Batch: 20, Loss: 0.975345
Train - Epoch 11, Batch: 30, Loss: 0.982726
Train - Epoch 12, Batch: 0, Loss: 0.966038
Train - Epoch 12, Batch: 10, Loss: 0.967116
Train - Epoch 12, Batch: 20, Loss: 0.968252
Train - Epoch 12, Batch: 30, Loss: 0.972422
Train - Epoch 13, Batch: 0, Loss: 0.967254
Train - Epoch 13, Batch: 10, Loss: 0.962227
Train - Epoch 13, Batch: 20, Loss: 0.964297
Train - Epoch 13, Batch: 30, Loss: 0.966079
Train - Epoch 14, Batch: 0, Loss: 0.958046
Train - Epoch 14, Batch: 10, Loss: 0.954645
Train - Epoch 14, Batch: 20, Loss: 0.955649
Train - Epoch 14, Batch: 30, Loss: 0.950703
Train - Epoch 15, Batch: 0, Loss: 0.961204
Train - Epoch 15, Batch: 10, Loss: 0.958644
Train - Epoch 15, Batch: 20, Loss: 0.946305
Train - Epoch 15, Batch: 30, Loss: 0.955290
Train - Epoch 16, Batch: 0, Loss: 0.947062
Train - Epoch 16, Batch: 10, Loss: 0.939888
Train - Epoch 16, Batch: 20, Loss: 0.941811
Train - Epoch 16, Batch: 30, Loss: 0.938966
Train - Epoch 17, Batch: 0, Loss: 0.944379
Train - Epoch 17, Batch: 10, Loss: 0.941332
Train - Epoch 17, Batch: 20, Loss: 0.936935
Train - Epoch 17, Batch: 30, Loss: 0.937305
Train - Epoch 18, Batch: 0, Loss: 0.933209
Train - Epoch 18, Batch: 10, Loss: 0.927827
Train - Epoch 18, Batch: 20, Loss: 0.936747
Train - Epoch 18, Batch: 30, Loss: 0.934399
Train - Epoch 19, Batch: 0, Loss: 0.920343
Train - Epoch 19, Batch: 10, Loss: 0.933431
Train - Epoch 19, Batch: 20, Loss: 0.924683
Train - Epoch 19, Batch: 30, Loss: 0.930644
Train - Epoch 20, Batch: 0, Loss: 0.932525
Train - Epoch 20, Batch: 10, Loss: 0.919850
Train - Epoch 20, Batch: 20, Loss: 0.923706
Train - Epoch 20, Batch: 30, Loss: 0.920247
Train - Epoch 21, Batch: 0, Loss: 0.927372
Train - Epoch 21, Batch: 10, Loss: 0.925006
Train - Epoch 21, Batch: 20, Loss: 0.926637
Train - Epoch 21, Batch: 30, Loss: 0.923408
Train - Epoch 22, Batch: 0, Loss: 0.921342
Train - Epoch 22, Batch: 10, Loss: 0.922147
Train - Epoch 22, Batch: 20, Loss: 0.920420
Train - Epoch 22, Batch: 30, Loss: 0.925962
Train - Epoch 23, Batch: 0, Loss: 0.936512
Train - Epoch 23, Batch: 10, Loss: 0.917112
Train - Epoch 23, Batch: 20, Loss: 0.918159
Train - Epoch 23, Batch: 30, Loss: 0.922919
Train - Epoch 24, Batch: 0, Loss: 0.909941
Train - Epoch 24, Batch: 10, Loss: 0.916471
Train - Epoch 24, Batch: 20, Loss: 0.917632
Train - Epoch 24, Batch: 30, Loss: 0.905908
Train - Epoch 25, Batch: 0, Loss: 0.907970
Train - Epoch 25, Batch: 10, Loss: 0.913125
Train - Epoch 25, Batch: 20, Loss: 0.908122
Train - Epoch 25, Batch: 30, Loss: 0.911367
Train - Epoch 26, Batch: 0, Loss: 0.907466
Train - Epoch 26, Batch: 10, Loss: 0.904647
Train - Epoch 26, Batch: 20, Loss: 0.904284
Train - Epoch 26, Batch: 30, Loss: 0.903906
Train - Epoch 27, Batch: 0, Loss: 0.907044
Train - Epoch 27, Batch: 10, Loss: 0.911079
Train - Epoch 27, Batch: 20, Loss: 0.901837
Train - Epoch 27, Batch: 30, Loss: 0.907568
Train - Epoch 28, Batch: 0, Loss: 0.912052
Train - Epoch 28, Batch: 10, Loss: 0.905265
Train - Epoch 28, Batch: 20, Loss: 0.900478
Train - Epoch 28, Batch: 30, Loss: 0.902044
Train - Epoch 29, Batch: 0, Loss: 0.899917
Train - Epoch 29, Batch: 10, Loss: 0.894802
Train - Epoch 29, Batch: 20, Loss: 0.888900
Train - Epoch 29, Batch: 30, Loss: 0.893493
Train - Epoch 30, Batch: 0, Loss: 0.897191
Train - Epoch 30, Batch: 10, Loss: 0.902764
Train - Epoch 30, Batch: 20, Loss: 0.895965
Train - Epoch 30, Batch: 30, Loss: 0.903808
Train - Epoch 31, Batch: 0, Loss: 0.900616
Train - Epoch 31, Batch: 10, Loss: 0.896901
Train - Epoch 31, Batch: 20, Loss: 0.892159
Train - Epoch 31, Batch: 30, Loss: 0.891546
training_time:: 3.866514205932617
training time full:: 3.8665573596954346
provenance prepare time:: 7.152557373046875e-07
Test Avg. Loss: 0.000070, Accuracy: 0.629994
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 104
training time is 2.2984135150909424
overhead:: 0
overhead2:: 0
time_baseline:: 2.3005409240722656
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629959
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.01694488525390625
overhead3:: 0.03384995460510254
overhead4:: 0.1155385971069336
overhead5:: 0
time_provenance:: 0.8618214130401611
curr_diff: 0 tensor(6.7075e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.7075e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629976
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.019206762313842773
overhead3:: 0.04115462303161621
overhead4:: 0.12690114974975586
overhead5:: 0
time_provenance:: 0.8879940509796143
curr_diff: 0 tensor(7.7893e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.7893e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629994
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.023856639862060547
overhead3:: 0.04642224311828613
overhead4:: 0.15457582473754883
overhead5:: 0
time_provenance:: 0.9801864624023438
curr_diff: 0 tensor(7.0701e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.0701e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629976
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.0257718563079834
overhead3:: 0.04122424125671387
overhead4:: 0.15261316299438477
overhead5:: 0
time_provenance:: 0.9671578407287598
curr_diff: 0 tensor(7.9311e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.9311e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629994
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.0301361083984375
overhead3:: 0.04816770553588867
overhead4:: 0.2317497730255127
overhead5:: 0
time_provenance:: 1.0126218795776367
curr_diff: 0 tensor(3.4804e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.4804e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629959
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.034653425216674805
overhead3:: 0.051015377044677734
overhead4:: 0.20066547393798828
overhead5:: 0
time_provenance:: 1.0282950401306152
curr_diff: 0 tensor(3.5613e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.5613e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629959
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.03545093536376953
overhead3:: 0.054589033126831055
overhead4:: 0.23155546188354492
overhead5:: 0
time_provenance:: 1.033717155456543
curr_diff: 0 tensor(3.6469e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.6469e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629959
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.03561520576477051
overhead3:: 0.06366372108459473
overhead4:: 0.24550938606262207
overhead5:: 0
time_provenance:: 0.9969918727874756
curr_diff: 0 tensor(3.5668e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.5668e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629959
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.059668540954589844
overhead3:: 0.09076356887817383
overhead4:: 0.3969240188598633
overhead5:: 0
time_provenance:: 1.2878813743591309
curr_diff: 0 tensor(1.6783e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6783e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629959
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.0762031078338623
overhead3:: 0.11023998260498047
overhead4:: 0.5163636207580566
overhead5:: 0
time_provenance:: 1.673705816268921
curr_diff: 0 tensor(1.6905e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6905e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629959
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.060808658599853516
overhead3:: 0.0918874740600586
overhead4:: 0.4110848903656006
overhead5:: 0
time_provenance:: 1.2882568836212158
curr_diff: 0 tensor(1.7064e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.7064e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629959
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.0821685791015625
overhead3:: 0.11610293388366699
overhead4:: 0.4871373176574707
overhead5:: 0
time_provenance:: 1.6367104053497314
curr_diff: 0 tensor(1.6893e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6893e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629959
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.13707780838012695
overhead3:: 0.2062525749206543
overhead4:: 0.9776287078857422
overhead5:: 0
time_provenance:: 2.0286543369293213
curr_diff: 0 tensor(2.8653e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.8653e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629959
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.14998316764831543
overhead3:: 0.2100369930267334
overhead4:: 0.9665915966033936
overhead5:: 0
time_provenance:: 2.025620460510254
curr_diff: 0 tensor(2.8857e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.8857e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629959
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.18723368644714355
overhead3:: 0.26905059814453125
overhead4:: 1.1667554378509521
overhead5:: 0
time_provenance:: 2.5872626304626465
curr_diff: 0 tensor(2.8467e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.8467e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629959
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.15179157257080078
overhead3:: 0.24492669105529785
overhead4:: 1.0091381072998047
overhead5:: 0
time_provenance:: 2.1661603450775146
curr_diff: 0 tensor(2.8027e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.8027e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629959
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.2519221305847168
overhead3:: 0.36157822608947754
overhead4:: 1.4764885902404785
overhead5:: 0
time_provenance:: 2.3436050415039062
curr_diff: 0 tensor(8.8962e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.8962e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629959
repetition 1
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 1 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 1.958577
Train - Epoch 0, Batch: 10, Loss: 1.371957
Train - Epoch 0, Batch: 20, Loss: 1.233702
Train - Epoch 0, Batch: 30, Loss: 1.181711
Train - Epoch 1, Batch: 0, Loss: 1.194145
Train - Epoch 1, Batch: 10, Loss: 1.156160
Train - Epoch 1, Batch: 20, Loss: 1.140163
Train - Epoch 1, Batch: 30, Loss: 1.120474
Train - Epoch 2, Batch: 0, Loss: 1.132004
Train - Epoch 2, Batch: 10, Loss: 1.115657
Train - Epoch 2, Batch: 20, Loss: 1.110200
Train - Epoch 2, Batch: 30, Loss: 1.105386
Train - Epoch 3, Batch: 0, Loss: 1.093767
Train - Epoch 3, Batch: 10, Loss: 1.086425
Train - Epoch 3, Batch: 20, Loss: 1.071424
Train - Epoch 3, Batch: 30, Loss: 1.071702
Train - Epoch 4, Batch: 0, Loss: 1.081530
Train - Epoch 4, Batch: 10, Loss: 1.060342
Train - Epoch 4, Batch: 20, Loss: 1.058145
Train - Epoch 4, Batch: 30, Loss: 1.048042
Train - Epoch 5, Batch: 0, Loss: 1.046544
Train - Epoch 5, Batch: 10, Loss: 1.056642
Train - Epoch 5, Batch: 20, Loss: 1.041205
Train - Epoch 5, Batch: 30, Loss: 1.037189
Train - Epoch 6, Batch: 0, Loss: 1.032689
Train - Epoch 6, Batch: 10, Loss: 1.026328
Train - Epoch 6, Batch: 20, Loss: 1.033192
Train - Epoch 6, Batch: 30, Loss: 1.022157
Train - Epoch 7, Batch: 0, Loss: 1.035155
Train - Epoch 7, Batch: 10, Loss: 1.016836
Train - Epoch 7, Batch: 20, Loss: 1.014104
Train - Epoch 7, Batch: 30, Loss: 1.006500
Train - Epoch 8, Batch: 0, Loss: 1.008122
Train - Epoch 8, Batch: 10, Loss: 0.995004
Train - Epoch 8, Batch: 20, Loss: 1.007565
Train - Epoch 8, Batch: 30, Loss: 1.004708
Train - Epoch 9, Batch: 0, Loss: 0.991117
Train - Epoch 9, Batch: 10, Loss: 1.000537
Train - Epoch 9, Batch: 20, Loss: 0.984211
Train - Epoch 9, Batch: 30, Loss: 0.986700
Train - Epoch 10, Batch: 0, Loss: 0.996980
Train - Epoch 10, Batch: 10, Loss: 0.977773
Train - Epoch 10, Batch: 20, Loss: 0.972047
Train - Epoch 10, Batch: 30, Loss: 0.983699
Train - Epoch 11, Batch: 0, Loss: 0.978904
Train - Epoch 11, Batch: 10, Loss: 0.981617
Train - Epoch 11, Batch: 20, Loss: 0.977502
Train - Epoch 11, Batch: 30, Loss: 0.972469
Train - Epoch 12, Batch: 0, Loss: 0.969073
Train - Epoch 12, Batch: 10, Loss: 0.968200
Train - Epoch 12, Batch: 20, Loss: 0.962426
Train - Epoch 12, Batch: 30, Loss: 0.964069
Train - Epoch 13, Batch: 0, Loss: 0.967614
Train - Epoch 13, Batch: 10, Loss: 0.965831
Train - Epoch 13, Batch: 20, Loss: 0.956869
Train - Epoch 13, Batch: 30, Loss: 0.950403
Train - Epoch 14, Batch: 0, Loss: 0.957836
Train - Epoch 14, Batch: 10, Loss: 0.952703
Train - Epoch 14, Batch: 20, Loss: 0.954195
Train - Epoch 14, Batch: 30, Loss: 0.949695
Train - Epoch 15, Batch: 0, Loss: 0.956538
Train - Epoch 15, Batch: 10, Loss: 0.944009
Train - Epoch 15, Batch: 20, Loss: 0.946611
Train - Epoch 15, Batch: 30, Loss: 0.945945
Train - Epoch 16, Batch: 0, Loss: 0.945338
Train - Epoch 16, Batch: 10, Loss: 0.935739
Train - Epoch 16, Batch: 20, Loss: 0.944127
Train - Epoch 16, Batch: 30, Loss: 0.940678
Train - Epoch 17, Batch: 0, Loss: 0.944606
Train - Epoch 17, Batch: 10, Loss: 0.939452
Train - Epoch 17, Batch: 20, Loss: 0.936005
Train - Epoch 17, Batch: 30, Loss: 0.942610
Train - Epoch 18, Batch: 0, Loss: 0.944881
Train - Epoch 18, Batch: 10, Loss: 0.944361
Train - Epoch 18, Batch: 20, Loss: 0.926929
Train - Epoch 18, Batch: 30, Loss: 0.944319
Train - Epoch 19, Batch: 0, Loss: 0.931845
Train - Epoch 19, Batch: 10, Loss: 0.920009
Train - Epoch 19, Batch: 20, Loss: 0.925228
Train - Epoch 19, Batch: 30, Loss: 0.927535
Train - Epoch 20, Batch: 0, Loss: 0.929497
Train - Epoch 20, Batch: 10, Loss: 0.925259
Train - Epoch 20, Batch: 20, Loss: 0.922301
Train - Epoch 20, Batch: 30, Loss: 0.915572
Train - Epoch 21, Batch: 0, Loss: 0.920548
Train - Epoch 21, Batch: 10, Loss: 0.919516
Train - Epoch 21, Batch: 20, Loss: 0.930761
Train - Epoch 21, Batch: 30, Loss: 0.924681
Train - Epoch 22, Batch: 0, Loss: 0.908911
Train - Epoch 22, Batch: 10, Loss: 0.927927
Train - Epoch 22, Batch: 20, Loss: 0.925264
Train - Epoch 22, Batch: 30, Loss: 0.920045
Train - Epoch 23, Batch: 0, Loss: 0.918902
Train - Epoch 23, Batch: 10, Loss: 0.920587
Train - Epoch 23, Batch: 20, Loss: 0.912475
Train - Epoch 23, Batch: 30, Loss: 0.902646
Train - Epoch 24, Batch: 0, Loss: 0.915600
Train - Epoch 24, Batch: 10, Loss: 0.905188
Train - Epoch 24, Batch: 20, Loss: 0.910807
Train - Epoch 24, Batch: 30, Loss: 0.909019
Train - Epoch 25, Batch: 0, Loss: 0.917383
Train - Epoch 25, Batch: 10, Loss: 0.905924
Train - Epoch 25, Batch: 20, Loss: 0.895902
Train - Epoch 25, Batch: 30, Loss: 0.907136
Train - Epoch 26, Batch: 0, Loss: 0.905088
Train - Epoch 26, Batch: 10, Loss: 0.896901
Train - Epoch 26, Batch: 20, Loss: 0.893176
Train - Epoch 26, Batch: 30, Loss: 0.904186
Train - Epoch 27, Batch: 0, Loss: 0.906910
Train - Epoch 27, Batch: 10, Loss: 0.901546
Train - Epoch 27, Batch: 20, Loss: 0.901034
Train - Epoch 27, Batch: 30, Loss: 0.904198
Train - Epoch 28, Batch: 0, Loss: 0.902951
Train - Epoch 28, Batch: 10, Loss: 0.910200
Train - Epoch 28, Batch: 20, Loss: 0.905548
Train - Epoch 28, Batch: 30, Loss: 0.907828
Train - Epoch 29, Batch: 0, Loss: 0.901039
Train - Epoch 29, Batch: 10, Loss: 0.894699
Train - Epoch 29, Batch: 20, Loss: 0.909081
Train - Epoch 29, Batch: 30, Loss: 0.892042
Train - Epoch 30, Batch: 0, Loss: 0.902482
Train - Epoch 30, Batch: 10, Loss: 0.893502
Train - Epoch 30, Batch: 20, Loss: 0.890706
Train - Epoch 30, Batch: 30, Loss: 0.904264
Train - Epoch 31, Batch: 0, Loss: 0.897344
Train - Epoch 31, Batch: 10, Loss: 0.889824
Train - Epoch 31, Batch: 20, Loss: 0.898683
Train - Epoch 31, Batch: 30, Loss: 0.888047
training_time:: 3.7091825008392334
training time full:: 3.7092278003692627
provenance prepare time:: 9.5367431640625e-07
Test Avg. Loss: 0.000070, Accuracy: 0.630217
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 104
training time is 1.9899418354034424
overhead:: 0
overhead2:: 0
time_baseline:: 1.9917631149291992
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630252
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.017072677612304688
overhead3:: 0.028640270233154297
overhead4:: 0.14182186126708984
overhead5:: 0
time_provenance:: 0.914963960647583
curr_diff: 0 tensor(8.9118e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.9118e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630217
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.021233320236206055
overhead3:: 0.032718658447265625
overhead4:: 0.12169361114501953
overhead5:: 0
time_provenance:: 0.9355835914611816
curr_diff: 0 tensor(5.8014e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.8014e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630252
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.021198749542236328
overhead3:: 0.046695709228515625
overhead4:: 0.15174341201782227
overhead5:: 0
time_provenance:: 0.8931818008422852
curr_diff: 0 tensor(9.2556e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.2556e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630217
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.026276111602783203
overhead3:: 0.05013275146484375
overhead4:: 0.16797113418579102
overhead5:: 0
time_provenance:: 1.0030946731567383
curr_diff: 0 tensor(6.2812e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.2812e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630252
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.030617952346801758
overhead3:: 0.04913735389709473
overhead4:: 0.22394657135009766
overhead5:: 0
time_provenance:: 1.0143578052520752
curr_diff: 0 tensor(3.5902e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.5902e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630252
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.03565716743469238
overhead3:: 0.05769681930541992
overhead4:: 0.2282121181488037
overhead5:: 0
time_provenance:: 1.105130910873413
curr_diff: 0 tensor(3.7142e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.7142e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630252
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.03723454475402832
overhead3:: 0.0620875358581543
overhead4:: 0.2633330821990967
overhead5:: 0
time_provenance:: 1.0627048015594482
curr_diff: 0 tensor(3.8262e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.8262e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630252
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.03777575492858887
overhead3:: 0.05703592300415039
overhead4:: 0.24161052703857422
overhead5:: 0
time_provenance:: 1.03810453414917
curr_diff: 0 tensor(3.8601e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.8601e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630252
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.058225154876708984
overhead3:: 0.08758687973022461
overhead4:: 0.3963158130645752
overhead5:: 0
time_provenance:: 1.2621400356292725
curr_diff: 0 tensor(1.1890e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1890e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630252
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.05945587158203125
overhead3:: 0.08807873725891113
overhead4:: 0.4337766170501709
overhead5:: 0
time_provenance:: 1.2958393096923828
curr_diff: 0 tensor(1.2172e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2172e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630252
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.07887601852416992
overhead3:: 0.1142277717590332
overhead4:: 0.48392558097839355
overhead5:: 0
time_provenance:: 1.6410961151123047
curr_diff: 0 tensor(1.2707e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2707e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630252
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.06421422958374023
overhead3:: 0.09715437889099121
overhead4:: 0.4365072250366211
overhead5:: 0
time_provenance:: 1.3263049125671387
curr_diff: 0 tensor(1.2835e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2835e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630252
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.14563274383544922
overhead3:: 0.20128512382507324
overhead4:: 0.959132194519043
overhead5:: 0
time_provenance:: 2.0072038173675537
curr_diff: 0 tensor(3.4226e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.4226e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630252
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.13362884521484375
overhead3:: 0.19872140884399414
overhead4:: 0.9515867233276367
overhead5:: 0
time_provenance:: 1.9618868827819824
curr_diff: 0 tensor(3.4874e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.4874e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630252
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.18577289581298828
overhead3:: 0.26373291015625
overhead4:: 1.1713109016418457
overhead5:: 0
time_provenance:: 2.5845611095428467
curr_diff: 0 tensor(3.6318e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.6318e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630252
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.165999174118042
overhead3:: 0.24536585807800293
overhead4:: 1.0895509719848633
overhead5:: 0
time_provenance:: 2.3369600772857666
curr_diff: 0 tensor(3.6808e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.6808e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630252
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.3100311756134033
overhead3:: 0.47330427169799805
overhead4:: 1.607555627822876
overhead5:: 0
time_provenance:: 2.652747869491577
curr_diff: 0 tensor(1.0354e-15, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0354e-15, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630252
repetition 2
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 1.912128
Train - Epoch 0, Batch: 10, Loss: 1.384267
Train - Epoch 0, Batch: 20, Loss: 1.237669
Train - Epoch 0, Batch: 30, Loss: 1.199025
Train - Epoch 1, Batch: 0, Loss: 1.198997
Train - Epoch 1, Batch: 10, Loss: 1.170379
Train - Epoch 1, Batch: 20, Loss: 1.150859
Train - Epoch 1, Batch: 30, Loss: 1.142089
Train - Epoch 2, Batch: 0, Loss: 1.129614
Train - Epoch 2, Batch: 10, Loss: 1.117769
Train - Epoch 2, Batch: 20, Loss: 1.113554
Train - Epoch 2, Batch: 30, Loss: 1.105643
Train - Epoch 3, Batch: 0, Loss: 1.112319
Train - Epoch 3, Batch: 10, Loss: 1.095518
Train - Epoch 3, Batch: 20, Loss: 1.089476
Train - Epoch 3, Batch: 30, Loss: 1.089698
Train - Epoch 4, Batch: 0, Loss: 1.082456
Train - Epoch 4, Batch: 10, Loss: 1.065450
Train - Epoch 4, Batch: 20, Loss: 1.064003
Train - Epoch 4, Batch: 30, Loss: 1.063405
Train - Epoch 5, Batch: 0, Loss: 1.057535
Train - Epoch 5, Batch: 10, Loss: 1.053560
Train - Epoch 5, Batch: 20, Loss: 1.049928
Train - Epoch 5, Batch: 30, Loss: 1.039631
Train - Epoch 6, Batch: 0, Loss: 1.043644
Train - Epoch 6, Batch: 10, Loss: 1.033533
Train - Epoch 6, Batch: 20, Loss: 1.034804
Train - Epoch 6, Batch: 30, Loss: 1.027983
Train - Epoch 7, Batch: 0, Loss: 1.026690
Train - Epoch 7, Batch: 10, Loss: 1.009364
Train - Epoch 7, Batch: 20, Loss: 1.011103
Train - Epoch 7, Batch: 30, Loss: 1.020548
Train - Epoch 8, Batch: 0, Loss: 1.015452
Train - Epoch 8, Batch: 10, Loss: 0.996931
Train - Epoch 8, Batch: 20, Loss: 1.001998
Train - Epoch 8, Batch: 30, Loss: 1.003012
Train - Epoch 9, Batch: 0, Loss: 1.011737
Train - Epoch 9, Batch: 10, Loss: 0.999654
Train - Epoch 9, Batch: 20, Loss: 0.995443
Train - Epoch 9, Batch: 30, Loss: 0.996596
Train - Epoch 10, Batch: 0, Loss: 0.987996
Train - Epoch 10, Batch: 10, Loss: 0.991808
Train - Epoch 10, Batch: 20, Loss: 0.993242
Train - Epoch 10, Batch: 30, Loss: 0.975606
Train - Epoch 11, Batch: 0, Loss: 0.993978
Train - Epoch 11, Batch: 10, Loss: 0.978509
Train - Epoch 11, Batch: 20, Loss: 0.973731
Train - Epoch 11, Batch: 30, Loss: 0.976420
Train - Epoch 12, Batch: 0, Loss: 0.966162
Train - Epoch 12, Batch: 10, Loss: 0.968755
Train - Epoch 12, Batch: 20, Loss: 0.965706
Train - Epoch 12, Batch: 30, Loss: 0.959310
Train - Epoch 13, Batch: 0, Loss: 0.963367
Train - Epoch 13, Batch: 10, Loss: 0.969675
Train - Epoch 13, Batch: 20, Loss: 0.969430
Train - Epoch 13, Batch: 30, Loss: 0.960089
Train - Epoch 14, Batch: 0, Loss: 0.961047
Train - Epoch 14, Batch: 10, Loss: 0.957954
Train - Epoch 14, Batch: 20, Loss: 0.955357
Train - Epoch 14, Batch: 30, Loss: 0.955245
Train - Epoch 15, Batch: 0, Loss: 0.941963
Train - Epoch 15, Batch: 10, Loss: 0.944580
Train - Epoch 15, Batch: 20, Loss: 0.949799
Train - Epoch 15, Batch: 30, Loss: 0.938424
Train - Epoch 16, Batch: 0, Loss: 0.942439
Train - Epoch 16, Batch: 10, Loss: 0.936463
Train - Epoch 16, Batch: 20, Loss: 0.949610
Train - Epoch 16, Batch: 30, Loss: 0.934795
Train - Epoch 17, Batch: 0, Loss: 0.944033
Train - Epoch 17, Batch: 10, Loss: 0.940493
Train - Epoch 17, Batch: 20, Loss: 0.930452
Train - Epoch 17, Batch: 30, Loss: 0.938648
Train - Epoch 18, Batch: 0, Loss: 0.933268
Train - Epoch 18, Batch: 10, Loss: 0.935816
Train - Epoch 18, Batch: 20, Loss: 0.938015
Train - Epoch 18, Batch: 30, Loss: 0.927004
Train - Epoch 19, Batch: 0, Loss: 0.936984
Train - Epoch 19, Batch: 10, Loss: 0.928441
Train - Epoch 19, Batch: 20, Loss: 0.927008
Train - Epoch 19, Batch: 30, Loss: 0.939426
Train - Epoch 20, Batch: 0, Loss: 0.919448
Train - Epoch 20, Batch: 10, Loss: 0.936352
Train - Epoch 20, Batch: 20, Loss: 0.920269
Train - Epoch 20, Batch: 30, Loss: 0.926253
Train - Epoch 21, Batch: 0, Loss: 0.917954
Train - Epoch 21, Batch: 10, Loss: 0.920893
Train - Epoch 21, Batch: 20, Loss: 0.920230
Train - Epoch 21, Batch: 30, Loss: 0.924551
Train - Epoch 22, Batch: 0, Loss: 0.912746
Train - Epoch 22, Batch: 10, Loss: 0.915171
Train - Epoch 22, Batch: 20, Loss: 0.924738
Train - Epoch 22, Batch: 30, Loss: 0.914550
Train - Epoch 23, Batch: 0, Loss: 0.916230
Train - Epoch 23, Batch: 10, Loss: 0.923347
Train - Epoch 23, Batch: 20, Loss: 0.921672
Train - Epoch 23, Batch: 30, Loss: 0.910874
Train - Epoch 24, Batch: 0, Loss: 0.914616
Train - Epoch 24, Batch: 10, Loss: 0.905033
Train - Epoch 24, Batch: 20, Loss: 0.915411
Train - Epoch 24, Batch: 30, Loss: 0.909299
Train - Epoch 25, Batch: 0, Loss: 0.903949
Train - Epoch 25, Batch: 10, Loss: 0.910072
Train - Epoch 25, Batch: 20, Loss: 0.911769
Train - Epoch 25, Batch: 30, Loss: 0.916557
Train - Epoch 26, Batch: 0, Loss: 0.911959
Train - Epoch 26, Batch: 10, Loss: 0.907409
Train - Epoch 26, Batch: 20, Loss: 0.903923
Train - Epoch 26, Batch: 30, Loss: 0.899353
Train - Epoch 27, Batch: 0, Loss: 0.911285
Train - Epoch 27, Batch: 10, Loss: 0.902125
Train - Epoch 27, Batch: 20, Loss: 0.899492
Train - Epoch 27, Batch: 30, Loss: 0.905463
Train - Epoch 28, Batch: 0, Loss: 0.892668
Train - Epoch 28, Batch: 10, Loss: 0.908955
Train - Epoch 28, Batch: 20, Loss: 0.902929
Train - Epoch 28, Batch: 30, Loss: 0.895000
Train - Epoch 29, Batch: 0, Loss: 0.894923
Train - Epoch 29, Batch: 10, Loss: 0.899693
Train - Epoch 29, Batch: 20, Loss: 0.893472
Train - Epoch 29, Batch: 30, Loss: 0.896013
Train - Epoch 30, Batch: 0, Loss: 0.898578
Train - Epoch 30, Batch: 10, Loss: 0.896978
Train - Epoch 30, Batch: 20, Loss: 0.885554
Train - Epoch 30, Batch: 30, Loss: 0.905407
Train - Epoch 31, Batch: 0, Loss: 0.900376
Train - Epoch 31, Batch: 10, Loss: 0.902285
Train - Epoch 31, Batch: 20, Loss: 0.893522
Train - Epoch 31, Batch: 30, Loss: 0.894741
training_time:: 3.930896043777466
training time full:: 3.930938959121704
provenance prepare time:: 1.1920928955078125e-06
Test Avg. Loss: 0.000070, Accuracy: 0.630183
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 104
training time is 2.444368600845337
overhead:: 0
overhead2:: 0
time_baseline:: 2.4457812309265137
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630200
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.01674509048461914
overhead3:: 0.02727508544921875
overhead4:: 0.10850954055786133
overhead5:: 0
time_provenance:: 0.8284149169921875
curr_diff: 0 tensor(6.3319e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.3319e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630200
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.019459962844848633
overhead3:: 0.03984260559082031
overhead4:: 0.12275385856628418
overhead5:: 0
time_provenance:: 0.8792979717254639
curr_diff: 0 tensor(8.4159e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.4159e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630235
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.022757291793823242
overhead3:: 0.04480743408203125
overhead4:: 0.1437373161315918
overhead5:: 0
time_provenance:: 0.9095368385314941
curr_diff: 0 tensor(6.7725e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.7725e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630200
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.02474832534790039
overhead3:: 0.048822641372680664
overhead4:: 0.16513586044311523
overhead5:: 0
time_provenance:: 0.9337763786315918
curr_diff: 0 tensor(8.8852e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.8852e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630235
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.030349254608154297
overhead3:: 0.04681801795959473
overhead4:: 0.1927495002746582
overhead5:: 0
time_provenance:: 0.962470531463623
curr_diff: 0 tensor(3.2944e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.2944e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630200
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.03481483459472656
overhead3:: 0.052820682525634766
overhead4:: 0.23654723167419434
overhead5:: 0
time_provenance:: 1.0409777164459229
curr_diff: 0 tensor(3.2554e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.2554e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630200
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.037329673767089844
overhead3:: 0.07078051567077637
overhead4:: 0.24194979667663574
overhead5:: 0
time_provenance:: 1.0763633251190186
curr_diff: 0 tensor(3.3987e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.3987e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630200
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.03948569297790527
overhead3:: 0.07007193565368652
overhead4:: 0.28822779655456543
overhead5:: 0
time_provenance:: 1.1303203105926514
curr_diff: 0 tensor(3.4898e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.4898e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630200
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.05774211883544922
overhead3:: 0.08818840980529785
overhead4:: 0.39987730979919434
overhead5:: 0
time_provenance:: 1.275451898574829
curr_diff: 0 tensor(1.3546e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3546e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630200
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.059973955154418945
overhead3:: 0.09520435333251953
overhead4:: 0.43021607398986816
overhead5:: 0
time_provenance:: 1.3082501888275146
curr_diff: 0 tensor(1.3517e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3517e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630200
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.06197476387023926
overhead3:: 0.09452581405639648
overhead4:: 0.4306337833404541
overhead5:: 0
time_provenance:: 1.3138537406921387
curr_diff: 0 tensor(1.3807e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3807e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630200
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.06414675712585449
overhead3:: 0.1053762435913086
overhead4:: 0.4239084720611572
overhead5:: 0
time_provenance:: 1.3142125606536865
curr_diff: 0 tensor(1.4042e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4042e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630200
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.1372663974761963
overhead3:: 0.19022917747497559
overhead4:: 0.9401791095733643
overhead5:: 0
time_provenance:: 1.961228370666504
curr_diff: 0 tensor(4.2724e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.2724e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630200
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.1523573398590088
overhead3:: 0.21317505836486816
overhead4:: 0.9773645401000977
overhead5:: 0
time_provenance:: 2.0815298557281494
curr_diff: 0 tensor(4.2810e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.2810e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630200
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.200667142868042
overhead3:: 0.27603745460510254
overhead4:: 1.1469957828521729
overhead5:: 0
time_provenance:: 2.597086191177368
curr_diff: 0 tensor(4.2797e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.2797e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630200
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.20622587203979492
overhead3:: 0.28029608726501465
overhead4:: 1.1588377952575684
overhead5:: 0
time_provenance:: 2.6259500980377197
curr_diff: 0 tensor(4.3066e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.3066e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630200
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.28287172317504883
overhead3:: 0.40143442153930664
overhead4:: 1.5561578273773193
overhead5:: 0
time_provenance:: 2.4978301525115967
curr_diff: 0 tensor(9.1507e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.1507e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630200
repetition 3
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 3 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 1.991319
Train - Epoch 0, Batch: 10, Loss: 1.390539
Train - Epoch 0, Batch: 20, Loss: 1.236171
Train - Epoch 0, Batch: 30, Loss: 1.191612
Train - Epoch 1, Batch: 0, Loss: 1.183690
Train - Epoch 1, Batch: 10, Loss: 1.166742
Train - Epoch 1, Batch: 20, Loss: 1.145985
Train - Epoch 1, Batch: 30, Loss: 1.134561
Train - Epoch 2, Batch: 0, Loss: 1.127286
Train - Epoch 2, Batch: 10, Loss: 1.118072
Train - Epoch 2, Batch: 20, Loss: 1.109778
Train - Epoch 2, Batch: 30, Loss: 1.100383
Train - Epoch 3, Batch: 0, Loss: 1.085159
Train - Epoch 3, Batch: 10, Loss: 1.093430
Train - Epoch 3, Batch: 20, Loss: 1.071304
Train - Epoch 3, Batch: 30, Loss: 1.069423
Train - Epoch 4, Batch: 0, Loss: 1.067937
Train - Epoch 4, Batch: 10, Loss: 1.070334
Train - Epoch 4, Batch: 20, Loss: 1.060536
Train - Epoch 4, Batch: 30, Loss: 1.052142
Train - Epoch 5, Batch: 0, Loss: 1.056987
Train - Epoch 5, Batch: 10, Loss: 1.057962
Train - Epoch 5, Batch: 20, Loss: 1.043977
Train - Epoch 5, Batch: 30, Loss: 1.032876
Train - Epoch 6, Batch: 0, Loss: 1.032304
Train - Epoch 6, Batch: 10, Loss: 1.027927
Train - Epoch 6, Batch: 20, Loss: 1.030074
Train - Epoch 6, Batch: 30, Loss: 1.034331
Train - Epoch 7, Batch: 0, Loss: 1.020032
Train - Epoch 7, Batch: 10, Loss: 1.025134
Train - Epoch 7, Batch: 20, Loss: 1.016277
Train - Epoch 7, Batch: 30, Loss: 1.014492
Train - Epoch 8, Batch: 0, Loss: 1.014886
Train - Epoch 8, Batch: 10, Loss: 0.989059
Train - Epoch 8, Batch: 20, Loss: 1.003296
Train - Epoch 8, Batch: 30, Loss: 0.998753
Train - Epoch 9, Batch: 0, Loss: 0.992280
Train - Epoch 9, Batch: 10, Loss: 0.987397
Train - Epoch 9, Batch: 20, Loss: 0.991505
Train - Epoch 9, Batch: 30, Loss: 0.991312
Train - Epoch 10, Batch: 0, Loss: 0.992541
Train - Epoch 10, Batch: 10, Loss: 0.988200
Train - Epoch 10, Batch: 20, Loss: 0.985620
Train - Epoch 10, Batch: 30, Loss: 0.982056
Train - Epoch 11, Batch: 0, Loss: 0.980999
Train - Epoch 11, Batch: 10, Loss: 0.976364
Train - Epoch 11, Batch: 20, Loss: 0.958303
Train - Epoch 11, Batch: 30, Loss: 0.964932
Train - Epoch 12, Batch: 0, Loss: 0.969921
Train - Epoch 12, Batch: 10, Loss: 0.972758
Train - Epoch 12, Batch: 20, Loss: 0.974822
Train - Epoch 12, Batch: 30, Loss: 0.965301
Train - Epoch 13, Batch: 0, Loss: 0.972709
Train - Epoch 13, Batch: 10, Loss: 0.956142
Train - Epoch 13, Batch: 20, Loss: 0.970673
Train - Epoch 13, Batch: 30, Loss: 0.958585
Train - Epoch 14, Batch: 0, Loss: 0.969425
Train - Epoch 14, Batch: 10, Loss: 0.952571
Train - Epoch 14, Batch: 20, Loss: 0.965925
Train - Epoch 14, Batch: 30, Loss: 0.950466
Train - Epoch 15, Batch: 0, Loss: 0.953126
Train - Epoch 15, Batch: 10, Loss: 0.949511
Train - Epoch 15, Batch: 20, Loss: 0.953286
Train - Epoch 15, Batch: 30, Loss: 0.940368
Train - Epoch 16, Batch: 0, Loss: 0.945871
Train - Epoch 16, Batch: 10, Loss: 0.948890
Train - Epoch 16, Batch: 20, Loss: 0.957884
Train - Epoch 16, Batch: 30, Loss: 0.945689
Train - Epoch 17, Batch: 0, Loss: 0.939679
Train - Epoch 17, Batch: 10, Loss: 0.943702
Train - Epoch 17, Batch: 20, Loss: 0.940903
Train - Epoch 17, Batch: 30, Loss: 0.934850
Train - Epoch 18, Batch: 0, Loss: 0.935324
Train - Epoch 18, Batch: 10, Loss: 0.934929
Train - Epoch 18, Batch: 20, Loss: 0.932066
Train - Epoch 18, Batch: 30, Loss: 0.929036
Train - Epoch 19, Batch: 0, Loss: 0.937825
Train - Epoch 19, Batch: 10, Loss: 0.930504
Train - Epoch 19, Batch: 20, Loss: 0.921047
Train - Epoch 19, Batch: 30, Loss: 0.933707
Train - Epoch 20, Batch: 0, Loss: 0.927790
Train - Epoch 20, Batch: 10, Loss: 0.934187
Train - Epoch 20, Batch: 20, Loss: 0.926527
Train - Epoch 20, Batch: 30, Loss: 0.931564
Train - Epoch 21, Batch: 0, Loss: 0.929266
Train - Epoch 21, Batch: 10, Loss: 0.921990
Train - Epoch 21, Batch: 20, Loss: 0.923974
Train - Epoch 21, Batch: 30, Loss: 0.926583
Train - Epoch 22, Batch: 0, Loss: 0.917355
Train - Epoch 22, Batch: 10, Loss: 0.920841
Train - Epoch 22, Batch: 20, Loss: 0.918491
Train - Epoch 22, Batch: 30, Loss: 0.922853
Train - Epoch 23, Batch: 0, Loss: 0.928006
Train - Epoch 23, Batch: 10, Loss: 0.922071
Train - Epoch 23, Batch: 20, Loss: 0.902143
Train - Epoch 23, Batch: 30, Loss: 0.914931
Train - Epoch 24, Batch: 0, Loss: 0.918346
Train - Epoch 24, Batch: 10, Loss: 0.918723
Train - Epoch 24, Batch: 20, Loss: 0.917703
Train - Epoch 24, Batch: 30, Loss: 0.916329
Train - Epoch 25, Batch: 0, Loss: 0.909320
Train - Epoch 25, Batch: 10, Loss: 0.913181
Train - Epoch 25, Batch: 20, Loss: 0.907043
Train - Epoch 25, Batch: 30, Loss: 0.910930
Train - Epoch 26, Batch: 0, Loss: 0.905372
Train - Epoch 26, Batch: 10, Loss: 0.906216
Train - Epoch 26, Batch: 20, Loss: 0.895847
Train - Epoch 26, Batch: 30, Loss: 0.914312
Train - Epoch 27, Batch: 0, Loss: 0.909715
Train - Epoch 27, Batch: 10, Loss: 0.896606
Train - Epoch 27, Batch: 20, Loss: 0.908331
Train - Epoch 27, Batch: 30, Loss: 0.910007
Train - Epoch 28, Batch: 0, Loss: 0.910128
Train - Epoch 28, Batch: 10, Loss: 0.894298
Train - Epoch 28, Batch: 20, Loss: 0.909967
Train - Epoch 28, Batch: 30, Loss: 0.894515
Train - Epoch 29, Batch: 0, Loss: 0.907337
Train - Epoch 29, Batch: 10, Loss: 0.906516
Train - Epoch 29, Batch: 20, Loss: 0.907559
Train - Epoch 29, Batch: 30, Loss: 0.902385
Train - Epoch 30, Batch: 0, Loss: 0.893098
Train - Epoch 30, Batch: 10, Loss: 0.909291
Train - Epoch 30, Batch: 20, Loss: 0.888810
Train - Epoch 30, Batch: 30, Loss: 0.894729
Train - Epoch 31, Batch: 0, Loss: 0.890616
Train - Epoch 31, Batch: 10, Loss: 0.903739
Train - Epoch 31, Batch: 20, Loss: 0.888332
Train - Epoch 31, Batch: 30, Loss: 0.895939
training_time:: 3.6007206439971924
training time full:: 3.600764513015747
provenance prepare time:: 7.152557373046875e-07
Test Avg. Loss: 0.000070, Accuracy: 0.629219
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 104
training time is 2.3828258514404297
overhead:: 0
overhead2:: 0
time_baseline:: 2.3845930099487305
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629185
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.017467021942138672
overhead3:: 0.027910232543945312
overhead4:: 0.10859131813049316
overhead5:: 0
time_provenance:: 0.8281350135803223
curr_diff: 0 tensor(6.6589e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.6589e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629202
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.019184589385986328
overhead3:: 0.031022071838378906
overhead4:: 0.12797975540161133
overhead5:: 0
time_provenance:: 0.8922638893127441
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629202
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.0231630802154541
overhead3:: 0.0355069637298584
overhead4:: 0.1495833396911621
overhead5:: 0
time_provenance:: 0.9324829578399658
curr_diff: 0 tensor(6.4404e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.4404e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629202
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.02744746208190918
overhead3:: 0.04088878631591797
overhead4:: 0.15800023078918457
overhead5:: 0
time_provenance:: 1.015526533126831
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629202
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.032736778259277344
overhead3:: 0.04905080795288086
overhead4:: 0.24883651733398438
overhead5:: 0
time_provenance:: 1.0549633502960205
curr_diff: 0 tensor(3.7028e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.7028e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629185
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.03421831130981445
overhead3:: 0.0695497989654541
overhead4:: 0.20463204383850098
overhead5:: 0
time_provenance:: 1.0033037662506104
curr_diff: 0 tensor(3.7290e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.7290e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629185
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.03796219825744629
overhead3:: 0.06280350685119629
overhead4:: 0.2637155055999756
overhead5:: 0
time_provenance:: 1.1050946712493896
curr_diff: 0 tensor(3.7323e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.7323e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629185
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.04609537124633789
overhead3:: 0.07060480117797852
overhead4:: 0.26328063011169434
overhead5:: 0
time_provenance:: 1.1024394035339355
curr_diff: 0 tensor(4.0164e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.0164e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629185
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.056868791580200195
overhead3:: 0.08608007431030273
overhead4:: 0.4100959300994873
overhead5:: 0
time_provenance:: 1.2712819576263428
curr_diff: 0 tensor(1.9258e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.9258e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629185
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.05945920944213867
overhead3:: 0.09020829200744629
overhead4:: 0.4170088768005371
overhead5:: 0
time_provenance:: 1.2945616245269775
curr_diff: 0 tensor(1.9322e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.9322e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629185
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.06556987762451172
overhead3:: 0.11193990707397461
overhead4:: 0.4216153621673584
overhead5:: 0
time_provenance:: 1.3585667610168457
curr_diff: 0 tensor(1.9513e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.9513e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629185
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.06523895263671875
overhead3:: 0.09733128547668457
overhead4:: 0.434283971786499
overhead5:: 0
time_provenance:: 1.3379766941070557
curr_diff: 0 tensor(1.9809e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.9809e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629185
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.1397397518157959
overhead3:: 0.21027112007141113
overhead4:: 0.9514961242675781
overhead5:: 0
time_provenance:: 1.9968140125274658
curr_diff: 0 tensor(2.7272e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.7272e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629185
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.14034414291381836
overhead3:: 0.20592927932739258
overhead4:: 0.9654157161712646
overhead5:: 0
time_provenance:: 1.9981448650360107
curr_diff: 0 tensor(2.7584e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.7584e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629185
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.18572735786437988
overhead3:: 0.26761579513549805
overhead4:: 1.2193450927734375
overhead5:: 0
time_provenance:: 2.6424989700317383
curr_diff: 0 tensor(2.6837e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.6837e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629185
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.14802050590515137
overhead3:: 0.22088170051574707
overhead4:: 0.9801521301269531
overhead5:: 0
time_provenance:: 2.064530611038208
curr_diff: 0 tensor(2.7820e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.7820e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629185
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.2978668212890625
overhead3:: 0.43947553634643555
overhead4:: 1.5831499099731445
overhead5:: 0
time_provenance:: 2.580230951309204
curr_diff: 0 tensor(8.5414e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.5414e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629185
repetition 4
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 4 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 1.971339
Train - Epoch 0, Batch: 10, Loss: 1.391661
Train - Epoch 0, Batch: 20, Loss: 1.251656
Train - Epoch 0, Batch: 30, Loss: 1.189620
Train - Epoch 1, Batch: 0, Loss: 1.183295
Train - Epoch 1, Batch: 10, Loss: 1.160035
Train - Epoch 1, Batch: 20, Loss: 1.148362
Train - Epoch 1, Batch: 30, Loss: 1.128557
Train - Epoch 2, Batch: 0, Loss: 1.126373
Train - Epoch 2, Batch: 10, Loss: 1.118883
Train - Epoch 2, Batch: 20, Loss: 1.108091
Train - Epoch 2, Batch: 30, Loss: 1.096357
Train - Epoch 3, Batch: 0, Loss: 1.101853
Train - Epoch 3, Batch: 10, Loss: 1.105911
Train - Epoch 3, Batch: 20, Loss: 1.084158
Train - Epoch 3, Batch: 30, Loss: 1.085709
Train - Epoch 4, Batch: 0, Loss: 1.066434
Train - Epoch 4, Batch: 10, Loss: 1.063259
Train - Epoch 4, Batch: 20, Loss: 1.073641
Train - Epoch 4, Batch: 30, Loss: 1.056008
Train - Epoch 5, Batch: 0, Loss: 1.060730
Train - Epoch 5, Batch: 10, Loss: 1.037019
Train - Epoch 5, Batch: 20, Loss: 1.046489
Train - Epoch 5, Batch: 30, Loss: 1.029039
Train - Epoch 6, Batch: 0, Loss: 1.043162
Train - Epoch 6, Batch: 10, Loss: 1.038268
Train - Epoch 6, Batch: 20, Loss: 1.023199
Train - Epoch 6, Batch: 30, Loss: 1.022749
Train - Epoch 7, Batch: 0, Loss: 1.017955
Train - Epoch 7, Batch: 10, Loss: 1.025799
Train - Epoch 7, Batch: 20, Loss: 1.014553
Train - Epoch 7, Batch: 30, Loss: 1.011243
Train - Epoch 8, Batch: 0, Loss: 1.015814
Train - Epoch 8, Batch: 10, Loss: 1.012164
Train - Epoch 8, Batch: 20, Loss: 1.008514
Train - Epoch 8, Batch: 30, Loss: 1.001483
Train - Epoch 9, Batch: 0, Loss: 0.996694
Train - Epoch 9, Batch: 10, Loss: 0.994355
Train - Epoch 9, Batch: 20, Loss: 0.995030
Train - Epoch 9, Batch: 30, Loss: 0.988279
Train - Epoch 10, Batch: 0, Loss: 0.981704
Train - Epoch 10, Batch: 10, Loss: 0.983969
Train - Epoch 10, Batch: 20, Loss: 0.977486
Train - Epoch 10, Batch: 30, Loss: 0.973413
Train - Epoch 11, Batch: 0, Loss: 0.978188
Train - Epoch 11, Batch: 10, Loss: 0.983550
Train - Epoch 11, Batch: 20, Loss: 0.978654
Train - Epoch 11, Batch: 30, Loss: 0.968616
Train - Epoch 12, Batch: 0, Loss: 0.972453
Train - Epoch 12, Batch: 10, Loss: 0.967417
Train - Epoch 12, Batch: 20, Loss: 0.955870
Train - Epoch 12, Batch: 30, Loss: 0.966963
Train - Epoch 13, Batch: 0, Loss: 0.966038
Train - Epoch 13, Batch: 10, Loss: 0.952624
Train - Epoch 13, Batch: 20, Loss: 0.959655
Train - Epoch 13, Batch: 30, Loss: 0.955646
Train - Epoch 14, Batch: 0, Loss: 0.971949
Train - Epoch 14, Batch: 10, Loss: 0.950739
Train - Epoch 14, Batch: 20, Loss: 0.953284
Train - Epoch 14, Batch: 30, Loss: 0.963978
Train - Epoch 15, Batch: 0, Loss: 0.965187
Train - Epoch 15, Batch: 10, Loss: 0.939133
Train - Epoch 15, Batch: 20, Loss: 0.947875
Train - Epoch 15, Batch: 30, Loss: 0.944335
Train - Epoch 16, Batch: 0, Loss: 0.942837
Train - Epoch 16, Batch: 10, Loss: 0.940082
Train - Epoch 16, Batch: 20, Loss: 0.935742
Train - Epoch 16, Batch: 30, Loss: 0.944509
Train - Epoch 17, Batch: 0, Loss: 0.936614
Train - Epoch 17, Batch: 10, Loss: 0.937217
Train - Epoch 17, Batch: 20, Loss: 0.938678
Train - Epoch 17, Batch: 30, Loss: 0.932183
Train - Epoch 18, Batch: 0, Loss: 0.941142
Train - Epoch 18, Batch: 10, Loss: 0.941563
Train - Epoch 18, Batch: 20, Loss: 0.931043
Train - Epoch 18, Batch: 30, Loss: 0.932787
Train - Epoch 19, Batch: 0, Loss: 0.932167
Train - Epoch 19, Batch: 10, Loss: 0.936365
Train - Epoch 19, Batch: 20, Loss: 0.924515
Train - Epoch 19, Batch: 30, Loss: 0.929697
Train - Epoch 20, Batch: 0, Loss: 0.930744
Train - Epoch 20, Batch: 10, Loss: 0.927434
Train - Epoch 20, Batch: 20, Loss: 0.933154
Train - Epoch 20, Batch: 30, Loss: 0.920811
Train - Epoch 21, Batch: 0, Loss: 0.930416
Train - Epoch 21, Batch: 10, Loss: 0.922193
Train - Epoch 21, Batch: 20, Loss: 0.914545
Train - Epoch 21, Batch: 30, Loss: 0.919774
Train - Epoch 22, Batch: 0, Loss: 0.919508
Train - Epoch 22, Batch: 10, Loss: 0.924182
Train - Epoch 22, Batch: 20, Loss: 0.922038
Train - Epoch 22, Batch: 30, Loss: 0.907772
Train - Epoch 23, Batch: 0, Loss: 0.916251
Train - Epoch 23, Batch: 10, Loss: 0.917362
Train - Epoch 23, Batch: 20, Loss: 0.916622
Train - Epoch 23, Batch: 30, Loss: 0.909148
Train - Epoch 24, Batch: 0, Loss: 0.908198
Train - Epoch 24, Batch: 10, Loss: 0.905809
Train - Epoch 24, Batch: 20, Loss: 0.903591
Train - Epoch 24, Batch: 30, Loss: 0.913932
Train - Epoch 25, Batch: 0, Loss: 0.908391
Train - Epoch 25, Batch: 10, Loss: 0.906849
Train - Epoch 25, Batch: 20, Loss: 0.903915
Train - Epoch 25, Batch: 30, Loss: 0.908296
Train - Epoch 26, Batch: 0, Loss: 0.921469
Train - Epoch 26, Batch: 10, Loss: 0.905975
Train - Epoch 26, Batch: 20, Loss: 0.907567
Train - Epoch 26, Batch: 30, Loss: 0.902669
Train - Epoch 27, Batch: 0, Loss: 0.917050
Train - Epoch 27, Batch: 10, Loss: 0.905243
Train - Epoch 27, Batch: 20, Loss: 0.891958
Train - Epoch 27, Batch: 30, Loss: 0.902588
Train - Epoch 28, Batch: 0, Loss: 0.905449
Train - Epoch 28, Batch: 10, Loss: 0.905483
Train - Epoch 28, Batch: 20, Loss: 0.905932
Train - Epoch 28, Batch: 30, Loss: 0.895640
Train - Epoch 29, Batch: 0, Loss: 0.892774
Train - Epoch 29, Batch: 10, Loss: 0.903850
Train - Epoch 29, Batch: 20, Loss: 0.897800
Train - Epoch 29, Batch: 30, Loss: 0.890852
Train - Epoch 30, Batch: 0, Loss: 0.900286
Train - Epoch 30, Batch: 10, Loss: 0.897865
Train - Epoch 30, Batch: 20, Loss: 0.886521
Train - Epoch 30, Batch: 30, Loss: 0.905784
Train - Epoch 31, Batch: 0, Loss: 0.894021
Train - Epoch 31, Batch: 10, Loss: 0.893945
Train - Epoch 31, Batch: 20, Loss: 0.894379
Train - Epoch 31, Batch: 30, Loss: 0.885615
training_time:: 3.763193368911743
training time full:: 3.7632389068603516
provenance prepare time:: 4.76837158203125e-07
Test Avg. Loss: 0.000070, Accuracy: 0.630458
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 104
training time is 2.2916390895843506
overhead:: 0
overhead2:: 0
time_baseline:: 2.293840169906616
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630441
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.01723456382751465
overhead3:: 0.02809596061706543
overhead4:: 0.10629940032958984
overhead5:: 0
time_provenance:: 0.8563194274902344
curr_diff: 0 tensor(8.3162e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.3162e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630441
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.020592927932739258
overhead3:: 0.03130388259887695
overhead4:: 0.11993551254272461
overhead5:: 0
time_provenance:: 0.8826277256011963
curr_diff: 0 tensor(5.2524e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.2524e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630441
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.021329641342163086
overhead3:: 0.04299139976501465
overhead4:: 0.13843655586242676
overhead5:: 0
time_provenance:: 0.8759100437164307
curr_diff: 0 tensor(8.6907e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.6907e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630441
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.025302410125732422
overhead3:: 0.04861736297607422
overhead4:: 0.15336132049560547
overhead5:: 0
time_provenance:: 0.944861888885498
curr_diff: 0 tensor(5.8605e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.8605e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630441
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.031691551208496094
overhead3:: 0.05547761917114258
overhead4:: 0.22522997856140137
overhead5:: 0
time_provenance:: 1.0217785835266113
curr_diff: 0 tensor(3.7887e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.7887e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630441
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.03281903266906738
overhead3:: 0.050455570220947266
overhead4:: 0.2337031364440918
overhead5:: 0
time_provenance:: 1.0348389148712158
curr_diff: 0 tensor(3.8811e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.8811e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630441
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.03731107711791992
overhead3:: 0.05427122116088867
overhead4:: 0.23680996894836426
overhead5:: 0
time_provenance:: 1.0365514755249023
curr_diff: 0 tensor(4.0408e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.0408e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630441
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.037277936935424805
overhead3:: 0.06512331962585449
overhead4:: 0.2685573101043701
overhead5:: 0
time_provenance:: 1.0585503578186035
curr_diff: 0 tensor(4.0395e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.0395e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630441
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.05847001075744629
overhead3:: 0.08909463882446289
overhead4:: 0.41327548027038574
overhead5:: 0
time_provenance:: 1.294227123260498
curr_diff: 0 tensor(1.6280e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6280e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630441
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.061548471450805664
overhead3:: 0.09456086158752441
overhead4:: 0.43877172470092773
overhead5:: 0
time_provenance:: 1.3162622451782227
curr_diff: 0 tensor(1.6405e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6405e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630441
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.07560014724731445
overhead3:: 0.10840916633605957
overhead4:: 0.4638020992279053
overhead5:: 0
time_provenance:: 1.5471396446228027
curr_diff: 0 tensor(1.7581e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.7581e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630441
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.06631994247436523
overhead3:: 0.11745333671569824
overhead4:: 0.4277036190032959
overhead5:: 0
time_provenance:: 1.3233568668365479
curr_diff: 0 tensor(1.7636e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.7636e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630441
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.1495213508605957
overhead3:: 0.21460223197937012
overhead4:: 0.9813375473022461
overhead5:: 0
time_provenance:: 2.0531365871429443
curr_diff: 0 tensor(3.8451e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.8451e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630441
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.1362137794494629
overhead3:: 0.20430874824523926
overhead4:: 0.9850530624389648
overhead5:: 0
time_provenance:: 2.0200493335723877
curr_diff: 0 tensor(3.9344e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.9344e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630441
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.13800621032714844
overhead3:: 0.20709514617919922
overhead4:: 0.9516100883483887
overhead5:: 0
time_provenance:: 1.9840924739837646
curr_diff: 0 tensor(4.1415e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.1415e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630441
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.1458134651184082
overhead3:: 0.23237872123718262
overhead4:: 0.9215853214263916
overhead5:: 0
time_provenance:: 1.9791405200958252
curr_diff: 0 tensor(4.1366e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.1366e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630441
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.3052396774291992
overhead3:: 0.4708847999572754
overhead4:: 1.6053736209869385
overhead5:: 0
time_provenance:: 2.6478309631347656
curr_diff: 0 tensor(8.0196e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.0196e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630441
deletion rate:: 0.0005
python3 generate_rand_ids 0.0005  covtype 0
tensor([417795, 353286, 514063, 270357, 107541, 145442, 255018, 298026, 379948,
        455730, 230451, 440378, 473147, 277563, 382013, 121918, 201791,  77887,
        453696, 343104, 453698,  31809,  81989, 430155, 315473, 373849,  29788,
        235618, 187493, 328810, 237675, 222316, 282740, 228470, 445573,  35978,
        341131,  87191,  14487, 148635, 181406, 196767, 154787, 480419, 424102,
         73896, 187565, 203950, 264375, 464060, 395454, 264394, 284879, 325842,
        461014,  71902, 292067, 111844, 158950, 146667, 335868, 139507,  38131,
        322807, 521472,  71937,   5378, 391428, 508165, 468230, 229638,  43276,
        170255, 432399, 464152, 389411, 155940, 279846, 175402,  50475, 146736,
        159027,  36150, 317753,  82236,   3392, 480582, 106823,  32072,  13645,
        445778,  79188, 150868, 367960, 208217,  25946, 339294, 258401, 408932,
        108905, 175469, 405870, 151921, 254322,  86385, 314748,  30076, 227713,
        448897,  96643, 160130,  96645, 273797, 289157, 183688, 467343, 416169,
         64940, 132524, 257454, 442799, 484793, 473532,  53692, 511422, 392639,
        126401, 101827, 489924, 137669, 361926, 313799,  14790, 191943, 314825,
        443848, 186832, 172497, 210384, 372179, 216531, 390617, 394714, 176603,
         86493, 206309, 442853, 104937, 456175, 479743, 297483, 330258, 487955,
        136725, 362011, 323104, 198179, 238122, 465455, 177711,  52792, 486973,
        170568, 463436, 352847,  49744, 478806, 312919, 392801, 127591, 148074,
         78443, 124530, 486006, 106108, 277122, 507524,  17030,  83604, 210589,
        425651, 171708, 342720, 348865, 175813,  59079, 296648, 353996, 243409,
         31444,  76500, 342752,    738, 134883, 274148, 195302, 487144, 340713,
        509674, 280305, 288499, 441076, 517876, 249588, 143094, 340725, 159484,
        417533, 246529, 125704, 476940,  89871, 304919,   2847, 280352, 162597,
         55078, 518951,  22311, 116521, 450347, 201537, 298818, 342861, 276302,
        127832, 172888,  84827, 336748, 116592,  67443, 134007, 201595, 139132,
        338822, 379784, 344968, 214928,  14224,  42901, 461721,  30625, 182182,
        132015, 281520,    954, 125882,  56260,  15305, 485331,  89047, 425947,
        229340, 261088, 105442,  63458, 418793,  25581, 317427, 489463, 262140])
batch size:: 16384
repetition 0
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 0 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 1.956272
Train - Epoch 0, Batch: 10, Loss: 1.392312
Train - Epoch 0, Batch: 20, Loss: 1.256504
Train - Epoch 0, Batch: 30, Loss: 1.201946
Train - Epoch 1, Batch: 0, Loss: 1.190837
Train - Epoch 1, Batch: 10, Loss: 1.174622
Train - Epoch 1, Batch: 20, Loss: 1.152705
Train - Epoch 1, Batch: 30, Loss: 1.137105
Train - Epoch 2, Batch: 0, Loss: 1.128724
Train - Epoch 2, Batch: 10, Loss: 1.121162
Train - Epoch 2, Batch: 20, Loss: 1.115067
Train - Epoch 2, Batch: 30, Loss: 1.106062
Train - Epoch 3, Batch: 0, Loss: 1.110280
Train - Epoch 3, Batch: 10, Loss: 1.085252
Train - Epoch 3, Batch: 20, Loss: 1.079186
Train - Epoch 3, Batch: 30, Loss: 1.068588
Train - Epoch 4, Batch: 0, Loss: 1.072800
Train - Epoch 4, Batch: 10, Loss: 1.071629
Train - Epoch 4, Batch: 20, Loss: 1.064493
Train - Epoch 4, Batch: 30, Loss: 1.055092
Train - Epoch 5, Batch: 0, Loss: 1.056479
Train - Epoch 5, Batch: 10, Loss: 1.050136
Train - Epoch 5, Batch: 20, Loss: 1.039365
Train - Epoch 5, Batch: 30, Loss: 1.036740
Train - Epoch 6, Batch: 0, Loss: 1.039550
Train - Epoch 6, Batch: 10, Loss: 1.033020
Train - Epoch 6, Batch: 20, Loss: 1.037195
Train - Epoch 6, Batch: 30, Loss: 1.018183
Train - Epoch 7, Batch: 0, Loss: 1.024988
Train - Epoch 7, Batch: 10, Loss: 1.014421
Train - Epoch 7, Batch: 20, Loss: 1.015264
Train - Epoch 7, Batch: 30, Loss: 1.012464
Train - Epoch 8, Batch: 0, Loss: 1.005160
Train - Epoch 8, Batch: 10, Loss: 0.998526
Train - Epoch 8, Batch: 20, Loss: 1.011201
Train - Epoch 8, Batch: 30, Loss: 1.005035
Train - Epoch 9, Batch: 0, Loss: 1.002645
Train - Epoch 9, Batch: 10, Loss: 0.992274
Train - Epoch 9, Batch: 20, Loss: 0.988160
Train - Epoch 9, Batch: 30, Loss: 0.988009
Train - Epoch 10, Batch: 0, Loss: 0.985915
Train - Epoch 10, Batch: 10, Loss: 0.997301
Train - Epoch 10, Batch: 20, Loss: 0.988534
Train - Epoch 10, Batch: 30, Loss: 0.983226
Train - Epoch 11, Batch: 0, Loss: 0.979236
Train - Epoch 11, Batch: 10, Loss: 0.977753
Train - Epoch 11, Batch: 20, Loss: 0.980504
Train - Epoch 11, Batch: 30, Loss: 0.971583
Train - Epoch 12, Batch: 0, Loss: 0.966654
Train - Epoch 12, Batch: 10, Loss: 0.971627
Train - Epoch 12, Batch: 20, Loss: 0.964698
Train - Epoch 12, Batch: 30, Loss: 0.958548
Train - Epoch 13, Batch: 0, Loss: 0.966786
Train - Epoch 13, Batch: 10, Loss: 0.964890
Train - Epoch 13, Batch: 20, Loss: 0.959597
Train - Epoch 13, Batch: 30, Loss: 0.956117
Train - Epoch 14, Batch: 0, Loss: 0.965368
Train - Epoch 14, Batch: 10, Loss: 0.955599
Train - Epoch 14, Batch: 20, Loss: 0.963751
Train - Epoch 14, Batch: 30, Loss: 0.947093
Train - Epoch 15, Batch: 0, Loss: 0.938414
Train - Epoch 15, Batch: 10, Loss: 0.949771
Train - Epoch 15, Batch: 20, Loss: 0.939799
Train - Epoch 15, Batch: 30, Loss: 0.953157
Train - Epoch 16, Batch: 0, Loss: 0.951364
Train - Epoch 16, Batch: 10, Loss: 0.948326
Train - Epoch 16, Batch: 20, Loss: 0.961728
Train - Epoch 16, Batch: 30, Loss: 0.934531
Train - Epoch 17, Batch: 0, Loss: 0.942133
Train - Epoch 17, Batch: 10, Loss: 0.937800
Train - Epoch 17, Batch: 20, Loss: 0.936383
Train - Epoch 17, Batch: 30, Loss: 0.931830
Train - Epoch 18, Batch: 0, Loss: 0.940412
Train - Epoch 18, Batch: 10, Loss: 0.946920
Train - Epoch 18, Batch: 20, Loss: 0.940452
Train - Epoch 18, Batch: 30, Loss: 0.929341
Train - Epoch 19, Batch: 0, Loss: 0.936541
Train - Epoch 19, Batch: 10, Loss: 0.930346
Train - Epoch 19, Batch: 20, Loss: 0.936269
Train - Epoch 19, Batch: 30, Loss: 0.935916
Train - Epoch 20, Batch: 0, Loss: 0.921221
Train - Epoch 20, Batch: 10, Loss: 0.927314
Train - Epoch 20, Batch: 20, Loss: 0.927517
Train - Epoch 20, Batch: 30, Loss: 0.924473
Train - Epoch 21, Batch: 0, Loss: 0.918299
Train - Epoch 21, Batch: 10, Loss: 0.918384
Train - Epoch 21, Batch: 20, Loss: 0.917168
Train - Epoch 21, Batch: 30, Loss: 0.925356
Train - Epoch 22, Batch: 0, Loss: 0.917299
Train - Epoch 22, Batch: 10, Loss: 0.924475
Train - Epoch 22, Batch: 20, Loss: 0.921460
Train - Epoch 22, Batch: 30, Loss: 0.908769
Train - Epoch 23, Batch: 0, Loss: 0.917808
Train - Epoch 23, Batch: 10, Loss: 0.909246
Train - Epoch 23, Batch: 20, Loss: 0.918160
Train - Epoch 23, Batch: 30, Loss: 0.918889
Train - Epoch 24, Batch: 0, Loss: 0.919203
Train - Epoch 24, Batch: 10, Loss: 0.900946
Train - Epoch 24, Batch: 20, Loss: 0.900551
Train - Epoch 24, Batch: 30, Loss: 0.919704
Train - Epoch 25, Batch: 0, Loss: 0.906939
Train - Epoch 25, Batch: 10, Loss: 0.912779
Train - Epoch 25, Batch: 20, Loss: 0.911605
Train - Epoch 25, Batch: 30, Loss: 0.914451
Train - Epoch 26, Batch: 0, Loss: 0.901233
Train - Epoch 26, Batch: 10, Loss: 0.905602
Train - Epoch 26, Batch: 20, Loss: 0.908801
Train - Epoch 26, Batch: 30, Loss: 0.899187
Train - Epoch 27, Batch: 0, Loss: 0.903590
Train - Epoch 27, Batch: 10, Loss: 0.906430
Train - Epoch 27, Batch: 20, Loss: 0.906031
Train - Epoch 27, Batch: 30, Loss: 0.910660
Train - Epoch 28, Batch: 0, Loss: 0.901759
Train - Epoch 28, Batch: 10, Loss: 0.902856
Train - Epoch 28, Batch: 20, Loss: 0.907603
Train - Epoch 28, Batch: 30, Loss: 0.898575
Train - Epoch 29, Batch: 0, Loss: 0.910265
Train - Epoch 29, Batch: 10, Loss: 0.909135
Train - Epoch 29, Batch: 20, Loss: 0.886017
Train - Epoch 29, Batch: 30, Loss: 0.898605
Train - Epoch 30, Batch: 0, Loss: 0.895532
Train - Epoch 30, Batch: 10, Loss: 0.898674
Train - Epoch 30, Batch: 20, Loss: 0.901773
Train - Epoch 30, Batch: 30, Loss: 0.897534
Train - Epoch 31, Batch: 0, Loss: 0.884092
Train - Epoch 31, Batch: 10, Loss: 0.893987
Train - Epoch 31, Batch: 20, Loss: 0.889111
Train - Epoch 31, Batch: 30, Loss: 0.899409
training_time:: 4.028568983078003
training time full:: 4.028615236282349
provenance prepare time:: 1.1920928955078125e-06
Test Avg. Loss: 0.000070, Accuracy: 0.631078
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 261
training time is 2.1385397911071777
overhead:: 0
overhead2:: 0
time_baseline:: 2.1399142742156982
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631095
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.021359920501708984
overhead3:: 0.043169498443603516
overhead4:: 0.11870360374450684
overhead5:: 0
time_provenance:: 0.9786701202392578
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631078
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.019731998443603516
overhead3:: 0.03429412841796875
overhead4:: 0.1154947280883789
overhead5:: 0
time_provenance:: 0.8885419368743896
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631112
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.02329087257385254
overhead3:: 0.045104026794433594
overhead4:: 0.14742255210876465
overhead5:: 0
time_provenance:: 0.9769518375396729
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631078
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.02517557144165039
overhead3:: 0.038954973220825195
overhead4:: 0.15510272979736328
overhead5:: 0
time_provenance:: 0.944624662399292
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631112
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.03173470497131348
overhead3:: 0.0482020378112793
overhead4:: 0.2134084701538086
overhead5:: 0
time_provenance:: 1.0458898544311523
curr_diff: 0 tensor(6.5213e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.5213e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631095
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.033760786056518555
overhead3:: 0.051300048828125
overhead4:: 0.2287299633026123
overhead5:: 0
time_provenance:: 1.0649259090423584
curr_diff: 0 tensor(6.5465e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.5465e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631095
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.03623676300048828
overhead3:: 0.07356595993041992
overhead4:: 0.24732565879821777
overhead5:: 0
time_provenance:: 1.0849368572235107
curr_diff: 0 tensor(6.5777e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.5777e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631095
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.038413047790527344
overhead3:: 0.058209896087646484
overhead4:: 0.26593613624572754
overhead5:: 0
time_provenance:: 1.092461109161377
curr_diff: 0 tensor(6.7349e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.7349e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631095
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.05910634994506836
overhead3:: 0.08607268333435059
overhead4:: 0.4031105041503906
overhead5:: 0
time_provenance:: 1.2927849292755127
curr_diff: 0 tensor(2.3663e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.3663e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631095
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.06237077713012695
overhead3:: 0.09288191795349121
overhead4:: 0.4264335632324219
overhead5:: 0
time_provenance:: 1.3198165893554688
curr_diff: 0 tensor(2.3750e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.3750e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631095
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.07361292839050293
overhead3:: 0.11301755905151367
overhead4:: 0.48494482040405273
overhead5:: 0
time_provenance:: 1.568310022354126
curr_diff: 0 tensor(2.3889e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.3889e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631095
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.07972407341003418
overhead3:: 0.11619353294372559
overhead4:: 0.4859030246734619
overhead5:: 0
time_provenance:: 1.59187650680542
curr_diff: 0 tensor(2.4198e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.4198e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631095
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.13966917991638184
overhead3:: 0.2013530731201172
overhead4:: 0.9609742164611816
overhead5:: 0
time_provenance:: 2.0040030479431152
curr_diff: 0 tensor(8.1640e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.1640e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631095
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.1526327133178711
overhead3:: 0.22596120834350586
overhead4:: 0.9925024509429932
overhead5:: 0
time_provenance:: 2.122919797897339
curr_diff: 0 tensor(8.1822e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.1822e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631095
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.19541025161743164
overhead3:: 0.2748541831970215
overhead4:: 1.1635019779205322
overhead5:: 0
time_provenance:: 2.609302043914795
curr_diff: 0 tensor(8.1778e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.1778e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631095
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.14906573295593262
overhead3:: 0.22687005996704102
overhead4:: 0.925208330154419
overhead5:: 0
time_provenance:: 1.9904696941375732
curr_diff: 0 tensor(8.2007e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.2007e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631095
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.32511377334594727
overhead3:: 0.48326754570007324
overhead4:: 1.6644270420074463
overhead5:: 0
time_provenance:: 2.7367777824401855
curr_diff: 0 tensor(9.8593e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.8593e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1., dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631095
repetition 1
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 1 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.043759
Train - Epoch 0, Batch: 10, Loss: 1.398253
Train - Epoch 0, Batch: 20, Loss: 1.238846
Train - Epoch 0, Batch: 30, Loss: 1.204748
Train - Epoch 1, Batch: 0, Loss: 1.185081
Train - Epoch 1, Batch: 10, Loss: 1.159289
Train - Epoch 1, Batch: 20, Loss: 1.140545
Train - Epoch 1, Batch: 30, Loss: 1.127156
Train - Epoch 2, Batch: 0, Loss: 1.134199
Train - Epoch 2, Batch: 10, Loss: 1.101381
Train - Epoch 2, Batch: 20, Loss: 1.104998
Train - Epoch 2, Batch: 30, Loss: 1.104171
Train - Epoch 3, Batch: 0, Loss: 1.101493
Train - Epoch 3, Batch: 10, Loss: 1.087373
Train - Epoch 3, Batch: 20, Loss: 1.079529
Train - Epoch 3, Batch: 30, Loss: 1.085827
Train - Epoch 4, Batch: 0, Loss: 1.069196
Train - Epoch 4, Batch: 10, Loss: 1.072431
Train - Epoch 4, Batch: 20, Loss: 1.055169
Train - Epoch 4, Batch: 30, Loss: 1.052693
Train - Epoch 5, Batch: 0, Loss: 1.059638
Train - Epoch 5, Batch: 10, Loss: 1.041138
Train - Epoch 5, Batch: 20, Loss: 1.035240
Train - Epoch 5, Batch: 30, Loss: 1.037586
Train - Epoch 6, Batch: 0, Loss: 1.036366
Train - Epoch 6, Batch: 10, Loss: 1.038202
Train - Epoch 6, Batch: 20, Loss: 1.035167
Train - Epoch 6, Batch: 30, Loss: 1.018835
Train - Epoch 7, Batch: 0, Loss: 1.031171
Train - Epoch 7, Batch: 10, Loss: 1.027519
Train - Epoch 7, Batch: 20, Loss: 1.008795
Train - Epoch 7, Batch: 30, Loss: 1.012003
Train - Epoch 8, Batch: 0, Loss: 1.010183
Train - Epoch 8, Batch: 10, Loss: 0.999071
Train - Epoch 8, Batch: 20, Loss: 0.998087
Train - Epoch 8, Batch: 30, Loss: 0.997475
Train - Epoch 9, Batch: 0, Loss: 0.993469
Train - Epoch 9, Batch: 10, Loss: 0.981568
Train - Epoch 9, Batch: 20, Loss: 0.995700
Train - Epoch 9, Batch: 30, Loss: 0.988357
Train - Epoch 10, Batch: 0, Loss: 0.998829
Train - Epoch 10, Batch: 10, Loss: 0.984429
Train - Epoch 10, Batch: 20, Loss: 0.983309
Train - Epoch 10, Batch: 30, Loss: 0.975450
Train - Epoch 11, Batch: 0, Loss: 0.976304
Train - Epoch 11, Batch: 10, Loss: 0.978091
Train - Epoch 11, Batch: 20, Loss: 0.976358
Train - Epoch 11, Batch: 30, Loss: 0.963277
Train - Epoch 12, Batch: 0, Loss: 0.973197
Train - Epoch 12, Batch: 10, Loss: 0.968304
Train - Epoch 12, Batch: 20, Loss: 0.971833
Train - Epoch 12, Batch: 30, Loss: 0.961518
Train - Epoch 13, Batch: 0, Loss: 0.960876
Train - Epoch 13, Batch: 10, Loss: 0.954317
Train - Epoch 13, Batch: 20, Loss: 0.962678
Train - Epoch 13, Batch: 30, Loss: 0.952206
Train - Epoch 14, Batch: 0, Loss: 0.952835
Train - Epoch 14, Batch: 10, Loss: 0.956374
Train - Epoch 14, Batch: 20, Loss: 0.957213
Train - Epoch 14, Batch: 30, Loss: 0.955701
Train - Epoch 15, Batch: 0, Loss: 0.953320
Train - Epoch 15, Batch: 10, Loss: 0.950470
Train - Epoch 15, Batch: 20, Loss: 0.950822
Train - Epoch 15, Batch: 30, Loss: 0.937063
Train - Epoch 16, Batch: 0, Loss: 0.953385
Train - Epoch 16, Batch: 10, Loss: 0.949418
Train - Epoch 16, Batch: 20, Loss: 0.939564
Train - Epoch 16, Batch: 30, Loss: 0.939227
Train - Epoch 17, Batch: 0, Loss: 0.939728
Train - Epoch 17, Batch: 10, Loss: 0.941290
Train - Epoch 17, Batch: 20, Loss: 0.936754
Train - Epoch 17, Batch: 30, Loss: 0.931024
Train - Epoch 18, Batch: 0, Loss: 0.935821
Train - Epoch 18, Batch: 10, Loss: 0.937967
Train - Epoch 18, Batch: 20, Loss: 0.937539
Train - Epoch 18, Batch: 30, Loss: 0.937377
Train - Epoch 19, Batch: 0, Loss: 0.925768
Train - Epoch 19, Batch: 10, Loss: 0.932006
Train - Epoch 19, Batch: 20, Loss: 0.930455
Train - Epoch 19, Batch: 30, Loss: 0.929643
Train - Epoch 20, Batch: 0, Loss: 0.922793
Train - Epoch 20, Batch: 10, Loss: 0.930527
Train - Epoch 20, Batch: 20, Loss: 0.925223
Train - Epoch 20, Batch: 30, Loss: 0.921413
Train - Epoch 21, Batch: 0, Loss: 0.929122
Train - Epoch 21, Batch: 10, Loss: 0.922128
Train - Epoch 21, Batch: 20, Loss: 0.910171
Train - Epoch 21, Batch: 30, Loss: 0.921381
Train - Epoch 22, Batch: 0, Loss: 0.921588
Train - Epoch 22, Batch: 10, Loss: 0.919576
Train - Epoch 22, Batch: 20, Loss: 0.915887
Train - Epoch 22, Batch: 30, Loss: 0.912523
Train - Epoch 23, Batch: 0, Loss: 0.918495
Train - Epoch 23, Batch: 10, Loss: 0.915070
Train - Epoch 23, Batch: 20, Loss: 0.919006
Train - Epoch 23, Batch: 30, Loss: 0.913999
Train - Epoch 24, Batch: 0, Loss: 0.913787
Train - Epoch 24, Batch: 10, Loss: 0.925868
Train - Epoch 24, Batch: 20, Loss: 0.908330
Train - Epoch 24, Batch: 30, Loss: 0.917673
Train - Epoch 25, Batch: 0, Loss: 0.917192
Train - Epoch 25, Batch: 10, Loss: 0.911229
Train - Epoch 25, Batch: 20, Loss: 0.905646
Train - Epoch 25, Batch: 30, Loss: 0.901087
Train - Epoch 26, Batch: 0, Loss: 0.901302
Train - Epoch 26, Batch: 10, Loss: 0.923673
Train - Epoch 26, Batch: 20, Loss: 0.907114
Train - Epoch 26, Batch: 30, Loss: 0.900647
Train - Epoch 27, Batch: 0, Loss: 0.907906
Train - Epoch 27, Batch: 10, Loss: 0.900615
Train - Epoch 27, Batch: 20, Loss: 0.894396
Train - Epoch 27, Batch: 30, Loss: 0.909024
Train - Epoch 28, Batch: 0, Loss: 0.904609
Train - Epoch 28, Batch: 10, Loss: 0.897507
Train - Epoch 28, Batch: 20, Loss: 0.904251
Train - Epoch 28, Batch: 30, Loss: 0.892527
Train - Epoch 29, Batch: 0, Loss: 0.907303
Train - Epoch 29, Batch: 10, Loss: 0.904058
Train - Epoch 29, Batch: 20, Loss: 0.896366
Train - Epoch 29, Batch: 30, Loss: 0.909949
Train - Epoch 30, Batch: 0, Loss: 0.904371
Train - Epoch 30, Batch: 10, Loss: 0.895058
Train - Epoch 30, Batch: 20, Loss: 0.897085
Train - Epoch 30, Batch: 30, Loss: 0.892056
Train - Epoch 31, Batch: 0, Loss: 0.897231
Train - Epoch 31, Batch: 10, Loss: 0.893357
Train - Epoch 31, Batch: 20, Loss: 0.902565
Train - Epoch 31, Batch: 30, Loss: 0.891883
training_time:: 3.8767025470733643
training time full:: 3.8767478466033936
provenance prepare time:: 7.152557373046875e-07
Test Avg. Loss: 0.000070, Accuracy: 0.631267
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 261
training time is 2.008324384689331
overhead:: 0
overhead2:: 0
time_baseline:: 2.009906053543091
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631302
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.01700305938720703
overhead3:: 0.027110815048217773
overhead4:: 0.10291171073913574
overhead5:: 0
time_provenance:: 0.8510799407958984
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631302
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.020349979400634766
overhead3:: 0.0403444766998291
overhead4:: 0.12400460243225098
overhead5:: 0
time_provenance:: 0.8805432319641113
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631302
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.0290524959564209
overhead3:: 0.03567147254943848
overhead4:: 0.14493322372436523
overhead5:: 0
time_provenance:: 0.8905351161956787
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631302
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.02726435661315918
overhead3:: 0.04176187515258789
overhead4:: 0.1473863124847412
overhead5:: 0
time_provenance:: 0.927159309387207
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631302
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.034284353256225586
overhead3:: 0.05019521713256836
overhead4:: 0.20644497871398926
overhead5:: 0
time_provenance:: 1.0625975131988525
curr_diff: 0 tensor(6.1003e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.1003e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631319
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.03619503974914551
overhead3:: 0.051603078842163086
overhead4:: 0.2387235164642334
overhead5:: 0
time_provenance:: 1.0936028957366943
curr_diff: 0 tensor(6.1553e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.1553e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631319
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.037128448486328125
overhead3:: 0.0641624927520752
overhead4:: 0.260662317276001
overhead5:: 0
time_provenance:: 1.0936203002929688
curr_diff: 0 tensor(6.1487e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.1487e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631319
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.04555463790893555
overhead3:: 0.05911588668823242
overhead4:: 0.2708926200866699
overhead5:: 0
time_provenance:: 1.104433536529541
curr_diff: 0 tensor(6.1813e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.1813e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631319
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.0785207748413086
overhead3:: 0.11080121994018555
overhead4:: 0.4901273250579834
overhead5:: 0
time_provenance:: 1.6895697116851807
curr_diff: 0 tensor(2.7282e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.7282e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631319
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.06051921844482422
overhead3:: 0.08816146850585938
overhead4:: 0.42899489402770996
overhead5:: 0
time_provenance:: 1.3033559322357178
curr_diff: 0 tensor(2.7751e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.7751e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631319
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.09099721908569336
overhead3:: 0.12008881568908691
overhead4:: 0.4876549243927002
overhead5:: 0
time_provenance:: 1.7349936962127686
curr_diff: 0 tensor(2.7812e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.7812e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631319
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.0675048828125
overhead3:: 0.10017204284667969
overhead4:: 0.42050933837890625
overhead5:: 0
time_provenance:: 1.340813159942627
curr_diff: 0 tensor(2.7833e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.7833e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631319
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.1435849666595459
overhead3:: 0.20354080200195312
overhead4:: 0.9465367794036865
overhead5:: 0
time_provenance:: 1.9934754371643066
curr_diff: 0 tensor(6.0969e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.0969e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631302
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.1604480743408203
overhead3:: 0.241194486618042
overhead4:: 0.9663100242614746
overhead5:: 0
time_provenance:: 2.097104787826538
curr_diff: 0 tensor(6.1338e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.1338e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631302
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.14551711082458496
overhead3:: 0.2079300880432129
overhead4:: 0.8856422901153564
overhead5:: 0
time_provenance:: 1.9185082912445068
curr_diff: 0 tensor(6.1285e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.1285e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631302
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.1461188793182373
overhead3:: 0.21411609649658203
overhead4:: 0.9634034633636475
overhead5:: 0
time_provenance:: 2.0108675956726074
curr_diff: 0 tensor(6.1427e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.1427e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631302
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.3334009647369385
overhead3:: 0.5105798244476318
overhead4:: 1.640794277191162
overhead5:: 0
time_provenance:: 2.747910976409912
curr_diff: 0 tensor(1.1414e-15, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1414e-15, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631302
repetition 2
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.063045
Train - Epoch 0, Batch: 10, Loss: 1.424893
Train - Epoch 0, Batch: 20, Loss: 1.264079
Train - Epoch 0, Batch: 30, Loss: 1.206252
Train - Epoch 1, Batch: 0, Loss: 1.195453
Train - Epoch 1, Batch: 10, Loss: 1.167863
Train - Epoch 1, Batch: 20, Loss: 1.149990
Train - Epoch 1, Batch: 30, Loss: 1.140074
Train - Epoch 2, Batch: 0, Loss: 1.125174
Train - Epoch 2, Batch: 10, Loss: 1.120377
Train - Epoch 2, Batch: 20, Loss: 1.114738
Train - Epoch 2, Batch: 30, Loss: 1.111994
Train - Epoch 3, Batch: 0, Loss: 1.087499
Train - Epoch 3, Batch: 10, Loss: 1.087802
Train - Epoch 3, Batch: 20, Loss: 1.086414
Train - Epoch 3, Batch: 30, Loss: 1.094473
Train - Epoch 4, Batch: 0, Loss: 1.081857
Train - Epoch 4, Batch: 10, Loss: 1.067002
Train - Epoch 4, Batch: 20, Loss: 1.064607
Train - Epoch 4, Batch: 30, Loss: 1.058054
Train - Epoch 5, Batch: 0, Loss: 1.054974
Train - Epoch 5, Batch: 10, Loss: 1.050893
Train - Epoch 5, Batch: 20, Loss: 1.045663
Train - Epoch 5, Batch: 30, Loss: 1.045730
Train - Epoch 6, Batch: 0, Loss: 1.039842
Train - Epoch 6, Batch: 10, Loss: 1.048815
Train - Epoch 6, Batch: 20, Loss: 1.037805
Train - Epoch 6, Batch: 30, Loss: 1.020298
Train - Epoch 7, Batch: 0, Loss: 1.020542
Train - Epoch 7, Batch: 10, Loss: 1.022511
Train - Epoch 7, Batch: 20, Loss: 1.026417
Train - Epoch 7, Batch: 30, Loss: 1.025554
Train - Epoch 8, Batch: 0, Loss: 1.012894
Train - Epoch 8, Batch: 10, Loss: 1.006488
Train - Epoch 8, Batch: 20, Loss: 1.004663
Train - Epoch 8, Batch: 30, Loss: 0.997564
Train - Epoch 9, Batch: 0, Loss: 1.000587
Train - Epoch 9, Batch: 10, Loss: 1.006014
Train - Epoch 9, Batch: 20, Loss: 0.991306
Train - Epoch 9, Batch: 30, Loss: 0.988226
Train - Epoch 10, Batch: 0, Loss: 0.989225
Train - Epoch 10, Batch: 10, Loss: 0.978911
Train - Epoch 10, Batch: 20, Loss: 0.982755
Train - Epoch 10, Batch: 30, Loss: 0.982494
Train - Epoch 11, Batch: 0, Loss: 0.976675
Train - Epoch 11, Batch: 10, Loss: 0.983893
Train - Epoch 11, Batch: 20, Loss: 0.967847
Train - Epoch 11, Batch: 30, Loss: 0.979775
Train - Epoch 12, Batch: 0, Loss: 0.974403
Train - Epoch 12, Batch: 10, Loss: 0.969791
Train - Epoch 12, Batch: 20, Loss: 0.962772
Train - Epoch 12, Batch: 30, Loss: 0.970375
Train - Epoch 13, Batch: 0, Loss: 0.970456
Train - Epoch 13, Batch: 10, Loss: 0.954842
Train - Epoch 13, Batch: 20, Loss: 0.964241
Train - Epoch 13, Batch: 30, Loss: 0.960280
Train - Epoch 14, Batch: 0, Loss: 0.964906
Train - Epoch 14, Batch: 10, Loss: 0.957141
Train - Epoch 14, Batch: 20, Loss: 0.957052
Train - Epoch 14, Batch: 30, Loss: 0.947676
Train - Epoch 15, Batch: 0, Loss: 0.957108
Train - Epoch 15, Batch: 10, Loss: 0.948150
Train - Epoch 15, Batch: 20, Loss: 0.939694
Train - Epoch 15, Batch: 30, Loss: 0.945406
Train - Epoch 16, Batch: 0, Loss: 0.938577
Train - Epoch 16, Batch: 10, Loss: 0.948288
Train - Epoch 16, Batch: 20, Loss: 0.943210
Train - Epoch 16, Batch: 30, Loss: 0.937001
Train - Epoch 17, Batch: 0, Loss: 0.951980
Train - Epoch 17, Batch: 10, Loss: 0.934419
Train - Epoch 17, Batch: 20, Loss: 0.943215
Train - Epoch 17, Batch: 30, Loss: 0.933526
Train - Epoch 18, Batch: 0, Loss: 0.943173
Train - Epoch 18, Batch: 10, Loss: 0.931127
Train - Epoch 18, Batch: 20, Loss: 0.936251
Train - Epoch 18, Batch: 30, Loss: 0.924686
Train - Epoch 19, Batch: 0, Loss: 0.933439
Train - Epoch 19, Batch: 10, Loss: 0.940852
Train - Epoch 19, Batch: 20, Loss: 0.929242
Train - Epoch 19, Batch: 30, Loss: 0.918859
Train - Epoch 20, Batch: 0, Loss: 0.935113
Train - Epoch 20, Batch: 10, Loss: 0.921543
Train - Epoch 20, Batch: 20, Loss: 0.916051
Train - Epoch 20, Batch: 30, Loss: 0.926447
Train - Epoch 21, Batch: 0, Loss: 0.931591
Train - Epoch 21, Batch: 10, Loss: 0.922332
Train - Epoch 21, Batch: 20, Loss: 0.921251
Train - Epoch 21, Batch: 30, Loss: 0.926279
Train - Epoch 22, Batch: 0, Loss: 0.929306
Train - Epoch 22, Batch: 10, Loss: 0.920345
Train - Epoch 22, Batch: 20, Loss: 0.924019
Train - Epoch 22, Batch: 30, Loss: 0.922763
Train - Epoch 23, Batch: 0, Loss: 0.920424
Train - Epoch 23, Batch: 10, Loss: 0.917981
Train - Epoch 23, Batch: 20, Loss: 0.928861
Train - Epoch 23, Batch: 30, Loss: 0.915402
Train - Epoch 24, Batch: 0, Loss: 0.908835
Train - Epoch 24, Batch: 10, Loss: 0.914622
Train - Epoch 24, Batch: 20, Loss: 0.911816
Train - Epoch 24, Batch: 30, Loss: 0.905291
Train - Epoch 25, Batch: 0, Loss: 0.907690
Train - Epoch 25, Batch: 10, Loss: 0.913601
Train - Epoch 25, Batch: 20, Loss: 0.908561
Train - Epoch 25, Batch: 30, Loss: 0.908708
Train - Epoch 26, Batch: 0, Loss: 0.911798
Train - Epoch 26, Batch: 10, Loss: 0.907770
Train - Epoch 26, Batch: 20, Loss: 0.899480
Train - Epoch 26, Batch: 30, Loss: 0.915001
Train - Epoch 27, Batch: 0, Loss: 0.906437
Train - Epoch 27, Batch: 10, Loss: 0.909250
Train - Epoch 27, Batch: 20, Loss: 0.902015
Train - Epoch 27, Batch: 30, Loss: 0.910299
Train - Epoch 28, Batch: 0, Loss: 0.901052
Train - Epoch 28, Batch: 10, Loss: 0.906179
Train - Epoch 28, Batch: 20, Loss: 0.904355
Train - Epoch 28, Batch: 30, Loss: 0.905233
Train - Epoch 29, Batch: 0, Loss: 0.900399
Train - Epoch 29, Batch: 10, Loss: 0.906130
Train - Epoch 29, Batch: 20, Loss: 0.899860
Train - Epoch 29, Batch: 30, Loss: 0.892733
Train - Epoch 30, Batch: 0, Loss: 0.897149
Train - Epoch 30, Batch: 10, Loss: 0.887171
Train - Epoch 30, Batch: 20, Loss: 0.891338
Train - Epoch 30, Batch: 30, Loss: 0.903929
Train - Epoch 31, Batch: 0, Loss: 0.903993
Train - Epoch 31, Batch: 10, Loss: 0.893400
Train - Epoch 31, Batch: 20, Loss: 0.891872
Train - Epoch 31, Batch: 30, Loss: 0.885332
training_time:: 3.614548683166504
training time full:: 3.6145923137664795
provenance prepare time:: 4.76837158203125e-07
Test Avg. Loss: 0.000070, Accuracy: 0.629512
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 261
training time is 2.05831241607666
overhead:: 0
overhead2:: 0
time_baseline:: 2.0595717430114746
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629546
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.01857137680053711
overhead3:: 0.04451298713684082
overhead4:: 0.11796069145202637
overhead5:: 0
time_provenance:: 0.9052393436431885
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629546
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.020392179489135742
overhead3:: 0.032242536544799805
overhead4:: 0.12403464317321777
overhead5:: 0
time_provenance:: 0.8991825580596924
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629581
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.02269268035888672
overhead3:: 0.05271649360656738
overhead4:: 0.13751888275146484
overhead5:: 0
time_provenance:: 0.9135499000549316
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629546
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.025463104248046875
overhead3:: 0.04692482948303223
overhead4:: 0.14341092109680176
overhead5:: 0
time_provenance:: 0.927544116973877
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629581
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.032866716384887695
overhead3:: 0.04931807518005371
overhead4:: 0.22166681289672852
overhead5:: 0
time_provenance:: 1.0488576889038086
curr_diff: 0 tensor(6.5905e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.5905e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629546
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.03791093826293945
overhead3:: 0.05495429039001465
overhead4:: 0.22647619247436523
overhead5:: 0
time_provenance:: 1.106337547302246
curr_diff: 0 tensor(6.5530e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.5530e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629546
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.03718400001525879
overhead3:: 0.055639028549194336
overhead4:: 0.22045612335205078
overhead5:: 0
time_provenance:: 1.0841963291168213
curr_diff: 0 tensor(6.5634e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.5634e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629546
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.03839898109436035
overhead3:: 0.0666043758392334
overhead4:: 0.2647731304168701
overhead5:: 0
time_provenance:: 1.098320484161377
curr_diff: 0 tensor(6.5781e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.5781e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629546
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.05694460868835449
overhead3:: 0.08372211456298828
overhead4:: 0.40764665603637695
overhead5:: 0
time_provenance:: 1.2785069942474365
curr_diff: 0 tensor(2.0558e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.0558e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629546
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.06157946586608887
overhead3:: 0.09095358848571777
overhead4:: 0.4355194568634033
overhead5:: 0
time_provenance:: 1.329751968383789
curr_diff: 0 tensor(2.0645e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.0645e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629546
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.0723724365234375
overhead3:: 0.10271620750427246
overhead4:: 0.4418516159057617
overhead5:: 0
time_provenance:: 1.4671316146850586
curr_diff: 0 tensor(2.0979e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.0979e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629546
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.0640103816986084
overhead3:: 0.09520864486694336
overhead4:: 0.45827341079711914
overhead5:: 0
time_provenance:: 1.355437994003296
curr_diff: 0 tensor(2.1589e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1589e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629546
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.14263319969177246
overhead3:: 0.2195289134979248
overhead4:: 0.9647839069366455
overhead5:: 0
time_provenance:: 2.0237224102020264
curr_diff: 0 tensor(9.4254e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.4254e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629546
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.14006948471069336
overhead3:: 0.2032606601715088
overhead4:: 0.9702143669128418
overhead5:: 0
time_provenance:: 2.0103328227996826
curr_diff: 0 tensor(9.4562e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.4562e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629546
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.1583859920501709
overhead3:: 0.22191119194030762
overhead4:: 1.0002281665802002
overhead5:: 0
time_provenance:: 2.145843982696533
curr_diff: 0 tensor(9.4984e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.4984e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629546
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.21377968788146973
overhead3:: 0.27849555015563965
overhead4:: 1.1800463199615479
overhead5:: 0
time_provenance:: 2.656615734100342
curr_diff: 0 tensor(9.6828e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.6828e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629546
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.30684566497802734
overhead3:: 0.4540848731994629
overhead4:: 1.5762581825256348
overhead5:: 0
time_provenance:: 2.599651575088501
curr_diff: 0 tensor(1.0267e-15, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0267e-15, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629546
repetition 3
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 3 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 1.922564
Train - Epoch 0, Batch: 10, Loss: 1.383880
Train - Epoch 0, Batch: 20, Loss: 1.249638
Train - Epoch 0, Batch: 30, Loss: 1.197665
Train - Epoch 1, Batch: 0, Loss: 1.197646
Train - Epoch 1, Batch: 10, Loss: 1.165227
Train - Epoch 1, Batch: 20, Loss: 1.161331
Train - Epoch 1, Batch: 30, Loss: 1.138821
Train - Epoch 2, Batch: 0, Loss: 1.133513
Train - Epoch 2, Batch: 10, Loss: 1.122534
Train - Epoch 2, Batch: 20, Loss: 1.111929
Train - Epoch 2, Batch: 30, Loss: 1.108874
Train - Epoch 3, Batch: 0, Loss: 1.098434
Train - Epoch 3, Batch: 10, Loss: 1.099097
Train - Epoch 3, Batch: 20, Loss: 1.100141
Train - Epoch 3, Batch: 30, Loss: 1.084468
Train - Epoch 4, Batch: 0, Loss: 1.074600
Train - Epoch 4, Batch: 10, Loss: 1.072238
Train - Epoch 4, Batch: 20, Loss: 1.066313
Train - Epoch 4, Batch: 30, Loss: 1.047124
Train - Epoch 5, Batch: 0, Loss: 1.063702
Train - Epoch 5, Batch: 10, Loss: 1.050223
Train - Epoch 5, Batch: 20, Loss: 1.048898
Train - Epoch 5, Batch: 30, Loss: 1.044451
Train - Epoch 6, Batch: 0, Loss: 1.041659
Train - Epoch 6, Batch: 10, Loss: 1.038143
Train - Epoch 6, Batch: 20, Loss: 1.020973
Train - Epoch 6, Batch: 30, Loss: 1.033632
Train - Epoch 7, Batch: 0, Loss: 1.035646
Train - Epoch 7, Batch: 10, Loss: 1.011137
Train - Epoch 7, Batch: 20, Loss: 1.020543
Train - Epoch 7, Batch: 30, Loss: 1.004153
Train - Epoch 8, Batch: 0, Loss: 1.005370
Train - Epoch 8, Batch: 10, Loss: 1.002098
Train - Epoch 8, Batch: 20, Loss: 1.008048
Train - Epoch 8, Batch: 30, Loss: 1.002518
Train - Epoch 9, Batch: 0, Loss: 1.008688
Train - Epoch 9, Batch: 10, Loss: 0.989948
Train - Epoch 9, Batch: 20, Loss: 0.992855
Train - Epoch 9, Batch: 30, Loss: 0.988711
Train - Epoch 10, Batch: 0, Loss: 0.990675
Train - Epoch 10, Batch: 10, Loss: 0.990026
Train - Epoch 10, Batch: 20, Loss: 0.987808
Train - Epoch 10, Batch: 30, Loss: 0.980146
Train - Epoch 11, Batch: 0, Loss: 0.970290
Train - Epoch 11, Batch: 10, Loss: 0.978195
Train - Epoch 11, Batch: 20, Loss: 0.969424
Train - Epoch 11, Batch: 30, Loss: 0.972372
Train - Epoch 12, Batch: 0, Loss: 0.973640
Train - Epoch 12, Batch: 10, Loss: 0.970559
Train - Epoch 12, Batch: 20, Loss: 0.967156
Train - Epoch 12, Batch: 30, Loss: 0.984068
Train - Epoch 13, Batch: 0, Loss: 0.970051
Train - Epoch 13, Batch: 10, Loss: 0.976874
Train - Epoch 13, Batch: 20, Loss: 0.965570
Train - Epoch 13, Batch: 30, Loss: 0.957465
Train - Epoch 14, Batch: 0, Loss: 0.970028
Train - Epoch 14, Batch: 10, Loss: 0.961445
Train - Epoch 14, Batch: 20, Loss: 0.949389
Train - Epoch 14, Batch: 30, Loss: 0.955353
Train - Epoch 15, Batch: 0, Loss: 0.953679
Train - Epoch 15, Batch: 10, Loss: 0.943928
Train - Epoch 15, Batch: 20, Loss: 0.947609
Train - Epoch 15, Batch: 30, Loss: 0.956375
Train - Epoch 16, Batch: 0, Loss: 0.956571
Train - Epoch 16, Batch: 10, Loss: 0.946673
Train - Epoch 16, Batch: 20, Loss: 0.940159
Train - Epoch 16, Batch: 30, Loss: 0.955197
Train - Epoch 17, Batch: 0, Loss: 0.941545
Train - Epoch 17, Batch: 10, Loss: 0.940282
Train - Epoch 17, Batch: 20, Loss: 0.933951
Train - Epoch 17, Batch: 30, Loss: 0.934343
Train - Epoch 18, Batch: 0, Loss: 0.939858
Train - Epoch 18, Batch: 10, Loss: 0.935215
Train - Epoch 18, Batch: 20, Loss: 0.930081
Train - Epoch 18, Batch: 30, Loss: 0.935975
Train - Epoch 19, Batch: 0, Loss: 0.938600
Train - Epoch 19, Batch: 10, Loss: 0.930807
Train - Epoch 19, Batch: 20, Loss: 0.934760
Train - Epoch 19, Batch: 30, Loss: 0.932251
Train - Epoch 20, Batch: 0, Loss: 0.925633
Train - Epoch 20, Batch: 10, Loss: 0.931188
Train - Epoch 20, Batch: 20, Loss: 0.926682
Train - Epoch 20, Batch: 30, Loss: 0.932373
Train - Epoch 21, Batch: 0, Loss: 0.920420
Train - Epoch 21, Batch: 10, Loss: 0.926257
Train - Epoch 21, Batch: 20, Loss: 0.921854
Train - Epoch 21, Batch: 30, Loss: 0.913074
Train - Epoch 22, Batch: 0, Loss: 0.921925
Train - Epoch 22, Batch: 10, Loss: 0.921780
Train - Epoch 22, Batch: 20, Loss: 0.930937
Train - Epoch 22, Batch: 30, Loss: 0.920745
Train - Epoch 23, Batch: 0, Loss: 0.921964
Train - Epoch 23, Batch: 10, Loss: 0.923541
Train - Epoch 23, Batch: 20, Loss: 0.913146
Train - Epoch 23, Batch: 30, Loss: 0.905783
Train - Epoch 24, Batch: 0, Loss: 0.902175
Train - Epoch 24, Batch: 10, Loss: 0.904478
Train - Epoch 24, Batch: 20, Loss: 0.909366
Train - Epoch 24, Batch: 30, Loss: 0.923272
Train - Epoch 25, Batch: 0, Loss: 0.909238
Train - Epoch 25, Batch: 10, Loss: 0.913823
Train - Epoch 25, Batch: 20, Loss: 0.916721
Train - Epoch 25, Batch: 30, Loss: 0.908499
Train - Epoch 26, Batch: 0, Loss: 0.905115
Train - Epoch 26, Batch: 10, Loss: 0.908402
Train - Epoch 26, Batch: 20, Loss: 0.909436
Train - Epoch 26, Batch: 30, Loss: 0.906447
Train - Epoch 27, Batch: 0, Loss: 0.912840
Train - Epoch 27, Batch: 10, Loss: 0.907775
Train - Epoch 27, Batch: 20, Loss: 0.911464
Train - Epoch 27, Batch: 30, Loss: 0.901417
Train - Epoch 28, Batch: 0, Loss: 0.900521
Train - Epoch 28, Batch: 10, Loss: 0.897476
Train - Epoch 28, Batch: 20, Loss: 0.904910
Train - Epoch 28, Batch: 30, Loss: 0.902662
Train - Epoch 29, Batch: 0, Loss: 0.907384
Train - Epoch 29, Batch: 10, Loss: 0.903609
Train - Epoch 29, Batch: 20, Loss: 0.894478
Train - Epoch 29, Batch: 30, Loss: 0.907088
Train - Epoch 30, Batch: 0, Loss: 0.899691
Train - Epoch 30, Batch: 10, Loss: 0.903182
Train - Epoch 30, Batch: 20, Loss: 0.897465
Train - Epoch 30, Batch: 30, Loss: 0.894734
Train - Epoch 31, Batch: 0, Loss: 0.896280
Train - Epoch 31, Batch: 10, Loss: 0.903370
Train - Epoch 31, Batch: 20, Loss: 0.893797
Train - Epoch 31, Batch: 30, Loss: 0.897979
training_time:: 3.8895480632781982
training time full:: 3.8895914554595947
provenance prepare time:: 4.76837158203125e-07
Test Avg. Loss: 0.000070, Accuracy: 0.629684
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 261
training time is 2.0720999240875244
overhead:: 0
overhead2:: 0
time_baseline:: 2.0736453533172607
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629735
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.01737689971923828
overhead3:: 0.04140305519104004
overhead4:: 0.11217689514160156
overhead5:: 0
time_provenance:: 0.8548729419708252
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629753
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.02138233184814453
overhead3:: 0.05033683776855469
overhead4:: 0.11744070053100586
overhead5:: 0
time_provenance:: 0.9089062213897705
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629753
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.022307634353637695
overhead3:: 0.051854610443115234
overhead4:: 0.13905763626098633
overhead5:: 0
time_provenance:: 0.9043242931365967
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629753
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.026476621627807617
overhead3:: 0.040814876556396484
overhead4:: 0.15285611152648926
overhead5:: 0
time_provenance:: 1.0028715133666992
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629753
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.031094789505004883
overhead3:: 0.04782843589782715
overhead4:: 0.246016263961792
overhead5:: 0
time_provenance:: 1.0603759288787842
curr_diff: 0 tensor(7.3521e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.3521e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629735
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.03478384017944336
overhead3:: 0.05282306671142578
overhead4:: 0.24774551391601562
overhead5:: 0
time_provenance:: 1.071732997894287
curr_diff: 0 tensor(7.3568e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.3568e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629735
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.036306142807006836
overhead3:: 0.05391502380371094
overhead4:: 0.26402831077575684
overhead5:: 0
time_provenance:: 1.1016364097595215
curr_diff: 0 tensor(7.4982e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.4982e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629735
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.04115104675292969
overhead3:: 0.07211852073669434
overhead4:: 0.24242925643920898
overhead5:: 0
time_provenance:: 1.1442830562591553
curr_diff: 0 tensor(7.5311e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.5311e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629735
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.06000685691833496
overhead3:: 0.08273577690124512
overhead4:: 0.39979028701782227
overhead5:: 0
time_provenance:: 1.2936711311340332
curr_diff: 0 tensor(2.8790e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.8790e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629735
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.061014413833618164
overhead3:: 0.09158873558044434
overhead4:: 0.45430636405944824
overhead5:: 0
time_provenance:: 1.3415188789367676
curr_diff: 0 tensor(2.8968e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.8968e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629735
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.06260395050048828
overhead3:: 0.09134602546691895
overhead4:: 0.4085257053375244
overhead5:: 0
time_provenance:: 1.2967157363891602
curr_diff: 0 tensor(2.9170e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.9170e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629735
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.08229255676269531
overhead3:: 0.12441277503967285
overhead4:: 0.4909191131591797
overhead5:: 0
time_provenance:: 1.6393792629241943
curr_diff: 0 tensor(2.9413e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.9413e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629735
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.14866423606872559
overhead3:: 0.19981718063354492
overhead4:: 0.9726533889770508
overhead5:: 0
time_provenance:: 2.0406038761138916
curr_diff: 0 tensor(5.7456e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.7456e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629735
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.15300297737121582
overhead3:: 0.20610642433166504
overhead4:: 0.9587173461914062
overhead5:: 0
time_provenance:: 2.0197865962982178
curr_diff: 0 tensor(5.7594e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.7594e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629735
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.1803596019744873
overhead3:: 0.2524127960205078
overhead4:: 1.1486046314239502
overhead5:: 0
time_provenance:: 2.5143637657165527
curr_diff: 0 tensor(5.7778e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.7778e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629735
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.15470504760742188
overhead3:: 0.23530983924865723
overhead4:: 0.9780244827270508
overhead5:: 0
time_provenance:: 2.116891860961914
curr_diff: 0 tensor(5.8240e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.8240e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629735
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.32242918014526367
overhead3:: 0.482525110244751
overhead4:: 1.6620900630950928
overhead5:: 0
time_provenance:: 2.738267421722412
curr_diff: 0 tensor(1.1312e-15, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1312e-15, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629735
repetition 4
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 4 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 1.954057
Train - Epoch 0, Batch: 10, Loss: 1.380565
Train - Epoch 0, Batch: 20, Loss: 1.242507
Train - Epoch 0, Batch: 30, Loss: 1.182712
Train - Epoch 1, Batch: 0, Loss: 1.190571
Train - Epoch 1, Batch: 10, Loss: 1.149574
Train - Epoch 1, Batch: 20, Loss: 1.143619
Train - Epoch 1, Batch: 30, Loss: 1.130702
Train - Epoch 2, Batch: 0, Loss: 1.129696
Train - Epoch 2, Batch: 10, Loss: 1.115441
Train - Epoch 2, Batch: 20, Loss: 1.106906
Train - Epoch 2, Batch: 30, Loss: 1.097354
Train - Epoch 3, Batch: 0, Loss: 1.096108
Train - Epoch 3, Batch: 10, Loss: 1.098103
Train - Epoch 3, Batch: 20, Loss: 1.090989
Train - Epoch 3, Batch: 30, Loss: 1.065571
Train - Epoch 4, Batch: 0, Loss: 1.078412
Train - Epoch 4, Batch: 10, Loss: 1.068820
Train - Epoch 4, Batch: 20, Loss: 1.054380
Train - Epoch 4, Batch: 30, Loss: 1.051985
Train - Epoch 5, Batch: 0, Loss: 1.049086
Train - Epoch 5, Batch: 10, Loss: 1.044527
Train - Epoch 5, Batch: 20, Loss: 1.039215
Train - Epoch 5, Batch: 30, Loss: 1.038468
Train - Epoch 6, Batch: 0, Loss: 1.027282
Train - Epoch 6, Batch: 10, Loss: 1.038512
Train - Epoch 6, Batch: 20, Loss: 1.026028
Train - Epoch 6, Batch: 30, Loss: 1.025073
Train - Epoch 7, Batch: 0, Loss: 1.022812
Train - Epoch 7, Batch: 10, Loss: 1.012189
Train - Epoch 7, Batch: 20, Loss: 1.021663
Train - Epoch 7, Batch: 30, Loss: 1.019038
Train - Epoch 8, Batch: 0, Loss: 1.011353
Train - Epoch 8, Batch: 10, Loss: 1.003903
Train - Epoch 8, Batch: 20, Loss: 1.005046
Train - Epoch 8, Batch: 30, Loss: 0.997915
Train - Epoch 9, Batch: 0, Loss: 0.998248
Train - Epoch 9, Batch: 10, Loss: 0.992834
Train - Epoch 9, Batch: 20, Loss: 0.994222
Train - Epoch 9, Batch: 30, Loss: 0.988428
Train - Epoch 10, Batch: 0, Loss: 0.991169
Train - Epoch 10, Batch: 10, Loss: 0.986706
Train - Epoch 10, Batch: 20, Loss: 0.991627
Train - Epoch 10, Batch: 30, Loss: 0.976399
Train - Epoch 11, Batch: 0, Loss: 0.981634
Train - Epoch 11, Batch: 10, Loss: 0.982967
Train - Epoch 11, Batch: 20, Loss: 0.980386
Train - Epoch 11, Batch: 30, Loss: 0.972533
Train - Epoch 12, Batch: 0, Loss: 0.976541
Train - Epoch 12, Batch: 10, Loss: 0.974310
Train - Epoch 12, Batch: 20, Loss: 0.966804
Train - Epoch 12, Batch: 30, Loss: 0.968309
Train - Epoch 13, Batch: 0, Loss: 0.964303
Train - Epoch 13, Batch: 10, Loss: 0.959961
Train - Epoch 13, Batch: 20, Loss: 0.960429
Train - Epoch 13, Batch: 30, Loss: 0.951193
Train - Epoch 14, Batch: 0, Loss: 0.947492
Train - Epoch 14, Batch: 10, Loss: 0.953402
Train - Epoch 14, Batch: 20, Loss: 0.960051
Train - Epoch 14, Batch: 30, Loss: 0.958586
Train - Epoch 15, Batch: 0, Loss: 0.945201
Train - Epoch 15, Batch: 10, Loss: 0.947075
Train - Epoch 15, Batch: 20, Loss: 0.951775
Train - Epoch 15, Batch: 30, Loss: 0.952635
Train - Epoch 16, Batch: 0, Loss: 0.949674
Train - Epoch 16, Batch: 10, Loss: 0.962144
Train - Epoch 16, Batch: 20, Loss: 0.944486
Train - Epoch 16, Batch: 30, Loss: 0.942525
Train - Epoch 17, Batch: 0, Loss: 0.939955
Train - Epoch 17, Batch: 10, Loss: 0.948587
Train - Epoch 17, Batch: 20, Loss: 0.926766
Train - Epoch 17, Batch: 30, Loss: 0.934207
Train - Epoch 18, Batch: 0, Loss: 0.947922
Train - Epoch 18, Batch: 10, Loss: 0.937846
Train - Epoch 18, Batch: 20, Loss: 0.937944
Train - Epoch 18, Batch: 30, Loss: 0.935473
Train - Epoch 19, Batch: 0, Loss: 0.941570
Train - Epoch 19, Batch: 10, Loss: 0.943032
Train - Epoch 19, Batch: 20, Loss: 0.930544
Train - Epoch 19, Batch: 30, Loss: 0.931801
Train - Epoch 20, Batch: 0, Loss: 0.932646
Train - Epoch 20, Batch: 10, Loss: 0.935057
Train - Epoch 20, Batch: 20, Loss: 0.924582
Train - Epoch 20, Batch: 30, Loss: 0.916845
Train - Epoch 21, Batch: 0, Loss: 0.919119
Train - Epoch 21, Batch: 10, Loss: 0.914871
Train - Epoch 21, Batch: 20, Loss: 0.925016
Train - Epoch 21, Batch: 30, Loss: 0.927202
Train - Epoch 22, Batch: 0, Loss: 0.924755
Train - Epoch 22, Batch: 10, Loss: 0.916767
Train - Epoch 22, Batch: 20, Loss: 0.918690
Train - Epoch 22, Batch: 30, Loss: 0.918375
Train - Epoch 23, Batch: 0, Loss: 0.913349
Train - Epoch 23, Batch: 10, Loss: 0.925877
Train - Epoch 23, Batch: 20, Loss: 0.917932
Train - Epoch 23, Batch: 30, Loss: 0.910072
Train - Epoch 24, Batch: 0, Loss: 0.914811
Train - Epoch 24, Batch: 10, Loss: 0.914846
Train - Epoch 24, Batch: 20, Loss: 0.919234
Train - Epoch 24, Batch: 30, Loss: 0.912363
Train - Epoch 25, Batch: 0, Loss: 0.903597
Train - Epoch 25, Batch: 10, Loss: 0.899599
Train - Epoch 25, Batch: 20, Loss: 0.913856
Train - Epoch 25, Batch: 30, Loss: 0.906728
Train - Epoch 26, Batch: 0, Loss: 0.899787
Train - Epoch 26, Batch: 10, Loss: 0.899654
Train - Epoch 26, Batch: 20, Loss: 0.903136
Train - Epoch 26, Batch: 30, Loss: 0.901260
Train - Epoch 27, Batch: 0, Loss: 0.910759
Train - Epoch 27, Batch: 10, Loss: 0.899143
Train - Epoch 27, Batch: 20, Loss: 0.912937
Train - Epoch 27, Batch: 30, Loss: 0.899247
Train - Epoch 28, Batch: 0, Loss: 0.896121
Train - Epoch 28, Batch: 10, Loss: 0.900041
Train - Epoch 28, Batch: 20, Loss: 0.897530
Train - Epoch 28, Batch: 30, Loss: 0.901443
Train - Epoch 29, Batch: 0, Loss: 0.897057
Train - Epoch 29, Batch: 10, Loss: 0.906195
Train - Epoch 29, Batch: 20, Loss: 0.901667
Train - Epoch 29, Batch: 30, Loss: 0.908543
Train - Epoch 30, Batch: 0, Loss: 0.903636
Train - Epoch 30, Batch: 10, Loss: 0.895359
Train - Epoch 30, Batch: 20, Loss: 0.895694
Train - Epoch 30, Batch: 30, Loss: 0.899070
Train - Epoch 31, Batch: 0, Loss: 0.899198
Train - Epoch 31, Batch: 10, Loss: 0.893950
Train - Epoch 31, Batch: 20, Loss: 0.895615
Train - Epoch 31, Batch: 30, Loss: 0.898627
training_time:: 3.8637452125549316
training time full:: 3.863790273666382
provenance prepare time:: 7.152557373046875e-07
Test Avg. Loss: 0.000070, Accuracy: 0.630596
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 261
training time is 2.1348416805267334
overhead:: 0
overhead2:: 0
time_baseline:: 2.1364939212799072
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630717
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.018855810165405273
overhead3:: 0.02827763557434082
overhead4:: 0.11051368713378906
overhead5:: 0
time_provenance:: 0.8982126712799072
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630768
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.020256996154785156
overhead3:: 0.04180455207824707
overhead4:: 0.1420729160308838
overhead5:: 0
time_provenance:: 0.9233541488647461
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630751
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.02310323715209961
overhead3:: 0.0447235107421875
overhead4:: 0.13730549812316895
overhead5:: 0
time_provenance:: 0.9253032207489014
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630768
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.0256650447845459
overhead3:: 0.04970073699951172
overhead4:: 0.14954686164855957
overhead5:: 0
time_provenance:: 0.9258937835693359
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630751
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.03127121925354004
overhead3:: 0.060419321060180664
overhead4:: 0.2250053882598877
overhead5:: 0
time_provenance:: 1.05303955078125
curr_diff: 0 tensor(5.5572e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.5572e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630717
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.03404569625854492
overhead3:: 0.051400184631347656
overhead4:: 0.23610854148864746
overhead5:: 0
time_provenance:: 1.0746071338653564
curr_diff: 0 tensor(5.7329e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.7329e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630717
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.03519868850708008
overhead3:: 0.052626848220825195
overhead4:: 0.23321247100830078
overhead5:: 0
time_provenance:: 1.0700483322143555
curr_diff: 0 tensor(5.8484e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.8484e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630717
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.03972005844116211
overhead3:: 0.0686182975769043
overhead4:: 0.2697184085845947
overhead5:: 0
time_provenance:: 1.1681067943572998
curr_diff: 0 tensor(5.9950e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.9950e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630717
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.06046581268310547
overhead3:: 0.10146141052246094
overhead4:: 0.39081358909606934
overhead5:: 0
time_provenance:: 1.2989590167999268
curr_diff: 0 tensor(2.2870e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.2870e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630734
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.06175518035888672
overhead3:: 0.09116888046264648
overhead4:: 0.4328877925872803
overhead5:: 0
time_provenance:: 1.3282201290130615
curr_diff: 0 tensor(2.3725e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.3725e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630734
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.09182262420654297
overhead3:: 0.12221646308898926
overhead4:: 0.48963332176208496
overhead5:: 0
time_provenance:: 1.7364535331726074
curr_diff: 0 tensor(2.3964e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.3964e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630734
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.0705721378326416
overhead3:: 0.10002517700195312
overhead4:: 0.4410581588745117
overhead5:: 0
time_provenance:: 1.4102537631988525
curr_diff: 0 tensor(2.4165e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.4165e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630734
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.13789987564086914
overhead3:: 0.19993996620178223
overhead4:: 0.9602246284484863
overhead5:: 0
time_provenance:: 1.9994442462921143
curr_diff: 0 tensor(7.0010e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.0010e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630717
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.1485731601715088
overhead3:: 0.22267627716064453
overhead4:: 0.984450101852417
overhead5:: 0
time_provenance:: 2.058828115463257
curr_diff: 0 tensor(7.0948e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.0948e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630717
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.14450836181640625
overhead3:: 0.20685625076293945
overhead4:: 0.9631659984588623
overhead5:: 0
time_provenance:: 2.0185751914978027
curr_diff: 0 tensor(7.2021e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.2021e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630717
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.1447136402130127
overhead3:: 0.2086317539215088
overhead4:: 0.9604790210723877
overhead5:: 0
time_provenance:: 2.0113613605499268
curr_diff: 0 tensor(7.1871e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.1871e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630717
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.28014636039733887
overhead3:: 0.39665699005126953
overhead4:: 1.5127284526824951
overhead5:: 0
time_provenance:: 2.441707134246826
curr_diff: 0 tensor(1.3359e-15, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3359e-15, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630717
deletion rate:: 0.001
python3 generate_rand_ids 0.001  covtype 0
tensor([417795, 353286, 306183, 514063, 424979, 107541, 270357, 363541, 274454,
        444442, 145442, 278568, 255018, 298026, 379948, 440365,  89134, 455730,
        230451, 504884, 161844, 440378, 277563, 473147, 382013, 121918,  77887,
        453696, 343104, 453698, 201791,  31809,  81989,  92222, 358464, 246850,
        107587, 311365, 430155, 162889, 431177,  59468,  72782, 315473, 373849,
         29788, 501856, 235618, 248931, 351332, 187493, 517221, 381031, 328810,
        237675, 222316, 366698, 211051, 151662,  58479, 282740, 228470, 128129,
        312450,  62595, 445573,  65669,  35978, 341131, 196746, 199819, 505996,
         62605, 432270, 212117,  87191,  14487, 178328, 214169, 148635, 196761,
           155, 181406, 196767, 421020,  89246, 418972, 154787, 480419,  38049,
        424102, 270499,  73896, 303270,  20648, 124072, 187565, 203950, 359597,
        264375,  94393, 305338, 464060, 512188, 395454, 155837, 264394, 284879,
        325842, 333011, 461014, 216285,  71902, 155869, 346338, 292067, 111844,
        158950, 146667, 335868, 495853, 139507,  38131, 307445, 449781, 322807,
        246005, 521472,  71937,   5378, 271616, 391428, 508165, 468230, 229638,
         43276, 170255, 432399,  70927, 298257,  23826, 464152, 432409, 435483,
        212252, 389411, 155940, 391461, 279846, 145702, 175402,  50475, 146736,
        159027, 134452,  36150, 317753, 316730,  82236,   3392, 346437, 480582,
        106823,  32072, 485702, 266567, 450889,    330,  13645, 445778, 251219,
         79188, 150868, 370005,  26963, 367960, 208217,  25946, 339294, 485728,
        258401, 475490, 103779, 408932,  58725, 483684, 132452, 108905, 175469,
        405870, 165230,  97647, 151921, 254322,  86385, 194929,  74095, 484730,
        314748,  30076,   6527, 368000, 227713, 448897,  96643, 160130,  96645,
        273797, 289157, 183688, 156032, 386436, 467343, 345488, 445842, 398749,
        364959,  84392, 416169,  64940, 132524, 257454, 442799, 410028, 426417,
        385452, 129459,   2486,  12728, 484793,  14776, 473532,  53692, 511422,
        392639, 418240, 126401, 512450, 101827, 489924, 137669, 361926, 313799,
         14790, 191943, 314825, 443848,  51658, 186832, 172497, 210384, 372179,
        216531, 205269, 232918, 229848, 390617, 394714, 176603, 338393,  86493,
        335320,  81913, 357856, 385507, 206309, 442853, 519653, 104937, 443886,
        456175, 128496,    497, 343545, 479743, 520704,  36351, 407046, 103944,
        297483, 326158,    527, 330258, 487955, 136725, 351767, 159257, 255514,
        362011, 475675, 323104, 198179, 369189, 502313, 238122, 408105, 465452,
        465455, 177711, 504368, 107059,  52792,  28217, 486973, 170568, 463436,
         45646, 352847,  49744, 478806, 312919, 128600,  36442, 322143, 156255,
        392801, 391776, 135779, 415332, 127591, 111209, 148074,  78443, 137836,
        118384, 307825, 124530, 321137, 486006, 106108, 190076, 286332, 277122,
        507524, 459396,  17030, 367237, 262792, 451212, 471693, 107148, 511631,
        297618, 287379,  83604, 282260,  33430, 160405,  81560,  10907, 210589,
        121504, 327330, 519843, 385703, 380583, 327339, 198317, 425651, 498357,
        475834, 185019, 171708, 342720, 348865, 376513, 209602, 366275, 175813,
        120517,  59079, 296648, 474823, 353996, 101068, 243409, 107219,  31444,
         76500, 201433, 370394, 342752,    738, 134883, 274148, 195302, 247526,
        487144, 340713, 509674, 210663, 210664, 510702, 280305,  25330, 288499,
        441076, 517876, 249588, 143094, 340725, 368371,  51960, 443128, 159484,
        417533, 246529,  20225,  57090, 443142, 125704, 101130, 476940,  89871,
        510735,  61204, 304919,   2847, 280352, 470816, 203553,  53025, 143140,
        162597,  55078, 518951,  22311, 116521, 258855, 450347,   2857, 234289,
        257841, 500532,  52025, 448317, 201537, 298818, 367432, 349002, 393034,
        342861, 276302, 415567, 352081, 492372, 465750, 127832, 172888,  84827,
        508764, 350047, 126816, 462689, 336748, 463725, 424815, 116592,  67443,
        225139, 456565,   9078, 134007, 201595, 139132, 476029, 139135, 180097,
        338822, 379784, 344968, 128908, 214928,  14224,  42901, 461721, 376729,
        418720,  30625, 286625, 400290, 505762, 182182, 204716, 278445, 132015,
        281520, 147375,   4015, 394165,    954, 125882, 107451,  56260, 104388,
         97222, 303046,  15305,  47050, 288717, 485331,  89047, 307160, 223193,
        425947, 229340, 261088, 155617, 105442,  63458, 445413, 418793,  46058,
         66539,  25581, 317427, 183285, 361462, 489463, 486393, 262140, 364542])
batch size:: 16384
repetition 0
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 0 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.012359
Train - Epoch 0, Batch: 10, Loss: 1.401632
Train - Epoch 0, Batch: 20, Loss: 1.250590
Train - Epoch 0, Batch: 30, Loss: 1.209926
Train - Epoch 1, Batch: 0, Loss: 1.180676
Train - Epoch 1, Batch: 10, Loss: 1.166231
Train - Epoch 1, Batch: 20, Loss: 1.146777
Train - Epoch 1, Batch: 30, Loss: 1.134414
Train - Epoch 2, Batch: 0, Loss: 1.135334
Train - Epoch 2, Batch: 10, Loss: 1.116314
Train - Epoch 2, Batch: 20, Loss: 1.109790
Train - Epoch 2, Batch: 30, Loss: 1.101870
Train - Epoch 3, Batch: 0, Loss: 1.102487
Train - Epoch 3, Batch: 10, Loss: 1.093240
Train - Epoch 3, Batch: 20, Loss: 1.079468
Train - Epoch 3, Batch: 30, Loss: 1.068024
Train - Epoch 4, Batch: 0, Loss: 1.078050
Train - Epoch 4, Batch: 10, Loss: 1.064977
Train - Epoch 4, Batch: 20, Loss: 1.065097
Train - Epoch 4, Batch: 30, Loss: 1.055822
Train - Epoch 5, Batch: 0, Loss: 1.046499
Train - Epoch 5, Batch: 10, Loss: 1.040568
Train - Epoch 5, Batch: 20, Loss: 1.044895
Train - Epoch 5, Batch: 30, Loss: 1.050157
Train - Epoch 6, Batch: 0, Loss: 1.034716
Train - Epoch 6, Batch: 10, Loss: 1.026960
Train - Epoch 6, Batch: 20, Loss: 1.022428
Train - Epoch 6, Batch: 30, Loss: 1.023292
Train - Epoch 7, Batch: 0, Loss: 1.029873
Train - Epoch 7, Batch: 10, Loss: 1.016690
Train - Epoch 7, Batch: 20, Loss: 1.009121
Train - Epoch 7, Batch: 30, Loss: 1.010186
Train - Epoch 8, Batch: 0, Loss: 1.005688
Train - Epoch 8, Batch: 10, Loss: 1.000938
Train - Epoch 8, Batch: 20, Loss: 1.004182
Train - Epoch 8, Batch: 30, Loss: 0.996182
Train - Epoch 9, Batch: 0, Loss: 0.998933
Train - Epoch 9, Batch: 10, Loss: 0.993036
Train - Epoch 9, Batch: 20, Loss: 0.997855
Train - Epoch 9, Batch: 30, Loss: 0.992001
Train - Epoch 10, Batch: 0, Loss: 0.993120
Train - Epoch 10, Batch: 10, Loss: 0.991330
Train - Epoch 10, Batch: 20, Loss: 0.983002
Train - Epoch 10, Batch: 30, Loss: 0.974617
Train - Epoch 11, Batch: 0, Loss: 0.976786
Train - Epoch 11, Batch: 10, Loss: 0.987950
Train - Epoch 11, Batch: 20, Loss: 0.972599
Train - Epoch 11, Batch: 30, Loss: 0.975792
Train - Epoch 12, Batch: 0, Loss: 0.968161
Train - Epoch 12, Batch: 10, Loss: 0.969254
Train - Epoch 12, Batch: 20, Loss: 0.970089
Train - Epoch 12, Batch: 30, Loss: 0.972826
Train - Epoch 13, Batch: 0, Loss: 0.976379
Train - Epoch 13, Batch: 10, Loss: 0.961990
Train - Epoch 13, Batch: 20, Loss: 0.957744
Train - Epoch 13, Batch: 30, Loss: 0.953682
Train - Epoch 14, Batch: 0, Loss: 0.966110
Train - Epoch 14, Batch: 10, Loss: 0.950780
Train - Epoch 14, Batch: 20, Loss: 0.948907
Train - Epoch 14, Batch: 30, Loss: 0.954702
Train - Epoch 15, Batch: 0, Loss: 0.969510
Train - Epoch 15, Batch: 10, Loss: 0.939487
Train - Epoch 15, Batch: 20, Loss: 0.942825
Train - Epoch 15, Batch: 30, Loss: 0.942678
Train - Epoch 16, Batch: 0, Loss: 0.936832
Train - Epoch 16, Batch: 10, Loss: 0.942798
Train - Epoch 16, Batch: 20, Loss: 0.939155
Train - Epoch 16, Batch: 30, Loss: 0.944859
Train - Epoch 17, Batch: 0, Loss: 0.945786
Train - Epoch 17, Batch: 10, Loss: 0.944886
Train - Epoch 17, Batch: 20, Loss: 0.940523
Train - Epoch 17, Batch: 30, Loss: 0.934872
Train - Epoch 18, Batch: 0, Loss: 0.937699
Train - Epoch 18, Batch: 10, Loss: 0.935621
Train - Epoch 18, Batch: 20, Loss: 0.924094
Train - Epoch 18, Batch: 30, Loss: 0.932094
Train - Epoch 19, Batch: 0, Loss: 0.927818
Train - Epoch 19, Batch: 10, Loss: 0.928140
Train - Epoch 19, Batch: 20, Loss: 0.935958
Train - Epoch 19, Batch: 30, Loss: 0.926980
Train - Epoch 20, Batch: 0, Loss: 0.928969
Train - Epoch 20, Batch: 10, Loss: 0.930696
Train - Epoch 20, Batch: 20, Loss: 0.919440
Train - Epoch 20, Batch: 30, Loss: 0.922594
Train - Epoch 21, Batch: 0, Loss: 0.930432
Train - Epoch 21, Batch: 10, Loss: 0.925730
Train - Epoch 21, Batch: 20, Loss: 0.920980
Train - Epoch 21, Batch: 30, Loss: 0.923500
Train - Epoch 22, Batch: 0, Loss: 0.927664
Train - Epoch 22, Batch: 10, Loss: 0.925146
Train - Epoch 22, Batch: 20, Loss: 0.915434
Train - Epoch 22, Batch: 30, Loss: 0.912552
Train - Epoch 23, Batch: 0, Loss: 0.916042
Train - Epoch 23, Batch: 10, Loss: 0.915170
Train - Epoch 23, Batch: 20, Loss: 0.919396
Train - Epoch 23, Batch: 30, Loss: 0.915111
Train - Epoch 24, Batch: 0, Loss: 0.915305
Train - Epoch 24, Batch: 10, Loss: 0.912708
Train - Epoch 24, Batch: 20, Loss: 0.912236
Train - Epoch 24, Batch: 30, Loss: 0.903161
Train - Epoch 25, Batch: 0, Loss: 0.909187
Train - Epoch 25, Batch: 10, Loss: 0.914318
Train - Epoch 25, Batch: 20, Loss: 0.910942
Train - Epoch 25, Batch: 30, Loss: 0.917142
Train - Epoch 26, Batch: 0, Loss: 0.911617
Train - Epoch 26, Batch: 10, Loss: 0.903035
Train - Epoch 26, Batch: 20, Loss: 0.903996
Train - Epoch 26, Batch: 30, Loss: 0.910791
Train - Epoch 27, Batch: 0, Loss: 0.907398
Train - Epoch 27, Batch: 10, Loss: 0.899891
Train - Epoch 27, Batch: 20, Loss: 0.915426
Train - Epoch 27, Batch: 30, Loss: 0.908003
Train - Epoch 28, Batch: 0, Loss: 0.899639
Train - Epoch 28, Batch: 10, Loss: 0.903078
Train - Epoch 28, Batch: 20, Loss: 0.900588
Train - Epoch 28, Batch: 30, Loss: 0.904398
Train - Epoch 29, Batch: 0, Loss: 0.903492
Train - Epoch 29, Batch: 10, Loss: 0.907513
Train - Epoch 29, Batch: 20, Loss: 0.907690
Train - Epoch 29, Batch: 30, Loss: 0.903497
Train - Epoch 30, Batch: 0, Loss: 0.889145
Train - Epoch 30, Batch: 10, Loss: 0.894107
Train - Epoch 30, Batch: 20, Loss: 0.899085
Train - Epoch 30, Batch: 30, Loss: 0.895140
Train - Epoch 31, Batch: 0, Loss: 0.896666
Train - Epoch 31, Batch: 10, Loss: 0.899001
Train - Epoch 31, Batch: 20, Loss: 0.897284
Train - Epoch 31, Batch: 30, Loss: 0.900502
training_time:: 3.8496007919311523
training time full:: 3.8496463298797607
provenance prepare time:: 4.76837158203125e-07
Test Avg. Loss: 0.000070, Accuracy: 0.629890
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 522
training time is 2.1134865283966064
overhead:: 0
overhead2:: 0
time_baseline:: 2.114985227584839
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629959
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.01707744598388672
overhead3:: 0.027644634246826172
overhead4:: 0.10476255416870117
overhead5:: 0
time_provenance:: 0.8431134223937988
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0021, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0021, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629942
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.020938396453857422
overhead3:: 0.03799724578857422
overhead4:: 0.14223670959472656
overhead5:: 0
time_provenance:: 0.9660241603851318
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0020, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0020, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629959
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.022479772567749023
overhead3:: 0.04534745216369629
overhead4:: 0.14111113548278809
overhead5:: 0
time_provenance:: 0.9290945529937744
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0021, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0021, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629942
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.025167226791381836
overhead3:: 0.04304957389831543
overhead4:: 0.16563773155212402
overhead5:: 0
time_provenance:: 0.9461324214935303
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0020, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0020, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629959
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.03115081787109375
overhead3:: 0.06001019477844238
overhead4:: 0.2424616813659668
overhead5:: 0
time_provenance:: 1.0625221729278564
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0019, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0019, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629959
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.03533816337585449
overhead3:: 0.05178046226501465
overhead4:: 0.2241978645324707
overhead5:: 0
time_provenance:: 1.0653958320617676
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0019, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0019, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629959
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.03828907012939453
overhead3:: 0.056380271911621094
overhead4:: 0.2434680461883545
overhead5:: 0
time_provenance:: 1.1080632209777832
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0019, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0019, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629959
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.03914666175842285
overhead3:: 0.05789780616760254
overhead4:: 0.24590420722961426
overhead5:: 0
time_provenance:: 1.0344486236572266
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0019, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0019, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629959
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.06521177291870117
overhead3:: 0.09075140953063965
overhead4:: 0.4002542495727539
overhead5:: 0
time_provenance:: 1.3226628303527832
curr_diff: 0 tensor(4.0931e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.0931e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0019, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0019, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629959
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.06506824493408203
overhead3:: 0.09122133255004883
overhead4:: 0.422652006149292
overhead5:: 0
time_provenance:: 1.3304738998413086
curr_diff: 0 tensor(4.1322e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.1322e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0019, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0019, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629959
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.06384444236755371
overhead3:: 0.09297895431518555
overhead4:: 0.40727925300598145
overhead5:: 0
time_provenance:: 1.3133134841918945
curr_diff: 0 tensor(4.1335e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.1335e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0019, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0019, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629959
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.06658530235290527
overhead3:: 0.09729123115539551
overhead4:: 0.43857502937316895
overhead5:: 0
time_provenance:: 1.3486168384552002
curr_diff: 0 tensor(4.1160e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.1160e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0019, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0019, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629959
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.14165496826171875
overhead3:: 0.20236897468566895
overhead4:: 0.9546921253204346
overhead5:: 0
time_provenance:: 2.004772186279297
curr_diff: 0 tensor(7.4722e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.4722e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629959
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.14705824851989746
overhead3:: 0.21820306777954102
overhead4:: 1.0081136226654053
overhead5:: 0
time_provenance:: 2.073188543319702
curr_diff: 0 tensor(7.5200e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.5200e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629959
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.20199847221374512
overhead3:: 0.2708749771118164
overhead4:: 1.1397418975830078
overhead5:: 0
time_provenance:: 2.5976626873016357
curr_diff: 0 tensor(7.5129e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.5129e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629959
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.15946340560913086
overhead3:: 0.24078774452209473
overhead4:: 0.9338412284851074
overhead5:: 0
time_provenance:: 2.041677474975586
curr_diff: 0 tensor(7.5539e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.5539e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629959
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.3239407539367676
overhead3:: 0.4606442451477051
overhead4:: 1.5948495864868164
overhead5:: 0
time_provenance:: 2.643172264099121
curr_diff: 0 tensor(9.8230e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.8230e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629959
repetition 1
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 1 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 1.920961
Train - Epoch 0, Batch: 10, Loss: 1.378317
Train - Epoch 0, Batch: 20, Loss: 1.236473
Train - Epoch 0, Batch: 30, Loss: 1.189968
Train - Epoch 1, Batch: 0, Loss: 1.189926
Train - Epoch 1, Batch: 10, Loss: 1.158100
Train - Epoch 1, Batch: 20, Loss: 1.152665
Train - Epoch 1, Batch: 30, Loss: 1.134683
Train - Epoch 2, Batch: 0, Loss: 1.131849
Train - Epoch 2, Batch: 10, Loss: 1.116751
Train - Epoch 2, Batch: 20, Loss: 1.104978
Train - Epoch 2, Batch: 30, Loss: 1.100787
Train - Epoch 3, Batch: 0, Loss: 1.090108
Train - Epoch 3, Batch: 10, Loss: 1.083589
Train - Epoch 3, Batch: 20, Loss: 1.080006
Train - Epoch 3, Batch: 30, Loss: 1.081054
Train - Epoch 4, Batch: 0, Loss: 1.077193
Train - Epoch 4, Batch: 10, Loss: 1.063523
Train - Epoch 4, Batch: 20, Loss: 1.064387
Train - Epoch 4, Batch: 30, Loss: 1.060328
Train - Epoch 5, Batch: 0, Loss: 1.051006
Train - Epoch 5, Batch: 10, Loss: 1.046276
Train - Epoch 5, Batch: 20, Loss: 1.042584
Train - Epoch 5, Batch: 30, Loss: 1.036424
Train - Epoch 6, Batch: 0, Loss: 1.033648
Train - Epoch 6, Batch: 10, Loss: 1.028988
Train - Epoch 6, Batch: 20, Loss: 1.031511
Train - Epoch 6, Batch: 30, Loss: 1.023922
Train - Epoch 7, Batch: 0, Loss: 1.011067
Train - Epoch 7, Batch: 10, Loss: 1.016273
Train - Epoch 7, Batch: 20, Loss: 1.016175
Train - Epoch 7, Batch: 30, Loss: 1.002543
Train - Epoch 8, Batch: 0, Loss: 1.008355
Train - Epoch 8, Batch: 10, Loss: 1.002248
Train - Epoch 8, Batch: 20, Loss: 0.989824
Train - Epoch 8, Batch: 30, Loss: 0.998776
Train - Epoch 9, Batch: 0, Loss: 0.997214
Train - Epoch 9, Batch: 10, Loss: 0.989547
Train - Epoch 9, Batch: 20, Loss: 0.979296
Train - Epoch 9, Batch: 30, Loss: 0.994035
Train - Epoch 10, Batch: 0, Loss: 0.988402
Train - Epoch 10, Batch: 10, Loss: 0.977584
Train - Epoch 10, Batch: 20, Loss: 0.967169
Train - Epoch 10, Batch: 30, Loss: 0.971123
Train - Epoch 11, Batch: 0, Loss: 0.978629
Train - Epoch 11, Batch: 10, Loss: 0.979045
Train - Epoch 11, Batch: 20, Loss: 0.966840
Train - Epoch 11, Batch: 30, Loss: 0.980781
Train - Epoch 12, Batch: 0, Loss: 0.965886
Train - Epoch 12, Batch: 10, Loss: 0.975201
Train - Epoch 12, Batch: 20, Loss: 0.956492
Train - Epoch 12, Batch: 30, Loss: 0.953477
Train - Epoch 13, Batch: 0, Loss: 0.968631
Train - Epoch 13, Batch: 10, Loss: 0.959586
Train - Epoch 13, Batch: 20, Loss: 0.952973
Train - Epoch 13, Batch: 30, Loss: 0.946220
Train - Epoch 14, Batch: 0, Loss: 0.965712
Train - Epoch 14, Batch: 10, Loss: 0.954204
Train - Epoch 14, Batch: 20, Loss: 0.958589
Train - Epoch 14, Batch: 30, Loss: 0.940445
Train - Epoch 15, Batch: 0, Loss: 0.942345
Train - Epoch 15, Batch: 10, Loss: 0.952992
Train - Epoch 15, Batch: 20, Loss: 0.952579
Train - Epoch 15, Batch: 30, Loss: 0.945955
Train - Epoch 16, Batch: 0, Loss: 0.943051
Train - Epoch 16, Batch: 10, Loss: 0.940785
Train - Epoch 16, Batch: 20, Loss: 0.935710
Train - Epoch 16, Batch: 30, Loss: 0.944300
Train - Epoch 17, Batch: 0, Loss: 0.934583
Train - Epoch 17, Batch: 10, Loss: 0.942985
Train - Epoch 17, Batch: 20, Loss: 0.932657
Train - Epoch 17, Batch: 30, Loss: 0.934166
Train - Epoch 18, Batch: 0, Loss: 0.939478
Train - Epoch 18, Batch: 10, Loss: 0.931318
Train - Epoch 18, Batch: 20, Loss: 0.932142
Train - Epoch 18, Batch: 30, Loss: 0.925056
Train - Epoch 19, Batch: 0, Loss: 0.932976
Train - Epoch 19, Batch: 10, Loss: 0.927078
Train - Epoch 19, Batch: 20, Loss: 0.927853
Train - Epoch 19, Batch: 30, Loss: 0.935011
Train - Epoch 20, Batch: 0, Loss: 0.918483
Train - Epoch 20, Batch: 10, Loss: 0.934005
Train - Epoch 20, Batch: 20, Loss: 0.919716
Train - Epoch 20, Batch: 30, Loss: 0.926626
Train - Epoch 21, Batch: 0, Loss: 0.912873
Train - Epoch 21, Batch: 10, Loss: 0.915707
Train - Epoch 21, Batch: 20, Loss: 0.919085
Train - Epoch 21, Batch: 30, Loss: 0.914061
Train - Epoch 22, Batch: 0, Loss: 0.915023
Train - Epoch 22, Batch: 10, Loss: 0.917001
Train - Epoch 22, Batch: 20, Loss: 0.926273
Train - Epoch 22, Batch: 30, Loss: 0.911013
Train - Epoch 23, Batch: 0, Loss: 0.919612
Train - Epoch 23, Batch: 10, Loss: 0.905453
Train - Epoch 23, Batch: 20, Loss: 0.910218
Train - Epoch 23, Batch: 30, Loss: 0.906009
Train - Epoch 24, Batch: 0, Loss: 0.916308
Train - Epoch 24, Batch: 10, Loss: 0.906099
Train - Epoch 24, Batch: 20, Loss: 0.911738
Train - Epoch 24, Batch: 30, Loss: 0.921029
Train - Epoch 25, Batch: 0, Loss: 0.908023
Train - Epoch 25, Batch: 10, Loss: 0.903225
Train - Epoch 25, Batch: 20, Loss: 0.908760
Train - Epoch 25, Batch: 30, Loss: 0.910540
Train - Epoch 26, Batch: 0, Loss: 0.908297
Train - Epoch 26, Batch: 10, Loss: 0.906821
Train - Epoch 26, Batch: 20, Loss: 0.909219
Train - Epoch 26, Batch: 30, Loss: 0.910078
Train - Epoch 27, Batch: 0, Loss: 0.898171
Train - Epoch 27, Batch: 10, Loss: 0.901412
Train - Epoch 27, Batch: 20, Loss: 0.904860
Train - Epoch 27, Batch: 30, Loss: 0.899879
Train - Epoch 28, Batch: 0, Loss: 0.892790
Train - Epoch 28, Batch: 10, Loss: 0.898577
Train - Epoch 28, Batch: 20, Loss: 0.894141
Train - Epoch 28, Batch: 30, Loss: 0.896082
Train - Epoch 29, Batch: 0, Loss: 0.898695
Train - Epoch 29, Batch: 10, Loss: 0.902420
Train - Epoch 29, Batch: 20, Loss: 0.896438
Train - Epoch 29, Batch: 30, Loss: 0.885459
Train - Epoch 30, Batch: 0, Loss: 0.901071
Train - Epoch 30, Batch: 10, Loss: 0.897733
Train - Epoch 30, Batch: 20, Loss: 0.892567
Train - Epoch 30, Batch: 30, Loss: 0.891675
Train - Epoch 31, Batch: 0, Loss: 0.887870
Train - Epoch 31, Batch: 10, Loss: 0.892048
Train - Epoch 31, Batch: 20, Loss: 0.898029
Train - Epoch 31, Batch: 30, Loss: 0.890223
training_time:: 3.585040807723999
training time full:: 3.5850846767425537
provenance prepare time:: 4.76837158203125e-07
Test Avg. Loss: 0.000070, Accuracy: 0.631319
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 522
training time is 2.010956287384033
overhead:: 0
overhead2:: 0
time_baseline:: 2.0125296115875244
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631302
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.017511606216430664
overhead3:: 0.028068065643310547
overhead4:: 0.10343694686889648
overhead5:: 0
time_provenance:: 0.8509831428527832
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0020, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0020, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631267
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.020439624786376953
overhead3:: 0.031761884689331055
overhead4:: 0.13127684593200684
overhead5:: 0
time_provenance:: 0.9109406471252441
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0020, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0020, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631267
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.02355217933654785
overhead3:: 0.036592721939086914
overhead4:: 0.14164471626281738
overhead5:: 0
time_provenance:: 0.9703695774078369
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0020, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0020, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631267
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.024408340454101562
overhead3:: 0.04661130905151367
overhead4:: 0.15816378593444824
overhead5:: 0
time_provenance:: 0.9191591739654541
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0020, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0020, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631267
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.030034303665161133
overhead3:: 0.04547405242919922
overhead4:: 0.22291922569274902
overhead5:: 0
time_provenance:: 1.0159873962402344
curr_diff: 0 tensor(8.7255e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.7255e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0019, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0019, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631285
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.03851461410522461
overhead3:: 0.053544044494628906
overhead4:: 0.23076152801513672
overhead5:: 0
time_provenance:: 1.0691287517547607
curr_diff: 0 tensor(9.0134e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.0134e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0019, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0019, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631285
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.03585672378540039
overhead3:: 0.06330013275146484
overhead4:: 0.24881672859191895
overhead5:: 0
time_provenance:: 1.0773675441741943
curr_diff: 0 tensor(9.4785e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.4785e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0019, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0019, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631285
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.038973093032836914
overhead3:: 0.0656580924987793
overhead4:: 0.24442815780639648
overhead5:: 0
time_provenance:: 1.0706536769866943
curr_diff: 0 tensor(9.3265e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.3265e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0019, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0019, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631285
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.058339595794677734
overhead3:: 0.09489011764526367
overhead4:: 0.40260887145996094
overhead5:: 0
time_provenance:: 1.2945160865783691
curr_diff: 0 tensor(3.0045e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.0045e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631285
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.06096053123474121
overhead3:: 0.0913240909576416
overhead4:: 0.4283761978149414
overhead5:: 0
time_provenance:: 1.325563669204712
curr_diff: 0 tensor(3.0526e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.0526e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631285
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.06461715698242188
overhead3:: 0.09856748580932617
overhead4:: 0.43540072441101074
overhead5:: 0
time_provenance:: 1.335740327835083
curr_diff: 0 tensor(3.2532e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.2532e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631285
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.06531667709350586
overhead3:: 0.09416031837463379
overhead4:: 0.4227755069732666
overhead5:: 0
time_provenance:: 1.3212618827819824
curr_diff: 0 tensor(3.1897e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.1897e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631285
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.16074013710021973
overhead3:: 0.20050048828125
overhead4:: 0.8874683380126953
overhead5:: 0
time_provenance:: 1.9539217948913574
curr_diff: 0 tensor(8.2574e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.2574e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631302
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.14935016632080078
overhead3:: 0.20877718925476074
overhead4:: 0.9451496601104736
overhead5:: 0
time_provenance:: 2.007718801498413
curr_diff: 0 tensor(8.3285e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.3285e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631302
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.17149710655212402
overhead3:: 0.2571549415588379
overhead4:: 0.968315839767456
overhead5:: 0
time_provenance:: 2.2247955799102783
curr_diff: 0 tensor(8.6242e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.6242e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631302
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.1441495418548584
overhead3:: 0.20809221267700195
overhead4:: 0.9858536720275879
overhead5:: 0
time_provenance:: 2.0289831161499023
curr_diff: 0 tensor(8.5202e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.5202e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631302
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.29979944229125977
overhead3:: 0.4184722900390625
overhead4:: 1.5714318752288818
overhead5:: 0
time_provenance:: 2.559018850326538
curr_diff: 0 tensor(8.4773e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.4773e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631302
repetition 2
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.043445
Train - Epoch 0, Batch: 10, Loss: 1.403161
Train - Epoch 0, Batch: 20, Loss: 1.245957
Train - Epoch 0, Batch: 30, Loss: 1.195091
Train - Epoch 1, Batch: 0, Loss: 1.186162
Train - Epoch 1, Batch: 10, Loss: 1.160474
Train - Epoch 1, Batch: 20, Loss: 1.146511
Train - Epoch 1, Batch: 30, Loss: 1.130397
Train - Epoch 2, Batch: 0, Loss: 1.128661
Train - Epoch 2, Batch: 10, Loss: 1.112117
Train - Epoch 2, Batch: 20, Loss: 1.109114
Train - Epoch 2, Batch: 30, Loss: 1.106310
Train - Epoch 3, Batch: 0, Loss: 1.097694
Train - Epoch 3, Batch: 10, Loss: 1.089539
Train - Epoch 3, Batch: 20, Loss: 1.086733
Train - Epoch 3, Batch: 30, Loss: 1.070316
Train - Epoch 4, Batch: 0, Loss: 1.063547
Train - Epoch 4, Batch: 10, Loss: 1.068179
Train - Epoch 4, Batch: 20, Loss: 1.062471
Train - Epoch 4, Batch: 30, Loss: 1.053626
Train - Epoch 5, Batch: 0, Loss: 1.048715
Train - Epoch 5, Batch: 10, Loss: 1.051469
Train - Epoch 5, Batch: 20, Loss: 1.042286
Train - Epoch 5, Batch: 30, Loss: 1.038896
Train - Epoch 6, Batch: 0, Loss: 1.028650
Train - Epoch 6, Batch: 10, Loss: 1.031885
Train - Epoch 6, Batch: 20, Loss: 1.025681
Train - Epoch 6, Batch: 30, Loss: 1.026882
Train - Epoch 7, Batch: 0, Loss: 1.020026
Train - Epoch 7, Batch: 10, Loss: 1.024840
Train - Epoch 7, Batch: 20, Loss: 1.012278
Train - Epoch 7, Batch: 30, Loss: 1.008414
Train - Epoch 8, Batch: 0, Loss: 1.022046
Train - Epoch 8, Batch: 10, Loss: 1.017145
Train - Epoch 8, Batch: 20, Loss: 0.999852
Train - Epoch 8, Batch: 30, Loss: 0.993148
Train - Epoch 9, Batch: 0, Loss: 0.996827
Train - Epoch 9, Batch: 10, Loss: 0.998967
Train - Epoch 9, Batch: 20, Loss: 0.983687
Train - Epoch 9, Batch: 30, Loss: 0.980861
Train - Epoch 10, Batch: 0, Loss: 0.979502
Train - Epoch 10, Batch: 10, Loss: 0.981549
Train - Epoch 10, Batch: 20, Loss: 0.984814
Train - Epoch 10, Batch: 30, Loss: 0.984527
Train - Epoch 11, Batch: 0, Loss: 0.979223
Train - Epoch 11, Batch: 10, Loss: 0.970157
Train - Epoch 11, Batch: 20, Loss: 0.971492
Train - Epoch 11, Batch: 30, Loss: 0.969105
Train - Epoch 12, Batch: 0, Loss: 0.976526
Train - Epoch 12, Batch: 10, Loss: 0.974112
Train - Epoch 12, Batch: 20, Loss: 0.954207
Train - Epoch 12, Batch: 30, Loss: 0.959714
Train - Epoch 13, Batch: 0, Loss: 0.950440
Train - Epoch 13, Batch: 10, Loss: 0.957037
Train - Epoch 13, Batch: 20, Loss: 0.958219
Train - Epoch 13, Batch: 30, Loss: 0.971270
Train - Epoch 14, Batch: 0, Loss: 0.954367
Train - Epoch 14, Batch: 10, Loss: 0.965730
Train - Epoch 14, Batch: 20, Loss: 0.960549
Train - Epoch 14, Batch: 30, Loss: 0.951656
Train - Epoch 15, Batch: 0, Loss: 0.958009
Train - Epoch 15, Batch: 10, Loss: 0.940500
Train - Epoch 15, Batch: 20, Loss: 0.952890
Train - Epoch 15, Batch: 30, Loss: 0.941729
Train - Epoch 16, Batch: 0, Loss: 0.942930
Train - Epoch 16, Batch: 10, Loss: 0.942836
Train - Epoch 16, Batch: 20, Loss: 0.937873
Train - Epoch 16, Batch: 30, Loss: 0.939637
Train - Epoch 17, Batch: 0, Loss: 0.944370
Train - Epoch 17, Batch: 10, Loss: 0.944969
Train - Epoch 17, Batch: 20, Loss: 0.944019
Train - Epoch 17, Batch: 30, Loss: 0.921189
Train - Epoch 18, Batch: 0, Loss: 0.934020
Train - Epoch 18, Batch: 10, Loss: 0.932556
Train - Epoch 18, Batch: 20, Loss: 0.926142
Train - Epoch 18, Batch: 30, Loss: 0.932867
Train - Epoch 19, Batch: 0, Loss: 0.926507
Train - Epoch 19, Batch: 10, Loss: 0.934324
Train - Epoch 19, Batch: 20, Loss: 0.937250
Train - Epoch 19, Batch: 30, Loss: 0.931484
Train - Epoch 20, Batch: 0, Loss: 0.920044
Train - Epoch 20, Batch: 10, Loss: 0.926714
Train - Epoch 20, Batch: 20, Loss: 0.931711
Train - Epoch 20, Batch: 30, Loss: 0.927796
Train - Epoch 21, Batch: 0, Loss: 0.927609
Train - Epoch 21, Batch: 10, Loss: 0.926447
Train - Epoch 21, Batch: 20, Loss: 0.936148
Train - Epoch 21, Batch: 30, Loss: 0.921330
Train - Epoch 22, Batch: 0, Loss: 0.916620
Train - Epoch 22, Batch: 10, Loss: 0.920947
Train - Epoch 22, Batch: 20, Loss: 0.919373
Train - Epoch 22, Batch: 30, Loss: 0.917198
Train - Epoch 23, Batch: 0, Loss: 0.923079
Train - Epoch 23, Batch: 10, Loss: 0.921877
Train - Epoch 23, Batch: 20, Loss: 0.920698
Train - Epoch 23, Batch: 30, Loss: 0.907604
Train - Epoch 24, Batch: 0, Loss: 0.913569
Train - Epoch 24, Batch: 10, Loss: 0.919970
Train - Epoch 24, Batch: 20, Loss: 0.918638
Train - Epoch 24, Batch: 30, Loss: 0.917643
Train - Epoch 25, Batch: 0, Loss: 0.910172
Train - Epoch 25, Batch: 10, Loss: 0.922087
Train - Epoch 25, Batch: 20, Loss: 0.913608
Train - Epoch 25, Batch: 30, Loss: 0.907312
Train - Epoch 26, Batch: 0, Loss: 0.906753
Train - Epoch 26, Batch: 10, Loss: 0.910473
Train - Epoch 26, Batch: 20, Loss: 0.901122
Train - Epoch 26, Batch: 30, Loss: 0.906179
Train - Epoch 27, Batch: 0, Loss: 0.906773
Train - Epoch 27, Batch: 10, Loss: 0.897911
Train - Epoch 27, Batch: 20, Loss: 0.895968
Train - Epoch 27, Batch: 30, Loss: 0.902132
Train - Epoch 28, Batch: 0, Loss: 0.909109
Train - Epoch 28, Batch: 10, Loss: 0.903837
Train - Epoch 28, Batch: 20, Loss: 0.910388
Train - Epoch 28, Batch: 30, Loss: 0.912642
Train - Epoch 29, Batch: 0, Loss: 0.911883
Train - Epoch 29, Batch: 10, Loss: 0.898281
Train - Epoch 29, Batch: 20, Loss: 0.891370
Train - Epoch 29, Batch: 30, Loss: 0.907587
Train - Epoch 30, Batch: 0, Loss: 0.900084
Train - Epoch 30, Batch: 10, Loss: 0.901830
Train - Epoch 30, Batch: 20, Loss: 0.898228
Train - Epoch 30, Batch: 30, Loss: 0.900465
Train - Epoch 31, Batch: 0, Loss: 0.892989
Train - Epoch 31, Batch: 10, Loss: 0.891015
Train - Epoch 31, Batch: 20, Loss: 0.899719
Train - Epoch 31, Batch: 30, Loss: 0.899990
training_time:: 3.7201430797576904
training time full:: 3.7201876640319824
provenance prepare time:: 7.152557373046875e-07
Test Avg. Loss: 0.000070, Accuracy: 0.629873
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 522
training time is 2.3801939487457275
overhead:: 0
overhead2:: 0
time_baseline:: 2.381925344467163
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629994
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.01965951919555664
overhead3:: 0.04110074043273926
overhead4:: 0.12310242652893066
overhead5:: 0
time_provenance:: 0.937387228012085
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0020, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0020, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629976
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.02464127540588379
overhead3:: 0.0370330810546875
overhead4:: 0.15224719047546387
overhead5:: 0
time_provenance:: 0.9510166645050049
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0020, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0020, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630097
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.022345304489135742
overhead3:: 0.041980743408203125
overhead4:: 0.13851165771484375
overhead5:: 0
time_provenance:: 0.900155782699585
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0020, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0020, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629976
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.02495741844177246
overhead3:: 0.04388308525085449
overhead4:: 0.1572253704071045
overhead5:: 0
time_provenance:: 0.9206192493438721
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0020, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0020, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630097
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.03437972068786621
overhead3:: 0.05236244201660156
overhead4:: 0.2293093204498291
overhead5:: 0
time_provenance:: 1.099802017211914
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0019, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0019, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629976
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.034157514572143555
overhead3:: 0.05121874809265137
overhead4:: 0.2361588478088379
overhead5:: 0
time_provenance:: 1.0604228973388672
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0019, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0019, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629976
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.03859829902648926
overhead3:: 0.0645437240600586
overhead4:: 0.25313639640808105
overhead5:: 0
time_provenance:: 1.1140477657318115
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0019, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0019, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629976
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.041654109954833984
overhead3:: 0.05973505973815918
overhead4:: 0.2411823272705078
overhead5:: 0
time_provenance:: 1.136134386062622
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0019, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0019, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629976
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.057219505310058594
overhead3:: 0.08428573608398438
overhead4:: 0.40053796768188477
overhead5:: 0
time_provenance:: 1.2717618942260742
curr_diff: 0 tensor(4.4901e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.4901e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0019, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0019, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629976
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.06062626838684082
overhead3:: 0.08850336074829102
overhead4:: 0.4171321392059326
overhead5:: 0
time_provenance:: 1.3020939826965332
curr_diff: 0 tensor(4.5281e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.5281e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0019, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0019, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629976
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.06314349174499512
overhead3:: 0.09258699417114258
overhead4:: 0.4218759536743164
overhead5:: 0
time_provenance:: 1.319211721420288
curr_diff: 0 tensor(4.5619e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.5619e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0019, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0019, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629976
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.06523537635803223
overhead3:: 0.09606337547302246
overhead4:: 0.43201327323913574
overhead5:: 0
time_provenance:: 1.3286051750183105
curr_diff: 0 tensor(4.6390e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.6390e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0019, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0019, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629976
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.1497941017150879
overhead3:: 0.20846247673034668
overhead4:: 0.9565486907958984
overhead5:: 0
time_provenance:: 2.0277063846588135
curr_diff: 0 tensor(1.8555e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.8555e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629994
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.15366172790527344
overhead3:: 0.20737385749816895
overhead4:: 0.9601535797119141
overhead5:: 0
time_provenance:: 2.0327086448669434
curr_diff: 0 tensor(1.8590e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.8590e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629994
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.15869665145874023
overhead3:: 0.22140884399414062
overhead4:: 0.9479680061340332
overhead5:: 0
time_provenance:: 2.0369019508361816
curr_diff: 0 tensor(1.8652e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.8652e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629994
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.1477186679840088
overhead3:: 0.22771000862121582
overhead4:: 0.980492353439331
overhead5:: 0
time_provenance:: 2.05229115486145
curr_diff: 0 tensor(1.8669e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.8669e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629994
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.3411140441894531
overhead3:: 0.5204112529754639
overhead4:: 1.6581003665924072
overhead5:: 0
time_provenance:: 2.78444242477417
curr_diff: 0 tensor(9.7035e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.7035e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629994
repetition 3
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 3 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.042878
Train - Epoch 0, Batch: 10, Loss: 1.390148
Train - Epoch 0, Batch: 20, Loss: 1.242114
Train - Epoch 0, Batch: 30, Loss: 1.185690
Train - Epoch 1, Batch: 0, Loss: 1.180168
Train - Epoch 1, Batch: 10, Loss: 1.151291
Train - Epoch 1, Batch: 20, Loss: 1.133933
Train - Epoch 1, Batch: 30, Loss: 1.130122
Train - Epoch 2, Batch: 0, Loss: 1.115508
Train - Epoch 2, Batch: 10, Loss: 1.115021
Train - Epoch 2, Batch: 20, Loss: 1.100276
Train - Epoch 2, Batch: 30, Loss: 1.099388
Train - Epoch 3, Batch: 0, Loss: 1.081944
Train - Epoch 3, Batch: 10, Loss: 1.080923
Train - Epoch 3, Batch: 20, Loss: 1.078310
Train - Epoch 3, Batch: 30, Loss: 1.072252
Train - Epoch 4, Batch: 0, Loss: 1.065921
Train - Epoch 4, Batch: 10, Loss: 1.050164
Train - Epoch 4, Batch: 20, Loss: 1.049689
Train - Epoch 4, Batch: 30, Loss: 1.043648
Train - Epoch 5, Batch: 0, Loss: 1.061820
Train - Epoch 5, Batch: 10, Loss: 1.046237
Train - Epoch 5, Batch: 20, Loss: 1.036140
Train - Epoch 5, Batch: 30, Loss: 1.040305
Train - Epoch 6, Batch: 0, Loss: 1.029178
Train - Epoch 6, Batch: 10, Loss: 1.034311
Train - Epoch 6, Batch: 20, Loss: 1.024572
Train - Epoch 6, Batch: 30, Loss: 1.016300
Train - Epoch 7, Batch: 0, Loss: 1.021574
Train - Epoch 7, Batch: 10, Loss: 1.012078
Train - Epoch 7, Batch: 20, Loss: 1.010642
Train - Epoch 7, Batch: 30, Loss: 1.004614
Train - Epoch 8, Batch: 0, Loss: 0.994978
Train - Epoch 8, Batch: 10, Loss: 0.998820
Train - Epoch 8, Batch: 20, Loss: 0.996774
Train - Epoch 8, Batch: 30, Loss: 1.000125
Train - Epoch 9, Batch: 0, Loss: 0.984990
Train - Epoch 9, Batch: 10, Loss: 1.004814
Train - Epoch 9, Batch: 20, Loss: 0.987833
Train - Epoch 9, Batch: 30, Loss: 0.982055
Train - Epoch 10, Batch: 0, Loss: 0.987251
Train - Epoch 10, Batch: 10, Loss: 0.986945
Train - Epoch 10, Batch: 20, Loss: 0.981865
Train - Epoch 10, Batch: 30, Loss: 0.977828
Train - Epoch 11, Batch: 0, Loss: 0.970619
Train - Epoch 11, Batch: 10, Loss: 0.977795
Train - Epoch 11, Batch: 20, Loss: 0.968165
Train - Epoch 11, Batch: 30, Loss: 0.951605
Train - Epoch 12, Batch: 0, Loss: 0.975235
Train - Epoch 12, Batch: 10, Loss: 0.972955
Train - Epoch 12, Batch: 20, Loss: 0.964924
Train - Epoch 12, Batch: 30, Loss: 0.962820
Train - Epoch 13, Batch: 0, Loss: 0.957522
Train - Epoch 13, Batch: 10, Loss: 0.963825
Train - Epoch 13, Batch: 20, Loss: 0.964753
Train - Epoch 13, Batch: 30, Loss: 0.962237
Train - Epoch 14, Batch: 0, Loss: 0.959495
Train - Epoch 14, Batch: 10, Loss: 0.960023
Train - Epoch 14, Batch: 20, Loss: 0.954204
Train - Epoch 14, Batch: 30, Loss: 0.952966
Train - Epoch 15, Batch: 0, Loss: 0.947837
Train - Epoch 15, Batch: 10, Loss: 0.952604
Train - Epoch 15, Batch: 20, Loss: 0.943375
Train - Epoch 15, Batch: 30, Loss: 0.952906
Train - Epoch 16, Batch: 0, Loss: 0.933782
Train - Epoch 16, Batch: 10, Loss: 0.942551
Train - Epoch 16, Batch: 20, Loss: 0.945472
Train - Epoch 16, Batch: 30, Loss: 0.935294
Train - Epoch 17, Batch: 0, Loss: 0.934068
Train - Epoch 17, Batch: 10, Loss: 0.945769
Train - Epoch 17, Batch: 20, Loss: 0.947460
Train - Epoch 17, Batch: 30, Loss: 0.940063
Train - Epoch 18, Batch: 0, Loss: 0.942954
Train - Epoch 18, Batch: 10, Loss: 0.946009
Train - Epoch 18, Batch: 20, Loss: 0.925879
Train - Epoch 18, Batch: 30, Loss: 0.935202
Train - Epoch 19, Batch: 0, Loss: 0.941559
Train - Epoch 19, Batch: 10, Loss: 0.928608
Train - Epoch 19, Batch: 20, Loss: 0.939707
Train - Epoch 19, Batch: 30, Loss: 0.922496
Train - Epoch 20, Batch: 0, Loss: 0.913662
Train - Epoch 20, Batch: 10, Loss: 0.928130
Train - Epoch 20, Batch: 20, Loss: 0.922824
Train - Epoch 20, Batch: 30, Loss: 0.926959
Train - Epoch 21, Batch: 0, Loss: 0.929949
Train - Epoch 21, Batch: 10, Loss: 0.927234
Train - Epoch 21, Batch: 20, Loss: 0.918842
Train - Epoch 21, Batch: 30, Loss: 0.917478
Train - Epoch 22, Batch: 0, Loss: 0.927579
Train - Epoch 22, Batch: 10, Loss: 0.911173
Train - Epoch 22, Batch: 20, Loss: 0.928335
Train - Epoch 22, Batch: 30, Loss: 0.918834
Train - Epoch 23, Batch: 0, Loss: 0.916004
Train - Epoch 23, Batch: 10, Loss: 0.909877
Train - Epoch 23, Batch: 20, Loss: 0.916513
Train - Epoch 23, Batch: 30, Loss: 0.910470
Train - Epoch 24, Batch: 0, Loss: 0.917261
Train - Epoch 24, Batch: 10, Loss: 0.912566
Train - Epoch 24, Batch: 20, Loss: 0.908529
Train - Epoch 24, Batch: 30, Loss: 0.906719
Train - Epoch 25, Batch: 0, Loss: 0.909165
Train - Epoch 25, Batch: 10, Loss: 0.900556
Train - Epoch 25, Batch: 20, Loss: 0.915190
Train - Epoch 25, Batch: 30, Loss: 0.909944
Train - Epoch 26, Batch: 0, Loss: 0.906120
Train - Epoch 26, Batch: 10, Loss: 0.912161
Train - Epoch 26, Batch: 20, Loss: 0.902824
Train - Epoch 26, Batch: 30, Loss: 0.901908
Train - Epoch 27, Batch: 0, Loss: 0.901723
Train - Epoch 27, Batch: 10, Loss: 0.905356
Train - Epoch 27, Batch: 20, Loss: 0.906624
Train - Epoch 27, Batch: 30, Loss: 0.907936
Train - Epoch 28, Batch: 0, Loss: 0.908705
Train - Epoch 28, Batch: 10, Loss: 0.896301
Train - Epoch 28, Batch: 20, Loss: 0.903272
Train - Epoch 28, Batch: 30, Loss: 0.910078
Train - Epoch 29, Batch: 0, Loss: 0.899011
Train - Epoch 29, Batch: 10, Loss: 0.901204
Train - Epoch 29, Batch: 20, Loss: 0.897130
Train - Epoch 29, Batch: 30, Loss: 0.888639
Train - Epoch 30, Batch: 0, Loss: 0.898441
Train - Epoch 30, Batch: 10, Loss: 0.902195
Train - Epoch 30, Batch: 20, Loss: 0.889748
Train - Epoch 30, Batch: 30, Loss: 0.897237
Train - Epoch 31, Batch: 0, Loss: 0.889075
Train - Epoch 31, Batch: 10, Loss: 0.895313
Train - Epoch 31, Batch: 20, Loss: 0.896149
Train - Epoch 31, Batch: 30, Loss: 0.887059
training_time:: 3.4638819694519043
training time full:: 3.463925361633301
provenance prepare time:: 1.1920928955078125e-06
Test Avg. Loss: 0.000070, Accuracy: 0.630596
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 522
training time is 2.2187201976776123
overhead:: 0
overhead2:: 0
time_baseline:: 2.2205300331115723
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630630
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.01732015609741211
overhead3:: 0.039301395416259766
overhead4:: 0.11466765403747559
overhead5:: 0
time_provenance:: 0.8713862895965576
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0020, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0020, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630682
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.020516633987426758
overhead3:: 0.05027508735656738
overhead4:: 0.12944698333740234
overhead5:: 0
time_provenance:: 0.9212963581085205
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0020, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0020, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630648
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.022637128829956055
overhead3:: 0.04219841957092285
overhead4:: 0.15425348281860352
overhead5:: 0
time_provenance:: 0.9307994842529297
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0020, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0020, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630682
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.025920867919921875
overhead3:: 0.0564730167388916
overhead4:: 0.14788126945495605
overhead5:: 0
time_provenance:: 0.9720456600189209
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0020, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0020, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630648
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.03256487846374512
overhead3:: 0.04942750930786133
overhead4:: 0.20952987670898438
overhead5:: 0
time_provenance:: 1.0445823669433594
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0019, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0019, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630665
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.03345012664794922
overhead3:: 0.05516624450683594
overhead4:: 0.23308753967285156
overhead5:: 0
time_provenance:: 1.0676820278167725
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0019, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0019, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630682
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.035674095153808594
overhead3:: 0.07040190696716309
overhead4:: 0.23250937461853027
overhead5:: 0
time_provenance:: 1.0516128540039062
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0019, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0019, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630682
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.046744346618652344
overhead3:: 0.05823349952697754
overhead4:: 0.27654290199279785
overhead5:: 0
time_provenance:: 1.1405088901519775
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0019, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0019, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630682
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.06173515319824219
overhead3:: 0.09174466133117676
overhead4:: 0.4082968235015869
overhead5:: 0
time_provenance:: 1.316612958908081
curr_diff: 0 tensor(3.6656e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.6656e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0019, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0019, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630630
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.06083989143371582
overhead3:: 0.08890771865844727
overhead4:: 0.42803478240966797
overhead5:: 0
time_provenance:: 1.3140716552734375
curr_diff: 0 tensor(3.7477e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.7477e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0019, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0019, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630630
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.06462383270263672
overhead3:: 0.09400272369384766
overhead4:: 0.41283488273620605
overhead5:: 0
time_provenance:: 1.3190171718597412
curr_diff: 0 tensor(3.7623e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.7623e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0019, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0019, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630630
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.06688952445983887
overhead3:: 0.09699606895446777
overhead4:: 0.4333772659301758
overhead5:: 0
time_provenance:: 1.3595221042633057
curr_diff: 0 tensor(3.7674e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.7674e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0019, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0019, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630630
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.15231657028198242
overhead3:: 0.20836853981018066
overhead4:: 0.9576199054718018
overhead5:: 0
time_provenance:: 2.0372323989868164
curr_diff: 0 tensor(8.7425e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.7425e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630630
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.14829206466674805
overhead3:: 0.2055492401123047
overhead4:: 0.970451831817627
overhead5:: 0
time_provenance:: 2.0293753147125244
curr_diff: 0 tensor(8.8992e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.8992e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630630
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.1454451084136963
overhead3:: 0.2085731029510498
overhead4:: 0.9578356742858887
overhead5:: 0
time_provenance:: 2.0179545879364014
curr_diff: 0 tensor(8.9339e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.9339e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630630
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.15914177894592285
overhead3:: 0.23433923721313477
overhead4:: 0.972059965133667
overhead5:: 0
time_provenance:: 2.081768274307251
curr_diff: 0 tensor(8.9175e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.9175e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630630
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.3122391700744629
overhead3:: 0.4350919723510742
overhead4:: 1.5735065937042236
overhead5:: 0
time_provenance:: 2.5853517055511475
curr_diff: 0 tensor(9.9613e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.9613e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630630
repetition 4
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 4 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.048245
Train - Epoch 0, Batch: 10, Loss: 1.412504
Train - Epoch 0, Batch: 20, Loss: 1.251365
Train - Epoch 0, Batch: 30, Loss: 1.180498
Train - Epoch 1, Batch: 0, Loss: 1.169798
Train - Epoch 1, Batch: 10, Loss: 1.162617
Train - Epoch 1, Batch: 20, Loss: 1.134396
Train - Epoch 1, Batch: 30, Loss: 1.149831
Train - Epoch 2, Batch: 0, Loss: 1.127898
Train - Epoch 2, Batch: 10, Loss: 1.110864
Train - Epoch 2, Batch: 20, Loss: 1.112691
Train - Epoch 2, Batch: 30, Loss: 1.090339
Train - Epoch 3, Batch: 0, Loss: 1.096760
Train - Epoch 3, Batch: 10, Loss: 1.095380
Train - Epoch 3, Batch: 20, Loss: 1.077237
Train - Epoch 3, Batch: 30, Loss: 1.071389
Train - Epoch 4, Batch: 0, Loss: 1.073965
Train - Epoch 4, Batch: 10, Loss: 1.070531
Train - Epoch 4, Batch: 20, Loss: 1.061270
Train - Epoch 4, Batch: 30, Loss: 1.067944
Train - Epoch 5, Batch: 0, Loss: 1.054192
Train - Epoch 5, Batch: 10, Loss: 1.053968
Train - Epoch 5, Batch: 20, Loss: 1.036504
Train - Epoch 5, Batch: 30, Loss: 1.039489
Train - Epoch 6, Batch: 0, Loss: 1.038183
Train - Epoch 6, Batch: 10, Loss: 1.024617
Train - Epoch 6, Batch: 20, Loss: 1.033005
Train - Epoch 6, Batch: 30, Loss: 1.028602
Train - Epoch 7, Batch: 0, Loss: 1.019324
Train - Epoch 7, Batch: 10, Loss: 1.022396
Train - Epoch 7, Batch: 20, Loss: 1.013942
Train - Epoch 7, Batch: 30, Loss: 1.005979
Train - Epoch 8, Batch: 0, Loss: 1.006739
Train - Epoch 8, Batch: 10, Loss: 1.003532
Train - Epoch 8, Batch: 20, Loss: 1.004467
Train - Epoch 8, Batch: 30, Loss: 0.996098
Train - Epoch 9, Batch: 0, Loss: 1.009808
Train - Epoch 9, Batch: 10, Loss: 0.994018
Train - Epoch 9, Batch: 20, Loss: 0.993028
Train - Epoch 9, Batch: 30, Loss: 0.990134
Train - Epoch 10, Batch: 0, Loss: 0.994467
Train - Epoch 10, Batch: 10, Loss: 0.996819
Train - Epoch 10, Batch: 20, Loss: 0.987079
Train - Epoch 10, Batch: 30, Loss: 0.984320
Train - Epoch 11, Batch: 0, Loss: 0.984529
Train - Epoch 11, Batch: 10, Loss: 0.966618
Train - Epoch 11, Batch: 20, Loss: 0.974310
Train - Epoch 11, Batch: 30, Loss: 0.977350
Train - Epoch 12, Batch: 0, Loss: 0.966847
Train - Epoch 12, Batch: 10, Loss: 0.967849
Train - Epoch 12, Batch: 20, Loss: 0.966767
Train - Epoch 12, Batch: 30, Loss: 0.973882
Train - Epoch 13, Batch: 0, Loss: 0.965645
Train - Epoch 13, Batch: 10, Loss: 0.963888
Train - Epoch 13, Batch: 20, Loss: 0.964941
Train - Epoch 13, Batch: 30, Loss: 0.962455
Train - Epoch 14, Batch: 0, Loss: 0.959791
Train - Epoch 14, Batch: 10, Loss: 0.951177
Train - Epoch 14, Batch: 20, Loss: 0.950833
Train - Epoch 14, Batch: 30, Loss: 0.952234
Train - Epoch 15, Batch: 0, Loss: 0.952412
Train - Epoch 15, Batch: 10, Loss: 0.941260
Train - Epoch 15, Batch: 20, Loss: 0.951059
Train - Epoch 15, Batch: 30, Loss: 0.948270
Train - Epoch 16, Batch: 0, Loss: 0.953320
Train - Epoch 16, Batch: 10, Loss: 0.949234
Train - Epoch 16, Batch: 20, Loss: 0.947591
Train - Epoch 16, Batch: 30, Loss: 0.942421
Train - Epoch 17, Batch: 0, Loss: 0.939565
Train - Epoch 17, Batch: 10, Loss: 0.941399
Train - Epoch 17, Batch: 20, Loss: 0.943239
Train - Epoch 17, Batch: 30, Loss: 0.937424
Train - Epoch 18, Batch: 0, Loss: 0.947380
Train - Epoch 18, Batch: 10, Loss: 0.939774
Train - Epoch 18, Batch: 20, Loss: 0.926004
Train - Epoch 18, Batch: 30, Loss: 0.936539
Train - Epoch 19, Batch: 0, Loss: 0.941770
Train - Epoch 19, Batch: 10, Loss: 0.937190
Train - Epoch 19, Batch: 20, Loss: 0.941422
Train - Epoch 19, Batch: 30, Loss: 0.934754
Train - Epoch 20, Batch: 0, Loss: 0.927525
Train - Epoch 20, Batch: 10, Loss: 0.919198
Train - Epoch 20, Batch: 20, Loss: 0.929384
Train - Epoch 20, Batch: 30, Loss: 0.927743
Train - Epoch 21, Batch: 0, Loss: 0.926577
Train - Epoch 21, Batch: 10, Loss: 0.922348
Train - Epoch 21, Batch: 20, Loss: 0.920548
Train - Epoch 21, Batch: 30, Loss: 0.911001
Train - Epoch 22, Batch: 0, Loss: 0.934966
Train - Epoch 22, Batch: 10, Loss: 0.922460
Train - Epoch 22, Batch: 20, Loss: 0.916214
Train - Epoch 22, Batch: 30, Loss: 0.909731
Train - Epoch 23, Batch: 0, Loss: 0.923950
Train - Epoch 23, Batch: 10, Loss: 0.924431
Train - Epoch 23, Batch: 20, Loss: 0.922693
Train - Epoch 23, Batch: 30, Loss: 0.902268
Train - Epoch 24, Batch: 0, Loss: 0.917889
Train - Epoch 24, Batch: 10, Loss: 0.910578
Train - Epoch 24, Batch: 20, Loss: 0.900462
Train - Epoch 24, Batch: 30, Loss: 0.914436
Train - Epoch 25, Batch: 0, Loss: 0.909378
Train - Epoch 25, Batch: 10, Loss: 0.910286
Train - Epoch 25, Batch: 20, Loss: 0.907502
Train - Epoch 25, Batch: 30, Loss: 0.918468
Train - Epoch 26, Batch: 0, Loss: 0.912905
Train - Epoch 26, Batch: 10, Loss: 0.913235
Train - Epoch 26, Batch: 20, Loss: 0.897314
Train - Epoch 26, Batch: 30, Loss: 0.908201
Train - Epoch 27, Batch: 0, Loss: 0.912664
Train - Epoch 27, Batch: 10, Loss: 0.909731
Train - Epoch 27, Batch: 20, Loss: 0.900428
Train - Epoch 27, Batch: 30, Loss: 0.908954
Train - Epoch 28, Batch: 0, Loss: 0.904846
Train - Epoch 28, Batch: 10, Loss: 0.906662
Train - Epoch 28, Batch: 20, Loss: 0.905423
Train - Epoch 28, Batch: 30, Loss: 0.900431
Train - Epoch 29, Batch: 0, Loss: 0.914465
Train - Epoch 29, Batch: 10, Loss: 0.901984
Train - Epoch 29, Batch: 20, Loss: 0.908716
Train - Epoch 29, Batch: 30, Loss: 0.890888
Train - Epoch 30, Batch: 0, Loss: 0.895230
Train - Epoch 30, Batch: 10, Loss: 0.897476
Train - Epoch 30, Batch: 20, Loss: 0.894129
Train - Epoch 30, Batch: 30, Loss: 0.897778
Train - Epoch 31, Batch: 0, Loss: 0.898535
Train - Epoch 31, Batch: 10, Loss: 0.893937
Train - Epoch 31, Batch: 20, Loss: 0.884590
Train - Epoch 31, Batch: 30, Loss: 0.891273
training_time:: 3.755455493927002
training time full:: 3.755499839782715
provenance prepare time:: 7.152557373046875e-07
Test Avg. Loss: 0.000070, Accuracy: 0.629598
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 522
training time is 2.4564616680145264
overhead:: 0
overhead2:: 0
time_baseline:: 2.45865797996521
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629615
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.017582416534423828
overhead3:: 0.03828859329223633
overhead4:: 0.11207294464111328
overhead5:: 0
time_provenance:: 0.8888943195343018
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0020, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0020, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629615
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.021435022354125977
overhead3:: 0.047551631927490234
overhead4:: 0.14953327178955078
overhead5:: 0
time_provenance:: 0.9648458957672119
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0020, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0020, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629632
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.02279210090637207
overhead3:: 0.049776554107666016
overhead4:: 0.1427621841430664
overhead5:: 0
time_provenance:: 0.9345595836639404
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0020, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0020, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629615
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.025704383850097656
overhead3:: 0.04038810729980469
overhead4:: 0.15012073516845703
overhead5:: 0
time_provenance:: 0.9515724182128906
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0020, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0020, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629632
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.034102439880371094
overhead3:: 0.05069875717163086
overhead4:: 0.2564060688018799
overhead5:: 0
time_provenance:: 1.100172519683838
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0019, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0019, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629615
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.037056684494018555
overhead3:: 0.052618980407714844
overhead4:: 0.1999530792236328
overhead5:: 0
time_provenance:: 1.0549542903900146
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0019, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0019, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629615
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.037713050842285156
overhead3:: 0.0520482063293457
overhead4:: 0.21200132369995117
overhead5:: 0
time_provenance:: 1.0351715087890625
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0019, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0019, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629615
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.04053092002868652
overhead3:: 0.0701742172241211
overhead4:: 0.2618236541748047
overhead5:: 0
time_provenance:: 1.1221792697906494
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0019, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0019, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629615
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.06073570251464844
overhead3:: 0.08236289024353027
overhead4:: 0.41623544692993164
overhead5:: 0
time_provenance:: 1.3101789951324463
curr_diff: 0 tensor(3.7338e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.7338e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0019, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0019, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629615
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.06206202507019043
overhead3:: 0.09054017066955566
overhead4:: 0.4387516975402832
overhead5:: 0
time_provenance:: 1.3492636680603027
curr_diff: 0 tensor(3.7455e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.7455e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0019, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0019, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629615
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.09132742881774902
overhead3:: 0.12082076072692871
overhead4:: 0.485060453414917
overhead5:: 0
time_provenance:: 1.7420754432678223
curr_diff: 0 tensor(3.8171e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.8171e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0019, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0019, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629615
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.06718754768371582
overhead3:: 0.09714221954345703
overhead4:: 0.4664318561553955
overhead5:: 0
time_provenance:: 1.3613464832305908
curr_diff: 0 tensor(3.8245e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.8245e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0019, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0019, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629615
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.14538812637329102
overhead3:: 0.20675086975097656
overhead4:: 0.9198868274688721
overhead5:: 0
time_provenance:: 1.9818038940429688
curr_diff: 0 tensor(1.0300e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0300e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629615
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.14237332344055176
overhead3:: 0.20500421524047852
overhead4:: 0.9964919090270996
overhead5:: 0
time_provenance:: 2.0500478744506836
curr_diff: 0 tensor(1.0357e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0357e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629615
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.14704012870788574
overhead3:: 0.2110919952392578
overhead4:: 0.9680330753326416
overhead5:: 0
time_provenance:: 2.043919324874878
curr_diff: 0 tensor(1.0544e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0544e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629615
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.14620327949523926
overhead3:: 0.21165776252746582
overhead4:: 0.9711320400238037
overhead5:: 0
time_provenance:: 2.0341620445251465
curr_diff: 0 tensor(1.0564e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0564e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629615
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.3485262393951416
overhead3:: 0.5341782569885254
overhead4:: 1.710571050643921
overhead5:: 0
time_provenance:: 2.864856004714966
curr_diff: 0 tensor(1.0550e-15, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0550e-15, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629615
deletion rate:: 0.002
python3 generate_rand_ids 0.002  covtype 0
tensor([417795, 514063, 307220,  ..., 335868, 272381, 364542])
batch size:: 16384
repetition 0
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 0 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 1.911379
Train - Epoch 0, Batch: 10, Loss: 1.369991
Train - Epoch 0, Batch: 20, Loss: 1.233584
Train - Epoch 0, Batch: 30, Loss: 1.180791
Train - Epoch 1, Batch: 0, Loss: 1.182185
Train - Epoch 1, Batch: 10, Loss: 1.160062
Train - Epoch 1, Batch: 20, Loss: 1.142273
Train - Epoch 1, Batch: 30, Loss: 1.128338
Train - Epoch 2, Batch: 0, Loss: 1.117193
Train - Epoch 2, Batch: 10, Loss: 1.123839
Train - Epoch 2, Batch: 20, Loss: 1.099292
Train - Epoch 2, Batch: 30, Loss: 1.114207
Train - Epoch 3, Batch: 0, Loss: 1.100025
Train - Epoch 3, Batch: 10, Loss: 1.088462
Train - Epoch 3, Batch: 20, Loss: 1.087339
Train - Epoch 3, Batch: 30, Loss: 1.070875
Train - Epoch 4, Batch: 0, Loss: 1.073644
Train - Epoch 4, Batch: 10, Loss: 1.065367
Train - Epoch 4, Batch: 20, Loss: 1.050211
Train - Epoch 4, Batch: 30, Loss: 1.057190
Train - Epoch 5, Batch: 0, Loss: 1.052878
Train - Epoch 5, Batch: 10, Loss: 1.043360
Train - Epoch 5, Batch: 20, Loss: 1.044394
Train - Epoch 5, Batch: 30, Loss: 1.032118
Train - Epoch 6, Batch: 0, Loss: 1.025519
Train - Epoch 6, Batch: 10, Loss: 1.041020
Train - Epoch 6, Batch: 20, Loss: 1.020634
Train - Epoch 6, Batch: 30, Loss: 1.033405
Train - Epoch 7, Batch: 0, Loss: 1.029217
Train - Epoch 7, Batch: 10, Loss: 1.011603
Train - Epoch 7, Batch: 20, Loss: 1.012735
Train - Epoch 7, Batch: 30, Loss: 1.009266
Train - Epoch 8, Batch: 0, Loss: 1.016699
Train - Epoch 8, Batch: 10, Loss: 1.001127
Train - Epoch 8, Batch: 20, Loss: 1.001448
Train - Epoch 8, Batch: 30, Loss: 1.000332
Train - Epoch 9, Batch: 0, Loss: 1.001261
Train - Epoch 9, Batch: 10, Loss: 0.987311
Train - Epoch 9, Batch: 20, Loss: 0.986608
Train - Epoch 9, Batch: 30, Loss: 0.988947
Train - Epoch 10, Batch: 0, Loss: 0.989295
Train - Epoch 10, Batch: 10, Loss: 0.979429
Train - Epoch 10, Batch: 20, Loss: 0.979474
Train - Epoch 10, Batch: 30, Loss: 0.985806
Train - Epoch 11, Batch: 0, Loss: 0.977423
Train - Epoch 11, Batch: 10, Loss: 0.976861
Train - Epoch 11, Batch: 20, Loss: 0.959578
Train - Epoch 11, Batch: 30, Loss: 0.962268
Train - Epoch 12, Batch: 0, Loss: 0.970003
Train - Epoch 12, Batch: 10, Loss: 0.960428
Train - Epoch 12, Batch: 20, Loss: 0.950630
Train - Epoch 12, Batch: 30, Loss: 0.961433
Train - Epoch 13, Batch: 0, Loss: 0.963710
Train - Epoch 13, Batch: 10, Loss: 0.963412
Train - Epoch 13, Batch: 20, Loss: 0.951828
Train - Epoch 13, Batch: 30, Loss: 0.954789
Train - Epoch 14, Batch: 0, Loss: 0.954366
Train - Epoch 14, Batch: 10, Loss: 0.955963
Train - Epoch 14, Batch: 20, Loss: 0.954963
Train - Epoch 14, Batch: 30, Loss: 0.943897
Train - Epoch 15, Batch: 0, Loss: 0.951950
Train - Epoch 15, Batch: 10, Loss: 0.948606
Train - Epoch 15, Batch: 20, Loss: 0.941253
Train - Epoch 15, Batch: 30, Loss: 0.947702
Train - Epoch 16, Batch: 0, Loss: 0.948203
Train - Epoch 16, Batch: 10, Loss: 0.936791
Train - Epoch 16, Batch: 20, Loss: 0.944541
Train - Epoch 16, Batch: 30, Loss: 0.934366
Train - Epoch 17, Batch: 0, Loss: 0.951675
Train - Epoch 17, Batch: 10, Loss: 0.938095
Train - Epoch 17, Batch: 20, Loss: 0.941243
Train - Epoch 17, Batch: 30, Loss: 0.926637
Train - Epoch 18, Batch: 0, Loss: 0.936847
Train - Epoch 18, Batch: 10, Loss: 0.928114
Train - Epoch 18, Batch: 20, Loss: 0.925821
Train - Epoch 18, Batch: 30, Loss: 0.928483
Train - Epoch 19, Batch: 0, Loss: 0.940635
Train - Epoch 19, Batch: 10, Loss: 0.937535
Train - Epoch 19, Batch: 20, Loss: 0.927291
Train - Epoch 19, Batch: 30, Loss: 0.923951
Train - Epoch 20, Batch: 0, Loss: 0.942005
Train - Epoch 20, Batch: 10, Loss: 0.930621
Train - Epoch 20, Batch: 20, Loss: 0.927715
Train - Epoch 20, Batch: 30, Loss: 0.929653
Train - Epoch 21, Batch: 0, Loss: 0.929117
Train - Epoch 21, Batch: 10, Loss: 0.912708
Train - Epoch 21, Batch: 20, Loss: 0.922501
Train - Epoch 21, Batch: 30, Loss: 0.916410
Train - Epoch 22, Batch: 0, Loss: 0.927090
Train - Epoch 22, Batch: 10, Loss: 0.912745
Train - Epoch 22, Batch: 20, Loss: 0.919715
Train - Epoch 22, Batch: 30, Loss: 0.913764
Train - Epoch 23, Batch: 0, Loss: 0.911496
Train - Epoch 23, Batch: 10, Loss: 0.914380
Train - Epoch 23, Batch: 20, Loss: 0.908723
Train - Epoch 23, Batch: 30, Loss: 0.915629
Train - Epoch 24, Batch: 0, Loss: 0.911475
Train - Epoch 24, Batch: 10, Loss: 0.910099
Train - Epoch 24, Batch: 20, Loss: 0.910528
Train - Epoch 24, Batch: 30, Loss: 0.906011
Train - Epoch 25, Batch: 0, Loss: 0.907576
Train - Epoch 25, Batch: 10, Loss: 0.922446
Train - Epoch 25, Batch: 20, Loss: 0.894122
Train - Epoch 25, Batch: 30, Loss: 0.913948
Train - Epoch 26, Batch: 0, Loss: 0.911810
Train - Epoch 26, Batch: 10, Loss: 0.904103
Train - Epoch 26, Batch: 20, Loss: 0.907901
Train - Epoch 26, Batch: 30, Loss: 0.909351
Train - Epoch 27, Batch: 0, Loss: 0.907382
Train - Epoch 27, Batch: 10, Loss: 0.906130
Train - Epoch 27, Batch: 20, Loss: 0.902893
Train - Epoch 27, Batch: 30, Loss: 0.900752
Train - Epoch 28, Batch: 0, Loss: 0.895823
Train - Epoch 28, Batch: 10, Loss: 0.902426
Train - Epoch 28, Batch: 20, Loss: 0.896822
Train - Epoch 28, Batch: 30, Loss: 0.905335
Train - Epoch 29, Batch: 0, Loss: 0.900015
Train - Epoch 29, Batch: 10, Loss: 0.899257
Train - Epoch 29, Batch: 20, Loss: 0.895880
Train - Epoch 29, Batch: 30, Loss: 0.896401
Train - Epoch 30, Batch: 0, Loss: 0.886746
Train - Epoch 30, Batch: 10, Loss: 0.895762
Train - Epoch 30, Batch: 20, Loss: 0.886803
Train - Epoch 30, Batch: 30, Loss: 0.892798
Train - Epoch 31, Batch: 0, Loss: 0.891301
Train - Epoch 31, Batch: 10, Loss: 0.886150
Train - Epoch 31, Batch: 20, Loss: 0.901586
Train - Epoch 31, Batch: 30, Loss: 0.904695
training_time:: 3.736356735229492
training time full:: 3.736398458480835
provenance prepare time:: 7.152557373046875e-07
Test Avg. Loss: 0.000070, Accuracy: 0.629942
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 1045
training time is 2.136409282684326
overhead:: 0
overhead2:: 0
time_baseline:: 2.1378657817840576
curr_diff: 0 tensor(0.0023, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0023, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630062
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.01754474639892578
overhead3:: 0.028449296951293945
overhead4:: 0.11732244491577148
overhead5:: 0
time_provenance:: 0.8788433074951172
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0026, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0026, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630097
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.022234678268432617
overhead3:: 0.03560209274291992
overhead4:: 0.13715362548828125
overhead5:: 0
time_provenance:: 0.9925861358642578
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0026, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0026, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630062
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.024032115936279297
overhead3:: 0.03510904312133789
overhead4:: 0.13645553588867188
overhead5:: 0
time_provenance:: 0.9633104801177979
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0026, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0026, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630097
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.02512049674987793
overhead3:: 0.04712033271789551
overhead4:: 0.1538400650024414
overhead5:: 0
time_provenance:: 0.9184796810150146
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0026, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0026, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630062
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.029665708541870117
overhead3:: 0.0634603500366211
overhead4:: 0.19510316848754883
overhead5:: 0
time_provenance:: 0.9769752025604248
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0024, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0024, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630062
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.03455829620361328
overhead3:: 0.05509328842163086
overhead4:: 0.21694183349609375
overhead5:: 0
time_provenance:: 1.016115665435791
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0024, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0024, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630062
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.03567814826965332
overhead3:: 0.05333089828491211
overhead4:: 0.2356429100036621
overhead5:: 0
time_provenance:: 1.082366943359375
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0024, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0024, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630062
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.039812564849853516
overhead3:: 0.05888962745666504
overhead4:: 0.2570188045501709
overhead5:: 0
time_provenance:: 1.0994012355804443
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0024, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0024, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630062
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.07862377166748047
overhead3:: 0.10972785949707031
overhead4:: 0.49512147903442383
overhead5:: 0
time_provenance:: 1.7039122581481934
curr_diff: 0 tensor(4.3498e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.3498e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0024, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0024, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630062
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.06293749809265137
overhead3:: 0.08964920043945312
overhead4:: 0.3798518180847168
overhead5:: 0
time_provenance:: 1.2527000904083252
curr_diff: 0 tensor(4.4744e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.4744e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0024, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0024, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630062
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.08143162727355957
overhead3:: 0.11473250389099121
overhead4:: 0.4998452663421631
overhead5:: 0
time_provenance:: 1.681988000869751
curr_diff: 0 tensor(4.5155e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.5155e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0024, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0024, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630062
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.06775832176208496
overhead3:: 0.09476494789123535
overhead4:: 0.43492770195007324
overhead5:: 0
time_provenance:: 1.3327696323394775
curr_diff: 0 tensor(4.5354e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.5354e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0024, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0024, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630062
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.1636180877685547
overhead3:: 0.22437286376953125
overhead4:: 1.0326111316680908
overhead5:: 0
time_provenance:: 2.232203245162964
curr_diff: 0 tensor(1.2214e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2214e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0023, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0023, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630062
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.1911482810974121
overhead3:: 0.26688146591186523
overhead4:: 1.190229892730713
overhead5:: 0
time_provenance:: 2.6445319652557373
curr_diff: 0 tensor(1.2322e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2322e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0023, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0023, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630062
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.14336252212524414
overhead3:: 0.2061474323272705
overhead4:: 0.996323823928833
overhead5:: 0
time_provenance:: 2.0579445362091064
curr_diff: 0 tensor(1.2316e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2316e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0023, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0023, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630062
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.1459956169128418
overhead3:: 0.20928430557250977
overhead4:: 0.9794278144836426
overhead5:: 0
time_provenance:: 2.029827356338501
curr_diff: 0 tensor(1.2521e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2521e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0023, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0023, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630062
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.29184889793395996
overhead3:: 0.3959958553314209
overhead4:: 1.5232279300689697
overhead5:: 0
time_provenance:: 2.486593246459961
curr_diff: 0 tensor(1.1008e-15, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1008e-15, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0023, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0023, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630062
repetition 1
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 1 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.077772
Train - Epoch 0, Batch: 10, Loss: 1.413297
Train - Epoch 0, Batch: 20, Loss: 1.251452
Train - Epoch 0, Batch: 30, Loss: 1.189992
Train - Epoch 1, Batch: 0, Loss: 1.187418
Train - Epoch 1, Batch: 10, Loss: 1.162610
Train - Epoch 1, Batch: 20, Loss: 1.149625
Train - Epoch 1, Batch: 30, Loss: 1.129589
Train - Epoch 2, Batch: 0, Loss: 1.131078
Train - Epoch 2, Batch: 10, Loss: 1.121472
Train - Epoch 2, Batch: 20, Loss: 1.104561
Train - Epoch 2, Batch: 30, Loss: 1.101145
Train - Epoch 3, Batch: 0, Loss: 1.094690
Train - Epoch 3, Batch: 10, Loss: 1.084890
Train - Epoch 3, Batch: 20, Loss: 1.081768
Train - Epoch 3, Batch: 30, Loss: 1.079640
Train - Epoch 4, Batch: 0, Loss: 1.081707
Train - Epoch 4, Batch: 10, Loss: 1.071599
Train - Epoch 4, Batch: 20, Loss: 1.047331
Train - Epoch 4, Batch: 30, Loss: 1.054786
Train - Epoch 5, Batch: 0, Loss: 1.045378
Train - Epoch 5, Batch: 10, Loss: 1.047208
Train - Epoch 5, Batch: 20, Loss: 1.046307
Train - Epoch 5, Batch: 30, Loss: 1.042161
Train - Epoch 6, Batch: 0, Loss: 1.041481
Train - Epoch 6, Batch: 10, Loss: 1.027478
Train - Epoch 6, Batch: 20, Loss: 1.027234
Train - Epoch 6, Batch: 30, Loss: 1.019314
Train - Epoch 7, Batch: 0, Loss: 1.027014
Train - Epoch 7, Batch: 10, Loss: 1.016582
Train - Epoch 7, Batch: 20, Loss: 1.013197
Train - Epoch 7, Batch: 30, Loss: 1.016553
Train - Epoch 8, Batch: 0, Loss: 1.003756
Train - Epoch 8, Batch: 10, Loss: 1.009400
Train - Epoch 8, Batch: 20, Loss: 1.008577
Train - Epoch 8, Batch: 30, Loss: 0.999798
Train - Epoch 9, Batch: 0, Loss: 0.999702
Train - Epoch 9, Batch: 10, Loss: 0.990147
Train - Epoch 9, Batch: 20, Loss: 0.994124
Train - Epoch 9, Batch: 30, Loss: 0.998200
Train - Epoch 10, Batch: 0, Loss: 0.996139
Train - Epoch 10, Batch: 10, Loss: 0.970745
Train - Epoch 10, Batch: 20, Loss: 0.980269
Train - Epoch 10, Batch: 30, Loss: 0.976941
Train - Epoch 11, Batch: 0, Loss: 0.982503
Train - Epoch 11, Batch: 10, Loss: 0.980437
Train - Epoch 11, Batch: 20, Loss: 0.972696
Train - Epoch 11, Batch: 30, Loss: 0.971717
Train - Epoch 12, Batch: 0, Loss: 0.977758
Train - Epoch 12, Batch: 10, Loss: 0.967930
Train - Epoch 12, Batch: 20, Loss: 0.958137
Train - Epoch 12, Batch: 30, Loss: 0.964311
Train - Epoch 13, Batch: 0, Loss: 0.954036
Train - Epoch 13, Batch: 10, Loss: 0.968855
Train - Epoch 13, Batch: 20, Loss: 0.961306
Train - Epoch 13, Batch: 30, Loss: 0.964130
Train - Epoch 14, Batch: 0, Loss: 0.975445
Train - Epoch 14, Batch: 10, Loss: 0.955096
Train - Epoch 14, Batch: 20, Loss: 0.947980
Train - Epoch 14, Batch: 30, Loss: 0.958979
Train - Epoch 15, Batch: 0, Loss: 0.955313
Train - Epoch 15, Batch: 10, Loss: 0.940182
Train - Epoch 15, Batch: 20, Loss: 0.950980
Train - Epoch 15, Batch: 30, Loss: 0.944071
Train - Epoch 16, Batch: 0, Loss: 0.941408
Train - Epoch 16, Batch: 10, Loss: 0.948275
Train - Epoch 16, Batch: 20, Loss: 0.950297
Train - Epoch 16, Batch: 30, Loss: 0.940754
Train - Epoch 17, Batch: 0, Loss: 0.946077
Train - Epoch 17, Batch: 10, Loss: 0.942729
Train - Epoch 17, Batch: 20, Loss: 0.942249
Train - Epoch 17, Batch: 30, Loss: 0.930776
Train - Epoch 18, Batch: 0, Loss: 0.940819
Train - Epoch 18, Batch: 10, Loss: 0.944942
Train - Epoch 18, Batch: 20, Loss: 0.925562
Train - Epoch 18, Batch: 30, Loss: 0.922664
Train - Epoch 19, Batch: 0, Loss: 0.931907
Train - Epoch 19, Batch: 10, Loss: 0.936369
Train - Epoch 19, Batch: 20, Loss: 0.924679
Train - Epoch 19, Batch: 30, Loss: 0.939410
Train - Epoch 20, Batch: 0, Loss: 0.926673
Train - Epoch 20, Batch: 10, Loss: 0.938490
Train - Epoch 20, Batch: 20, Loss: 0.923116
Train - Epoch 20, Batch: 30, Loss: 0.929866
Train - Epoch 21, Batch: 0, Loss: 0.938268
Train - Epoch 21, Batch: 10, Loss: 0.917062
Train - Epoch 21, Batch: 20, Loss: 0.925953
Train - Epoch 21, Batch: 30, Loss: 0.912092
Train - Epoch 22, Batch: 0, Loss: 0.906279
Train - Epoch 22, Batch: 10, Loss: 0.919694
Train - Epoch 22, Batch: 20, Loss: 0.920412
Train - Epoch 22, Batch: 30, Loss: 0.918131
Train - Epoch 23, Batch: 0, Loss: 0.923529
Train - Epoch 23, Batch: 10, Loss: 0.918320
Train - Epoch 23, Batch: 20, Loss: 0.908183
Train - Epoch 23, Batch: 30, Loss: 0.912152
Train - Epoch 24, Batch: 0, Loss: 0.922462
Train - Epoch 24, Batch: 10, Loss: 0.913693
Train - Epoch 24, Batch: 20, Loss: 0.914821
Train - Epoch 24, Batch: 30, Loss: 0.902577
Train - Epoch 25, Batch: 0, Loss: 0.920190
Train - Epoch 25, Batch: 10, Loss: 0.904043
Train - Epoch 25, Batch: 20, Loss: 0.913063
Train - Epoch 25, Batch: 30, Loss: 0.912625
Train - Epoch 26, Batch: 0, Loss: 0.911167
Train - Epoch 26, Batch: 10, Loss: 0.905483
Train - Epoch 26, Batch: 20, Loss: 0.898817
Train - Epoch 26, Batch: 30, Loss: 0.904543
Train - Epoch 27, Batch: 0, Loss: 0.903208
Train - Epoch 27, Batch: 10, Loss: 0.903044
Train - Epoch 27, Batch: 20, Loss: 0.912240
Train - Epoch 27, Batch: 30, Loss: 0.905667
Train - Epoch 28, Batch: 0, Loss: 0.902188
Train - Epoch 28, Batch: 10, Loss: 0.899512
Train - Epoch 28, Batch: 20, Loss: 0.902233
Train - Epoch 28, Batch: 30, Loss: 0.886095
Train - Epoch 29, Batch: 0, Loss: 0.893605
Train - Epoch 29, Batch: 10, Loss: 0.902846
Train - Epoch 29, Batch: 20, Loss: 0.898552
Train - Epoch 29, Batch: 30, Loss: 0.890410
Train - Epoch 30, Batch: 0, Loss: 0.891424
Train - Epoch 30, Batch: 10, Loss: 0.902359
Train - Epoch 30, Batch: 20, Loss: 0.898111
Train - Epoch 30, Batch: 30, Loss: 0.902496
Train - Epoch 31, Batch: 0, Loss: 0.888127
Train - Epoch 31, Batch: 10, Loss: 0.893170
Train - Epoch 31, Batch: 20, Loss: 0.890391
Train - Epoch 31, Batch: 30, Loss: 0.896050
training_time:: 3.879648447036743
training time full:: 3.8796939849853516
provenance prepare time:: 4.76837158203125e-07
Test Avg. Loss: 0.000070, Accuracy: 0.629753
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 1045
training time is 2.588961124420166
overhead:: 0
overhead2:: 0
time_baseline:: 2.5907630920410156
curr_diff: 0 tensor(0.0023, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0023, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629804
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.018512248992919922
overhead3:: 0.03785538673400879
overhead4:: 0.11208081245422363
overhead5:: 0
time_provenance:: 0.9187393188476562
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0025, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0025, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629839
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.021127700805664062
overhead3:: 0.033675432205200195
overhead4:: 0.11981868743896484
overhead5:: 0
time_provenance:: 0.9230592250823975
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0025, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0025, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629839
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.026021718978881836
overhead3:: 0.04146432876586914
overhead4:: 0.1403653621673584
overhead5:: 0
time_provenance:: 1.0295038223266602
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0025, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0025, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629839
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.02484583854675293
overhead3:: 0.04217648506164551
overhead4:: 0.15666866302490234
overhead5:: 0
time_provenance:: 0.9601445198059082
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0026, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0026, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629839
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.031157493591308594
overhead3:: 0.04809451103210449
overhead4:: 0.2373802661895752
overhead5:: 0
time_provenance:: 1.0603861808776855
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0024, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0024, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629804
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.033994197845458984
overhead3:: 0.05174875259399414
overhead4:: 0.2290487289428711
overhead5:: 0
time_provenance:: 1.0665397644042969
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0024, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0024, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629804
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.039247751235961914
overhead3:: 0.06534075736999512
overhead4:: 0.23489642143249512
overhead5:: 0
time_provenance:: 1.135284185409546
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0024, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0024, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629804
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.03702712059020996
overhead3:: 0.05561113357543945
overhead4:: 0.2417130470275879
overhead5:: 0
time_provenance:: 1.0926487445831299
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0024, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0024, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629804
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.06276774406433105
overhead3:: 0.10174369812011719
overhead4:: 0.4060370922088623
overhead5:: 0
time_provenance:: 1.3200716972351074
curr_diff: 0 tensor(5.2604e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.2604e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0024, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0024, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629804
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.0614924430847168
overhead3:: 0.08987212181091309
overhead4:: 0.4193427562713623
overhead5:: 0
time_provenance:: 1.313659906387329
curr_diff: 0 tensor(5.3988e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.3988e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0024, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0024, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629804
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.07883977890014648
overhead3:: 0.10986185073852539
overhead4:: 0.4698495864868164
overhead5:: 0
time_provenance:: 1.5767302513122559
curr_diff: 0 tensor(5.4629e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.4629e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0024, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0024, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629804
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.0699167251586914
overhead3:: 0.0959618091583252
overhead4:: 0.42310428619384766
overhead5:: 0
time_provenance:: 1.340050458908081
curr_diff: 0 tensor(5.4719e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.4719e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0024, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0024, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629804
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.1509103775024414
overhead3:: 0.20450878143310547
overhead4:: 0.9779007434844971
overhead5:: 0
time_provenance:: 2.05525803565979
curr_diff: 0 tensor(1.3960e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3960e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0023, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0023, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629804
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.18733549118041992
overhead3:: 0.26132965087890625
overhead4:: 1.14577317237854
overhead5:: 0
time_provenance:: 2.5663843154907227
curr_diff: 0 tensor(1.4121e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4121e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0023, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0023, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629804
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.15549778938293457
overhead3:: 0.20772647857666016
overhead4:: 0.9060711860656738
overhead5:: 0
time_provenance:: 1.964160442352295
curr_diff: 0 tensor(1.4166e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4166e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0023, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0023, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629804
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.14455866813659668
overhead3:: 0.20543909072875977
overhead4:: 0.981020450592041
overhead5:: 0
time_provenance:: 2.0340280532836914
curr_diff: 0 tensor(1.4155e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4155e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0023, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0023, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629804
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.30266499519348145
overhead3:: 0.41721296310424805
overhead4:: 1.5412242412567139
overhead5:: 0
time_provenance:: 2.528083324432373
curr_diff: 0 tensor(1.0022e-15, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0022e-15, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0023, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0023, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629804
repetition 2
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 1.917346
Train - Epoch 0, Batch: 10, Loss: 1.375758
Train - Epoch 0, Batch: 20, Loss: 1.245983
Train - Epoch 0, Batch: 30, Loss: 1.196707
Train - Epoch 1, Batch: 0, Loss: 1.192440
Train - Epoch 1, Batch: 10, Loss: 1.166968
Train - Epoch 1, Batch: 20, Loss: 1.145070
Train - Epoch 1, Batch: 30, Loss: 1.134859
Train - Epoch 2, Batch: 0, Loss: 1.136048
Train - Epoch 2, Batch: 10, Loss: 1.120206
Train - Epoch 2, Batch: 20, Loss: 1.120625
Train - Epoch 2, Batch: 30, Loss: 1.099179
Train - Epoch 3, Batch: 0, Loss: 1.106458
Train - Epoch 3, Batch: 10, Loss: 1.090735
Train - Epoch 3, Batch: 20, Loss: 1.082153
Train - Epoch 3, Batch: 30, Loss: 1.072284
Train - Epoch 4, Batch: 0, Loss: 1.078575
Train - Epoch 4, Batch: 10, Loss: 1.074576
Train - Epoch 4, Batch: 20, Loss: 1.068770
Train - Epoch 4, Batch: 30, Loss: 1.056209
Train - Epoch 5, Batch: 0, Loss: 1.059161
Train - Epoch 5, Batch: 10, Loss: 1.050821
Train - Epoch 5, Batch: 20, Loss: 1.043100
Train - Epoch 5, Batch: 30, Loss: 1.031479
Train - Epoch 6, Batch: 0, Loss: 1.033772
Train - Epoch 6, Batch: 10, Loss: 1.034079
Train - Epoch 6, Batch: 20, Loss: 1.037494
Train - Epoch 6, Batch: 30, Loss: 1.019743
Train - Epoch 7, Batch: 0, Loss: 1.020275
Train - Epoch 7, Batch: 10, Loss: 1.027812
Train - Epoch 7, Batch: 20, Loss: 1.016555
Train - Epoch 7, Batch: 30, Loss: 1.010498
Train - Epoch 8, Batch: 0, Loss: 1.010043
Train - Epoch 8, Batch: 10, Loss: 1.005076
Train - Epoch 8, Batch: 20, Loss: 1.002525
Train - Epoch 8, Batch: 30, Loss: 0.994053
Train - Epoch 9, Batch: 0, Loss: 0.989194
Train - Epoch 9, Batch: 10, Loss: 0.991147
Train - Epoch 9, Batch: 20, Loss: 0.985809
Train - Epoch 9, Batch: 30, Loss: 0.988154
Train - Epoch 10, Batch: 0, Loss: 0.983142
Train - Epoch 10, Batch: 10, Loss: 0.988072
Train - Epoch 10, Batch: 20, Loss: 0.986568
Train - Epoch 10, Batch: 30, Loss: 0.969809
Train - Epoch 11, Batch: 0, Loss: 0.984118
Train - Epoch 11, Batch: 10, Loss: 0.982366
Train - Epoch 11, Batch: 20, Loss: 0.969882
Train - Epoch 11, Batch: 30, Loss: 0.959559
Train - Epoch 12, Batch: 0, Loss: 0.977233
Train - Epoch 12, Batch: 10, Loss: 0.969371
Train - Epoch 12, Batch: 20, Loss: 0.966808
Train - Epoch 12, Batch: 30, Loss: 0.966788
Train - Epoch 13, Batch: 0, Loss: 0.963243
Train - Epoch 13, Batch: 10, Loss: 0.959852
Train - Epoch 13, Batch: 20, Loss: 0.962133
Train - Epoch 13, Batch: 30, Loss: 0.954142
Train - Epoch 14, Batch: 0, Loss: 0.953597
Train - Epoch 14, Batch: 10, Loss: 0.964653
Train - Epoch 14, Batch: 20, Loss: 0.960579
Train - Epoch 14, Batch: 30, Loss: 0.946852
Train - Epoch 15, Batch: 0, Loss: 0.946816
Train - Epoch 15, Batch: 10, Loss: 0.955891
Train - Epoch 15, Batch: 20, Loss: 0.939071
Train - Epoch 15, Batch: 30, Loss: 0.942921
Train - Epoch 16, Batch: 0, Loss: 0.945227
Train - Epoch 16, Batch: 10, Loss: 0.946059
Train - Epoch 16, Batch: 20, Loss: 0.936441
Train - Epoch 16, Batch: 30, Loss: 0.948789
Train - Epoch 17, Batch: 0, Loss: 0.944889
Train - Epoch 17, Batch: 10, Loss: 0.934503
Train - Epoch 17, Batch: 20, Loss: 0.944263
Train - Epoch 17, Batch: 30, Loss: 0.951634
Train - Epoch 18, Batch: 0, Loss: 0.932853
Train - Epoch 18, Batch: 10, Loss: 0.932892
Train - Epoch 18, Batch: 20, Loss: 0.933415
Train - Epoch 18, Batch: 30, Loss: 0.926223
Train - Epoch 19, Batch: 0, Loss: 0.920565
Train - Epoch 19, Batch: 10, Loss: 0.927986
Train - Epoch 19, Batch: 20, Loss: 0.934508
Train - Epoch 19, Batch: 30, Loss: 0.928392
Train - Epoch 20, Batch: 0, Loss: 0.923985
Train - Epoch 20, Batch: 10, Loss: 0.936350
Train - Epoch 20, Batch: 20, Loss: 0.923564
Train - Epoch 20, Batch: 30, Loss: 0.919394
Train - Epoch 21, Batch: 0, Loss: 0.925281
Train - Epoch 21, Batch: 10, Loss: 0.924653
Train - Epoch 21, Batch: 20, Loss: 0.929725
Train - Epoch 21, Batch: 30, Loss: 0.918268
Train - Epoch 22, Batch: 0, Loss: 0.926152
Train - Epoch 22, Batch: 10, Loss: 0.913454
Train - Epoch 22, Batch: 20, Loss: 0.912946
Train - Epoch 22, Batch: 30, Loss: 0.908441
Train - Epoch 23, Batch: 0, Loss: 0.926839
Train - Epoch 23, Batch: 10, Loss: 0.913150
Train - Epoch 23, Batch: 20, Loss: 0.919612
Train - Epoch 23, Batch: 30, Loss: 0.905556
Train - Epoch 24, Batch: 0, Loss: 0.908819
Train - Epoch 24, Batch: 10, Loss: 0.918689
Train - Epoch 24, Batch: 20, Loss: 0.912368
Train - Epoch 24, Batch: 30, Loss: 0.912871
Train - Epoch 25, Batch: 0, Loss: 0.909925
Train - Epoch 25, Batch: 10, Loss: 0.918787
Train - Epoch 25, Batch: 20, Loss: 0.909458
Train - Epoch 25, Batch: 30, Loss: 0.910008
Train - Epoch 26, Batch: 0, Loss: 0.903648
Train - Epoch 26, Batch: 10, Loss: 0.907353
Train - Epoch 26, Batch: 20, Loss: 0.904542
Train - Epoch 26, Batch: 30, Loss: 0.915502
Train - Epoch 27, Batch: 0, Loss: 0.906224
Train - Epoch 27, Batch: 10, Loss: 0.907265
Train - Epoch 27, Batch: 20, Loss: 0.905842
Train - Epoch 27, Batch: 30, Loss: 0.901697
Train - Epoch 28, Batch: 0, Loss: 0.901047
Train - Epoch 28, Batch: 10, Loss: 0.892581
Train - Epoch 28, Batch: 20, Loss: 0.904088
Train - Epoch 28, Batch: 30, Loss: 0.900133
Train - Epoch 29, Batch: 0, Loss: 0.901117
Train - Epoch 29, Batch: 10, Loss: 0.901894
Train - Epoch 29, Batch: 20, Loss: 0.903975
Train - Epoch 29, Batch: 30, Loss: 0.893280
Train - Epoch 30, Batch: 0, Loss: 0.893558
Train - Epoch 30, Batch: 10, Loss: 0.894355
Train - Epoch 30, Batch: 20, Loss: 0.905222
Train - Epoch 30, Batch: 30, Loss: 0.886771
Train - Epoch 31, Batch: 0, Loss: 0.901058
Train - Epoch 31, Batch: 10, Loss: 0.879149
Train - Epoch 31, Batch: 20, Loss: 0.893302
Train - Epoch 31, Batch: 30, Loss: 0.896090
training_time:: 3.802762746810913
training time full:: 3.802807092666626
provenance prepare time:: 4.76837158203125e-07
Test Avg. Loss: 0.000070, Accuracy: 0.629770
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 1045
training time is 2.5412044525146484
overhead:: 0
overhead2:: 0
time_baseline:: 2.5429656505584717
curr_diff: 0 tensor(0.0023, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0023, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629890
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.017945528030395508
overhead3:: 0.027295351028442383
overhead4:: 0.11678218841552734
overhead5:: 0
time_provenance:: 0.8947062492370605
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0025, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0025, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629925
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.019774675369262695
overhead3:: 0.029954195022583008
overhead4:: 0.11570191383361816
overhead5:: 0
time_provenance:: 0.8807892799377441
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0026, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0026, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629925
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.022998571395874023
overhead3:: 0.03527235984802246
overhead4:: 0.14329147338867188
overhead5:: 0
time_provenance:: 0.9929866790771484
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0026, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0026, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629925
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.026355743408203125
overhead3:: 0.04980134963989258
overhead4:: 0.1686570644378662
overhead5:: 0
time_provenance:: 1.0063629150390625
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0026, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0026, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629925
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.04039406776428223
overhead3:: 0.04867124557495117
overhead4:: 0.21480202674865723
overhead5:: 0
time_provenance:: 1.0933077335357666
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0024, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0024, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629908
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.03398394584655762
overhead3:: 0.05113053321838379
overhead4:: 0.24238801002502441
overhead5:: 0
time_provenance:: 1.0744662284851074
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0024, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0024, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629908
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.035610198974609375
overhead3:: 0.05679059028625488
overhead4:: 0.24088644981384277
overhead5:: 0
time_provenance:: 1.0803446769714355
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0024, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0024, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629908
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.0400538444519043
overhead3:: 0.05770301818847656
overhead4:: 0.2508563995361328
overhead5:: 0
time_provenance:: 1.1093246936798096
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0024, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0024, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629908
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.05787205696105957
overhead3:: 0.08468055725097656
overhead4:: 0.4037933349609375
overhead5:: 0
time_provenance:: 1.2875499725341797
curr_diff: 0 tensor(6.2353e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.2353e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0024, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0024, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629908
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.06264519691467285
overhead3:: 0.09322547912597656
overhead4:: 0.41941261291503906
overhead5:: 0
time_provenance:: 1.316969633102417
curr_diff: 0 tensor(6.2753e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.2753e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0024, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0024, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629908
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.06384396553039551
overhead3:: 0.09141349792480469
overhead4:: 0.40114545822143555
overhead5:: 0
time_provenance:: 1.2941198348999023
curr_diff: 0 tensor(6.2972e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.2972e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0024, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0024, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629908
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.092926025390625
overhead3:: 0.12276911735534668
overhead4:: 0.4927396774291992
overhead5:: 0
time_provenance:: 1.745180606842041
curr_diff: 0 tensor(6.2982e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.2982e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0024, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0024, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629908
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.14011693000793457
overhead3:: 0.20296955108642578
overhead4:: 0.9638135433197021
overhead5:: 0
time_provenance:: 2.009547472000122
curr_diff: 0 tensor(1.3552e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3552e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0023, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0023, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629890
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.18773245811462402
overhead3:: 0.2597777843475342
overhead4:: 1.1685280799865723
overhead5:: 0
time_provenance:: 2.5910468101501465
curr_diff: 0 tensor(1.3527e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3527e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0023, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0023, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629890
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.14811277389526367
overhead3:: 0.2180018424987793
overhead4:: 0.976987361907959
overhead5:: 0
time_provenance:: 2.060856580734253
curr_diff: 0 tensor(1.3813e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3813e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0023, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0023, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629890
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.1522679328918457
overhead3:: 0.21704411506652832
overhead4:: 0.9771544933319092
overhead5:: 0
time_provenance:: 2.058086395263672
curr_diff: 0 tensor(1.3894e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3894e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0023, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0023, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629890
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.3680245876312256
overhead3:: 0.5624313354492188
overhead4:: 1.7131340503692627
overhead5:: 0
time_provenance:: 2.9209539890289307
curr_diff: 0 tensor(9.9405e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.9405e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0023, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0023, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629890
repetition 3
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 3 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 1.878800
Train - Epoch 0, Batch: 10, Loss: 1.371979
Train - Epoch 0, Batch: 20, Loss: 1.243006
Train - Epoch 0, Batch: 30, Loss: 1.202606
Train - Epoch 1, Batch: 0, Loss: 1.189057
Train - Epoch 1, Batch: 10, Loss: 1.165064
Train - Epoch 1, Batch: 20, Loss: 1.146948
Train - Epoch 1, Batch: 30, Loss: 1.133915
Train - Epoch 2, Batch: 0, Loss: 1.140836
Train - Epoch 2, Batch: 10, Loss: 1.119474
Train - Epoch 2, Batch: 20, Loss: 1.117382
Train - Epoch 2, Batch: 30, Loss: 1.103618
Train - Epoch 3, Batch: 0, Loss: 1.103917
Train - Epoch 3, Batch: 10, Loss: 1.091293
Train - Epoch 3, Batch: 20, Loss: 1.085720
Train - Epoch 3, Batch: 30, Loss: 1.076745
Train - Epoch 4, Batch: 0, Loss: 1.093012
Train - Epoch 4, Batch: 10, Loss: 1.076909
Train - Epoch 4, Batch: 20, Loss: 1.059427
Train - Epoch 4, Batch: 30, Loss: 1.065202
Train - Epoch 5, Batch: 0, Loss: 1.066850
Train - Epoch 5, Batch: 10, Loss: 1.053361
Train - Epoch 5, Batch: 20, Loss: 1.050456
Train - Epoch 5, Batch: 30, Loss: 1.034751
Train - Epoch 6, Batch: 0, Loss: 1.049964
Train - Epoch 6, Batch: 10, Loss: 1.026488
Train - Epoch 6, Batch: 20, Loss: 1.030098
Train - Epoch 6, Batch: 30, Loss: 1.029971
Train - Epoch 7, Batch: 0, Loss: 1.029973
Train - Epoch 7, Batch: 10, Loss: 1.026475
Train - Epoch 7, Batch: 20, Loss: 1.021614
Train - Epoch 7, Batch: 30, Loss: 1.009231
Train - Epoch 8, Batch: 0, Loss: 1.014794
Train - Epoch 8, Batch: 10, Loss: 1.022550
Train - Epoch 8, Batch: 20, Loss: 1.002754
Train - Epoch 8, Batch: 30, Loss: 1.004881
Train - Epoch 9, Batch: 0, Loss: 0.995346
Train - Epoch 9, Batch: 10, Loss: 0.999011
Train - Epoch 9, Batch: 20, Loss: 1.006539
Train - Epoch 9, Batch: 30, Loss: 0.991255
Train - Epoch 10, Batch: 0, Loss: 0.987032
Train - Epoch 10, Batch: 10, Loss: 0.986255
Train - Epoch 10, Batch: 20, Loss: 0.994206
Train - Epoch 10, Batch: 30, Loss: 0.978535
Train - Epoch 11, Batch: 0, Loss: 0.979063
Train - Epoch 11, Batch: 10, Loss: 0.987267
Train - Epoch 11, Batch: 20, Loss: 0.978711
Train - Epoch 11, Batch: 30, Loss: 0.985196
Train - Epoch 12, Batch: 0, Loss: 0.973222
Train - Epoch 12, Batch: 10, Loss: 0.967335
Train - Epoch 12, Batch: 20, Loss: 0.978218
Train - Epoch 12, Batch: 30, Loss: 0.963343
Train - Epoch 13, Batch: 0, Loss: 0.971147
Train - Epoch 13, Batch: 10, Loss: 0.966400
Train - Epoch 13, Batch: 20, Loss: 0.957333
Train - Epoch 13, Batch: 30, Loss: 0.954938
Train - Epoch 14, Batch: 0, Loss: 0.970201
Train - Epoch 14, Batch: 10, Loss: 0.945997
Train - Epoch 14, Batch: 20, Loss: 0.954160
Train - Epoch 14, Batch: 30, Loss: 0.943969
Train - Epoch 15, Batch: 0, Loss: 0.948165
Train - Epoch 15, Batch: 10, Loss: 0.953929
Train - Epoch 15, Batch: 20, Loss: 0.951737
Train - Epoch 15, Batch: 30, Loss: 0.958762
Train - Epoch 16, Batch: 0, Loss: 0.950855
Train - Epoch 16, Batch: 10, Loss: 0.960872
Train - Epoch 16, Batch: 20, Loss: 0.947181
Train - Epoch 16, Batch: 30, Loss: 0.938360
Train - Epoch 17, Batch: 0, Loss: 0.950098
Train - Epoch 17, Batch: 10, Loss: 0.935564
Train - Epoch 17, Batch: 20, Loss: 0.934104
Train - Epoch 17, Batch: 30, Loss: 0.946869
Train - Epoch 18, Batch: 0, Loss: 0.946433
Train - Epoch 18, Batch: 10, Loss: 0.942338
Train - Epoch 18, Batch: 20, Loss: 0.935433
Train - Epoch 18, Batch: 30, Loss: 0.935586
Train - Epoch 19, Batch: 0, Loss: 0.938935
Train - Epoch 19, Batch: 10, Loss: 0.926320
Train - Epoch 19, Batch: 20, Loss: 0.942036
Train - Epoch 19, Batch: 30, Loss: 0.931793
Train - Epoch 20, Batch: 0, Loss: 0.923086
Train - Epoch 20, Batch: 10, Loss: 0.926099
Train - Epoch 20, Batch: 20, Loss: 0.929058
Train - Epoch 20, Batch: 30, Loss: 0.921245
Train - Epoch 21, Batch: 0, Loss: 0.919616
Train - Epoch 21, Batch: 10, Loss: 0.932961
Train - Epoch 21, Batch: 20, Loss: 0.921676
Train - Epoch 21, Batch: 30, Loss: 0.915570
Train - Epoch 22, Batch: 0, Loss: 0.921926
Train - Epoch 22, Batch: 10, Loss: 0.912680
Train - Epoch 22, Batch: 20, Loss: 0.916154
Train - Epoch 22, Batch: 30, Loss: 0.920529
Train - Epoch 23, Batch: 0, Loss: 0.915938
Train - Epoch 23, Batch: 10, Loss: 0.926226
Train - Epoch 23, Batch: 20, Loss: 0.917267
Train - Epoch 23, Batch: 30, Loss: 0.919036
Train - Epoch 24, Batch: 0, Loss: 0.918410
Train - Epoch 24, Batch: 10, Loss: 0.913504
Train - Epoch 24, Batch: 20, Loss: 0.911999
Train - Epoch 24, Batch: 30, Loss: 0.923973
Train - Epoch 25, Batch: 0, Loss: 0.922105
Train - Epoch 25, Batch: 10, Loss: 0.909922
Train - Epoch 25, Batch: 20, Loss: 0.910451
Train - Epoch 25, Batch: 30, Loss: 0.907370
Train - Epoch 26, Batch: 0, Loss: 0.911676
Train - Epoch 26, Batch: 10, Loss: 0.907703
Train - Epoch 26, Batch: 20, Loss: 0.911323
Train - Epoch 26, Batch: 30, Loss: 0.901323
Train - Epoch 27, Batch: 0, Loss: 0.907087
Train - Epoch 27, Batch: 10, Loss: 0.898890
Train - Epoch 27, Batch: 20, Loss: 0.907780
Train - Epoch 27, Batch: 30, Loss: 0.905624
Train - Epoch 28, Batch: 0, Loss: 0.910225
Train - Epoch 28, Batch: 10, Loss: 0.903174
Train - Epoch 28, Batch: 20, Loss: 0.901913
Train - Epoch 28, Batch: 30, Loss: 0.892265
Train - Epoch 29, Batch: 0, Loss: 0.896401
Train - Epoch 29, Batch: 10, Loss: 0.907532
Train - Epoch 29, Batch: 20, Loss: 0.891130
Train - Epoch 29, Batch: 30, Loss: 0.901078
Train - Epoch 30, Batch: 0, Loss: 0.891118
Train - Epoch 30, Batch: 10, Loss: 0.898833
Train - Epoch 30, Batch: 20, Loss: 0.895178
Train - Epoch 30, Batch: 30, Loss: 0.902527
Train - Epoch 31, Batch: 0, Loss: 0.898751
Train - Epoch 31, Batch: 10, Loss: 0.892532
Train - Epoch 31, Batch: 20, Loss: 0.895109
Train - Epoch 31, Batch: 30, Loss: 0.910089
training_time:: 3.743687152862549
training time full:: 3.743731737136841
provenance prepare time:: 4.76837158203125e-07
Test Avg. Loss: 0.000070, Accuracy: 0.629512
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 1045
training time is 2.1341869831085205
overhead:: 0
overhead2:: 0
time_baseline:: 2.13570499420166
curr_diff: 0 tensor(0.0023, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0023, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629615
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.018513917922973633
overhead3:: 0.02986431121826172
overhead4:: 0.10273551940917969
overhead5:: 0
time_provenance:: 0.8807146549224854
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0025, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0025, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629581
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.021192312240600586
overhead3:: 0.03276634216308594
overhead4:: 0.12293744087219238
overhead5:: 0
time_provenance:: 0.9160130023956299
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0025, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0025, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629615
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.023425817489624023
overhead3:: 0.04814887046813965
overhead4:: 0.14630818367004395
overhead5:: 0
time_provenance:: 0.9596319198608398
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0025, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0025, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629598
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.02465081214904785
overhead3:: 0.04731416702270508
overhead4:: 0.1623213291168213
overhead5:: 0
time_provenance:: 0.9399228096008301
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0025, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0025, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629615
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.031076431274414062
overhead3:: 0.04822707176208496
overhead4:: 0.23955774307250977
overhead5:: 0
time_provenance:: 1.0614545345306396
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0024, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0024, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629632
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.03354644775390625
overhead3:: 0.05092501640319824
overhead4:: 0.24489450454711914
overhead5:: 0
time_provenance:: 1.0679965019226074
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0024, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0024, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629632
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.036232948303222656
overhead3:: 0.06356215476989746
overhead4:: 0.2532823085784912
overhead5:: 0
time_provenance:: 1.0936038494110107
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0024, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0024, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629632
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.03895211219787598
overhead3:: 0.05791640281677246
overhead4:: 0.26377320289611816
overhead5:: 0
time_provenance:: 1.115022897720337
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0024, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0024, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629632
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.06379866600036621
overhead3:: 0.08920979499816895
overhead4:: 0.4039921760559082
overhead5:: 0
time_provenance:: 1.3192338943481445
curr_diff: 0 tensor(3.1127e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.1127e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0024, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0024, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629615
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.06203484535217285
overhead3:: 0.09197521209716797
overhead4:: 0.42726826667785645
overhead5:: 0
time_provenance:: 1.3200485706329346
curr_diff: 0 tensor(3.2389e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.2389e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0024, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0024, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629615
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.06959009170532227
overhead3:: 0.09804511070251465
overhead4:: 0.4312424659729004
overhead5:: 0
time_provenance:: 1.417841911315918
curr_diff: 0 tensor(3.3790e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.3790e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0024, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0024, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629615
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.07289743423461914
overhead3:: 0.10078597068786621
overhead4:: 0.43425512313842773
overhead5:: 0
time_provenance:: 1.4116661548614502
curr_diff: 0 tensor(3.4059e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.4059e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0024, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0024, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629615
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.1456468105316162
overhead3:: 0.21327662467956543
overhead4:: 0.9676132202148438
overhead5:: 0
time_provenance:: 2.048701763153076
curr_diff: 0 tensor(1.3960e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3960e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0023, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0023, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629615
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.14145231246948242
overhead3:: 0.21057772636413574
overhead4:: 0.9281148910522461
overhead5:: 0
time_provenance:: 1.9660534858703613
curr_diff: 0 tensor(1.4006e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4006e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0023, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0023, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629615
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.20765423774719238
overhead3:: 0.2749021053314209
overhead4:: 1.1313934326171875
overhead5:: 0
time_provenance:: 2.5959880352020264
curr_diff: 0 tensor(1.4105e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4105e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0023, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0023, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629615
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.1530168056488037
overhead3:: 0.21395659446716309
overhead4:: 0.9532499313354492
overhead5:: 0
time_provenance:: 2.023296594619751
curr_diff: 0 tensor(1.4153e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4153e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0023, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0023, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629615
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.3207693099975586
overhead3:: 0.4620203971862793
overhead4:: 1.6206977367401123
overhead5:: 0
time_provenance:: 2.6851048469543457
curr_diff: 0 tensor(1.1907e-15, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1907e-15, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1., dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0023, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0023, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629615
repetition 4
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 4 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 1.923897
Train - Epoch 0, Batch: 10, Loss: 1.380906
Train - Epoch 0, Batch: 20, Loss: 1.240043
Train - Epoch 0, Batch: 30, Loss: 1.189775
Train - Epoch 1, Batch: 0, Loss: 1.175908
Train - Epoch 1, Batch: 10, Loss: 1.157327
Train - Epoch 1, Batch: 20, Loss: 1.138859
Train - Epoch 1, Batch: 30, Loss: 1.129525
Train - Epoch 2, Batch: 0, Loss: 1.126365
Train - Epoch 2, Batch: 10, Loss: 1.111824
Train - Epoch 2, Batch: 20, Loss: 1.108629
Train - Epoch 2, Batch: 30, Loss: 1.092158
Train - Epoch 3, Batch: 0, Loss: 1.103667
Train - Epoch 3, Batch: 10, Loss: 1.090738
Train - Epoch 3, Batch: 20, Loss: 1.079343
Train - Epoch 3, Batch: 30, Loss: 1.078051
Train - Epoch 4, Batch: 0, Loss: 1.063787
Train - Epoch 4, Batch: 10, Loss: 1.074533
Train - Epoch 4, Batch: 20, Loss: 1.062113
Train - Epoch 4, Batch: 30, Loss: 1.037618
Train - Epoch 5, Batch: 0, Loss: 1.048038
Train - Epoch 5, Batch: 10, Loss: 1.048758
Train - Epoch 5, Batch: 20, Loss: 1.034430
Train - Epoch 5, Batch: 30, Loss: 1.034331
Train - Epoch 6, Batch: 0, Loss: 1.033490
Train - Epoch 6, Batch: 10, Loss: 1.026266
Train - Epoch 6, Batch: 20, Loss: 1.022798
Train - Epoch 6, Batch: 30, Loss: 1.023121
Train - Epoch 7, Batch: 0, Loss: 1.021978
Train - Epoch 7, Batch: 10, Loss: 1.018467
Train - Epoch 7, Batch: 20, Loss: 1.012831
Train - Epoch 7, Batch: 30, Loss: 1.008099
Train - Epoch 8, Batch: 0, Loss: 1.004081
Train - Epoch 8, Batch: 10, Loss: 1.001759
Train - Epoch 8, Batch: 20, Loss: 0.995079
Train - Epoch 8, Batch: 30, Loss: 0.992773
Train - Epoch 9, Batch: 0, Loss: 1.001450
Train - Epoch 9, Batch: 10, Loss: 0.992236
Train - Epoch 9, Batch: 20, Loss: 0.992449
Train - Epoch 9, Batch: 30, Loss: 0.990597
Train - Epoch 10, Batch: 0, Loss: 0.988122
Train - Epoch 10, Batch: 10, Loss: 0.988050
Train - Epoch 10, Batch: 20, Loss: 0.980818
Train - Epoch 10, Batch: 30, Loss: 0.973491
Train - Epoch 11, Batch: 0, Loss: 0.982973
Train - Epoch 11, Batch: 10, Loss: 0.975885
Train - Epoch 11, Batch: 20, Loss: 0.981273
Train - Epoch 11, Batch: 30, Loss: 0.965824
Train - Epoch 12, Batch: 0, Loss: 0.961990
Train - Epoch 12, Batch: 10, Loss: 0.981560
Train - Epoch 12, Batch: 20, Loss: 0.968131
Train - Epoch 12, Batch: 30, Loss: 0.976201
Train - Epoch 13, Batch: 0, Loss: 0.957809
Train - Epoch 13, Batch: 10, Loss: 0.965362
Train - Epoch 13, Batch: 20, Loss: 0.964794
Train - Epoch 13, Batch: 30, Loss: 0.954029
Train - Epoch 14, Batch: 0, Loss: 0.949162
Train - Epoch 14, Batch: 10, Loss: 0.951742
Train - Epoch 14, Batch: 20, Loss: 0.947697
Train - Epoch 14, Batch: 30, Loss: 0.950695
Train - Epoch 15, Batch: 0, Loss: 0.952057
Train - Epoch 15, Batch: 10, Loss: 0.949956
Train - Epoch 15, Batch: 20, Loss: 0.948539
Train - Epoch 15, Batch: 30, Loss: 0.955171
Train - Epoch 16, Batch: 0, Loss: 0.946142
Train - Epoch 16, Batch: 10, Loss: 0.932456
Train - Epoch 16, Batch: 20, Loss: 0.947198
Train - Epoch 16, Batch: 30, Loss: 0.946812
Train - Epoch 17, Batch: 0, Loss: 0.940540
Train - Epoch 17, Batch: 10, Loss: 0.937396
Train - Epoch 17, Batch: 20, Loss: 0.944911
Train - Epoch 17, Batch: 30, Loss: 0.927324
Train - Epoch 18, Batch: 0, Loss: 0.935993
Train - Epoch 18, Batch: 10, Loss: 0.932928
Train - Epoch 18, Batch: 20, Loss: 0.930982
Train - Epoch 18, Batch: 30, Loss: 0.935736
Train - Epoch 19, Batch: 0, Loss: 0.931096
Train - Epoch 19, Batch: 10, Loss: 0.929943
Train - Epoch 19, Batch: 20, Loss: 0.918776
Train - Epoch 19, Batch: 30, Loss: 0.927614
Train - Epoch 20, Batch: 0, Loss: 0.926047
Train - Epoch 20, Batch: 10, Loss: 0.924104
Train - Epoch 20, Batch: 20, Loss: 0.926932
Train - Epoch 20, Batch: 30, Loss: 0.922849
Train - Epoch 21, Batch: 0, Loss: 0.923523
Train - Epoch 21, Batch: 10, Loss: 0.937228
Train - Epoch 21, Batch: 20, Loss: 0.920360
Train - Epoch 21, Batch: 30, Loss: 0.922005
Train - Epoch 22, Batch: 0, Loss: 0.926175
Train - Epoch 22, Batch: 10, Loss: 0.916505
Train - Epoch 22, Batch: 20, Loss: 0.919287
Train - Epoch 22, Batch: 30, Loss: 0.917015
Train - Epoch 23, Batch: 0, Loss: 0.919655
Train - Epoch 23, Batch: 10, Loss: 0.909533
Train - Epoch 23, Batch: 20, Loss: 0.910901
Train - Epoch 23, Batch: 30, Loss: 0.911615
Train - Epoch 24, Batch: 0, Loss: 0.917225
Train - Epoch 24, Batch: 10, Loss: 0.908408
Train - Epoch 24, Batch: 20, Loss: 0.906217
Train - Epoch 24, Batch: 30, Loss: 0.911871
Train - Epoch 25, Batch: 0, Loss: 0.914755
Train - Epoch 25, Batch: 10, Loss: 0.898528
Train - Epoch 25, Batch: 20, Loss: 0.901699
Train - Epoch 25, Batch: 30, Loss: 0.904522
Train - Epoch 26, Batch: 0, Loss: 0.902714
Train - Epoch 26, Batch: 10, Loss: 0.920187
Train - Epoch 26, Batch: 20, Loss: 0.912208
Train - Epoch 26, Batch: 30, Loss: 0.898329
Train - Epoch 27, Batch: 0, Loss: 0.897992
Train - Epoch 27, Batch: 10, Loss: 0.903592
Train - Epoch 27, Batch: 20, Loss: 0.900309
Train - Epoch 27, Batch: 30, Loss: 0.903647
Train - Epoch 28, Batch: 0, Loss: 0.906151
Train - Epoch 28, Batch: 10, Loss: 0.909230
Train - Epoch 28, Batch: 20, Loss: 0.896334
Train - Epoch 28, Batch: 30, Loss: 0.906338
Train - Epoch 29, Batch: 0, Loss: 0.901527
Train - Epoch 29, Batch: 10, Loss: 0.904609
Train - Epoch 29, Batch: 20, Loss: 0.901685
Train - Epoch 29, Batch: 30, Loss: 0.901442
Train - Epoch 30, Batch: 0, Loss: 0.893378
Train - Epoch 30, Batch: 10, Loss: 0.894278
Train - Epoch 30, Batch: 20, Loss: 0.896913
Train - Epoch 30, Batch: 30, Loss: 0.893791
Train - Epoch 31, Batch: 0, Loss: 0.889455
Train - Epoch 31, Batch: 10, Loss: 0.892793
Train - Epoch 31, Batch: 20, Loss: 0.892101
Train - Epoch 31, Batch: 30, Loss: 0.886518
training_time:: 4.0707619190216064
training time full:: 4.070804595947266
provenance prepare time:: 4.76837158203125e-07
Test Avg. Loss: 0.000070, Accuracy: 0.630837
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 1045
training time is 2.271502733230591
overhead:: 0
overhead2:: 0
time_baseline:: 2.272901773452759
curr_diff: 0 tensor(0.0023, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0023, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630906
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.02229762077331543
overhead3:: 0.05296683311462402
overhead4:: 0.1197352409362793
overhead5:: 0
time_provenance:: 1.027785301208496
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0025, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0025, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630906
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.021842241287231445
overhead3:: 0.034101247787475586
overhead4:: 0.12941384315490723
overhead5:: 0
time_provenance:: 0.9709811210632324
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0025, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0025, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630854
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.022084474563598633
overhead3:: 0.04760146141052246
overhead4:: 0.1372368335723877
overhead5:: 0
time_provenance:: 0.8953065872192383
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0026, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0026, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630906
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.026851654052734375
overhead3:: 0.047626495361328125
overhead4:: 0.16266417503356934
overhead5:: 0
time_provenance:: 1.0253345966339111
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0025, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0025, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630854
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.033765554428100586
overhead3:: 0.04625749588012695
overhead4:: 0.19773530960083008
overhead5:: 0
time_provenance:: 1.010352373123169
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0024, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0024, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630889
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.034787893295288086
overhead3:: 0.050334930419921875
overhead4:: 0.23482346534729004
overhead5:: 0
time_provenance:: 1.0763046741485596
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0025, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0025, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630889
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.04657578468322754
overhead3:: 0.060228586196899414
overhead4:: 0.24659061431884766
overhead5:: 0
time_provenance:: 1.1128289699554443
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0025, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0025, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630889
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.041077613830566406
overhead3:: 0.057732582092285156
overhead4:: 0.234358549118042
overhead5:: 0
time_provenance:: 1.1231706142425537
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0025, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0025, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630889
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.07694602012634277
overhead3:: 0.1083829402923584
overhead4:: 0.4737422466278076
overhead5:: 0
time_provenance:: 1.653296709060669
curr_diff: 0 tensor(5.0479e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.0479e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0024, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0024, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630906
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.06280183792114258
overhead3:: 0.08946728706359863
overhead4:: 0.43909525871276855
overhead5:: 0
time_provenance:: 1.3483250141143799
curr_diff: 0 tensor(5.0923e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.0923e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0024, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0024, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630906
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.0645134449005127
overhead3:: 0.09545660018920898
overhead4:: 0.42727065086364746
overhead5:: 0
time_provenance:: 1.3226447105407715
curr_diff: 0 tensor(5.3229e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.3229e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0024, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0024, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630906
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.09361529350280762
overhead3:: 0.1242058277130127
overhead4:: 0.49956369400024414
overhead5:: 0
time_provenance:: 1.7598984241485596
curr_diff: 0 tensor(5.3456e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.3456e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0024, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0024, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630906
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.14430570602416992
overhead3:: 0.22618842124938965
overhead4:: 0.9622259140014648
overhead5:: 0
time_provenance:: 2.051093101501465
curr_diff: 0 tensor(1.0695e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0695e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0023, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0023, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630906
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.15041756629943848
overhead3:: 0.20950531959533691
overhead4:: 0.9080667495727539
overhead5:: 0
time_provenance:: 1.9585156440734863
curr_diff: 0 tensor(1.0940e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0940e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0023, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0023, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630906
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.14955878257751465
overhead3:: 0.21733641624450684
overhead4:: 0.9499208927154541
overhead5:: 0
time_provenance:: 2.0327510833740234
curr_diff: 0 tensor(1.1297e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1297e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0023, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0023, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630906
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.15329337120056152
overhead3:: 0.21796512603759766
overhead4:: 0.9762239456176758
overhead5:: 0
time_provenance:: 2.080746650695801
curr_diff: 0 tensor(1.1478e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1478e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0023, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0023, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630906
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.34520983695983887
overhead3:: 0.532261848449707
overhead4:: 1.6638696193695068
overhead5:: 0
time_provenance:: 2.82191801071167
curr_diff: 0 tensor(9.1901e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.1901e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0023, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0023, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630906
deletion rate:: 0.01
python3 generate_rand_ids 0.01  covtype 0
tensor([ 98311, 344071,  81935,  ..., 212987, 262140,  16383])
batch size:: 16384
repetition 0
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 0 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.019799
Train - Epoch 0, Batch: 10, Loss: 1.398495
Train - Epoch 0, Batch: 20, Loss: 1.238542
Train - Epoch 0, Batch: 30, Loss: 1.188138
Train - Epoch 1, Batch: 0, Loss: 1.179836
Train - Epoch 1, Batch: 10, Loss: 1.159015
Train - Epoch 1, Batch: 20, Loss: 1.134623
Train - Epoch 1, Batch: 30, Loss: 1.134107
Train - Epoch 2, Batch: 0, Loss: 1.120677
Train - Epoch 2, Batch: 10, Loss: 1.119038
Train - Epoch 2, Batch: 20, Loss: 1.105423
Train - Epoch 2, Batch: 30, Loss: 1.091487
Train - Epoch 3, Batch: 0, Loss: 1.086730
Train - Epoch 3, Batch: 10, Loss: 1.080032
Train - Epoch 3, Batch: 20, Loss: 1.069158
Train - Epoch 3, Batch: 30, Loss: 1.068308
Train - Epoch 4, Batch: 0, Loss: 1.063486
Train - Epoch 4, Batch: 10, Loss: 1.072743
Train - Epoch 4, Batch: 20, Loss: 1.057208
Train - Epoch 4, Batch: 30, Loss: 1.054339
Train - Epoch 5, Batch: 0, Loss: 1.047048
Train - Epoch 5, Batch: 10, Loss: 1.047732
Train - Epoch 5, Batch: 20, Loss: 1.037697
Train - Epoch 5, Batch: 30, Loss: 1.030871
Train - Epoch 6, Batch: 0, Loss: 1.033725
Train - Epoch 6, Batch: 10, Loss: 1.024550
Train - Epoch 6, Batch: 20, Loss: 1.015818
Train - Epoch 6, Batch: 30, Loss: 1.035359
Train - Epoch 7, Batch: 0, Loss: 1.033055
Train - Epoch 7, Batch: 10, Loss: 1.012312
Train - Epoch 7, Batch: 20, Loss: 1.011444
Train - Epoch 7, Batch: 30, Loss: 1.014277
Train - Epoch 8, Batch: 0, Loss: 1.007783
Train - Epoch 8, Batch: 10, Loss: 1.000487
Train - Epoch 8, Batch: 20, Loss: 1.004515
Train - Epoch 8, Batch: 30, Loss: 0.997284
Train - Epoch 9, Batch: 0, Loss: 1.000610
Train - Epoch 9, Batch: 10, Loss: 0.990349
Train - Epoch 9, Batch: 20, Loss: 0.986736
Train - Epoch 9, Batch: 30, Loss: 0.983875
Train - Epoch 10, Batch: 0, Loss: 0.991177
Train - Epoch 10, Batch: 10, Loss: 0.969988
Train - Epoch 10, Batch: 20, Loss: 0.982179
Train - Epoch 10, Batch: 30, Loss: 0.979649
Train - Epoch 11, Batch: 0, Loss: 0.976266
Train - Epoch 11, Batch: 10, Loss: 0.978386
Train - Epoch 11, Batch: 20, Loss: 0.980359
Train - Epoch 11, Batch: 30, Loss: 0.970319
Train - Epoch 12, Batch: 0, Loss: 0.976652
Train - Epoch 12, Batch: 10, Loss: 0.973633
Train - Epoch 12, Batch: 20, Loss: 0.963142
Train - Epoch 12, Batch: 30, Loss: 0.959002
Train - Epoch 13, Batch: 0, Loss: 0.957054
Train - Epoch 13, Batch: 10, Loss: 0.962501
Train - Epoch 13, Batch: 20, Loss: 0.966994
Train - Epoch 13, Batch: 30, Loss: 0.957670
Train - Epoch 14, Batch: 0, Loss: 0.959260
Train - Epoch 14, Batch: 10, Loss: 0.948005
Train - Epoch 14, Batch: 20, Loss: 0.956593
Train - Epoch 14, Batch: 30, Loss: 0.942569
Train - Epoch 15, Batch: 0, Loss: 0.956495
Train - Epoch 15, Batch: 10, Loss: 0.941251
Train - Epoch 15, Batch: 20, Loss: 0.936342
Train - Epoch 15, Batch: 30, Loss: 0.941419
Train - Epoch 16, Batch: 0, Loss: 0.951585
Train - Epoch 16, Batch: 10, Loss: 0.955865
Train - Epoch 16, Batch: 20, Loss: 0.947927
Train - Epoch 16, Batch: 30, Loss: 0.944205
Train - Epoch 17, Batch: 0, Loss: 0.953060
Train - Epoch 17, Batch: 10, Loss: 0.934277
Train - Epoch 17, Batch: 20, Loss: 0.936670
Train - Epoch 17, Batch: 30, Loss: 0.945451
Train - Epoch 18, Batch: 0, Loss: 0.933945
Train - Epoch 18, Batch: 10, Loss: 0.939139
Train - Epoch 18, Batch: 20, Loss: 0.935258
Train - Epoch 18, Batch: 30, Loss: 0.920618
Train - Epoch 19, Batch: 0, Loss: 0.934140
Train - Epoch 19, Batch: 10, Loss: 0.931722
Train - Epoch 19, Batch: 20, Loss: 0.930478
Train - Epoch 19, Batch: 30, Loss: 0.924859
Train - Epoch 20, Batch: 0, Loss: 0.934289
Train - Epoch 20, Batch: 10, Loss: 0.922019
Train - Epoch 20, Batch: 20, Loss: 0.913738
Train - Epoch 20, Batch: 30, Loss: 0.928522
Train - Epoch 21, Batch: 0, Loss: 0.920020
Train - Epoch 21, Batch: 10, Loss: 0.924841
Train - Epoch 21, Batch: 20, Loss: 0.917090
Train - Epoch 21, Batch: 30, Loss: 0.913057
Train - Epoch 22, Batch: 0, Loss: 0.924188
Train - Epoch 22, Batch: 10, Loss: 0.910490
Train - Epoch 22, Batch: 20, Loss: 0.919556
Train - Epoch 22, Batch: 30, Loss: 0.905712
Train - Epoch 23, Batch: 0, Loss: 0.916887
Train - Epoch 23, Batch: 10, Loss: 0.914365
Train - Epoch 23, Batch: 20, Loss: 0.912554
Train - Epoch 23, Batch: 30, Loss: 0.910467
Train - Epoch 24, Batch: 0, Loss: 0.912192
Train - Epoch 24, Batch: 10, Loss: 0.904812
Train - Epoch 24, Batch: 20, Loss: 0.911689
Train - Epoch 24, Batch: 30, Loss: 0.909425
Train - Epoch 25, Batch: 0, Loss: 0.917798
Train - Epoch 25, Batch: 10, Loss: 0.918853
Train - Epoch 25, Batch: 20, Loss: 0.913617
Train - Epoch 25, Batch: 30, Loss: 0.901137
Train - Epoch 26, Batch: 0, Loss: 0.905629
Train - Epoch 26, Batch: 10, Loss: 0.903983
Train - Epoch 26, Batch: 20, Loss: 0.897758
Train - Epoch 26, Batch: 30, Loss: 0.894875
Train - Epoch 27, Batch: 0, Loss: 0.902066
Train - Epoch 27, Batch: 10, Loss: 0.897061
Train - Epoch 27, Batch: 20, Loss: 0.904423
Train - Epoch 27, Batch: 30, Loss: 0.897311
Train - Epoch 28, Batch: 0, Loss: 0.903988
Train - Epoch 28, Batch: 10, Loss: 0.904338
Train - Epoch 28, Batch: 20, Loss: 0.905031
Train - Epoch 28, Batch: 30, Loss: 0.893405
Train - Epoch 29, Batch: 0, Loss: 0.900266
Train - Epoch 29, Batch: 10, Loss: 0.903300
Train - Epoch 29, Batch: 20, Loss: 0.900329
Train - Epoch 29, Batch: 30, Loss: 0.894910
Train - Epoch 30, Batch: 0, Loss: 0.896204
Train - Epoch 30, Batch: 10, Loss: 0.894775
Train - Epoch 30, Batch: 20, Loss: 0.892044
Train - Epoch 30, Batch: 30, Loss: 0.890573
Train - Epoch 31, Batch: 0, Loss: 0.894850
Train - Epoch 31, Batch: 10, Loss: 0.896200
Train - Epoch 31, Batch: 20, Loss: 0.905881
Train - Epoch 31, Batch: 30, Loss: 0.898358
training_time:: 4.074519395828247
training time full:: 4.074561595916748
provenance prepare time:: 7.152557373046875e-07
Test Avg. Loss: 0.000070, Accuracy: 0.630131
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 5229
training time is 2.2723705768585205
overhead:: 0
overhead2:: 0
time_baseline:: 2.274146318435669
curr_diff: 0 tensor(0.0053, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0053, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630252
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.018909215927124023
overhead3:: 0.02886676788330078
overhead4:: 0.1154334545135498
overhead5:: 0
time_provenance:: 0.9705498218536377
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0059, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0059, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630286
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.021543502807617188
overhead3:: 0.04694771766662598
overhead4:: 0.14168357849121094
overhead5:: 0
time_provenance:: 0.9990391731262207
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0058, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0058, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630252
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.023585081100463867
overhead3:: 0.03626370429992676
overhead4:: 0.15137720108032227
overhead5:: 0
time_provenance:: 0.9996943473815918
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0060, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0060, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630286
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.026453495025634766
overhead3:: 0.048986196517944336
overhead4:: 0.16602158546447754
overhead5:: 0
time_provenance:: 1.069270133972168
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0058, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0058, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630252
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.036447763442993164
overhead3:: 0.05660867691040039
overhead4:: 0.22289037704467773
overhead5:: 0
time_provenance:: 1.130993366241455
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0056, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0056, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630252
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.03397989273071289
overhead3:: 0.060362815856933594
overhead4:: 0.24672794342041016
overhead5:: 0
time_provenance:: 1.1253442764282227
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0056, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0056, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630252
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.03714561462402344
overhead3:: 0.05438852310180664
overhead4:: 0.2447354793548584
overhead5:: 0
time_provenance:: 1.1375768184661865
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0056, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0056, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630252
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.04248166084289551
overhead3:: 0.0623934268951416
overhead4:: 0.2451930046081543
overhead5:: 0
time_provenance:: 1.201704978942871
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0056, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0056, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630252
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.061537981033325195
overhead3:: 0.0882258415222168
overhead4:: 0.4087066650390625
overhead5:: 0
time_provenance:: 1.3567113876342773
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0054, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0054, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630252
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.07592988014221191
overhead3:: 0.10519266128540039
overhead4:: 0.43944287300109863
overhead5:: 0
time_provenance:: 1.5073621273040771
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0054, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0054, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630252
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.09249019622802734
overhead3:: 0.12137889862060547
overhead4:: 0.49885058403015137
overhead5:: 0
time_provenance:: 1.8203105926513672
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0054, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0054, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630252
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.07059311866760254
overhead3:: 0.1042931079864502
overhead4:: 0.446852445602417
overhead5:: 0
time_provenance:: 1.421928882598877
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0054, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0054, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630252
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.1430821418762207
overhead3:: 0.20241785049438477
overhead4:: 0.966087818145752
overhead5:: 0
time_provenance:: 2.0644936561584473
curr_diff: 0 tensor(3.6141e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.6141e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0053, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0053, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630252
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.14689993858337402
overhead3:: 0.2064518928527832
overhead4:: 0.9889340400695801
overhead5:: 0
time_provenance:: 2.0977683067321777
curr_diff: 0 tensor(3.6811e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.6811e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0053, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0053, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630252
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.20817065238952637
overhead3:: 0.2692396640777588
overhead4:: 1.1499855518341064
overhead5:: 0
time_provenance:: 2.6552493572235107
curr_diff: 0 tensor(3.7121e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.7121e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0053, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0053, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630252
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.16150593757629395
overhead3:: 0.23340106010437012
overhead4:: 0.9512686729431152
overhead5:: 0
time_provenance:: 2.112370491027832
curr_diff: 0 tensor(3.7007e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.7007e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0053, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0053, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630252
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.3761577606201172
overhead3:: 0.581791877746582
overhead4:: 1.731041669845581
overhead5:: 0
time_provenance:: 3.0136337280273438
curr_diff: 0 tensor(7.8669e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.8669e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0053, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0053, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630252
repetition 1
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 1 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 1.893553
Train - Epoch 0, Batch: 10, Loss: 1.378991
Train - Epoch 0, Batch: 20, Loss: 1.245349
Train - Epoch 0, Batch: 30, Loss: 1.201828
Train - Epoch 1, Batch: 0, Loss: 1.196149
Train - Epoch 1, Batch: 10, Loss: 1.172828
Train - Epoch 1, Batch: 20, Loss: 1.142377
Train - Epoch 1, Batch: 30, Loss: 1.146972
Train - Epoch 2, Batch: 0, Loss: 1.127873
Train - Epoch 2, Batch: 10, Loss: 1.124759
Train - Epoch 2, Batch: 20, Loss: 1.116741
Train - Epoch 2, Batch: 30, Loss: 1.102021
Train - Epoch 3, Batch: 0, Loss: 1.099466
Train - Epoch 3, Batch: 10, Loss: 1.105611
Train - Epoch 3, Batch: 20, Loss: 1.094271
Train - Epoch 3, Batch: 30, Loss: 1.086899
Train - Epoch 4, Batch: 0, Loss: 1.066030
Train - Epoch 4, Batch: 10, Loss: 1.072645
Train - Epoch 4, Batch: 20, Loss: 1.062201
Train - Epoch 4, Batch: 30, Loss: 1.066118
Train - Epoch 5, Batch: 0, Loss: 1.064826
Train - Epoch 5, Batch: 10, Loss: 1.058790
Train - Epoch 5, Batch: 20, Loss: 1.044280
Train - Epoch 5, Batch: 30, Loss: 1.041951
Train - Epoch 6, Batch: 0, Loss: 1.039462
Train - Epoch 6, Batch: 10, Loss: 1.032848
Train - Epoch 6, Batch: 20, Loss: 1.038146
Train - Epoch 6, Batch: 30, Loss: 1.034840
Train - Epoch 7, Batch: 0, Loss: 1.018311
Train - Epoch 7, Batch: 10, Loss: 1.025052
Train - Epoch 7, Batch: 20, Loss: 1.015553
Train - Epoch 7, Batch: 30, Loss: 1.021387
Train - Epoch 8, Batch: 0, Loss: 1.015744
Train - Epoch 8, Batch: 10, Loss: 0.998536
Train - Epoch 8, Batch: 20, Loss: 1.007325
Train - Epoch 8, Batch: 30, Loss: 0.986263
Train - Epoch 9, Batch: 0, Loss: 1.008104
Train - Epoch 9, Batch: 10, Loss: 1.001244
Train - Epoch 9, Batch: 20, Loss: 0.994390
Train - Epoch 9, Batch: 30, Loss: 0.989696
Train - Epoch 10, Batch: 0, Loss: 0.987626
Train - Epoch 10, Batch: 10, Loss: 0.989787
Train - Epoch 10, Batch: 20, Loss: 0.981815
Train - Epoch 10, Batch: 30, Loss: 0.982535
Train - Epoch 11, Batch: 0, Loss: 0.987930
Train - Epoch 11, Batch: 10, Loss: 0.980562
Train - Epoch 11, Batch: 20, Loss: 0.984095
Train - Epoch 11, Batch: 30, Loss: 0.968558
Train - Epoch 12, Batch: 0, Loss: 0.969786
Train - Epoch 12, Batch: 10, Loss: 0.972533
Train - Epoch 12, Batch: 20, Loss: 0.964393
Train - Epoch 12, Batch: 30, Loss: 0.979861
Train - Epoch 13, Batch: 0, Loss: 0.965457
Train - Epoch 13, Batch: 10, Loss: 0.963683
Train - Epoch 13, Batch: 20, Loss: 0.955647
Train - Epoch 13, Batch: 30, Loss: 0.965117
Train - Epoch 14, Batch: 0, Loss: 0.956846
Train - Epoch 14, Batch: 10, Loss: 0.957682
Train - Epoch 14, Batch: 20, Loss: 0.952335
Train - Epoch 14, Batch: 30, Loss: 0.951277
Train - Epoch 15, Batch: 0, Loss: 0.962660
Train - Epoch 15, Batch: 10, Loss: 0.956306
Train - Epoch 15, Batch: 20, Loss: 0.946061
Train - Epoch 15, Batch: 30, Loss: 0.953914
Train - Epoch 16, Batch: 0, Loss: 0.943159
Train - Epoch 16, Batch: 10, Loss: 0.947749
Train - Epoch 16, Batch: 20, Loss: 0.945773
Train - Epoch 16, Batch: 30, Loss: 0.939060
Train - Epoch 17, Batch: 0, Loss: 0.949040
Train - Epoch 17, Batch: 10, Loss: 0.945714
Train - Epoch 17, Batch: 20, Loss: 0.940176
Train - Epoch 17, Batch: 30, Loss: 0.937558
Train - Epoch 18, Batch: 0, Loss: 0.939371
Train - Epoch 18, Batch: 10, Loss: 0.930094
Train - Epoch 18, Batch: 20, Loss: 0.939706
Train - Epoch 18, Batch: 30, Loss: 0.930800
Train - Epoch 19, Batch: 0, Loss: 0.938958
Train - Epoch 19, Batch: 10, Loss: 0.929507
Train - Epoch 19, Batch: 20, Loss: 0.935255
Train - Epoch 19, Batch: 30, Loss: 0.924620
Train - Epoch 20, Batch: 0, Loss: 0.927239
Train - Epoch 20, Batch: 10, Loss: 0.926238
Train - Epoch 20, Batch: 20, Loss: 0.923209
Train - Epoch 20, Batch: 30, Loss: 0.928064
Train - Epoch 21, Batch: 0, Loss: 0.936768
Train - Epoch 21, Batch: 10, Loss: 0.917335
Train - Epoch 21, Batch: 20, Loss: 0.923308
Train - Epoch 21, Batch: 30, Loss: 0.919568
Train - Epoch 22, Batch: 0, Loss: 0.921574
Train - Epoch 22, Batch: 10, Loss: 0.922058
Train - Epoch 22, Batch: 20, Loss: 0.921653
Train - Epoch 22, Batch: 30, Loss: 0.914639
Train - Epoch 23, Batch: 0, Loss: 0.921586
Train - Epoch 23, Batch: 10, Loss: 0.922933
Train - Epoch 23, Batch: 20, Loss: 0.908655
Train - Epoch 23, Batch: 30, Loss: 0.928545
Train - Epoch 24, Batch: 0, Loss: 0.910492
Train - Epoch 24, Batch: 10, Loss: 0.912539
Train - Epoch 24, Batch: 20, Loss: 0.912632
Train - Epoch 24, Batch: 30, Loss: 0.916416
Train - Epoch 25, Batch: 0, Loss: 0.911489
Train - Epoch 25, Batch: 10, Loss: 0.916264
Train - Epoch 25, Batch: 20, Loss: 0.917771
Train - Epoch 25, Batch: 30, Loss: 0.906320
Train - Epoch 26, Batch: 0, Loss: 0.918164
Train - Epoch 26, Batch: 10, Loss: 0.906494
Train - Epoch 26, Batch: 20, Loss: 0.907485
Train - Epoch 26, Batch: 30, Loss: 0.900375
Train - Epoch 27, Batch: 0, Loss: 0.902216
Train - Epoch 27, Batch: 10, Loss: 0.907929
Train - Epoch 27, Batch: 20, Loss: 0.904685
Train - Epoch 27, Batch: 30, Loss: 0.906797
Train - Epoch 28, Batch: 0, Loss: 0.911001
Train - Epoch 28, Batch: 10, Loss: 0.904990
Train - Epoch 28, Batch: 20, Loss: 0.899064
Train - Epoch 28, Batch: 30, Loss: 0.897540
Train - Epoch 29, Batch: 0, Loss: 0.901594
Train - Epoch 29, Batch: 10, Loss: 0.900386
Train - Epoch 29, Batch: 20, Loss: 0.900142
Train - Epoch 29, Batch: 30, Loss: 0.892773
Train - Epoch 30, Batch: 0, Loss: 0.902002
Train - Epoch 30, Batch: 10, Loss: 0.905586
Train - Epoch 30, Batch: 20, Loss: 0.901334
Train - Epoch 30, Batch: 30, Loss: 0.881892
Train - Epoch 31, Batch: 0, Loss: 0.896421
Train - Epoch 31, Batch: 10, Loss: 0.889351
Train - Epoch 31, Batch: 20, Loss: 0.897231
Train - Epoch 31, Batch: 30, Loss: 0.894337
training_time:: 3.584170341491699
training time full:: 3.584212303161621
provenance prepare time:: 4.76837158203125e-07
Test Avg. Loss: 0.000070, Accuracy: 0.629340
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 5229
training time is 2.0406036376953125
overhead:: 0
overhead2:: 0
time_baseline:: 2.042670965194702
curr_diff: 0 tensor(0.0053, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0053, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629494
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.01838064193725586
overhead3:: 0.028883934020996094
overhead4:: 0.1112060546875
overhead5:: 0
time_provenance:: 0.9206035137176514
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0056, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0056, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629443
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.02283644676208496
overhead3:: 0.035021066665649414
overhead4:: 0.13531208038330078
overhead5:: 0
time_provenance:: 0.992621898651123
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0058, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0058, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629443
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.025199413299560547
overhead3:: 0.05663251876831055
overhead4:: 0.12919139862060547
overhead5:: 0
time_provenance:: 1.009690523147583
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0057, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0057, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629563
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.02587103843688965
overhead3:: 0.03931593894958496
overhead4:: 0.15178537368774414
overhead5:: 0
time_provenance:: 0.988316535949707
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0059, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0059, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629443
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.03205084800720215
overhead3:: 0.04845309257507324
overhead4:: 0.22331714630126953
overhead5:: 0
time_provenance:: 1.0907647609710693
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0055, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0055, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629460
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.03685808181762695
overhead3:: 0.052594661712646484
overhead4:: 0.22100114822387695
overhead5:: 0
time_provenance:: 1.1317589282989502
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0055, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0055, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629460
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.039846181869506836
overhead3:: 0.060558319091796875
overhead4:: 0.27134227752685547
overhead5:: 0
time_provenance:: 1.1803596019744873
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0056, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0056, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629460
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.03911995887756348
overhead3:: 0.059627532958984375
overhead4:: 0.28260159492492676
overhead5:: 0
time_provenance:: 1.1573631763458252
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0056, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0056, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629460
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.06294870376586914
overhead3:: 0.08576560020446777
overhead4:: 0.4071831703186035
overhead5:: 0
time_provenance:: 1.3571534156799316
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0054, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0054, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629477
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.06130838394165039
overhead3:: 0.0877082347869873
overhead4:: 0.39640212059020996
overhead5:: 0
time_provenance:: 1.3159689903259277
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0054, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0054, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629477
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.06910133361816406
overhead3:: 0.09462547302246094
overhead4:: 0.41338419914245605
overhead5:: 0
time_provenance:: 1.3799026012420654
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0054, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0054, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629477
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.07907795906066895
overhead3:: 0.11342835426330566
overhead4:: 0.45337462425231934
overhead5:: 0
time_provenance:: 1.5621297359466553
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0054, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0054, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629477
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.14334917068481445
overhead3:: 0.20363402366638184
overhead4:: 0.9660918712615967
overhead5:: 0
time_provenance:: 2.0611412525177
curr_diff: 0 tensor(3.6930e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.6930e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0053, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0053, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629494
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.14626765251159668
overhead3:: 0.20365333557128906
overhead4:: 0.9622938632965088
overhead5:: 0
time_provenance:: 2.0661168098449707
curr_diff: 0 tensor(3.7807e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.7807e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0053, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0053, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629494
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.15505361557006836
overhead3:: 0.22067952156066895
overhead4:: 0.957284688949585
overhead5:: 0
time_provenance:: 2.0924723148345947
curr_diff: 0 tensor(3.8741e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.8741e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0053, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0053, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629494
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.16910576820373535
overhead3:: 0.22579598426818848
overhead4:: 1.0453259944915771
overhead5:: 0
time_provenance:: 2.2587921619415283
curr_diff: 0 tensor(3.9039e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.9039e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0053, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0053, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629494
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.32820773124694824
overhead3:: 0.4593384265899658
overhead4:: 1.5921189785003662
overhead5:: 0
time_provenance:: 2.685520648956299
curr_diff: 0 tensor(1.3624e-15, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3624e-15, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0053, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0053, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629494
repetition 2
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 1.969875
Train - Epoch 0, Batch: 10, Loss: 1.385090
Train - Epoch 0, Batch: 20, Loss: 1.243390
Train - Epoch 0, Batch: 30, Loss: 1.174767
Train - Epoch 1, Batch: 0, Loss: 1.184246
Train - Epoch 1, Batch: 10, Loss: 1.162901
Train - Epoch 1, Batch: 20, Loss: 1.146951
Train - Epoch 1, Batch: 30, Loss: 1.121130
Train - Epoch 2, Batch: 0, Loss: 1.127411
Train - Epoch 2, Batch: 10, Loss: 1.125783
Train - Epoch 2, Batch: 20, Loss: 1.101687
Train - Epoch 2, Batch: 30, Loss: 1.092866
Train - Epoch 3, Batch: 0, Loss: 1.103187
Train - Epoch 3, Batch: 10, Loss: 1.093212
Train - Epoch 3, Batch: 20, Loss: 1.079618
Train - Epoch 3, Batch: 30, Loss: 1.073647
Train - Epoch 4, Batch: 0, Loss: 1.067249
Train - Epoch 4, Batch: 10, Loss: 1.057279
Train - Epoch 4, Batch: 20, Loss: 1.065308
Train - Epoch 4, Batch: 30, Loss: 1.067563
Train - Epoch 5, Batch: 0, Loss: 1.048730
Train - Epoch 5, Batch: 10, Loss: 1.041847
Train - Epoch 5, Batch: 20, Loss: 1.046131
Train - Epoch 5, Batch: 30, Loss: 1.029320
Train - Epoch 6, Batch: 0, Loss: 1.035705
Train - Epoch 6, Batch: 10, Loss: 1.032474
Train - Epoch 6, Batch: 20, Loss: 1.018128
Train - Epoch 6, Batch: 30, Loss: 1.021532
Train - Epoch 7, Batch: 0, Loss: 1.015873
Train - Epoch 7, Batch: 10, Loss: 1.028121
Train - Epoch 7, Batch: 20, Loss: 1.007694
Train - Epoch 7, Batch: 30, Loss: 1.005829
Train - Epoch 8, Batch: 0, Loss: 1.002940
Train - Epoch 8, Batch: 10, Loss: 1.002399
Train - Epoch 8, Batch: 20, Loss: 1.005184
Train - Epoch 8, Batch: 30, Loss: 1.000314
Train - Epoch 9, Batch: 0, Loss: 0.986956
Train - Epoch 9, Batch: 10, Loss: 0.992752
Train - Epoch 9, Batch: 20, Loss: 0.994624
Train - Epoch 9, Batch: 30, Loss: 0.986458
Train - Epoch 10, Batch: 0, Loss: 0.991879
Train - Epoch 10, Batch: 10, Loss: 0.983172
Train - Epoch 10, Batch: 20, Loss: 0.974966
Train - Epoch 10, Batch: 30, Loss: 0.974050
Train - Epoch 11, Batch: 0, Loss: 0.974804
Train - Epoch 11, Batch: 10, Loss: 0.979498
Train - Epoch 11, Batch: 20, Loss: 0.977129
Train - Epoch 11, Batch: 30, Loss: 0.968152
Train - Epoch 12, Batch: 0, Loss: 0.965222
Train - Epoch 12, Batch: 10, Loss: 0.967804
Train - Epoch 12, Batch: 20, Loss: 0.974200
Train - Epoch 12, Batch: 30, Loss: 0.963577
Train - Epoch 13, Batch: 0, Loss: 0.959435
Train - Epoch 13, Batch: 10, Loss: 0.967314
Train - Epoch 13, Batch: 20, Loss: 0.966721
Train - Epoch 13, Batch: 30, Loss: 0.957283
Train - Epoch 14, Batch: 0, Loss: 0.967436
Train - Epoch 14, Batch: 10, Loss: 0.956798
Train - Epoch 14, Batch: 20, Loss: 0.962208
Train - Epoch 14, Batch: 30, Loss: 0.944831
Train - Epoch 15, Batch: 0, Loss: 0.943511
Train - Epoch 15, Batch: 10, Loss: 0.956649
Train - Epoch 15, Batch: 20, Loss: 0.949719
Train - Epoch 15, Batch: 30, Loss: 0.952931
Train - Epoch 16, Batch: 0, Loss: 0.936368
Train - Epoch 16, Batch: 10, Loss: 0.951780
Train - Epoch 16, Batch: 20, Loss: 0.951955
Train - Epoch 16, Batch: 30, Loss: 0.931056
Train - Epoch 17, Batch: 0, Loss: 0.949790
Train - Epoch 17, Batch: 10, Loss: 0.943640
Train - Epoch 17, Batch: 20, Loss: 0.944644
Train - Epoch 17, Batch: 30, Loss: 0.935814
Train - Epoch 18, Batch: 0, Loss: 0.933835
Train - Epoch 18, Batch: 10, Loss: 0.938027
Train - Epoch 18, Batch: 20, Loss: 0.928455
Train - Epoch 18, Batch: 30, Loss: 0.926265
Train - Epoch 19, Batch: 0, Loss: 0.935967
Train - Epoch 19, Batch: 10, Loss: 0.938823
Train - Epoch 19, Batch: 20, Loss: 0.929148
Train - Epoch 19, Batch: 30, Loss: 0.923261
Train - Epoch 20, Batch: 0, Loss: 0.924986
Train - Epoch 20, Batch: 10, Loss: 0.914712
Train - Epoch 20, Batch: 20, Loss: 0.929928
Train - Epoch 20, Batch: 30, Loss: 0.917150
Train - Epoch 21, Batch: 0, Loss: 0.932299
Train - Epoch 21, Batch: 10, Loss: 0.920676
Train - Epoch 21, Batch: 20, Loss: 0.920745
Train - Epoch 21, Batch: 30, Loss: 0.922336
Train - Epoch 22, Batch: 0, Loss: 0.923743
Train - Epoch 22, Batch: 10, Loss: 0.917624
Train - Epoch 22, Batch: 20, Loss: 0.918091
Train - Epoch 22, Batch: 30, Loss: 0.914893
Train - Epoch 23, Batch: 0, Loss: 0.910525
Train - Epoch 23, Batch: 10, Loss: 0.914421
Train - Epoch 23, Batch: 20, Loss: 0.909250
Train - Epoch 23, Batch: 30, Loss: 0.912715
Train - Epoch 24, Batch: 0, Loss: 0.913882
Train - Epoch 24, Batch: 10, Loss: 0.923969
Train - Epoch 24, Batch: 20, Loss: 0.905611
Train - Epoch 24, Batch: 30, Loss: 0.915882
Train - Epoch 25, Batch: 0, Loss: 0.909261
Train - Epoch 25, Batch: 10, Loss: 0.911524
Train - Epoch 25, Batch: 20, Loss: 0.904314
Train - Epoch 25, Batch: 30, Loss: 0.915670
Train - Epoch 26, Batch: 0, Loss: 0.902436
Train - Epoch 26, Batch: 10, Loss: 0.909638
Train - Epoch 26, Batch: 20, Loss: 0.900846
Train - Epoch 26, Batch: 30, Loss: 0.913396
Train - Epoch 27, Batch: 0, Loss: 0.912292
Train - Epoch 27, Batch: 10, Loss: 0.901853
Train - Epoch 27, Batch: 20, Loss: 0.893994
Train - Epoch 27, Batch: 30, Loss: 0.900786
Train - Epoch 28, Batch: 0, Loss: 0.900031
Train - Epoch 28, Batch: 10, Loss: 0.902679
Train - Epoch 28, Batch: 20, Loss: 0.907983
Train - Epoch 28, Batch: 30, Loss: 0.901434
Train - Epoch 29, Batch: 0, Loss: 0.897694
Train - Epoch 29, Batch: 10, Loss: 0.906330
Train - Epoch 29, Batch: 20, Loss: 0.901833
Train - Epoch 29, Batch: 30, Loss: 0.893189
Train - Epoch 30, Batch: 0, Loss: 0.904891
Train - Epoch 30, Batch: 10, Loss: 0.906217
Train - Epoch 30, Batch: 20, Loss: 0.902899
Train - Epoch 30, Batch: 30, Loss: 0.889565
Train - Epoch 31, Batch: 0, Loss: 0.900219
Train - Epoch 31, Batch: 10, Loss: 0.891311
Train - Epoch 31, Batch: 20, Loss: 0.904377
Train - Epoch 31, Batch: 30, Loss: 0.893801
training_time:: 3.9844419956207275
training time full:: 3.9844865798950195
provenance prepare time:: 7.152557373046875e-07
Test Avg. Loss: 0.000070, Accuracy: 0.630544
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 5229
training time is 2.0585439205169678
overhead:: 0
overhead2:: 0
time_baseline:: 2.0601954460144043
curr_diff: 0 tensor(0.0053, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0053, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630682
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.018463850021362305
overhead3:: 0.03825974464416504
overhead4:: 0.10698246955871582
overhead5:: 0
time_provenance:: 0.9495208263397217
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0057, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0057, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630630
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.02295398712158203
overhead3:: 0.035176992416381836
overhead4:: 0.12997698783874512
overhead5:: 0
time_provenance:: 1.0626237392425537
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0057, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0057, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630665
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.022535324096679688
overhead3:: 0.044155120849609375
overhead4:: 0.13703441619873047
overhead5:: 0
time_provenance:: 0.9331073760986328
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0058, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0058, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630630
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.026047945022583008
overhead3:: 0.040384769439697266
overhead4:: 0.16017365455627441
overhead5:: 0
time_provenance:: 1.0182054042816162
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0058, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0058, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630665
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.03371453285217285
overhead3:: 0.04926919937133789
overhead4:: 0.24372339248657227
overhead5:: 0
time_provenance:: 1.125511884689331
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0055, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0055, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630648
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.03386092185974121
overhead3:: 0.049802541732788086
overhead4:: 0.22504925727844238
overhead5:: 0
time_provenance:: 1.066460371017456
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0055, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0055, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630648
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.039337873458862305
overhead3:: 0.05636000633239746
overhead4:: 0.24956440925598145
overhead5:: 0
time_provenance:: 1.149204969406128
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0056, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0056, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630648
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.043779850006103516
overhead3:: 0.07171368598937988
overhead4:: 0.258603572845459
overhead5:: 0
time_provenance:: 1.165564775466919
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0056, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0056, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630648
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.062135934829711914
overhead3:: 0.08619856834411621
overhead4:: 0.40491175651550293
overhead5:: 0
time_provenance:: 1.3451015949249268
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0054, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0054, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630699
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.06066441535949707
overhead3:: 0.09478545188903809
overhead4:: 0.4410409927368164
overhead5:: 0
time_provenance:: 1.378079891204834
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0054, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0054, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630699
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.08909940719604492
overhead3:: 0.11804771423339844
overhead4:: 0.5175294876098633
overhead5:: 0
time_provenance:: 1.8199443817138672
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0054, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0054, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630699
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.07951998710632324
overhead3:: 0.1092524528503418
overhead4:: 0.47721123695373535
overhead5:: 0
time_provenance:: 1.6104412078857422
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0054, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0054, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630699
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.14109063148498535
overhead3:: 0.20038628578186035
overhead4:: 0.9744381904602051
overhead5:: 0
time_provenance:: 2.0593249797821045
curr_diff: 0 tensor(4.2429e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.2429e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0053, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0053, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630665
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.14570045471191406
overhead3:: 0.21232819557189941
overhead4:: 0.991448163986206
overhead5:: 0
time_provenance:: 2.102977752685547
curr_diff: 0 tensor(4.3580e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.3580e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0053, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0053, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630665
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.14639639854431152
overhead3:: 0.22422075271606445
overhead4:: 0.9494833946228027
overhead5:: 0
time_provenance:: 2.083472728729248
curr_diff: 0 tensor(4.4459e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.4459e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0053, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0053, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630665
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.147064208984375
overhead3:: 0.22777223587036133
overhead4:: 0.9667341709136963
overhead5:: 0
time_provenance:: 2.0895707607269287
curr_diff: 0 tensor(4.5367e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.5367e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0053, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0053, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630665
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.33640098571777344
overhead3:: 0.5056896209716797
overhead4:: 1.676626205444336
overhead5:: 0
time_provenance:: 2.831552267074585
curr_diff: 0 tensor(1.1299e-15, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1299e-15, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0053, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0053, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630682
repetition 3
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 3 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.049233
Train - Epoch 0, Batch: 10, Loss: 1.402010
Train - Epoch 0, Batch: 20, Loss: 1.241362
Train - Epoch 0, Batch: 30, Loss: 1.176109
Train - Epoch 1, Batch: 0, Loss: 1.181793
Train - Epoch 1, Batch: 10, Loss: 1.162831
Train - Epoch 1, Batch: 20, Loss: 1.135204
Train - Epoch 1, Batch: 30, Loss: 1.132026
Train - Epoch 2, Batch: 0, Loss: 1.125306
Train - Epoch 2, Batch: 10, Loss: 1.119996
Train - Epoch 2, Batch: 20, Loss: 1.104421
Train - Epoch 2, Batch: 30, Loss: 1.093555
Train - Epoch 3, Batch: 0, Loss: 1.082825
Train - Epoch 3, Batch: 10, Loss: 1.082418
Train - Epoch 3, Batch: 20, Loss: 1.073816
Train - Epoch 3, Batch: 30, Loss: 1.064462
Train - Epoch 4, Batch: 0, Loss: 1.082489
Train - Epoch 4, Batch: 10, Loss: 1.065328
Train - Epoch 4, Batch: 20, Loss: 1.058211
Train - Epoch 4, Batch: 30, Loss: 1.045987
Train - Epoch 5, Batch: 0, Loss: 1.053603
Train - Epoch 5, Batch: 10, Loss: 1.041780
Train - Epoch 5, Batch: 20, Loss: 1.050642
Train - Epoch 5, Batch: 30, Loss: 1.042290
Train - Epoch 6, Batch: 0, Loss: 1.031534
Train - Epoch 6, Batch: 10, Loss: 1.035453
Train - Epoch 6, Batch: 20, Loss: 1.027103
Train - Epoch 6, Batch: 30, Loss: 1.019412
Train - Epoch 7, Batch: 0, Loss: 1.023937
Train - Epoch 7, Batch: 10, Loss: 1.016967
Train - Epoch 7, Batch: 20, Loss: 1.013443
Train - Epoch 7, Batch: 30, Loss: 1.005234
Train - Epoch 8, Batch: 0, Loss: 1.007476
Train - Epoch 8, Batch: 10, Loss: 1.001693
Train - Epoch 8, Batch: 20, Loss: 1.001668
Train - Epoch 8, Batch: 30, Loss: 0.998769
Train - Epoch 9, Batch: 0, Loss: 1.003679
Train - Epoch 9, Batch: 10, Loss: 0.983415
Train - Epoch 9, Batch: 20, Loss: 0.988977
Train - Epoch 9, Batch: 30, Loss: 0.995694
Train - Epoch 10, Batch: 0, Loss: 0.989816
Train - Epoch 10, Batch: 10, Loss: 0.979588
Train - Epoch 10, Batch: 20, Loss: 0.985154
Train - Epoch 10, Batch: 30, Loss: 0.975958
Train - Epoch 11, Batch: 0, Loss: 0.980562
Train - Epoch 11, Batch: 10, Loss: 0.981538
Train - Epoch 11, Batch: 20, Loss: 0.982775
Train - Epoch 11, Batch: 30, Loss: 0.968042
Train - Epoch 12, Batch: 0, Loss: 0.961943
Train - Epoch 12, Batch: 10, Loss: 0.973131
Train - Epoch 12, Batch: 20, Loss: 0.964542
Train - Epoch 12, Batch: 30, Loss: 0.967114
Train - Epoch 13, Batch: 0, Loss: 0.967418
Train - Epoch 13, Batch: 10, Loss: 0.967841
Train - Epoch 13, Batch: 20, Loss: 0.950130
Train - Epoch 13, Batch: 30, Loss: 0.950372
Train - Epoch 14, Batch: 0, Loss: 0.955471
Train - Epoch 14, Batch: 10, Loss: 0.954956
Train - Epoch 14, Batch: 20, Loss: 0.948901
Train - Epoch 14, Batch: 30, Loss: 0.948829
Train - Epoch 15, Batch: 0, Loss: 0.954755
Train - Epoch 15, Batch: 10, Loss: 0.949821
Train - Epoch 15, Batch: 20, Loss: 0.946767
Train - Epoch 15, Batch: 30, Loss: 0.942337
Train - Epoch 16, Batch: 0, Loss: 0.943814
Train - Epoch 16, Batch: 10, Loss: 0.944416
Train - Epoch 16, Batch: 20, Loss: 0.942268
Train - Epoch 16, Batch: 30, Loss: 0.939318
Train - Epoch 17, Batch: 0, Loss: 0.948075
Train - Epoch 17, Batch: 10, Loss: 0.932153
Train - Epoch 17, Batch: 20, Loss: 0.934893
Train - Epoch 17, Batch: 30, Loss: 0.928978
Train - Epoch 18, Batch: 0, Loss: 0.940965
Train - Epoch 18, Batch: 10, Loss: 0.942298
Train - Epoch 18, Batch: 20, Loss: 0.933773
Train - Epoch 18, Batch: 30, Loss: 0.930009
Train - Epoch 19, Batch: 0, Loss: 0.929401
Train - Epoch 19, Batch: 10, Loss: 0.932591
Train - Epoch 19, Batch: 20, Loss: 0.927383
Train - Epoch 19, Batch: 30, Loss: 0.918616
Train - Epoch 20, Batch: 0, Loss: 0.933263
Train - Epoch 20, Batch: 10, Loss: 0.925934
Train - Epoch 20, Batch: 20, Loss: 0.917796
Train - Epoch 20, Batch: 30, Loss: 0.927310
Train - Epoch 21, Batch: 0, Loss: 0.920078
Train - Epoch 21, Batch: 10, Loss: 0.924945
Train - Epoch 21, Batch: 20, Loss: 0.921550
Train - Epoch 21, Batch: 30, Loss: 0.909115
Train - Epoch 22, Batch: 0, Loss: 0.929925
Train - Epoch 22, Batch: 10, Loss: 0.930606
Train - Epoch 22, Batch: 20, Loss: 0.930304
Train - Epoch 22, Batch: 30, Loss: 0.917162
Train - Epoch 23, Batch: 0, Loss: 0.920630
Train - Epoch 23, Batch: 10, Loss: 0.905169
Train - Epoch 23, Batch: 20, Loss: 0.916304
Train - Epoch 23, Batch: 30, Loss: 0.917298
Train - Epoch 24, Batch: 0, Loss: 0.919919
Train - Epoch 24, Batch: 10, Loss: 0.918145
Train - Epoch 24, Batch: 20, Loss: 0.918177
Train - Epoch 24, Batch: 30, Loss: 0.905987
Train - Epoch 25, Batch: 0, Loss: 0.908206
Train - Epoch 25, Batch: 10, Loss: 0.909343
Train - Epoch 25, Batch: 20, Loss: 0.905130
Train - Epoch 25, Batch: 30, Loss: 0.905301
Train - Epoch 26, Batch: 0, Loss: 0.918372
Train - Epoch 26, Batch: 10, Loss: 0.902363
Train - Epoch 26, Batch: 20, Loss: 0.904160
Train - Epoch 26, Batch: 30, Loss: 0.902329
Train - Epoch 27, Batch: 0, Loss: 0.917433
Train - Epoch 27, Batch: 10, Loss: 0.909221
Train - Epoch 27, Batch: 20, Loss: 0.897299
Train - Epoch 27, Batch: 30, Loss: 0.908337
Train - Epoch 28, Batch: 0, Loss: 0.900051
Train - Epoch 28, Batch: 10, Loss: 0.900825
Train - Epoch 28, Batch: 20, Loss: 0.890750
Train - Epoch 28, Batch: 30, Loss: 0.898305
Train - Epoch 29, Batch: 0, Loss: 0.910147
Train - Epoch 29, Batch: 10, Loss: 0.886581
Train - Epoch 29, Batch: 20, Loss: 0.905651
Train - Epoch 29, Batch: 30, Loss: 0.890087
Train - Epoch 30, Batch: 0, Loss: 0.900071
Train - Epoch 30, Batch: 10, Loss: 0.889155
Train - Epoch 30, Batch: 20, Loss: 0.886632
Train - Epoch 30, Batch: 30, Loss: 0.893088
Train - Epoch 31, Batch: 0, Loss: 0.902863
Train - Epoch 31, Batch: 10, Loss: 0.882723
Train - Epoch 31, Batch: 20, Loss: 0.894445
Train - Epoch 31, Batch: 30, Loss: 0.881842
training_time:: 3.880444288253784
training time full:: 3.8804872035980225
provenance prepare time:: 4.76837158203125e-07
Test Avg. Loss: 0.000070, Accuracy: 0.630820
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 5229
training time is 2.1055514812469482
overhead:: 0
overhead2:: 0
time_baseline:: 2.106846570968628
curr_diff: 0 tensor(0.0053, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0053, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630958
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.018464088439941406
overhead3:: 0.028539419174194336
overhead4:: 0.10920453071594238
overhead5:: 0
time_provenance:: 0.9541893005371094
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0058, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0058, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631044
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.020247459411621094
overhead3:: 0.03117513656616211
overhead4:: 0.12611699104309082
overhead5:: 0
time_provenance:: 0.9491264820098877
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0059, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0059, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631044
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.022799253463745117
overhead3:: 0.04038500785827637
overhead4:: 0.14187002182006836
overhead5:: 0
time_provenance:: 0.9431273937225342
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0058, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0058, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631044
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.025612354278564453
overhead3:: 0.04805421829223633
overhead4:: 0.16675806045532227
overhead5:: 0
time_provenance:: 1.0045421123504639
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0059, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0059, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.631044
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.03371453285217285
overhead3:: 0.0586245059967041
overhead4:: 0.19772839546203613
overhead5:: 0
time_provenance:: 1.0789225101470947
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0056, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0056, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630992
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.04095864295959473
overhead3:: 0.07166314125061035
overhead4:: 0.2272348403930664
overhead5:: 0
time_provenance:: 1.1684551239013672
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0056, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0056, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630992
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.04011273384094238
overhead3:: 0.05680108070373535
overhead4:: 0.2529599666595459
overhead5:: 0
time_provenance:: 1.1684587001800537
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0056, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0056, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630992
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.03830909729003906
overhead3:: 0.07717108726501465
overhead4:: 0.24978137016296387
overhead5:: 0
time_provenance:: 1.1205081939697266
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0056, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0056, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630992
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.058196067810058594
overhead3:: 0.08478832244873047
overhead4:: 0.40451550483703613
overhead5:: 0
time_provenance:: 1.330216646194458
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0054, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0054, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630958
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.0805809497833252
overhead3:: 0.11217403411865234
overhead4:: 0.4903678894042969
overhead5:: 0
time_provenance:: 1.748265266418457
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0054, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0054, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630958
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.08229470252990723
overhead3:: 0.11342644691467285
overhead4:: 0.5333991050720215
overhead5:: 0
time_provenance:: 1.790318489074707
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0054, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0054, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630958
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.07791018486022949
overhead3:: 0.11490321159362793
overhead4:: 0.45500826835632324
overhead5:: 0
time_provenance:: 1.4831469058990479
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0054, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0054, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630958
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.1457831859588623
overhead3:: 0.21492528915405273
overhead4:: 0.9836335182189941
overhead5:: 0
time_provenance:: 2.1094064712524414
curr_diff: 0 tensor(2.1816e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1816e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0053, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0053, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630958
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.1446831226348877
overhead3:: 0.2042238712310791
overhead4:: 0.9524288177490234
overhead5:: 0
time_provenance:: 2.0458462238311768
curr_diff: 0 tensor(2.2530e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.2530e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0053, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0053, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630958
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.19717812538146973
overhead3:: 0.2676677703857422
overhead4:: 1.1712512969970703
overhead5:: 0
time_provenance:: 2.68270206451416
curr_diff: 0 tensor(2.2495e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.2495e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0053, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0053, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630958
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.14745140075683594
overhead3:: 0.20831298828125
overhead4:: 0.9588603973388672
overhead5:: 0
time_provenance:: 2.0616607666015625
curr_diff: 0 tensor(2.2828e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.2828e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0053, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0053, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630958
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.33664608001708984
overhead3:: 0.5024282932281494
overhead4:: 1.621140956878662
overhead5:: 0
time_provenance:: 2.767055034637451
curr_diff: 0 tensor(9.5707e-16, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.5707e-16, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0053, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0053, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630958
repetition 4
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 4 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 1.867209
Train - Epoch 0, Batch: 10, Loss: 1.349724
Train - Epoch 0, Batch: 20, Loss: 1.212916
Train - Epoch 0, Batch: 30, Loss: 1.168420
Train - Epoch 1, Batch: 0, Loss: 1.170535
Train - Epoch 1, Batch: 10, Loss: 1.159531
Train - Epoch 1, Batch: 20, Loss: 1.136488
Train - Epoch 1, Batch: 30, Loss: 1.118116
Train - Epoch 2, Batch: 0, Loss: 1.120339
Train - Epoch 2, Batch: 10, Loss: 1.115835
Train - Epoch 2, Batch: 20, Loss: 1.098222
Train - Epoch 2, Batch: 30, Loss: 1.086024
Train - Epoch 3, Batch: 0, Loss: 1.099805
Train - Epoch 3, Batch: 10, Loss: 1.092083
Train - Epoch 3, Batch: 20, Loss: 1.077551
Train - Epoch 3, Batch: 30, Loss: 1.069655
Train - Epoch 4, Batch: 0, Loss: 1.064866
Train - Epoch 4, Batch: 10, Loss: 1.059205
Train - Epoch 4, Batch: 20, Loss: 1.061656
Train - Epoch 4, Batch: 30, Loss: 1.052375
Train - Epoch 5, Batch: 0, Loss: 1.052794
Train - Epoch 5, Batch: 10, Loss: 1.046944
Train - Epoch 5, Batch: 20, Loss: 1.042438
Train - Epoch 5, Batch: 30, Loss: 1.033116
Train - Epoch 6, Batch: 0, Loss: 1.035458
Train - Epoch 6, Batch: 10, Loss: 1.026993
Train - Epoch 6, Batch: 20, Loss: 1.020618
Train - Epoch 6, Batch: 30, Loss: 1.015060
Train - Epoch 7, Batch: 0, Loss: 1.015237
Train - Epoch 7, Batch: 10, Loss: 1.011935
Train - Epoch 7, Batch: 20, Loss: 1.005750
Train - Epoch 7, Batch: 30, Loss: 1.012458
Train - Epoch 8, Batch: 0, Loss: 1.008142
Train - Epoch 8, Batch: 10, Loss: 0.997579
Train - Epoch 8, Batch: 20, Loss: 1.001160
Train - Epoch 8, Batch: 30, Loss: 1.000936
Train - Epoch 9, Batch: 0, Loss: 0.999833
Train - Epoch 9, Batch: 10, Loss: 0.992634
Train - Epoch 9, Batch: 20, Loss: 0.986542
Train - Epoch 9, Batch: 30, Loss: 0.987960
Train - Epoch 10, Batch: 0, Loss: 0.977494
Train - Epoch 10, Batch: 10, Loss: 0.985298
Train - Epoch 10, Batch: 20, Loss: 0.989561
Train - Epoch 10, Batch: 30, Loss: 0.982687
Train - Epoch 11, Batch: 0, Loss: 0.975475
Train - Epoch 11, Batch: 10, Loss: 0.979948
Train - Epoch 11, Batch: 20, Loss: 0.976669
Train - Epoch 11, Batch: 30, Loss: 0.967350
Train - Epoch 12, Batch: 0, Loss: 0.969943
Train - Epoch 12, Batch: 10, Loss: 0.969113
Train - Epoch 12, Batch: 20, Loss: 0.964612
Train - Epoch 12, Batch: 30, Loss: 0.968720
Train - Epoch 13, Batch: 0, Loss: 0.960547
Train - Epoch 13, Batch: 10, Loss: 0.959766
Train - Epoch 13, Batch: 20, Loss: 0.949762
Train - Epoch 13, Batch: 30, Loss: 0.967875
Train - Epoch 14, Batch: 0, Loss: 0.966223
Train - Epoch 14, Batch: 10, Loss: 0.954733
Train - Epoch 14, Batch: 20, Loss: 0.943051
Train - Epoch 14, Batch: 30, Loss: 0.955666
Train - Epoch 15, Batch: 0, Loss: 0.956564
Train - Epoch 15, Batch: 10, Loss: 0.948109
Train - Epoch 15, Batch: 20, Loss: 0.956747
Train - Epoch 15, Batch: 30, Loss: 0.949545
Train - Epoch 16, Batch: 0, Loss: 0.947575
Train - Epoch 16, Batch: 10, Loss: 0.950915
Train - Epoch 16, Batch: 20, Loss: 0.943630
Train - Epoch 16, Batch: 30, Loss: 0.950490
Train - Epoch 17, Batch: 0, Loss: 0.943330
Train - Epoch 17, Batch: 10, Loss: 0.944113
Train - Epoch 17, Batch: 20, Loss: 0.936417
Train - Epoch 17, Batch: 30, Loss: 0.949279
Train - Epoch 18, Batch: 0, Loss: 0.940752
Train - Epoch 18, Batch: 10, Loss: 0.940202
Train - Epoch 18, Batch: 20, Loss: 0.932064
Train - Epoch 18, Batch: 30, Loss: 0.937147
Train - Epoch 19, Batch: 0, Loss: 0.936607
Train - Epoch 19, Batch: 10, Loss: 0.931647
Train - Epoch 19, Batch: 20, Loss: 0.932059
Train - Epoch 19, Batch: 30, Loss: 0.935398
Train - Epoch 20, Batch: 0, Loss: 0.921503
Train - Epoch 20, Batch: 10, Loss: 0.923741
Train - Epoch 20, Batch: 20, Loss: 0.927147
Train - Epoch 20, Batch: 30, Loss: 0.932213
Train - Epoch 21, Batch: 0, Loss: 0.918287
Train - Epoch 21, Batch: 10, Loss: 0.925340
Train - Epoch 21, Batch: 20, Loss: 0.922049
Train - Epoch 21, Batch: 30, Loss: 0.918677
Train - Epoch 22, Batch: 0, Loss: 0.931031
Train - Epoch 22, Batch: 10, Loss: 0.922839
Train - Epoch 22, Batch: 20, Loss: 0.910460
Train - Epoch 22, Batch: 30, Loss: 0.906041
Train - Epoch 23, Batch: 0, Loss: 0.916524
Train - Epoch 23, Batch: 10, Loss: 0.920047
Train - Epoch 23, Batch: 20, Loss: 0.904264
Train - Epoch 23, Batch: 30, Loss: 0.918314
Train - Epoch 24, Batch: 0, Loss: 0.912229
Train - Epoch 24, Batch: 10, Loss: 0.911111
Train - Epoch 24, Batch: 20, Loss: 0.918821
Train - Epoch 24, Batch: 30, Loss: 0.912230
Train - Epoch 25, Batch: 0, Loss: 0.906197
Train - Epoch 25, Batch: 10, Loss: 0.911605
Train - Epoch 25, Batch: 20, Loss: 0.905666
Train - Epoch 25, Batch: 30, Loss: 0.909061
Train - Epoch 26, Batch: 0, Loss: 0.911386
Train - Epoch 26, Batch: 10, Loss: 0.915026
Train - Epoch 26, Batch: 20, Loss: 0.902615
Train - Epoch 26, Batch: 30, Loss: 0.902873
Train - Epoch 27, Batch: 0, Loss: 0.902948
Train - Epoch 27, Batch: 10, Loss: 0.906806
Train - Epoch 27, Batch: 20, Loss: 0.897070
Train - Epoch 27, Batch: 30, Loss: 0.914664
Train - Epoch 28, Batch: 0, Loss: 0.897037
Train - Epoch 28, Batch: 10, Loss: 0.903048
Train - Epoch 28, Batch: 20, Loss: 0.905729
Train - Epoch 28, Batch: 30, Loss: 0.903692
Train - Epoch 29, Batch: 0, Loss: 0.898294
Train - Epoch 29, Batch: 10, Loss: 0.901722
Train - Epoch 29, Batch: 20, Loss: 0.905749
Train - Epoch 29, Batch: 30, Loss: 0.898882
Train - Epoch 30, Batch: 0, Loss: 0.902779
Train - Epoch 30, Batch: 10, Loss: 0.902502
Train - Epoch 30, Batch: 20, Loss: 0.896431
Train - Epoch 30, Batch: 30, Loss: 0.895426
Train - Epoch 31, Batch: 0, Loss: 0.898033
Train - Epoch 31, Batch: 10, Loss: 0.890667
Train - Epoch 31, Batch: 20, Loss: 0.898298
Train - Epoch 31, Batch: 30, Loss: 0.888440
training_time:: 3.849989891052246
training time full:: 3.8500359058380127
provenance prepare time:: 4.76837158203125e-07
Test Avg. Loss: 0.000070, Accuracy: 0.629701
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta data size:: 5229
training time is 2.047673225402832
overhead:: 0
overhead2:: 0
time_baseline:: 2.0493414402008057
curr_diff: 0 tensor(0.0053, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0053, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629735
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.017436742782592773
overhead3:: 0.027220964431762695
overhead4:: 0.11069035530090332
overhead5:: 0
time_provenance:: 0.9126486778259277
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0058, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0058, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629735
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.021604061126708984
overhead3:: 0.0438847541809082
overhead4:: 0.12440896034240723
overhead5:: 0
time_provenance:: 0.958441972732544
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0058, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0058, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629787
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.02861952781677246
overhead3:: 0.0506443977355957
overhead4:: 0.15704345703125
overhead5:: 0
time_provenance:: 1.288095235824585
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0059, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0059, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629735
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.02632737159729004
overhead3:: 0.04962515830993652
overhead4:: 0.15909433364868164
overhead5:: 0
time_provenance:: 1.0258643627166748
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0059, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0059, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629787
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.03237009048461914
overhead3:: 0.04857587814331055
overhead4:: 0.222764253616333
overhead5:: 0
time_provenance:: 1.0915963649749756
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0056, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0056, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629735
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.033078670501708984
overhead3:: 0.04896140098571777
overhead4:: 0.22601890563964844
overhead5:: 0
time_provenance:: 1.0997695922851562
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0056, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0056, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629735
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.03669238090515137
overhead3:: 0.0619814395904541
overhead4:: 0.23089098930358887
overhead5:: 0
time_provenance:: 1.101029872894287
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0056, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0056, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629735
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.03815603256225586
overhead3:: 0.05703926086425781
overhead4:: 0.24393463134765625
overhead5:: 0
time_provenance:: 1.130924940109253
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0056, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0056, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629735
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.060658931732177734
overhead3:: 0.08853840827941895
overhead4:: 0.414945125579834
overhead5:: 0
time_provenance:: 1.3626577854156494
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0054, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0054, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629735
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.060694217681884766
overhead3:: 0.08797407150268555
overhead4:: 0.4340956211090088
overhead5:: 0
time_provenance:: 1.3659312725067139
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0054, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0054, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629735
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.06410622596740723
overhead3:: 0.09180736541748047
overhead4:: 0.4073910713195801
overhead5:: 0
time_provenance:: 1.3513238430023193
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0054, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0054, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629735
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.09341883659362793
overhead3:: 0.12217187881469727
overhead4:: 0.5053784847259521
overhead5:: 0
time_provenance:: 1.8078866004943848
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0054, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0054, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629735
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.19709086418151855
overhead3:: 0.26890110969543457
overhead4:: 1.1782679557800293
overhead5:: 0
time_provenance:: 2.7117490768432617
curr_diff: 0 tensor(3.9541e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.9541e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0053, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0053, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629735
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.1454179286956787
overhead3:: 0.2056443691253662
overhead4:: 0.9278116226196289
overhead5:: 0
time_provenance:: 2.0309691429138184
curr_diff: 0 tensor(4.0232e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.0232e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0053, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0053, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629735
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.15003085136413574
overhead3:: 0.2170271873474121
overhead4:: 0.9429471492767334
overhead5:: 0
time_provenance:: 2.058147668838501
curr_diff: 0 tensor(4.0545e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.0545e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0053, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0053, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629735
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.1507434844970703
overhead3:: 0.21085786819458008
overhead4:: 0.9864246845245361
overhead5:: 0
time_provenance:: 2.1108598709106445
curr_diff: 0 tensor(4.0526e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.0526e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0053, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0053, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629735
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.32984399795532227
overhead3:: 0.4753303527832031
overhead4:: 1.6386358737945557
overhead5:: 0
time_provenance:: 2.7542357444763184
curr_diff: 0 tensor(1.0598e-15, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0598e-15, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1., dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0053, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0053, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629735
