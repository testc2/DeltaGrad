varied deletion rate::
varied number of samples::
deletion rate:: 0.00002
python3 generate_rand_ids 0.00002  MNIST5 1
tensor([6575])
batch size:: 60000
repetition 1
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "

Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 109.56511521339417
training time full:: 109.57640337944031
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 120
delta_size:: 1
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 106.19623327255249
time_baseline:: 106.46522068977356
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 120
delta_size:: 1
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 54.619351863861084
curr_diff: 0 tensor(9.0160e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.0160e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
repetition 2
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "

Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 110.17484831809998
training time full:: 110.18644976615906
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 120
delta_size:: 1
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 105.9843270778656
time_baseline:: 106.25384283065796
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 120
delta_size:: 1
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 54.152862310409546
curr_diff: 0 tensor(9.0160e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.0160e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
repetition 3
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "

Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 109.35578989982605
training time full:: 109.36811947822571
provenance prepare time:: 2.384185791015625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 120
delta_size:: 1
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 105.81003952026367
time_baseline:: 106.0803108215332
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 120
delta_size:: 1
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 53.830209493637085
curr_diff: 0 tensor(9.0160e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.0160e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
repetition 4
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "

Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 111.03205847740173
training time full:: 111.04444074630737
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 120
delta_size:: 1
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 105.97602891921997
time_baseline:: 106.25864624977112
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 120
delta_size:: 1
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 54.79591608047485
curr_diff: 0 tensor(9.0160e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.0160e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
repetition 5
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "

Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 108.63987803459167
training time full:: 108.65173506736755
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 120
delta_size:: 1
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 106.39312219619751
time_baseline:: 106.66434049606323
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 120
delta_size:: 1
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 54.50882172584534
curr_diff: 0 tensor(9.0160e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.0160e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
batch size:: 30000
repetition 1
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.944952726364136
training time full:: 52.95203614234924
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 60
delta_size:: 1
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.099724769592285
time_baseline:: 48.23517560958862
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 1
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.542737007141113
curr_diff: 0 tensor(9.7578e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.7578e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
repetition 2
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.29066777229309
training time full:: 52.296947956085205
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 60
delta_size:: 1
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.29586696624756
time_baseline:: 48.43565034866333
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 1
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.5038001537323
curr_diff: 0 tensor(9.7578e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.7578e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
repetition 3
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.65879511833191
training time full:: 52.66528511047363
provenance prepare time:: 2.86102294921875e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 60
delta_size:: 1
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.45739197731018
time_baseline:: 48.59378409385681
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 1
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.8583824634552
curr_diff: 0 tensor(9.7578e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.7578e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
repetition 4
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.616732358932495
training time full:: 52.622945070266724
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 60
delta_size:: 1
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.03816270828247
time_baseline:: 48.17344784736633
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 1
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.707409381866455
curr_diff: 0 tensor(9.7578e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.7578e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
repetition 5
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.46008515357971
training time full:: 52.46625280380249
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 60
delta_size:: 1
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.20837426185608
time_baseline:: 48.34444808959961
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 1
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.896531105041504
curr_diff: 0 tensor(9.7578e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.7578e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
batch size:: 16384
repetition 1
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.355198621749878
training time full:: 27.357648849487305
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.48575711250305
time_baseline:: 24.557531595230103
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 13.12383484840393
curr_diff: 0 tensor(7.7011e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.7011e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 2
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.026532411575317
training time full:: 27.028801202774048
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.151414155960083
time_baseline:: 24.22646188735962
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 12.867957353591919
curr_diff: 0 tensor(7.7011e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.7011e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 3
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.574784755706787
training time full:: 27.577077865600586
provenance prepare time:: 2.384185791015625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.670047998428345
time_baseline:: 24.741413116455078
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 13.128331422805786
curr_diff: 0 tensor(7.7011e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.7011e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 4
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.28405523300171
training time full:: 27.28627038002014
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.193939924240112
time_baseline:: 24.264702796936035
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 12.885791301727295
curr_diff: 0 tensor(7.7011e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.7011e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 5
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.555988311767578
training time full:: 27.558204412460327
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.327447175979614
time_baseline:: 24.398374319076538
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 12.906787157058716
curr_diff: 0 tensor(7.7011e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.7011e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
batch size:: 4096
repetition 1
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.8948609828948975
training time full:: 5.8948974609375
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.087699890136719
time_baseline:: 5.10518217086792
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.1828784942626953
curr_diff: 0 tensor(1.2199e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2199e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
repetition 2
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.860741853713989
training time full:: 5.860779523849487
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.139725208282471
time_baseline:: 5.157294034957886
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.1123766899108887
curr_diff: 0 tensor(1.2199e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2199e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
repetition 3
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.902469873428345
training time full:: 5.902510404586792
provenance prepare time:: 2.6226043701171875e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.00519871711731
time_baseline:: 5.022791862487793
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.103670358657837
curr_diff: 0 tensor(1.2199e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2199e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
repetition 4
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 6.013097047805786
training time full:: 6.013135671615601
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.203134775161743
time_baseline:: 5.2205729484558105
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.179687976837158
curr_diff: 0 tensor(1.2199e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2199e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
repetition 5
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.885486364364624
training time full:: 5.885526418685913
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.464390277862549
time_baseline:: 5.482214450836182
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.09592604637146
curr_diff: 0 tensor(1.2199e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2199e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
batch size:: 1024
repetition 1
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.6854944229125977
training time full:: 1.6855278015136719
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 2
delta_size:: 1
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4273273944854736
time_baseline:: 1.4322586059570312
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 2
delta_size:: 1
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.0276331901550293
curr_diff: 0 tensor(7.4399e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.4399e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
repetition 2
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.6677217483520508
training time full:: 1.6677582263946533
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 2
delta_size:: 1
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.427431583404541
time_baseline:: 1.4322478771209717
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 2
delta_size:: 1
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.0584802627563477
curr_diff: 0 tensor(7.4399e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.4399e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
repetition 3
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.677027702331543
training time full:: 1.6770660877227783
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 2
delta_size:: 1
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.5404694080352783
time_baseline:: 1.5453026294708252
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 2
delta_size:: 1
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.0156283378601074
curr_diff: 0 tensor(7.4399e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.4399e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
repetition 4
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.672161340713501
training time full:: 1.6721992492675781
provenance prepare time:: 1.1920928955078125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 2
delta_size:: 1
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.450763463973999
time_baseline:: 1.4556124210357666
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 2
delta_size:: 1
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.025193691253662
curr_diff: 0 tensor(7.4399e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.4399e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
repetition 5
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.6780884265899658
training time full:: 1.6781268119812012
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 2
delta_size:: 1
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4987983703613281
time_baseline:: 1.503753423690796
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 2
delta_size:: 1
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.0162603855133057
curr_diff: 0 tensor(7.4399e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.4399e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
deletion rate:: 0.00005
python3 generate_rand_ids 0.00005  MNIST5 0
tensor([54552, 47617,  6575])
batch size:: 60000
repetition 1
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "

Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 110.49391055107117
training time full:: 110.50412845611572
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 120
delta_size:: 3
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 106.74815273284912
time_baseline:: 107.03033542633057
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 120
delta_size:: 3
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 54.581581354141235
curr_diff: 0 tensor(5.6880e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.6880e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
repetition 2
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "

Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 109.14723086357117
training time full:: 109.15886688232422
provenance prepare time:: 2.384185791015625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 120
delta_size:: 3
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 105.08505010604858
time_baseline:: 105.36604285240173
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 120
delta_size:: 3
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 53.80166697502136
curr_diff: 0 tensor(5.6880e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.6880e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
repetition 3
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "

Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 108.29744124412537
training time full:: 108.30962610244751
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 120
delta_size:: 3
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 105.04375791549683
time_baseline:: 105.31381273269653
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 120
delta_size:: 3
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 54.61712193489075
curr_diff: 0 tensor(5.6880e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.6880e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
repetition 4
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "

Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 110.08971190452576
training time full:: 110.1018078327179
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 120
delta_size:: 3
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 106.03054332733154
time_baseline:: 106.30289459228516
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 120
delta_size:: 3
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 54.89809250831604
curr_diff: 0 tensor(5.6880e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.6880e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
repetition 5
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "

Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 109.65565705299377
training time full:: 109.66813898086548
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 120
delta_size:: 3
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 106.3056743144989
time_baseline:: 106.57805490493774
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 120
delta_size:: 3
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 54.555909395217896
curr_diff: 0 tensor(5.6880e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.6880e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
batch size:: 30000
repetition 1
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 53.024709939956665
training time full:: 53.03188180923462
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.03792977333069
time_baseline:: 48.17397117614746
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.551398515701294
curr_diff: 0 tensor(6.1453e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.1453e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
repetition 2
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.74624848365784
training time full:: 52.752601861953735
provenance prepare time:: 2.6226043701171875e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.72819805145264
time_baseline:: 48.86942267417908
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.38322353363037
curr_diff: 0 tensor(6.1453e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.1453e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
repetition 3
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.58407258987427
training time full:: 52.59040021896362
provenance prepare time:: 2.384185791015625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.505035161972046
time_baseline:: 48.64191246032715
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.754146575927734
curr_diff: 0 tensor(6.1453e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.1453e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
repetition 4
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.545330286026
training time full:: 52.551496744155884
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.62207627296448
time_baseline:: 48.76022744178772
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.24834966659546
curr_diff: 0 tensor(6.1453e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.1453e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
repetition 5
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.42872953414917
training time full:: 52.43505787849426
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 49.04947066307068
time_baseline:: 49.184895515441895
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.322667360305786
curr_diff: 0 tensor(6.1453e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.1453e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
batch size:: 16384
repetition 1
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.52590584754944
training time full:: 27.528475761413574
provenance prepare time:: 1.1920928955078125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.219086408615112
time_baseline:: 24.292460441589355
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 12.914584398269653
curr_diff: 0 tensor(5.0030e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.0030e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 2
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.648627758026123
training time full:: 27.65117383003235
provenance prepare time:: 2.384185791015625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.204211950302124
time_baseline:: 24.276574850082397
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 12.99525260925293
curr_diff: 0 tensor(5.0030e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.0030e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 3
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.738877534866333
training time full:: 27.74128746986389
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.093263149261475
time_baseline:: 24.164371967315674
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 12.982661008834839
curr_diff: 0 tensor(5.0030e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.0030e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 4
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.69809341430664
training time full:: 27.700446367263794
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.45479440689087
time_baseline:: 24.52770447731018
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 12.866905212402344
curr_diff: 0 tensor(5.0030e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.0030e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 5
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.246877670288086
training time full:: 27.249134302139282
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.294922828674316
time_baseline:: 24.365705251693726
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 13.237238645553589
curr_diff: 0 tensor(5.0030e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.0030e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
batch size:: 4096
repetition 1
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.839572191238403
training time full:: 5.839610815048218
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 8
delta_size:: 3
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.151529312133789
time_baseline:: 5.169719696044922
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 8
delta_size:: 3
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.1113381385803223
curr_diff: 0 tensor(6.7315e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.7315e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
repetition 2
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.825823068618774
training time full:: 5.825864791870117
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 8
delta_size:: 3
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.050883531570435
time_baseline:: 5.068681716918945
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 8
delta_size:: 3
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.071444272994995
curr_diff: 0 tensor(6.7315e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.7315e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
repetition 3
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.819498062133789
training time full:: 5.819535732269287
provenance prepare time:: 2.6226043701171875e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 8
delta_size:: 3
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.123237133026123
time_baseline:: 5.14092755317688
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 8
delta_size:: 3
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.159848928451538
curr_diff: 0 tensor(6.7315e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.7315e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
repetition 4
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.883944034576416
training time full:: 5.88398289680481
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 8
delta_size:: 3
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.292071104049683
time_baseline:: 5.311943292617798
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 8
delta_size:: 3
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.0884838104248047
curr_diff: 0 tensor(6.7315e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.7315e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
repetition 5
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 6.160099983215332
training time full:: 6.160139799118042
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 8
delta_size:: 3
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.173732757568359
time_baseline:: 5.19138240814209
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 8
delta_size:: 3
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.1237049102783203
curr_diff: 0 tensor(6.7315e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.7315e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
batch size:: 1024
repetition 1
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.7081305980682373
training time full:: 1.708167314529419
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 2
delta_size:: 3
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4111318588256836
time_baseline:: 1.416041374206543
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 2
delta_size:: 3
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.0523195266723633
curr_diff: 0 tensor(7.0890e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.0890e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
repetition 2
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.689448356628418
training time full:: 1.689483642578125
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 2
delta_size:: 3
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4448564052581787
time_baseline:: 1.449814796447754
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 2
delta_size:: 3
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.020397663116455
curr_diff: 0 tensor(7.0890e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.0890e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
repetition 3
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.7238967418670654
training time full:: 1.7239322662353516
provenance prepare time:: 1.1920928955078125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 2
delta_size:: 3
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.425069808959961
time_baseline:: 1.4300014972686768
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 2
delta_size:: 3
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.0248935222625732
curr_diff: 0 tensor(7.0890e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.0890e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
repetition 4
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.6486971378326416
training time full:: 1.6487321853637695
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 2
delta_size:: 3
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4517953395843506
time_baseline:: 1.4567720890045166
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 2
delta_size:: 3
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.0238068103790283
curr_diff: 0 tensor(7.0890e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.0890e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
repetition 5
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.6572391986846924
training time full:: 1.6572766304016113
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 2
delta_size:: 3
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4419753551483154
time_baseline:: 1.4468305110931396
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 2
delta_size:: 3
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.009723424911499
curr_diff: 0 tensor(7.0890e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.0890e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
deletion rate:: 0.0001
python3 generate_rand_ids 0.0001  MNIST5 0
tensor([52880, 47617, 25793, 39583, 54552,  6575])
batch size:: 60000
repetition 1
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "

Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 108.9875693321228
training time full:: 108.99762535095215
provenance prepare time:: 2.384185791015625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 120
delta_size:: 6
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 105.83844804763794
time_baseline:: 106.10618209838867
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 120
delta_size:: 6
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 53.77880072593689
curr_diff: 0 tensor(7.8141e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.8141e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875300
repetition 2
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "

Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 108.96165823936462
training time full:: 108.97301745414734
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 120
delta_size:: 6
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 106.13952016830444
time_baseline:: 106.41592359542847
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 120
delta_size:: 6
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 53.71926808357239
curr_diff: 0 tensor(7.8141e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.8141e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875300
repetition 3
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "

Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 110.14425945281982
training time full:: 110.15667605400085
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 120
delta_size:: 6
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 106.29445672035217
time_baseline:: 106.56483340263367
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 120
delta_size:: 6
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 54.338014125823975
curr_diff: 0 tensor(7.8141e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.8141e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875300
repetition 4
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "

Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 109.23232078552246
training time full:: 109.24424743652344
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 120
delta_size:: 6
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 106.26013207435608
time_baseline:: 106.53143858909607
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 120
delta_size:: 6
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 53.370381355285645
curr_diff: 0 tensor(7.8141e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.8141e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875300
repetition 5
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "

Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 109.18933200836182
training time full:: 109.20102739334106
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 120
delta_size:: 6
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 107.0040717124939
time_baseline:: 107.27501845359802
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 120
delta_size:: 6
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 54.41733980178833
curr_diff: 0 tensor(7.8141e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.8141e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875300
batch size:: 30000
repetition 1
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 53.42532920837402
training time full:: 53.43243360519409
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.62565493583679
time_baseline:: 48.760749101638794
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.7372407913208
curr_diff: 0 tensor(8.4716e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.4716e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
repetition 2
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.77174615859985
training time full:: 52.77802038192749
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 47.97598123550415
time_baseline:: 48.11094522476196
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.977349042892456
curr_diff: 0 tensor(8.4716e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.4716e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
repetition 3
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.343679428100586
training time full:: 52.34995150566101
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.34083557128906
time_baseline:: 48.4777934551239
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.502440214157104
curr_diff: 0 tensor(8.4716e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.4716e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
repetition 4
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.391969442367554
training time full:: 52.398157835006714
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 47.75321364402771
time_baseline:: 47.888023376464844
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.750574350357056
curr_diff: 0 tensor(8.4716e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.4716e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
repetition 5
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.526641845703125
training time full:: 52.532824993133545
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.78610014915466
time_baseline:: 48.92227387428284
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.460111618041992
curr_diff: 0 tensor(8.4716e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.4716e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
batch size:: 16384
repetition 1
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.540691614151
training time full:: 27.543217658996582
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.61324119567871
time_baseline:: 24.68537187576294
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 12.876146793365479
curr_diff: 0 tensor(7.9005e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.9005e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 2
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.64712953567505
training time full:: 27.64964461326599
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.789563417434692
time_baseline:: 24.861419677734375
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 12.893203258514404
curr_diff: 0 tensor(7.9005e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.9005e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 3
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.41927433013916
training time full:: 27.42177939414978
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.626823902130127
time_baseline:: 24.69918942451477
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 13.04100513458252
curr_diff: 0 tensor(7.9005e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.9005e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 4
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.273550271987915
training time full:: 27.27588129043579
provenance prepare time:: 2.384185791015625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.3467218875885
time_baseline:: 24.41897201538086
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 12.913761138916016
curr_diff: 0 tensor(7.9005e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.9005e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 5
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.6007661819458
training time full:: 27.602951765060425
provenance prepare time:: 2.384185791015625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.68204355239868
time_baseline:: 24.7562575340271
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 13.059613943099976
curr_diff: 0 tensor(7.9005e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.9005e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
batch size:: 4096
repetition 1
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.748654365539551
training time full:: 5.748693943023682
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 8
delta_size:: 6
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.222797393798828
time_baseline:: 5.240302085876465
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 8
delta_size:: 6
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.1744420528411865
curr_diff: 0 tensor(9.4441e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.4441e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
repetition 2
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.86338472366333
training time full:: 5.863423824310303
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 8
delta_size:: 6
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.255371332168579
time_baseline:: 5.2726829051971436
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 8
delta_size:: 6
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.093977928161621
curr_diff: 0 tensor(9.4441e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.4441e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
repetition 3
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.786705017089844
training time full:: 5.7867431640625
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 8
delta_size:: 6
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.386525630950928
time_baseline:: 5.403918266296387
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 8
delta_size:: 6
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.1164674758911133
curr_diff: 0 tensor(9.4441e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.4441e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
repetition 4
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.962843894958496
training time full:: 5.9628822803497314
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 8
delta_size:: 6
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 4.99133038520813
time_baseline:: 5.008731126785278
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 8
delta_size:: 6
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.1625373363494873
curr_diff: 0 tensor(9.4441e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.4441e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
repetition 5
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.926111936569214
training time full:: 5.926150321960449
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 8
delta_size:: 6
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.335138320922852
time_baseline:: 5.352546691894531
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 8
delta_size:: 6
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.1249196529388428
curr_diff: 0 tensor(9.4441e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.4441e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
batch size:: 1024
repetition 1
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.642946481704712
training time full:: 1.6429827213287354
provenance prepare time:: 2.6226043701171875e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 2
delta_size:: 6
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4114136695861816
time_baseline:: 1.4162123203277588
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 2
delta_size:: 6
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.0143828392028809
curr_diff: 0 tensor(9.7392e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.7392e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
repetition 2
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.7325801849365234
training time full:: 1.7326159477233887
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 2
delta_size:: 6
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4461338520050049
time_baseline:: 1.4509639739990234
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 2
delta_size:: 6
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.082305908203125
curr_diff: 0 tensor(9.7392e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.7392e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
repetition 3
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.6590816974639893
training time full:: 1.6591167449951172
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 2
delta_size:: 6
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.48106050491333
time_baseline:: 1.4859225749969482
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 2
delta_size:: 6
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.00982666015625
curr_diff: 0 tensor(9.7392e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.7392e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
repetition 4
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.7113409042358398
training time full:: 1.7113778591156006
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 2
delta_size:: 6
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4445502758026123
time_baseline:: 1.4492661952972412
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 2
delta_size:: 6
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.0227282047271729
curr_diff: 0 tensor(9.7392e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.7392e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
repetition 5
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.710352897644043
training time full:: 1.710390567779541
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 2
delta_size:: 6
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4505107402801514
time_baseline:: 1.4554815292358398
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 2
delta_size:: 6
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.021019458770752
curr_diff: 0 tensor(9.7392e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.7392e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
deletion rate:: 0.0002
python3 generate_rand_ids 0.0002  MNIST5 0
tensor([47617, 25793,  5505, 16076, 37134,  6575, 52880,   659, 48055, 54552,
        29177, 39583])
batch size:: 60000
repetition 1
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "

Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 109.22628569602966
training time full:: 109.23636794090271
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 120
delta_size:: 12
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 107.04738903045654
time_baseline:: 107.32711172103882
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875400
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 120
delta_size:: 12
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 53.67544937133789
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875200
repetition 2
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "

Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 109.24075889587402
training time full:: 109.25231981277466
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 120
delta_size:: 12
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 105.72148442268372
time_baseline:: 106.00137662887573
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875400
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 120
delta_size:: 12
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 54.19385886192322
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875200
repetition 3
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "

Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 109.16172313690186
training time full:: 109.1746027469635
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 120
delta_size:: 12
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 105.73483920097351
time_baseline:: 106.00286173820496
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875400
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 120
delta_size:: 12
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 54.08073115348816
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875200
repetition 4
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "

Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 109.58508372306824
training time full:: 109.59705996513367
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 120
delta_size:: 12
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 106.42663908004761
time_baseline:: 106.69544768333435
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875400
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 120
delta_size:: 12
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 53.45834708213806
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875200
repetition 5
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "

Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 109.70338082313538
training time full:: 109.71570754051208
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 120
delta_size:: 12
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 106.18149638175964
time_baseline:: 106.45294165611267
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875400
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 120
delta_size:: 12
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 53.85917782783508
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875200
batch size:: 30000
repetition 1
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.936063051223755
training time full:: 52.943278312683105
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 60
delta_size:: 12
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 47.8509783744812
time_baseline:: 47.98622250556946
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 12
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.52342677116394
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0019, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0019, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
repetition 2
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.450462341308594
training time full:: 52.45672249794006
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 60
delta_size:: 12
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.30029082298279
time_baseline:: 48.435967206954956
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 12
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.724472999572754
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0019, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0019, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
repetition 3
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.89934182167053
training time full:: 52.90563154220581
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 60
delta_size:: 12
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.6422905921936
time_baseline:: 48.7790629863739
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 12
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.70839238166809
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0019, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0019, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
repetition 4
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.80345010757446
training time full:: 52.809653520584106
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 60
delta_size:: 12
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.64886212348938
time_baseline:: 48.78330993652344
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 12
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.78717565536499
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0019, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0019, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
repetition 5
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 53.30448317527771
training time full:: 53.310770988464355
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 60
delta_size:: 12
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.57170009613037
time_baseline:: 48.70802593231201
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 12
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.449788093566895
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0019, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0019, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
batch size:: 16384
repetition 1
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.12553644180298
training time full:: 27.127992630004883
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.614160537719727
time_baseline:: 24.68983244895935
curr_diff: 0 tensor(0.0019, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0019, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 12.982391357421875
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0020, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0020, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 2
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.215094089508057
training time full:: 27.217523097991943
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.677999258041382
time_baseline:: 24.74924921989441
curr_diff: 0 tensor(0.0019, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0019, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 13.18272590637207
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0020, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0020, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 3
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.365772247314453
training time full:: 27.367952585220337
provenance prepare time:: 2.86102294921875e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.863677501678467
time_baseline:: 24.937079906463623
curr_diff: 0 tensor(0.0019, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0019, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 12.976703882217407
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0020, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0020, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 4
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 26.989182472229004
training time full:: 26.991289377212524
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.400623321533203
time_baseline:: 24.471317529678345
curr_diff: 0 tensor(0.0019, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0019, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 12.974563837051392
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0020, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0020, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 5
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.458917140960693
training time full:: 27.46116805076599
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.76151394844055
time_baseline:: 24.832529067993164
curr_diff: 0 tensor(0.0019, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0019, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 13.086364030838013
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0020, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0020, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
batch size:: 4096
repetition 1
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.952694654464722
training time full:: 5.952735424041748
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 8
delta_size:: 12
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.068305730819702
time_baseline:: 5.085822343826294
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 8
delta_size:: 12
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.1495351791381836
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0019, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0019, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
repetition 2
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.9625561237335205
training time full:: 5.962592840194702
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 8
delta_size:: 12
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.038708448410034
time_baseline:: 5.056260108947754
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 8
delta_size:: 12
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.1540377140045166
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0019, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0019, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
repetition 3
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.898979902267456
training time full:: 5.899020195007324
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 8
delta_size:: 12
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.169047832489014
time_baseline:: 5.186396837234497
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 8
delta_size:: 12
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.20879864692688
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0019, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0019, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
repetition 4
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.826450347900391
training time full:: 5.826487064361572
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 8
delta_size:: 12
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.053558588027954
time_baseline:: 5.0712456703186035
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 8
delta_size:: 12
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.1128292083740234
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0019, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0019, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
repetition 5
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.805005311965942
training time full:: 5.805044412612915
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 8
delta_size:: 12
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.182200193405151
time_baseline:: 5.199756860733032
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 8
delta_size:: 12
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.1464056968688965
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0019, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0019, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
batch size:: 1024
repetition 1
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.6969294548034668
training time full:: 1.6969647407531738
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 2
delta_size:: 12
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.433246374130249
time_baseline:: 1.4381015300750732
curr_diff: 0 tensor(0.0019, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0019, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 2
delta_size:: 12
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.0309028625488281
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0019, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0019, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
repetition 2
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.654496192932129
training time full:: 1.6545312404632568
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 2
delta_size:: 12
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4528884887695312
time_baseline:: 1.4578633308410645
curr_diff: 0 tensor(0.0019, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0019, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 2
delta_size:: 12
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.0370123386383057
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0019, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0019, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
repetition 3
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.6655113697052002
training time full:: 1.6655454635620117
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 2
delta_size:: 12
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4502253532409668
time_baseline:: 1.4549405574798584
curr_diff: 0 tensor(0.0019, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0019, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 2
delta_size:: 12
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.0213861465454102
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0019, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0019, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
repetition 4
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.647385597229004
training time full:: 1.6474218368530273
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 2
delta_size:: 12
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4416682720184326
time_baseline:: 1.4465229511260986
curr_diff: 0 tensor(0.0019, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0019, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 2
delta_size:: 12
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.0266456604003906
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0019, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0019, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
repetition 5
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.686929702758789
training time full:: 1.6869680881500244
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 2
delta_size:: 12
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.430046796798706
time_baseline:: 1.4349186420440674
curr_diff: 0 tensor(0.0019, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0019, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 2
delta_size:: 12
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.0301787853240967
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0019, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0019, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
deletion rate:: 0.0005
python3 generate_rand_ids 0.0005  MNIST5 0
tensor([47617,  5505, 25793,  9035, 16076, 37134, 52880, 30226,   659, 42835,
        54552, 11352,  4766, 39583, 58336,  2487, 58917, 55912, 30254,  6575,
        23986, 59699, 19193, 48055, 25720, 29177, 35066, 59003,  8125, 28350])
batch size:: 60000
repetition 1
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "

Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 108.25577068328857
training time full:: 108.26585865020752
provenance prepare time:: 2.384185791015625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 120
delta_size:: 30
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 106.30398154258728
time_baseline:: 106.57398390769958
curr_diff: 0 tensor(0.0025, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0025, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 120
delta_size:: 30
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 55.05679225921631
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0026, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0026, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875300
repetition 2
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "

Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 109.60263466835022
training time full:: 109.61419177055359
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 120
delta_size:: 30
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 106.8905336856842
time_baseline:: 107.16946840286255
curr_diff: 0 tensor(0.0025, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0025, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 120
delta_size:: 30
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 55.37471008300781
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0026, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0026, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875300
repetition 3
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "

Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 112.2137246131897
training time full:: 112.22553634643555
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 120
delta_size:: 30
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 106.26115775108337
time_baseline:: 106.54655933380127
curr_diff: 0 tensor(0.0025, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0025, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 120
delta_size:: 30
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 54.68799805641174
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0026, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0026, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875300
repetition 4
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "

Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 109.33903431892395
training time full:: 109.35093426704407
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 120
delta_size:: 30
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 106.06135106086731
time_baseline:: 106.33368110656738
curr_diff: 0 tensor(0.0025, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0025, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 120
delta_size:: 30
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 54.23524355888367
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0026, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0026, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875300
repetition 5
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "

Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 109.9952130317688
training time full:: 110.00733256340027
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 120
delta_size:: 30
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 106.29369640350342
time_baseline:: 106.56686067581177
curr_diff: 0 tensor(0.0025, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0025, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 120
delta_size:: 30
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 53.82367253303528
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0026, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0026, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875300
batch size:: 30000
repetition 1
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.49977135658264
training time full:: 52.506747245788574
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.30774116516113
time_baseline:: 48.44822573661804
curr_diff: 0 tensor(0.0025, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0025, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.51441526412964
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0027, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0027, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
repetition 2
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.189136266708374
training time full:: 52.19554877281189
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.708109855651855
time_baseline:: 48.84500789642334
curr_diff: 0 tensor(0.0025, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0025, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.506605863571167
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0027, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0027, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
repetition 3
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.33586311340332
training time full:: 52.34205484390259
provenance prepare time:: 2.6226043701171875e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.32939124107361
time_baseline:: 48.464744329452515
curr_diff: 0 tensor(0.0025, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0025, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.77711534500122
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0027, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0027, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
repetition 4
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.94274163246155
training time full:: 52.949066162109375
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.29052948951721
time_baseline:: 48.42565894126892
curr_diff: 0 tensor(0.0025, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0025, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 26.198758602142334
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0027, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0027, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
repetition 5
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.002601623535156
training time full:: 52.008846282958984
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.52155780792236
time_baseline:: 48.65647292137146
curr_diff: 0 tensor(0.0025, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0025, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.475196361541748
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0027, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0027, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
batch size:: 16384
repetition 1
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.45135474205017
training time full:: 27.453866958618164
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.295520544052124
time_baseline:: 24.368131160736084
curr_diff: 0 tensor(0.0027, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0027, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 12.91571855545044
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0028, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0028, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 2
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.479939699172974
training time full:: 27.482449293136597
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.657214879989624
time_baseline:: 24.73065209388733
curr_diff: 0 tensor(0.0027, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0027, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 13.095237731933594
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0028, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0028, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 3
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.189289331436157
training time full:: 27.191802263259888
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.2448468208313
time_baseline:: 24.3156578540802
curr_diff: 0 tensor(0.0027, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0027, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 12.904649496078491
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0028, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0028, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 4
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.596410751342773
training time full:: 27.598828077316284
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.345368146896362
time_baseline:: 24.416602849960327
curr_diff: 0 tensor(0.0027, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0027, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 13.142397403717041
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0028, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0028, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 5
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.281477689743042
training time full:: 27.283689260482788
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.131141185760498
time_baseline:: 24.20151114463806
curr_diff: 0 tensor(0.0027, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0027, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 13.238314867019653
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0028, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0028, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
batch size:: 4096
repetition 1
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.819677352905273
training time full:: 5.819715976715088
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 8
delta_size:: 30
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.070527076721191
time_baseline:: 5.088027477264404
curr_diff: 0 tensor(0.0026, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0026, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 8
delta_size:: 30
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.1804685592651367
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0027, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0027, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
repetition 2
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.781969308853149
training time full:: 5.7820093631744385
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 8
delta_size:: 30
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 4.965360164642334
time_baseline:: 4.9830567836761475
curr_diff: 0 tensor(0.0026, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0026, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 8
delta_size:: 30
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.2203421592712402
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0027, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0027, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
repetition 3
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.819403171539307
training time full:: 5.819442510604858
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 8
delta_size:: 30
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 4.974776029586792
time_baseline:: 4.992696285247803
curr_diff: 0 tensor(0.0026, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0026, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 8
delta_size:: 30
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.2725670337677
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0027, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0027, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
repetition 4
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 6.049971580505371
training time full:: 6.050010919570923
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 8
delta_size:: 30
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.102722406387329
time_baseline:: 5.120407342910767
curr_diff: 0 tensor(0.0026, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0026, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 8
delta_size:: 30
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.2101147174835205
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0027, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0027, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
repetition 5
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.80434513092041
training time full:: 5.8043835163116455
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 8
delta_size:: 30
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 4.989419937133789
time_baseline:: 5.006918668746948
curr_diff: 0 tensor(0.0026, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0026, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 8
delta_size:: 30
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.195145606994629
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0027, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0027, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
batch size:: 1024
repetition 1
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.6952459812164307
training time full:: 1.6952893733978271
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 2
delta_size:: 30
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4222495555877686
time_baseline:: 1.4270555973052979
curr_diff: 0 tensor(0.0027, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0027, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 2
delta_size:: 30
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.060915470123291
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0028, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0028, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
repetition 2
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.6913490295410156
training time full:: 1.6913845539093018
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 2
delta_size:: 30
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4430079460144043
time_baseline:: 1.4481840133666992
curr_diff: 0 tensor(0.0027, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0027, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 2
delta_size:: 30
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.0479614734649658
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0028, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0028, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
repetition 3
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.6797029972076416
training time full:: 1.679738998413086
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 2
delta_size:: 30
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.454141616821289
time_baseline:: 1.459036111831665
curr_diff: 0 tensor(0.0027, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0027, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 2
delta_size:: 30
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.0488121509552002
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0028, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0028, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
repetition 4
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.6627957820892334
training time full:: 1.6628332138061523
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 2
delta_size:: 30
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4528286457061768
time_baseline:: 1.45782470703125
curr_diff: 0 tensor(0.0027, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0027, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 2
delta_size:: 30
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.0412604808807373
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0028, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0028, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
repetition 5
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.6552352905273438
training time full:: 1.6552717685699463
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 2
delta_size:: 30
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4030303955078125
time_baseline:: 1.4078314304351807
curr_diff: 0 tensor(0.0027, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0027, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 2
delta_size:: 30
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.0468809604644775
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0028, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0028, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
deletion rate:: 0.001
python3 generate_rand_ids 0.001  MNIST5 0
tensor([47617,  5505, 22529, 16257, 15110, 25352, 37134, 52880, 30226,   659,
        12562, 56596, 53269, 33685, 54552, 11543,  4766, 39583, 49439, 25120,
        44452, 58917, 28458, 44458, 28203, 30254,  6575, 54448, 23986, 59699,
        35122, 32051, 48055,  2487, 37049,  8125, 28350, 25793,  9035, 16076,
        25296, 56914, 42835, 47315, 55254, 11352, 25820, 58336, 56804, 55912,
        31978, 29177,   622, 55410, 38003,  6260, 25720, 19193, 35066, 59003])
batch size:: 60000
repetition 1
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "

Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 108.76501488685608
training time full:: 108.77521705627441
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 120
delta_size:: 60
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 105.87499284744263
time_baseline:: 106.14246726036072
curr_diff: 0 tensor(0.0041, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0041, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875200
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 120
delta_size:: 60
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 54.43530797958374
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875200
repetition 2
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "

Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 108.22126960754395
training time full:: 108.23302173614502
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 120
delta_size:: 60
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 106.1047043800354
time_baseline:: 106.37381196022034
curr_diff: 0 tensor(0.0041, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0041, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875200
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 120
delta_size:: 60
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 53.83138656616211
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875200
repetition 3
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "

Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 110.3843343257904
training time full:: 110.39617204666138
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 120
delta_size:: 60
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 106.83242726325989
time_baseline:: 107.10450768470764
curr_diff: 0 tensor(0.0041, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0041, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875200
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 120
delta_size:: 60
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 54.44962453842163
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875200
repetition 4
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "

Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 110.24821758270264
training time full:: 110.26032185554504
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 120
delta_size:: 60
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 108.04916834831238
time_baseline:: 108.33099937438965
curr_diff: 0 tensor(0.0041, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0041, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875200
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 120
delta_size:: 60
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 54.4036500453949
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875200
repetition 5
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "

Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 109.57728266716003
training time full:: 109.58902192115784
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 120
delta_size:: 60
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 106.0109474658966
time_baseline:: 106.28068947792053
curr_diff: 0 tensor(0.0041, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0041, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875200
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 120
delta_size:: 60
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 54.78457307815552
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875200
batch size:: 30000
repetition 1
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.47166895866394
training time full:: 52.478736877441406
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.18075251579285
time_baseline:: 48.31767916679382
curr_diff: 0 tensor(0.0041, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0041, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875300
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 26.074233531951904
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
repetition 2
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.64642882347107
training time full:: 52.652687549591064
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.6614408493042
time_baseline:: 48.79616665840149
curr_diff: 0 tensor(0.0041, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0041, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875300
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.88782238960266
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
repetition 3
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.251904249191284
training time full:: 52.25812339782715
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.80834627151489
time_baseline:: 48.944817304611206
curr_diff: 0 tensor(0.0041, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0041, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875300
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.350892543792725
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
repetition 4
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.255789279937744
training time full:: 52.262001276016235
provenance prepare time:: 2.384185791015625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.053412199020386
time_baseline:: 48.18977761268616
curr_diff: 0 tensor(0.0041, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0041, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875300
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.77198839187622
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
repetition 5
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 51.912447929382324
training time full:: 51.91861343383789
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.014771699905396
time_baseline:: 48.15058159828186
curr_diff: 0 tensor(0.0041, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0041, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875300
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.636882543563843
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
batch size:: 16384
repetition 1
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.201013803482056
training time full:: 27.20356059074402
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.380377531051636
time_baseline:: 24.451509475708008
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 13.086908102035522
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 2
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.38408899307251
training time full:: 27.386592626571655
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.335410594940186
time_baseline:: 24.40706753730774
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 13.16594386100769
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 3
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.584381341934204
training time full:: 27.586870193481445
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 23.995317459106445
time_baseline:: 24.066503524780273
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 12.966701745986938
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 4
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.182172298431396
training time full:: 27.184528827667236
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.361059427261353
time_baseline:: 24.432248830795288
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 13.17352032661438
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 5
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.126206159591675
training time full:: 27.128328561782837
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.330587148666382
time_baseline:: 24.403130054473877
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 13.015712976455688
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
batch size:: 4096
repetition 1
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.804442882537842
training time full:: 5.804487943649292
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 8
delta_size:: 60
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.112755298614502
time_baseline:: 5.130517244338989
curr_diff: 0 tensor(0.0042, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0042, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874800
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 8
delta_size:: 60
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.231945514678955
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874800
repetition 2
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.93551230430603
training time full:: 5.935551166534424
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 8
delta_size:: 60
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.251935005187988
time_baseline:: 5.269641876220703
curr_diff: 0 tensor(0.0042, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0042, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874800
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 8
delta_size:: 60
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.2126944065093994
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874800
repetition 3
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.911606073379517
training time full:: 5.911649227142334
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 8
delta_size:: 60
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.24519157409668
time_baseline:: 5.262907028198242
curr_diff: 0 tensor(0.0042, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0042, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874800
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 8
delta_size:: 60
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.1993637084960938
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874800
repetition 4
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.8533384799957275
training time full:: 5.853379487991333
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 8
delta_size:: 60
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.0224525928497314
time_baseline:: 5.039793252944946
curr_diff: 0 tensor(0.0042, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0042, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874800
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 8
delta_size:: 60
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.196032762527466
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874800
repetition 5
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.937190294265747
training time full:: 5.937229633331299
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 8
delta_size:: 60
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.062218904495239
time_baseline:: 5.079721927642822
curr_diff: 0 tensor(0.0042, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0042, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874800
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 8
delta_size:: 60
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.287440299987793
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874800
batch size:: 1024
repetition 1
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.7143537998199463
training time full:: 1.7143895626068115
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 2
delta_size:: 60
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4287493228912354
time_baseline:: 1.4334585666656494
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.872900
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 2
delta_size:: 60
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.0708582401275635
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0046, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0046, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
repetition 2
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.674919605255127
training time full:: 1.674957036972046
provenance prepare time:: 1.1920928955078125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 2
delta_size:: 60
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4213132858276367
time_baseline:: 1.4260683059692383
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.872900
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 2
delta_size:: 60
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.1652214527130127
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0046, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0046, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
repetition 3
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.6756248474121094
training time full:: 1.6756641864776611
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 2
delta_size:: 60
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.413182020187378
time_baseline:: 1.4180114269256592
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.872900
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 2
delta_size:: 60
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.0675981044769287
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0046, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0046, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
repetition 4
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.7003605365753174
training time full:: 1.70039701461792
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 2
delta_size:: 60
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4371919631958008
time_baseline:: 1.4419898986816406
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.872900
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 2
delta_size:: 60
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.114013671875
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0046, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0046, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
repetition 5
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.682133436203003
training time full:: 1.6821708679199219
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 2
delta_size:: 60
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4359688758850098
time_baseline:: 1.4409875869750977
curr_diff: 0 tensor(0.0044, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0044, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.872900
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 2
delta_size:: 60
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.0817780494689941
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0046, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0046, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
deletion rate:: 0.002
python3 generate_rand_ids 0.002  MNIST5 0
tensor([47617, 22529, 52737,  6915, 15110, 10502, 25352, 18444, 43788, 37134,
        30226, 12562, 56596, 53269, 23059, 11543, 54552, 36632,  3612, 49439,
        25120, 12320, 58917, 59429, 24357,  2856, 45608, 28458, 28203, 30254,
        35122, 59699, 32051, 23090,  7734, 16190, 53312,    66, 46406,  8520,
        15176,  9035, 38987, 25166, 20817, 56914, 42835, 55890, 18772, 38995,
        11352, 34137,   861, 42082, 12643, 56164, 55912, 29177,   622, 55410,
        38003,  6260, 15733, 19318, 25720, 59003, 25723,  5505, 16257, 16770,
        10373, 31373, 52880, 21138,   659, 59795, 33685, 47770, 19610, 55964,
         4766, 39583, 45473, 44452, 22692, 38821, 27559, 44458,  6575, 54448,
        34736, 23986, 48055,  2487, 37049,  8125, 28350, 25793,  5827, 16076,
        14030, 25296, 31185, 47315, 55254, 28630,  7384, 35034, 25820, 53981,
        58336, 56804, 31978, 14571,  4843, 43247, 19193, 35066, 13308, 36093])
batch size:: 60000
repetition 1
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "

Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 108.50262808799744
training time full:: 108.51279544830322
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 120
delta_size:: 120
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 105.16916346549988
time_baseline:: 105.43932437896729
curr_diff: 0 tensor(0.0067, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0067, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875100
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 120
delta_size:: 120
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 53.897273540496826
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0068, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0068, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875200
repetition 2
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "

Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 110.3394627571106
training time full:: 110.35099387168884
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 120
delta_size:: 120
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 106.32575702667236
time_baseline:: 106.59582781791687
curr_diff: 0 tensor(0.0067, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0067, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875100
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 120
delta_size:: 120
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 54.23434782028198
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0068, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0068, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875200
repetition 3
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "

Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 109.5528621673584
training time full:: 109.5652847290039
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 120
delta_size:: 120
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 106.95414972305298
time_baseline:: 107.22808647155762
curr_diff: 0 tensor(0.0067, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0067, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875100
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 120
delta_size:: 120
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 53.83676314353943
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0068, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0068, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875200
repetition 4
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "

Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 110.75092387199402
training time full:: 110.76342582702637
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 120
delta_size:: 120
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 105.58675718307495
time_baseline:: 105.85527610778809
curr_diff: 0 tensor(0.0067, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0067, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875100
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 120
delta_size:: 120
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 53.73680281639099
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0068, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0068, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875200
repetition 5
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "

Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 109.05807614326477
training time full:: 109.06994104385376
provenance prepare time:: 2.384185791015625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 120
delta_size:: 120
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 106.81147646903992
time_baseline:: 107.08224725723267
curr_diff: 0 tensor(0.0067, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0067, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875100
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 120
delta_size:: 120
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 54.71322989463806
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0068, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0068, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875200
batch size:: 30000
repetition 1
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.622987508773804
training time full:: 52.629953384399414
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 60
delta_size:: 120
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 47.9268434047699
time_baseline:: 48.061187744140625
curr_diff: 0 tensor(0.0067, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0067, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 120
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.943246126174927
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0070, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0070, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875200
repetition 2
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.29267621040344
training time full:: 52.299068450927734
provenance prepare time:: 2.6226043701171875e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 60
delta_size:: 120
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.43611431121826
time_baseline:: 48.57081866264343
curr_diff: 0 tensor(0.0067, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0067, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 120
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.88324213027954
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0070, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0070, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875200
repetition 3
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.62895226478577
training time full:: 52.63515067100525
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 60
delta_size:: 120
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.318413496017456
time_baseline:: 48.452640771865845
curr_diff: 0 tensor(0.0067, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0067, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 120
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.703669548034668
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0070, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0070, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875200
repetition 4
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.18451452255249
training time full:: 52.19069504737854
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 60
delta_size:: 120
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.35202884674072
time_baseline:: 48.49187731742859
curr_diff: 0 tensor(0.0067, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0067, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 120
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.901788473129272
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0070, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0070, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875200
repetition 5
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 53.0032114982605
training time full:: 53.009493827819824
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 60
delta_size:: 120
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.69558572769165
time_baseline:: 48.83087754249573
curr_diff: 0 tensor(0.0067, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0067, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875000
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 120
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.72448444366455
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0070, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0070, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875200
batch size:: 16384
repetition 1
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.593142986297607
training time full:: 27.59572434425354
provenance prepare time:: 2.384185791015625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.30224370956421
time_baseline:: 24.374141693115234
curr_diff: 0 tensor(0.0070, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0070, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 13.11689567565918
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0073, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0073, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 2
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.245537996292114
training time full:: 27.247997283935547
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.30709958076477
time_baseline:: 24.37885046005249
curr_diff: 0 tensor(0.0070, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0070, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 13.23730182647705
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0073, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0073, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 3
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.52627944946289
training time full:: 27.528709411621094
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.375043869018555
time_baseline:: 24.44563913345337
curr_diff: 0 tensor(0.0070, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0070, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 13.18255066871643
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0073, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0073, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 4
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.468501091003418
training time full:: 27.47087550163269
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.821151971817017
time_baseline:: 24.89523458480835
curr_diff: 0 tensor(0.0070, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0070, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 12.896518468856812
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0073, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0073, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 5
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.554460287094116
training time full:: 27.556731462478638
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.175872802734375
time_baseline:: 24.24680185317993
curr_diff: 0 tensor(0.0070, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0070, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 13.06460952758789
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0073, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0073, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
batch size:: 4096
repetition 1
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.863348007202148
training time full:: 5.863387584686279
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 8
delta_size:: 120
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.02690863609314
time_baseline:: 5.044279098510742
curr_diff: 0 tensor(0.0068, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0068, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874700
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 8
delta_size:: 120
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.1435437202453613
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0072, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0072, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874700
repetition 2
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.796328783035278
training time full:: 5.796368360519409
provenance prepare time:: 2.384185791015625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 8
delta_size:: 120
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 4.978464603424072
time_baseline:: 4.9957098960876465
curr_diff: 0 tensor(0.0068, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0068, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874700
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 8
delta_size:: 120
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.193652868270874
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0072, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0072, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874700
repetition 3
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 6.068509101867676
training time full:: 6.068546772003174
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 8
delta_size:: 120
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.050078392028809
time_baseline:: 5.06769061088562
curr_diff: 0 tensor(0.0068, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0068, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874700
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 8
delta_size:: 120
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.292045831680298
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0072, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0072, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874700
repetition 4
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.951329469680786
training time full:: 5.951367139816284
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 8
delta_size:: 120
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.070423126220703
time_baseline:: 5.088044166564941
curr_diff: 0 tensor(0.0068, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0068, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874700
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 8
delta_size:: 120
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.1669650077819824
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0072, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0072, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874700
repetition 5
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.7834248542785645
training time full:: 5.7834632396698
provenance prepare time:: 2.6226043701171875e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 8
delta_size:: 120
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.063726425170898
time_baseline:: 5.082115411758423
curr_diff: 0 tensor(0.0068, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0068, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874700
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 8
delta_size:: 120
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.14971923828125
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0072, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0072, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874700
batch size:: 1024
repetition 1
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.7128069400787354
training time full:: 1.7128431797027588
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 2
delta_size:: 120
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4199285507202148
time_baseline:: 1.4247581958770752
curr_diff: 0 tensor(0.0069, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0069, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.872800
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 2
delta_size:: 120
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.1323225498199463
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0071, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0071, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.872800
repetition 2
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.6604843139648438
training time full:: 1.660517692565918
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 2
delta_size:: 120
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.401733636856079
time_baseline:: 1.4070067405700684
curr_diff: 0 tensor(0.0069, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0069, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.872800
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 2
delta_size:: 120
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.1066386699676514
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0071, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0071, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.872800
repetition 3
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.7182891368865967
training time full:: 1.7183241844177246
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 2
delta_size:: 120
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.461087942123413
time_baseline:: 1.465810775756836
curr_diff: 0 tensor(0.0069, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0069, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.872800
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 2
delta_size:: 120
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.1634531021118164
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0071, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0071, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.872800
repetition 4
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.6676311492919922
training time full:: 1.6676688194274902
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 2
delta_size:: 120
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4688522815704346
time_baseline:: 1.4736711978912354
curr_diff: 0 tensor(0.0069, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0069, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.872800
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 2
delta_size:: 120
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.0707933902740479
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0071, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0071, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.872800
repetition 5
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.674377202987671
training time full:: 1.6744132041931152
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 2
delta_size:: 120
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.452906608581543
time_baseline:: 1.457737922668457
curr_diff: 0 tensor(0.0069, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0069, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.872800
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 2
delta_size:: 120
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.10679030418396
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0071, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0071, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.872800
deletion rate:: 0.01
python3 generate_rand_ids 0.01  MNIST5 0
tensor([22529, 45060, 14342, 12299, 18444, 28690, 53269, 49177, 28701, 59422,
        12320, 59429, 12327, 18479, 14387, 32831, 53312, 49216,    66, 24643,
        38979, 45125,  6212, 36931, 30793, 38987, 53324, 45132,  2129, 38995,
         8284, 53341, 10332, 34908, 28770, 32866, 28779, 55410,  6260, 16514,
        10373, 47237, 55431, 30863,  4243,  8344, 24734, 22692,  2212, 34983,
        22700, 37049, 32969, 47315, 35034, 51422, 35045, 20709, 14571, 43247,
        53491, 35066, 22784, 10496, 10502, 20746, 49418, 37134, 12562, 16668,
        49439, 37153, 18722, 33062,   296, 53550, 33070, 35122, 59699, 24883,
        59701,   312, 16698,  4411,  8520, 20817, 18772, 33109,  2393, 12637,
        55650, 12643, 31075, 51560, 49514, 18797, 10612, 12662, 14717,  8573,
        59773,  4480, 16770, 57733,  8586, 31116, 27022, 10639, 41362, 59795,
        35221, 43414, 18842, 10655, 45473, 45474,  6575, 33202,  8627,   437,
         2487,  6584, 14787, 49605, 51654,  4554, 31179,  4557, 31185, 49617,
        31189, 57814,  6615, 47577, 45537, 18922, 33259,  6639, 39410, 27128,
        29177, 47617, 49666, 43529, 49674,  6667,  4617, 57870, 33295, 23059,
        51731, 10774, 10775, 14877, 59933, 21021, 25120, 12829, 16934, 39462,
        45608, 55850, 51756, 29230, 25135, 41520, 23090, 53815, 55865,  4670,
        14917,  8774, 47687, 25166, 55890, 43603, 23135, 37472, 21090, 55912,
        17002,   622,  4719, 19058,  8827, 57982, 47745,  6788, 23176, 45708,
        31373, 39565, 21138,   659, 51858, 47770, 21147, 55964,  4766, 39583,
        19106, 15015, 47785,  4780, 55981, 25262, 43705, 23230, 51904, 10945,
        25281, 31427, 21190, 56012, 25296,   729,   732, 53981, 21218, 53988,
        51941,  8934, 19176,  4843, 31468, 35569,   758, 19193, 49914, 51962,
        37628,  6915, 29445, 15110, 25352, 43788, 23309, 15118, 23311, 33551,
        19214, 51991, 45848, 27415, 43802, 37661,  4896,  2856, 11051, 13100,
        25414, 47943, 15176, 39752,  4934,  9035, 13132, 11087, 21331, 17236,
        17238,   861,  2913, 56164, 19310,  2932, 19318,  4993,  9090,  4995,
        41866, 21388, 43917,  5012, 33685,   922, 27547, 31643, 11170, 27559,
        48055, 37818, 19386,  3004,  9148, 27582, 33731, 35782, 43984, 48081,
        43991, 29660, 54239, 58336, 56298, 37867, 48107, 19437, 39919, 19439,
        46065, 35826, 27638, 19448, 13308, 56327, 44039, 44042, 58382, 31764,
        27669, 54294, 50200, 33817,  7198, 37920, 35877, 42030, 46137, 52285,
         3136, 40004, 19529, 58444, 11352, 21593, 37977, 25692, 42082,  3170,
        52322, 54373, 29810, 38003, 35956, 48247, 25720, 25723, 44156, 29824,
        25731, 46212,  9347, 19596, 11408,  1171, 42136, 19610, 48286,  9378,
        21672, 50345,  9387,  5294, 54448, 13496, 38074, 11452, 25793, 11466,
         5327,  9427, 56534,  7384, 25820, 36060, 56541, 48352, 29922,  9444,
        15590, 31978, 27884, 54509, 13548, 58610, 13558, 38134, 36093, 15614,
         5380, 58638, 32016, 46353, 56596, 32022, 11543, 54552,  5400, 58654,
        34081, 21797, 13611, 42285, 32051, 38200, 46396,  9536, 13635, 46406,
         9547, 36180,  7512, 34137, 52572, 19804, 58716, 44389, 30060,  7535,
        15733, 28023, 19834, 28030,  5505, 50571, 54668, 44429, 15762, 46483,
        34196, 52637, 13729, 44451, 44452, 44458, 17834, 23986, 58802, 15794,
        26043, 46531,  3529,  7628, 48595,  7637,  7641, 30171, 48607, 21984,
        19938, 56804, 24048, 48629, 19960, 52732, 15872, 52737, 42500, 58888,
        38411, 13835, 30226, 34325, 28182,  9755,  3612, 32286, 38436, 58917,
        20004, 44586, 28203, 30254, 26160, 24116,  7734, 50744,  7736, 17978,
        44620, 38478, 56914, 42578,  3667, 15958, 48727, 50778, 13915, 56923,
        38503, 11886, 50804,  1654, 20090, 59003, 52860, 22144, 22148, 13956,
        11912, 28299, 50831, 52880, 48784, 56978, 11927,  1690, 11934, 28320,
        52896, 18082, 16034,  9897, 50858,  1710, 40623, 36529, 46771, 32437,
        38582, 30392, 28350,  5827,  1735, 28363, 16076, 14030, 42708, 16086,
        46810, 28379,  7903, 22240, 22244, 26341, 44774, 26364, 46847,  7939,
        26375, 22282, 12043, 59148,  3854, 32531,  1813, 36632, 34585, 18206,
        22308, 24357, 36644, 28458,  7978, 32554, 28462, 38706, 12085, 16190,
        34631, 10057, 22346, 40782, 42835, 53076, 32600, 59228, 12125, 53091,
        36714, 51051, 46956, 30580, 18300, 46975, 16257,  3969, 20355, 24451,
        40838, 26504,  6026, 38801, 59285, 24470, 47001,  1947,  6045, 38821,
        26540, 34736, 44978,  8125, 24510, 14276, 30661, 34759, 51144, 42958,
         4049, 22484, 55254, 28630, 49113, 14300, 42984, 40936,  4082,  8178])
batch size:: 60000
repetition 1
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "

Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 109.02424097061157
training time full:: 109.03424382209778
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 120
delta_size:: 600
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 105.8459677696228
time_baseline:: 106.11763215065002
curr_diff: 0 tensor(0.0141, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0141, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875100
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 120
delta_size:: 600
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 54.42839241027832
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0146, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0146, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.874800
repetition 2
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "

Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 107.94120955467224
training time full:: 107.95249581336975
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 120
delta_size:: 600
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 104.746018409729
time_baseline:: 105.01383996009827
curr_diff: 0 tensor(0.0141, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0141, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875100
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 120
delta_size:: 600
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 54.253342628479004
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0146, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0146, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.874800
repetition 3
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "

Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 109.3633496761322
training time full:: 109.37592124938965
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 120
delta_size:: 600
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 106.2416729927063
time_baseline:: 106.51230311393738
curr_diff: 0 tensor(0.0141, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0141, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875100
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 120
delta_size:: 600
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 54.192142724990845
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0146, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0146, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.874800
repetition 4
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "

Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 108.44564771652222
training time full:: 108.45843696594238
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 120
delta_size:: 600
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 104.75778365135193
time_baseline:: 105.02533626556396
curr_diff: 0 tensor(0.0141, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0141, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875100
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 120
delta_size:: 600
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 54.92276120185852
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0146, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0146, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.874800
repetition 5
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "

Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 110.40770363807678
training time full:: 110.42001390457153
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 120
delta_size:: 600
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 105.09568548202515
time_baseline:: 105.36487293243408
curr_diff: 0 tensor(0.0141, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0141, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875100
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 120
delta_size:: 600
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 54.83344793319702
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0146, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0146, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.874800
batch size:: 30000
repetition 1
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.323384046554565
training time full:: 52.33052349090576
provenance prepare time:: 2.384185791015625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 47.56462550163269
time_baseline:: 47.70122194290161
curr_diff: 0 tensor(0.0141, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0141, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875100
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.934673070907593
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0148, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0148, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.874600
repetition 2
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.56692028045654
training time full:: 52.57302284240723
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 47.828497886657715
time_baseline:: 47.963552713394165
curr_diff: 0 tensor(0.0141, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0141, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875100
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 26.118551015853882
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0148, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0148, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.874600
repetition 3
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.38392448425293
training time full:: 52.390281200408936
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.269837617874146
time_baseline:: 48.40398192405701
curr_diff: 0 tensor(0.0141, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0141, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875100
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.997334957122803
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0148, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0148, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.874600
repetition 4
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.04721641540527
training time full:: 52.05339479446411
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 47.52533292770386
time_baseline:: 47.65913510322571
curr_diff: 0 tensor(0.0141, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0141, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875100
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.784615516662598
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0148, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0148, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.874600
repetition 5
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.88407850265503
training time full:: 52.890376329422
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 47.4058301448822
time_baseline:: 47.54108214378357
curr_diff: 0 tensor(0.0141, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0141, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875100
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.933104038238525
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0148, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0148, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.874600
batch size:: 16384
repetition 1
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.07838225364685
training time full:: 27.080944299697876
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 23.902837991714478
time_baseline:: 23.97342824935913
curr_diff: 0 tensor(0.0146, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0146, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 13.320615530014038
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0154, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0154, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
repetition 2
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.439228534698486
training time full:: 27.44164252281189
provenance prepare time:: 2.384185791015625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.04713201522827
time_baseline:: 24.118961334228516
curr_diff: 0 tensor(0.0146, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0146, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 13.371234655380249
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0154, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0154, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
repetition 3
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.14103627204895
training time full:: 27.14345955848694
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.431142568588257
time_baseline:: 24.503296613693237
curr_diff: 0 tensor(0.0146, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0146, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 13.365419387817383
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0154, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0154, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
repetition 4
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.2738196849823
training time full:: 27.276183128356934
provenance prepare time:: 1.1920928955078125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 23.890206575393677
time_baseline:: 23.960726499557495
curr_diff: 0 tensor(0.0146, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0146, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 13.197395324707031
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0154, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0154, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
repetition 5
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.18347406387329
training time full:: 27.185806035995483
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 23.91686701774597
time_baseline:: 23.98704719543457
curr_diff: 0 tensor(0.0146, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0146, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 13.28424620628357
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0154, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0154, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876100
batch size:: 4096
repetition 1
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.860250234603882
training time full:: 5.860287666320801
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 8
delta_size:: 600
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.001688718795776
time_baseline:: 5.018937349319458
curr_diff: 0 tensor(0.0142, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0142, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874800
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 8
delta_size:: 600
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.2745442390441895
curr_diff: 0 tensor(0.0020, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0020, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0151, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0151, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
repetition 2
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.881619691848755
training time full:: 5.881656646728516
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 8
delta_size:: 600
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 4.98696494102478
time_baseline:: 5.004320383071899
curr_diff: 0 tensor(0.0142, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0142, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874800
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 8
delta_size:: 600
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.3415873050689697
curr_diff: 0 tensor(0.0020, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0020, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0151, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0151, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
repetition 3
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.856091022491455
training time full:: 5.856129884719849
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 8
delta_size:: 600
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.031256198883057
time_baseline:: 5.048715114593506
curr_diff: 0 tensor(0.0142, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0142, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874800
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 8
delta_size:: 600
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.2268683910369873
curr_diff: 0 tensor(0.0020, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0020, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0151, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0151, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
repetition 4
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.880100727081299
training time full:: 5.880156755447388
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 8
delta_size:: 600
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.040562868118286
time_baseline:: 5.0586912631988525
curr_diff: 0 tensor(0.0142, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0142, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874800
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 8
delta_size:: 600
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.3017184734344482
curr_diff: 0 tensor(0.0020, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0020, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0151, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0151, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
repetition 5
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.877227783203125
training time full:: 5.877266883850098
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 8
delta_size:: 600
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.0417468547821045
time_baseline:: 5.058889150619507
curr_diff: 0 tensor(0.0142, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0142, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874800
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 8
delta_size:: 600
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.4992403984069824
curr_diff: 0 tensor(0.0020, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0020, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0151, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0151, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
batch size:: 1024
repetition 1
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.661113977432251
training time full:: 1.6611506938934326
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 2
delta_size:: 600
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.43699049949646
time_baseline:: 1.4417171478271484
curr_diff: 0 tensor(0.0142, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0142, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.872800
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 2
delta_size:: 600
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.0872652530670166
curr_diff: 0 tensor(0.0023, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0023, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0146, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0146, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.872800
repetition 2
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.695533037185669
training time full:: 1.6955711841583252
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 2
delta_size:: 600
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4194328784942627
time_baseline:: 1.4241952896118164
curr_diff: 0 tensor(0.0142, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0142, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.872800
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 2
delta_size:: 600
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.0963668823242188
curr_diff: 0 tensor(0.0023, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0023, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0146, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0146, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.872800
repetition 3
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.6589515209197998
training time full:: 1.6589882373809814
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 2
delta_size:: 600
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4182324409484863
time_baseline:: 1.423095941543579
curr_diff: 0 tensor(0.0142, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0142, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.872800
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 2
delta_size:: 600
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.1277549266815186
curr_diff: 0 tensor(0.0023, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0023, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0146, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0146, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.872800
repetition 4
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.6538662910461426
training time full:: 1.6539018154144287
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 2
delta_size:: 600
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4356024265289307
time_baseline:: 1.4405734539031982
curr_diff: 0 tensor(0.0142, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0142, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.872800
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 2
delta_size:: 600
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.0850231647491455
curr_diff: 0 tensor(0.0023, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0023, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0146, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0146, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.872800
repetition 5
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NLLLoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.6371665000915527
training time full:: 1.6372034549713135
provenance prepare time:: 1.1920928955078125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Logistic_regression. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LogSoftmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
max_epoch:: 2
delta_size:: 600
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4497921466827393
time_baseline:: 1.4546034336090088
curr_diff: 0 tensor(0.0142, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0142, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.872800
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 2
delta_size:: 600
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.073448657989502
curr_diff: 0 tensor(0.0023, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0023, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0146, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0146, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.872800
