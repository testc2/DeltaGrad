period::
init_iters::
varied deletion rate::
varied number of samples::
python3 generate_dataset_train_test.py Logistic_regression covtype 16384 120 5
start loading data...
normalization start!!
deletion rate:: 0.00002
python3 generate_rand_ids 0.00002  covtype 1
start loading data...
normalization start!!
tensor([469568,  51651, 388422, 266441, 103094, 364534, 333306, 136699, 507038,
        137087])
batch size:: 16384
repetition 0
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 0 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 1.978409
Train - Epoch 0, Batch: 10, Loss: 1.387576
Train - Epoch 0, Batch: 20, Loss: 1.246914
Train - Epoch 0, Batch: 30, Loss: 1.198132
Train - Epoch 1, Batch: 0, Loss: 1.178864
Train - Epoch 1, Batch: 10, Loss: 1.161919
Train - Epoch 1, Batch: 20, Loss: 1.136248
Train - Epoch 1, Batch: 30, Loss: 1.124277
Train - Epoch 2, Batch: 0, Loss: 1.137988
Train - Epoch 2, Batch: 10, Loss: 1.107748
Train - Epoch 2, Batch: 20, Loss: 1.117720
Train - Epoch 2, Batch: 30, Loss: 1.096334
Train - Epoch 3, Batch: 0, Loss: 1.098870
Train - Epoch 3, Batch: 10, Loss: 1.088066
Train - Epoch 3, Batch: 20, Loss: 1.086280
Train - Epoch 3, Batch: 30, Loss: 1.068023
Train - Epoch 4, Batch: 0, Loss: 1.075710
Train - Epoch 4, Batch: 10, Loss: 1.070952
Train - Epoch 4, Batch: 20, Loss: 1.064482
Train - Epoch 4, Batch: 30, Loss: 1.049990
Train - Epoch 5, Batch: 0, Loss: 1.062279
Train - Epoch 5, Batch: 10, Loss: 1.045241
Train - Epoch 5, Batch: 20, Loss: 1.045271
Train - Epoch 5, Batch: 30, Loss: 1.034009
Train - Epoch 6, Batch: 0, Loss: 1.036682
Train - Epoch 6, Batch: 10, Loss: 1.024502
Train - Epoch 6, Batch: 20, Loss: 1.018889
Train - Epoch 6, Batch: 30, Loss: 1.018066
Train - Epoch 7, Batch: 0, Loss: 1.021773
Train - Epoch 7, Batch: 10, Loss: 1.017729
Train - Epoch 7, Batch: 20, Loss: 1.021932
Train - Epoch 7, Batch: 30, Loss: 1.006185
Train - Epoch 8, Batch: 0, Loss: 1.009602
Train - Epoch 8, Batch: 10, Loss: 1.000678
Train - Epoch 8, Batch: 20, Loss: 1.010893
Train - Epoch 8, Batch: 30, Loss: 0.994796
Train - Epoch 9, Batch: 0, Loss: 1.000773
Train - Epoch 9, Batch: 10, Loss: 1.005734
Train - Epoch 9, Batch: 20, Loss: 0.996779
Train - Epoch 9, Batch: 30, Loss: 0.986163
Train - Epoch 10, Batch: 0, Loss: 0.980191
Train - Epoch 10, Batch: 10, Loss: 0.980362
Train - Epoch 10, Batch: 20, Loss: 0.997067
Train - Epoch 10, Batch: 30, Loss: 0.995085
Train - Epoch 11, Batch: 0, Loss: 0.974835
Train - Epoch 11, Batch: 10, Loss: 0.982447
Train - Epoch 11, Batch: 20, Loss: 0.974574
Train - Epoch 11, Batch: 30, Loss: 0.975427
Train - Epoch 12, Batch: 0, Loss: 0.963547
Train - Epoch 12, Batch: 10, Loss: 0.962926
Train - Epoch 12, Batch: 20, Loss: 0.965675
Train - Epoch 12, Batch: 30, Loss: 0.964908
Train - Epoch 13, Batch: 0, Loss: 0.959950
Train - Epoch 13, Batch: 10, Loss: 0.972395
Train - Epoch 13, Batch: 20, Loss: 0.960603
Train - Epoch 13, Batch: 30, Loss: 0.963067
Train - Epoch 14, Batch: 0, Loss: 0.959511
Train - Epoch 14, Batch: 10, Loss: 0.964972
Train - Epoch 14, Batch: 20, Loss: 0.949290
Train - Epoch 14, Batch: 30, Loss: 0.955097
Train - Epoch 15, Batch: 0, Loss: 0.953895
Train - Epoch 15, Batch: 10, Loss: 0.949214
Train - Epoch 15, Batch: 20, Loss: 0.944222
Train - Epoch 15, Batch: 30, Loss: 0.955433
Train - Epoch 16, Batch: 0, Loss: 0.941960
Train - Epoch 16, Batch: 10, Loss: 0.948343
Train - Epoch 16, Batch: 20, Loss: 0.946727
Train - Epoch 16, Batch: 30, Loss: 0.939192
Train - Epoch 17, Batch: 0, Loss: 0.948161
Train - Epoch 17, Batch: 10, Loss: 0.932830
Train - Epoch 17, Batch: 20, Loss: 0.943154
Train - Epoch 17, Batch: 30, Loss: 0.936618
Train - Epoch 18, Batch: 0, Loss: 0.931188
Train - Epoch 18, Batch: 10, Loss: 0.937416
Train - Epoch 18, Batch: 20, Loss: 0.936414
Train - Epoch 18, Batch: 30, Loss: 0.932230
Train - Epoch 19, Batch: 0, Loss: 0.937161
Train - Epoch 19, Batch: 10, Loss: 0.931480
Train - Epoch 19, Batch: 20, Loss: 0.926447
Train - Epoch 19, Batch: 30, Loss: 0.931056
Train - Epoch 20, Batch: 0, Loss: 0.926551
Train - Epoch 20, Batch: 10, Loss: 0.933387
Train - Epoch 20, Batch: 20, Loss: 0.921654
Train - Epoch 20, Batch: 30, Loss: 0.924972
Train - Epoch 21, Batch: 0, Loss: 0.922185
Train - Epoch 21, Batch: 10, Loss: 0.927429
Train - Epoch 21, Batch: 20, Loss: 0.922796
Train - Epoch 21, Batch: 30, Loss: 0.926176
Train - Epoch 22, Batch: 0, Loss: 0.923575
Train - Epoch 22, Batch: 10, Loss: 0.918407
Train - Epoch 22, Batch: 20, Loss: 0.926666
Train - Epoch 22, Batch: 30, Loss: 0.922469
Train - Epoch 23, Batch: 0, Loss: 0.914522
Train - Epoch 23, Batch: 10, Loss: 0.924981
Train - Epoch 23, Batch: 20, Loss: 0.921825
Train - Epoch 23, Batch: 30, Loss: 0.910071
Train - Epoch 24, Batch: 0, Loss: 0.916795
Train - Epoch 24, Batch: 10, Loss: 0.911980
Train - Epoch 24, Batch: 20, Loss: 0.917566
Train - Epoch 24, Batch: 30, Loss: 0.919156
Train - Epoch 25, Batch: 0, Loss: 0.914058
Train - Epoch 25, Batch: 10, Loss: 0.911645
Train - Epoch 25, Batch: 20, Loss: 0.900875
Train - Epoch 25, Batch: 30, Loss: 0.914496
Train - Epoch 26, Batch: 0, Loss: 0.912086
Train - Epoch 26, Batch: 10, Loss: 0.906192
Train - Epoch 26, Batch: 20, Loss: 0.916036
Train - Epoch 26, Batch: 30, Loss: 0.906962
Train - Epoch 27, Batch: 0, Loss: 0.898528
Train - Epoch 27, Batch: 10, Loss: 0.913980
Train - Epoch 27, Batch: 20, Loss: 0.902253
Train - Epoch 27, Batch: 30, Loss: 0.902477
Train - Epoch 28, Batch: 0, Loss: 0.896749
Train - Epoch 28, Batch: 10, Loss: 0.898847
Train - Epoch 28, Batch: 20, Loss: 0.898094
Train - Epoch 28, Batch: 30, Loss: 0.902719
Train - Epoch 29, Batch: 0, Loss: 0.901839
Train - Epoch 29, Batch: 10, Loss: 0.903283
Train - Epoch 29, Batch: 20, Loss: 0.899621
Train - Epoch 29, Batch: 30, Loss: 0.894298
Train - Epoch 30, Batch: 0, Loss: 0.888480
Train - Epoch 30, Batch: 10, Loss: 0.893969
Train - Epoch 30, Batch: 20, Loss: 0.890443
Train - Epoch 30, Batch: 30, Loss: 0.902645
Train - Epoch 31, Batch: 0, Loss: 0.886125
Train - Epoch 31, Batch: 10, Loss: 0.900624
Train - Epoch 31, Batch: 20, Loss: 0.887503
Train - Epoch 31, Batch: 30, Loss: 0.892132
Test Avg. Loss: 0.000070, Accuracy: 0.629890
training_time:: 3.243598222732544
training time full:: 3.2436602115631104
provenance prepare time:: 6.9141387939453125e-06
Test Avg. Loss: 0.000070, Accuracy: 0.629890
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
batch_size:: 16384
epoch:: 0
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 2.5907199382781982
overhead:: 0
overhead2:: 0.5006353855133057
overhead3:: 0
time_baseline:: 2.591214418411255
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629873
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.005391120910644531
overhead3:: 0.04174041748046875
overhead4:: 0.1644906997680664
overhead5:: 0
memory usage:: 3762966528
time_provenance:: 0.7872836589813232
curr_diff: 0 tensor(2.7393e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.7393e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629856
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.007379770278930664
overhead3:: 0.05490827560424805
overhead4:: 0.1769418716430664
overhead5:: 0
memory usage:: 3791065088
time_provenance:: 0.8393807411193848
curr_diff: 0 tensor(2.5201e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.5201e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629873
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.0064945220947265625
overhead3:: 0.06409955024719238
overhead4:: 0.18540453910827637
overhead5:: 0
memory usage:: 3764412416
time_provenance:: 0.8391139507293701
curr_diff: 0 tensor(2.8354e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.8354e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629856
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.008297204971313477
overhead3:: 0.053994178771972656
overhead4:: 0.2242894172668457
overhead5:: 0
memory usage:: 3789373440
time_provenance:: 0.8463406562805176
curr_diff: 0 tensor(2.6014e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.6014e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629873
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.008156061172485352
overhead3:: 0.07204508781433105
overhead4:: 0.22084474563598633
overhead5:: 0
memory usage:: 3763867648
time_provenance:: 0.8596107959747314
curr_diff: 0 tensor(2.8296e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.8296e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629856
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.010882139205932617
overhead3:: 0.07096719741821289
overhead4:: 0.3008267879486084
overhead5:: 0
memory usage:: 3776958464
time_provenance:: 0.9763767719268799
curr_diff: 0 tensor(1.4601e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4601e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629873
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.011992931365966797
overhead3:: 0.06758761405944824
overhead4:: 0.3096005916595459
overhead5:: 0
memory usage:: 3816472576
time_provenance:: 0.9752826690673828
curr_diff: 0 tensor(1.4650e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4650e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629873
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.011606454849243164
overhead3:: 0.06575679779052734
overhead4:: 0.3328847885131836
overhead5:: 0
memory usage:: 3789987840
time_provenance:: 0.9818220138549805
curr_diff: 0 tensor(1.4902e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4902e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629873
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.016449928283691406
overhead3:: 0.07419848442077637
overhead4:: 0.3416576385498047
overhead5:: 0
memory usage:: 3762544640
time_provenance:: 1.0144932270050049
curr_diff: 0 tensor(1.4859e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4859e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629873
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.013927936553955078
overhead3:: 0.07813143730163574
overhead4:: 0.3592383861541748
overhead5:: 0
memory usage:: 3783110656
time_provenance:: 1.0130627155303955
curr_diff: 0 tensor(1.4894e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4894e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629873
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.020444393157958984
overhead3:: 0.10487937927246094
overhead4:: 0.5358564853668213
overhead5:: 0
memory usage:: 3764408320
time_provenance:: 1.2696137428283691
curr_diff: 0 tensor(6.4512e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.4512e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629873
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.021141767501831055
overhead3:: 0.11329174041748047
overhead4:: 0.5569441318511963
overhead5:: 0
memory usage:: 3775836160
time_provenance:: 1.3115026950836182
curr_diff: 0 tensor(6.4768e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.4768e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629873
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.022054195404052734
overhead3:: 0.10699868202209473
overhead4:: 0.5531961917877197
overhead5:: 0
memory usage:: 3799961600
time_provenance:: 1.278665542602539
curr_diff: 0 tensor(6.6942e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.6942e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629873
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.021823644638061523
overhead3:: 0.10067081451416016
overhead4:: 0.5272085666656494
overhead5:: 0
memory usage:: 3769622528
time_provenance:: 1.2299599647521973
curr_diff: 0 tensor(6.6841e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.6841e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629873
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.022034645080566406
overhead3:: 0.1096498966217041
overhead4:: 0.5948140621185303
overhead5:: 0
memory usage:: 3783110656
time_provenance:: 1.3276753425598145
curr_diff: 0 tensor(6.6999e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.6999e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629873
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.04156231880187988
overhead3:: 0.21593666076660156
overhead4:: 1.312328577041626
overhead5:: 0
memory usage:: 3782397952
time_provenance:: 2.242002248764038
curr_diff: 0 tensor(2.0450e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.0450e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629873
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.04162240028381348
overhead3:: 0.2076396942138672
overhead4:: 1.1336569786071777
overhead5:: 0
memory usage:: 3781951488
time_provenance:: 2.011784553527832
curr_diff: 0 tensor(2.0497e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.0497e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629873
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.06429195404052734
overhead3:: 0.28578996658325195
overhead4:: 1.6697051525115967
overhead5:: 0
memory usage:: 3790426112
time_provenance:: 2.876152992248535
curr_diff: 0 tensor(2.0862e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.0862e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629873
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.0445857048034668
overhead3:: 0.2201228141784668
overhead4:: 1.3405516147613525
overhead5:: 0
memory usage:: 3798228992
time_provenance:: 2.29142689704895
curr_diff: 0 tensor(2.0884e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.0884e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629873
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.04985976219177246
overhead3:: 0.23389148712158203
overhead4:: 1.1897046566009521
overhead5:: 0
memory usage:: 3791060992
time_provenance:: 2.1118624210357666
curr_diff: 0 tensor(2.0871e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.0871e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629873
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 0 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.09558868408203125
overhead3:: 0.5294642448425293
overhead4:: 2.208338499069214
overhead5:: 0
memory usage:: 3803930624
time_provenance:: 3.056924343109131
curr_diff: 0 tensor(1.0119e-13, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0119e-13, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629873
repetition 1
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 1 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 1.825744
Train - Epoch 0, Batch: 10, Loss: 1.345401
Train - Epoch 0, Batch: 20, Loss: 1.218651
Train - Epoch 0, Batch: 30, Loss: 1.178651
Train - Epoch 1, Batch: 0, Loss: 1.162383
Train - Epoch 1, Batch: 10, Loss: 1.136451
Train - Epoch 1, Batch: 20, Loss: 1.138177
Train - Epoch 1, Batch: 30, Loss: 1.113028
Train - Epoch 2, Batch: 0, Loss: 1.107800
Train - Epoch 2, Batch: 10, Loss: 1.110995
Train - Epoch 2, Batch: 20, Loss: 1.100107
Train - Epoch 2, Batch: 30, Loss: 1.087674
Train - Epoch 3, Batch: 0, Loss: 1.101025
Train - Epoch 3, Batch: 10, Loss: 1.078509
Train - Epoch 3, Batch: 20, Loss: 1.074809
Train - Epoch 3, Batch: 30, Loss: 1.066894
Train - Epoch 4, Batch: 0, Loss: 1.073204
Train - Epoch 4, Batch: 10, Loss: 1.059793
Train - Epoch 4, Batch: 20, Loss: 1.053129
Train - Epoch 4, Batch: 30, Loss: 1.047037
Train - Epoch 5, Batch: 0, Loss: 1.039064
Train - Epoch 5, Batch: 10, Loss: 1.040571
Train - Epoch 5, Batch: 20, Loss: 1.035775
Train - Epoch 5, Batch: 30, Loss: 1.033660
Train - Epoch 6, Batch: 0, Loss: 1.042055
Train - Epoch 6, Batch: 10, Loss: 1.025185
Train - Epoch 6, Batch: 20, Loss: 1.024544
Train - Epoch 6, Batch: 30, Loss: 1.018072
Train - Epoch 7, Batch: 0, Loss: 1.014377
Train - Epoch 7, Batch: 10, Loss: 1.007664
Train - Epoch 7, Batch: 20, Loss: 1.012262
Train - Epoch 7, Batch: 30, Loss: 0.999541
Train - Epoch 8, Batch: 0, Loss: 1.007392
Train - Epoch 8, Batch: 10, Loss: 0.993975
Train - Epoch 8, Batch: 20, Loss: 1.006609
Train - Epoch 8, Batch: 30, Loss: 0.987970
Train - Epoch 9, Batch: 0, Loss: 0.989801
Train - Epoch 9, Batch: 10, Loss: 0.989780
Train - Epoch 9, Batch: 20, Loss: 0.979662
Train - Epoch 9, Batch: 30, Loss: 0.987912
Train - Epoch 10, Batch: 0, Loss: 0.985910
Train - Epoch 10, Batch: 10, Loss: 0.978054
Train - Epoch 10, Batch: 20, Loss: 0.976042
Train - Epoch 10, Batch: 30, Loss: 0.968138
Train - Epoch 11, Batch: 0, Loss: 0.976859
Train - Epoch 11, Batch: 10, Loss: 0.975908
Train - Epoch 11, Batch: 20, Loss: 0.972779
Train - Epoch 11, Batch: 30, Loss: 0.962463
Train - Epoch 12, Batch: 0, Loss: 0.976394
Train - Epoch 12, Batch: 10, Loss: 0.962550
Train - Epoch 12, Batch: 20, Loss: 0.961313
Train - Epoch 12, Batch: 30, Loss: 0.950939
Train - Epoch 13, Batch: 0, Loss: 0.958574
Train - Epoch 13, Batch: 10, Loss: 0.961741
Train - Epoch 13, Batch: 20, Loss: 0.965351
Train - Epoch 13, Batch: 30, Loss: 0.956973
Train - Epoch 14, Batch: 0, Loss: 0.950109
Train - Epoch 14, Batch: 10, Loss: 0.954367
Train - Epoch 14, Batch: 20, Loss: 0.957656
Train - Epoch 14, Batch: 30, Loss: 0.955826
Train - Epoch 15, Batch: 0, Loss: 0.951000
Train - Epoch 15, Batch: 10, Loss: 0.952369
Train - Epoch 15, Batch: 20, Loss: 0.946256
Train - Epoch 15, Batch: 30, Loss: 0.940474
Train - Epoch 16, Batch: 0, Loss: 0.947762
Train - Epoch 16, Batch: 10, Loss: 0.946099
Train - Epoch 16, Batch: 20, Loss: 0.943726
Train - Epoch 16, Batch: 30, Loss: 0.938340
Train - Epoch 17, Batch: 0, Loss: 0.938740
Train - Epoch 17, Batch: 10, Loss: 0.938030
Train - Epoch 17, Batch: 20, Loss: 0.938779
Train - Epoch 17, Batch: 30, Loss: 0.936061
Train - Epoch 18, Batch: 0, Loss: 0.935815
Train - Epoch 18, Batch: 10, Loss: 0.940249
Train - Epoch 18, Batch: 20, Loss: 0.919763
Train - Epoch 18, Batch: 30, Loss: 0.930754
Train - Epoch 19, Batch: 0, Loss: 0.934812
Train - Epoch 19, Batch: 10, Loss: 0.928171
Train - Epoch 19, Batch: 20, Loss: 0.918541
Train - Epoch 19, Batch: 30, Loss: 0.919957
Train - Epoch 20, Batch: 0, Loss: 0.928685
Train - Epoch 20, Batch: 10, Loss: 0.922645
Train - Epoch 20, Batch: 20, Loss: 0.930164
Train - Epoch 20, Batch: 30, Loss: 0.916196
Train - Epoch 21, Batch: 0, Loss: 0.923948
Train - Epoch 21, Batch: 10, Loss: 0.920508
Train - Epoch 21, Batch: 20, Loss: 0.920751
Train - Epoch 21, Batch: 30, Loss: 0.922879
Train - Epoch 22, Batch: 0, Loss: 0.911855
Train - Epoch 22, Batch: 10, Loss: 0.920762
Train - Epoch 22, Batch: 20, Loss: 0.914295
Train - Epoch 22, Batch: 30, Loss: 0.921251
Train - Epoch 23, Batch: 0, Loss: 0.925174
Train - Epoch 23, Batch: 10, Loss: 0.918713
Train - Epoch 23, Batch: 20, Loss: 0.920277
Train - Epoch 23, Batch: 30, Loss: 0.900517
Train - Epoch 24, Batch: 0, Loss: 0.911242
Train - Epoch 24, Batch: 10, Loss: 0.906349
Train - Epoch 24, Batch: 20, Loss: 0.910959
Train - Epoch 24, Batch: 30, Loss: 0.912563
Train - Epoch 25, Batch: 0, Loss: 0.911579
Train - Epoch 25, Batch: 10, Loss: 0.918274
Train - Epoch 25, Batch: 20, Loss: 0.912139
Train - Epoch 25, Batch: 30, Loss: 0.906382
Train - Epoch 26, Batch: 0, Loss: 0.913251
Train - Epoch 26, Batch: 10, Loss: 0.900636
Train - Epoch 26, Batch: 20, Loss: 0.908170
Train - Epoch 26, Batch: 30, Loss: 0.904471
Train - Epoch 27, Batch: 0, Loss: 0.906525
Train - Epoch 27, Batch: 10, Loss: 0.907686
Train - Epoch 27, Batch: 20, Loss: 0.902163
Train - Epoch 27, Batch: 30, Loss: 0.898150
Train - Epoch 28, Batch: 0, Loss: 0.908271
Train - Epoch 28, Batch: 10, Loss: 0.893464
Train - Epoch 28, Batch: 20, Loss: 0.903760
Train - Epoch 28, Batch: 30, Loss: 0.894440
Train - Epoch 29, Batch: 0, Loss: 0.901540
Train - Epoch 29, Batch: 10, Loss: 0.918059
Train - Epoch 29, Batch: 20, Loss: 0.891420
Train - Epoch 29, Batch: 30, Loss: 0.891619
Train - Epoch 30, Batch: 0, Loss: 0.901931
Train - Epoch 30, Batch: 10, Loss: 0.890958
Train - Epoch 30, Batch: 20, Loss: 0.889833
Train - Epoch 30, Batch: 30, Loss: 0.893815
Train - Epoch 31, Batch: 0, Loss: 0.887921
Train - Epoch 31, Batch: 10, Loss: 0.894868
Train - Epoch 31, Batch: 20, Loss: 0.904409
Train - Epoch 31, Batch: 30, Loss: 0.891939
Test Avg. Loss: 0.000070, Accuracy: 0.629477
training_time:: 3.5367448329925537
training time full:: 3.5368094444274902
provenance prepare time:: 6.9141387939453125e-06
Test Avg. Loss: 0.000070, Accuracy: 0.629477
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
batch_size:: 16384
epoch:: 0
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 2.3692007064819336
overhead:: 0
overhead2:: 0.5005359649658203
overhead3:: 0
time_baseline:: 2.3697028160095215
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629477
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.00494694709777832
overhead3:: 0.052785396575927734
overhead4:: 0.14395475387573242
overhead5:: 0
memory usage:: 3764178944
time_provenance:: 0.758948802947998
curr_diff: 0 tensor(2.0430e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.0430e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629477
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.008120059967041016
overhead3:: 0.06754684448242188
overhead4:: 0.1700458526611328
overhead5:: 0
memory usage:: 3767353344
time_provenance:: 0.8233203887939453
curr_diff: 0 tensor(2.0623e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.0623e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629477
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.005823850631713867
overhead3:: 0.05461311340332031
overhead4:: 0.1922605037689209
overhead5:: 0
memory usage:: 3768832000
time_provenance:: 0.8610570430755615
curr_diff: 0 tensor(2.2106e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.2106e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629477
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.008462905883789062
overhead3:: 0.06550908088684082
overhead4:: 0.21733927726745605
overhead5:: 0
memory usage:: 3765321728
time_provenance:: 0.8215878009796143
curr_diff: 0 tensor(2.0232e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.0232e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629477
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.007244110107421875
overhead3:: 0.06332564353942871
overhead4:: 0.2148592472076416
overhead5:: 0
memory usage:: 3768741888
time_provenance:: 0.8734951019287109
curr_diff: 0 tensor(2.1797e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1797e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629477
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.009760379791259766
overhead3:: 0.08367276191711426
overhead4:: 0.31766557693481445
overhead5:: 0
memory usage:: 3775188992
time_provenance:: 1.0361487865447998
curr_diff: 0 tensor(1.3366e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3366e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629477
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.009856462478637695
overhead3:: 0.06249809265136719
overhead4:: 0.3077239990234375
overhead5:: 0
memory usage:: 3763879936
time_provenance:: 0.9772968292236328
curr_diff: 0 tensor(1.3501e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3501e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629477
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.010615825653076172
overhead3:: 0.06603074073791504
overhead4:: 0.33347296714782715
overhead5:: 0
memory usage:: 3782103040
time_provenance:: 1.0062124729156494
curr_diff: 0 tensor(1.3534e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3534e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629477
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.011388301849365234
overhead3:: 0.07694363594055176
overhead4:: 0.3504648208618164
overhead5:: 0
memory usage:: 3783446528
time_provenance:: 1.056011438369751
curr_diff: 0 tensor(1.3529e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3529e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629477
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.012154579162597656
overhead3:: 0.07216119766235352
overhead4:: 0.3337442874908447
overhead5:: 0
memory usage:: 3770208256
time_provenance:: 0.996877908706665
curr_diff: 0 tensor(1.3500e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3500e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629477
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.016296863555908203
overhead3:: 0.10252976417541504
overhead4:: 0.5200927257537842
overhead5:: 0
memory usage:: 3782168576
time_provenance:: 1.2519125938415527
curr_diff: 0 tensor(7.2670e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.2670e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629477
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.021820783615112305
overhead3:: 0.12298250198364258
overhead4:: 0.6621978282928467
overhead5:: 0
memory usage:: 3764539392
time_provenance:: 1.6120448112487793
curr_diff: 0 tensor(7.2935e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.2935e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629477
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.0173647403717041
overhead3:: 0.11186480522155762
overhead4:: 0.5367221832275391
overhead5:: 0
memory usage:: 3767668736
time_provenance:: 1.283879041671753
curr_diff: 0 tensor(7.3145e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.3145e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629477
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.019547700881958008
overhead3:: 0.10568666458129883
overhead4:: 0.5203502178192139
overhead5:: 0
memory usage:: 3800113152
time_provenance:: 1.2535450458526611
curr_diff: 0 tensor(7.3216e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.3216e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629477
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.018281221389770508
overhead3:: 0.109161376953125
overhead4:: 0.5678343772888184
overhead5:: 0
memory usage:: 3790401536
time_provenance:: 1.3051731586456299
curr_diff: 0 tensor(7.3262e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.3262e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629477
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.05319857597351074
overhead3:: 0.25929784774780273
overhead4:: 1.5537636280059814
overhead5:: 0
memory usage:: 3775569920
time_provenance:: 2.7081217765808105
curr_diff: 0 tensor(1.5749e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5749e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629477
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.04114580154418945
overhead3:: 0.21472930908203125
overhead4:: 1.2103979587554932
overhead5:: 0
memory usage:: 3774726144
time_provenance:: 2.131070613861084
curr_diff: 0 tensor(1.5921e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5921e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629477
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.04259371757507324
overhead3:: 0.22501373291015625
overhead4:: 1.265737533569336
overhead5:: 0
memory usage:: 3769880576
time_provenance:: 2.224961042404175
curr_diff: 0 tensor(1.5947e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5947e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629477
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.04387164115905762
overhead3:: 0.22190070152282715
overhead4:: 1.1505849361419678
overhead5:: 0
memory usage:: 3770253312
time_provenance:: 2.02913236618042
curr_diff: 0 tensor(1.5996e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5996e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629477
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.04255986213684082
overhead3:: 0.23520469665527344
overhead4:: 1.2727928161621094
overhead5:: 0
memory usage:: 3770269696
time_provenance:: 2.2144601345062256
curr_diff: 0 tensor(1.5985e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5985e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629477
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 1 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.0968465805053711
overhead3:: 0.515723705291748
overhead4:: 2.2303197383880615
overhead5:: 0
memory usage:: 3760562176
time_provenance:: 3.070871591567993
curr_diff: 0 tensor(1.0037e-13, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0037e-13, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629477
repetition 2
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 2 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 1.951470
Train - Epoch 0, Batch: 10, Loss: 1.395031
Train - Epoch 0, Batch: 20, Loss: 1.260148
Train - Epoch 0, Batch: 30, Loss: 1.201164
Train - Epoch 1, Batch: 0, Loss: 1.189960
Train - Epoch 1, Batch: 10, Loss: 1.167276
Train - Epoch 1, Batch: 20, Loss: 1.151501
Train - Epoch 1, Batch: 30, Loss: 1.138167
Train - Epoch 2, Batch: 0, Loss: 1.138510
Train - Epoch 2, Batch: 10, Loss: 1.128338
Train - Epoch 2, Batch: 20, Loss: 1.105665
Train - Epoch 2, Batch: 30, Loss: 1.121284
Train - Epoch 3, Batch: 0, Loss: 1.106751
Train - Epoch 3, Batch: 10, Loss: 1.106827
Train - Epoch 3, Batch: 20, Loss: 1.086242
Train - Epoch 3, Batch: 30, Loss: 1.091040
Train - Epoch 4, Batch: 0, Loss: 1.080607
Train - Epoch 4, Batch: 10, Loss: 1.066302
Train - Epoch 4, Batch: 20, Loss: 1.063785
Train - Epoch 4, Batch: 30, Loss: 1.054178
Train - Epoch 5, Batch: 0, Loss: 1.058376
Train - Epoch 5, Batch: 10, Loss: 1.058332
Train - Epoch 5, Batch: 20, Loss: 1.041128
Train - Epoch 5, Batch: 30, Loss: 1.042802
Train - Epoch 6, Batch: 0, Loss: 1.044813
Train - Epoch 6, Batch: 10, Loss: 1.028407
Train - Epoch 6, Batch: 20, Loss: 1.035854
Train - Epoch 6, Batch: 30, Loss: 1.020379
Train - Epoch 7, Batch: 0, Loss: 1.020431
Train - Epoch 7, Batch: 10, Loss: 1.027415
Train - Epoch 7, Batch: 20, Loss: 1.016539
Train - Epoch 7, Batch: 30, Loss: 1.016817
Train - Epoch 8, Batch: 0, Loss: 1.007414
Train - Epoch 8, Batch: 10, Loss: 1.007561
Train - Epoch 8, Batch: 20, Loss: 1.004520
Train - Epoch 8, Batch: 30, Loss: 1.003712
Train - Epoch 9, Batch: 0, Loss: 1.000483
Train - Epoch 9, Batch: 10, Loss: 1.001233
Train - Epoch 9, Batch: 20, Loss: 1.000669
Train - Epoch 9, Batch: 30, Loss: 0.986304
Train - Epoch 10, Batch: 0, Loss: 0.991503
Train - Epoch 10, Batch: 10, Loss: 0.987957
Train - Epoch 10, Batch: 20, Loss: 0.974609
Train - Epoch 10, Batch: 30, Loss: 0.987167
Train - Epoch 11, Batch: 0, Loss: 0.981916
Train - Epoch 11, Batch: 10, Loss: 0.972728
Train - Epoch 11, Batch: 20, Loss: 0.975618
Train - Epoch 11, Batch: 30, Loss: 0.971781
Train - Epoch 12, Batch: 0, Loss: 0.970556
Train - Epoch 12, Batch: 10, Loss: 0.976695
Train - Epoch 12, Batch: 20, Loss: 0.966877
Train - Epoch 12, Batch: 30, Loss: 0.974286
Train - Epoch 13, Batch: 0, Loss: 0.968130
Train - Epoch 13, Batch: 10, Loss: 0.956858
Train - Epoch 13, Batch: 20, Loss: 0.960289
Train - Epoch 13, Batch: 30, Loss: 0.973774
Train - Epoch 14, Batch: 0, Loss: 0.957574
Train - Epoch 14, Batch: 10, Loss: 0.964309
Train - Epoch 14, Batch: 20, Loss: 0.956943
Train - Epoch 14, Batch: 30, Loss: 0.962045
Train - Epoch 15, Batch: 0, Loss: 0.955673
Train - Epoch 15, Batch: 10, Loss: 0.948547
Train - Epoch 15, Batch: 20, Loss: 0.943669
Train - Epoch 15, Batch: 30, Loss: 0.944838
Train - Epoch 16, Batch: 0, Loss: 0.954684
Train - Epoch 16, Batch: 10, Loss: 0.943278
Train - Epoch 16, Batch: 20, Loss: 0.935466
Train - Epoch 16, Batch: 30, Loss: 0.948074
Train - Epoch 17, Batch: 0, Loss: 0.949652
Train - Epoch 17, Batch: 10, Loss: 0.944783
Train - Epoch 17, Batch: 20, Loss: 0.924991
Train - Epoch 17, Batch: 30, Loss: 0.937915
Train - Epoch 18, Batch: 0, Loss: 0.933046
Train - Epoch 18, Batch: 10, Loss: 0.932774
Train - Epoch 18, Batch: 20, Loss: 0.932482
Train - Epoch 18, Batch: 30, Loss: 0.934703
Train - Epoch 19, Batch: 0, Loss: 0.930491
Train - Epoch 19, Batch: 10, Loss: 0.922860
Train - Epoch 19, Batch: 20, Loss: 0.933626
Train - Epoch 19, Batch: 30, Loss: 0.922446
Train - Epoch 20, Batch: 0, Loss: 0.926774
Train - Epoch 20, Batch: 10, Loss: 0.922338
Train - Epoch 20, Batch: 20, Loss: 0.919842
Train - Epoch 20, Batch: 30, Loss: 0.922704
Train - Epoch 21, Batch: 0, Loss: 0.927523
Train - Epoch 21, Batch: 10, Loss: 0.919900
Train - Epoch 21, Batch: 20, Loss: 0.916846
Train - Epoch 21, Batch: 30, Loss: 0.916265
Train - Epoch 22, Batch: 0, Loss: 0.921488
Train - Epoch 22, Batch: 10, Loss: 0.924399
Train - Epoch 22, Batch: 20, Loss: 0.922194
Train - Epoch 22, Batch: 30, Loss: 0.915348
Train - Epoch 23, Batch: 0, Loss: 0.917448
Train - Epoch 23, Batch: 10, Loss: 0.915188
Train - Epoch 23, Batch: 20, Loss: 0.910497
Train - Epoch 23, Batch: 30, Loss: 0.919448
Train - Epoch 24, Batch: 0, Loss: 0.909554
Train - Epoch 24, Batch: 10, Loss: 0.920014
Train - Epoch 24, Batch: 20, Loss: 0.906736
Train - Epoch 24, Batch: 30, Loss: 0.909684
Train - Epoch 25, Batch: 0, Loss: 0.904979
Train - Epoch 25, Batch: 10, Loss: 0.916817
Train - Epoch 25, Batch: 20, Loss: 0.916151
Train - Epoch 25, Batch: 30, Loss: 0.918809
Train - Epoch 26, Batch: 0, Loss: 0.905957
Train - Epoch 26, Batch: 10, Loss: 0.906129
Train - Epoch 26, Batch: 20, Loss: 0.900239
Train - Epoch 26, Batch: 30, Loss: 0.909002
Train - Epoch 27, Batch: 0, Loss: 0.909857
Train - Epoch 27, Batch: 10, Loss: 0.901264
Train - Epoch 27, Batch: 20, Loss: 0.917405
Train - Epoch 27, Batch: 30, Loss: 0.897495
Train - Epoch 28, Batch: 0, Loss: 0.906139
Train - Epoch 28, Batch: 10, Loss: 0.903465
Train - Epoch 28, Batch: 20, Loss: 0.909698
Train - Epoch 28, Batch: 30, Loss: 0.904257
Train - Epoch 29, Batch: 0, Loss: 0.900640
Train - Epoch 29, Batch: 10, Loss: 0.893216
Train - Epoch 29, Batch: 20, Loss: 0.900633
Train - Epoch 29, Batch: 30, Loss: 0.898654
Train - Epoch 30, Batch: 0, Loss: 0.897880
Train - Epoch 30, Batch: 10, Loss: 0.899579
Train - Epoch 30, Batch: 20, Loss: 0.902767
Train - Epoch 30, Batch: 30, Loss: 0.889794
Train - Epoch 31, Batch: 0, Loss: 0.890537
Train - Epoch 31, Batch: 10, Loss: 0.891862
Train - Epoch 31, Batch: 20, Loss: 0.894734
Train - Epoch 31, Batch: 30, Loss: 0.882113
Test Avg. Loss: 0.000070, Accuracy: 0.629787
training_time:: 3.547132730484009
training time full:: 3.547193765640259
provenance prepare time:: 7.3909759521484375e-06
Test Avg. Loss: 0.000070, Accuracy: 0.629787
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
batch_size:: 16384
epoch:: 0
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 2.3563907146453857
overhead:: 0
overhead2:: 0.4938077926635742
overhead3:: 0
time_baseline:: 2.3569371700286865
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629787
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.007051706314086914
overhead3:: 0.049825191497802734
overhead4:: 0.16579508781433105
overhead5:: 0
memory usage:: 3800907776
time_provenance:: 0.7803781032562256
curr_diff: 0 tensor(2.6981e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.6981e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629787
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.00690150260925293
overhead3:: 0.04411172866821289
overhead4:: 0.17933392524719238
overhead5:: 0
memory usage:: 3797962752
time_provenance:: 0.8024461269378662
curr_diff: 0 tensor(2.8398e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.8398e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629787
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.008540868759155273
overhead3:: 0.056001901626586914
overhead4:: 0.19145679473876953
overhead5:: 0
memory usage:: 3766235136
time_provenance:: 0.7898445129394531
curr_diff: 0 tensor(2.6966e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.6966e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629787
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.008036375045776367
overhead3:: 0.05177807807922363
overhead4:: 0.21166706085205078
overhead5:: 0
memory usage:: 3816017920
time_provenance:: 0.8765451908111572
curr_diff: 0 tensor(2.9985e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.9985e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629787
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.010504961013793945
overhead3:: 0.07462668418884277
overhead4:: 0.2502439022064209
overhead5:: 0
memory usage:: 3790721024
time_provenance:: 0.9144439697265625
curr_diff: 0 tensor(2.7607e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.7607e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629787
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.010848522186279297
overhead3:: 0.056372880935668945
overhead4:: 0.2703557014465332
overhead5:: 0
memory usage:: 3772411904
time_provenance:: 0.9018001556396484
curr_diff: 0 tensor(1.4074e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4074e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629787
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.011614799499511719
overhead3:: 0.061350345611572266
overhead4:: 0.32886672019958496
overhead5:: 0
memory usage:: 3763699712
time_provenance:: 0.9911961555480957
curr_diff: 0 tensor(1.4082e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4082e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629787
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.014348983764648438
overhead3:: 0.07020354270935059
overhead4:: 0.36214470863342285
overhead5:: 0
memory usage:: 3769683968
time_provenance:: 1.0940921306610107
curr_diff: 0 tensor(1.4142e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4142e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629787
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.014564752578735352
overhead3:: 0.07020211219787598
overhead4:: 0.3345770835876465
overhead5:: 0
memory usage:: 3769610240
time_provenance:: 1.0031359195709229
curr_diff: 0 tensor(1.4453e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4453e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629787
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.013898611068725586
overhead3:: 0.07151412963867188
overhead4:: 0.3795356750488281
overhead5:: 0
memory usage:: 3782844416
time_provenance:: 1.0311694145202637
curr_diff: 0 tensor(1.4641e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4641e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629787
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.02294158935546875
overhead3:: 0.12519311904907227
overhead4:: 0.6959900856018066
overhead5:: 0
memory usage:: 3796840448
time_provenance:: 1.6490819454193115
curr_diff: 0 tensor(4.9332e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.9332e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629787
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.019027233123779297
overhead3:: 0.11834216117858887
overhead4:: 0.5374677181243896
overhead5:: 0
memory usage:: 3764477952
time_provenance:: 1.2865071296691895
curr_diff: 0 tensor(5.0454e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.0454e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629787
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.019247055053710938
overhead3:: 0.1026303768157959
overhead4:: 0.5465617179870605
overhead5:: 0
memory usage:: 3766378496
time_provenance:: 1.2768440246582031
curr_diff: 0 tensor(5.0650e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.0650e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629787
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.02232813835144043
overhead3:: 0.12499356269836426
overhead4:: 0.5141205787658691
overhead5:: 0
memory usage:: 3763007488
time_provenance:: 1.2556180953979492
curr_diff: 0 tensor(5.1123e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.1123e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629787
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.021178483963012695
overhead3:: 0.11222457885742188
overhead4:: 0.6109440326690674
overhead5:: 0
memory usage:: 3790618624
time_provenance:: 1.3502342700958252
curr_diff: 0 tensor(5.1294e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.1294e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629787
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.04001355171203613
overhead3:: 0.219984769821167
overhead4:: 1.155874490737915
overhead5:: 0
memory usage:: 3827638272
time_provenance:: 2.0616238117218018
curr_diff: 0 tensor(1.3044e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3044e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629787
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.04437136650085449
overhead3:: 0.2277512550354004
overhead4:: 1.2953863143920898
overhead5:: 0
memory usage:: 3763068928
time_provenance:: 2.2865068912506104
curr_diff: 0 tensor(1.3561e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3561e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629787
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.044077396392822266
overhead3:: 0.2520256042480469
overhead4:: 1.3297820091247559
overhead5:: 0
memory usage:: 3798802432
time_provenance:: 2.3614065647125244
curr_diff: 0 tensor(1.3766e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3766e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629787
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.040972232818603516
overhead3:: 0.21937942504882812
overhead4:: 1.2445247173309326
overhead5:: 0
memory usage:: 3768590336
time_provenance:: 2.1897048950195312
curr_diff: 0 tensor(1.3721e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3721e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629787
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.05576944351196289
overhead3:: 0.28052806854248047
overhead4:: 1.616154432296753
overhead5:: 0
memory usage:: 3762524160
time_provenance:: 2.8151285648345947
curr_diff: 0 tensor(1.3717e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3717e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629787
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 2 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.10532450675964355
overhead3:: 0.6019401550292969
overhead4:: 2.3299336433410645
overhead5:: 0
memory usage:: 3761111040
time_provenance:: 3.258715867996216
curr_diff: 0 tensor(9.9671e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.9671e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629787
repetition 3
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 3 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 1.895014
Train - Epoch 0, Batch: 10, Loss: 1.358362
Train - Epoch 0, Batch: 20, Loss: 1.228149
Train - Epoch 0, Batch: 30, Loss: 1.184613
Train - Epoch 1, Batch: 0, Loss: 1.165726
Train - Epoch 1, Batch: 10, Loss: 1.152237
Train - Epoch 1, Batch: 20, Loss: 1.124350
Train - Epoch 1, Batch: 30, Loss: 1.123262
Train - Epoch 2, Batch: 0, Loss: 1.124951
Train - Epoch 2, Batch: 10, Loss: 1.108547
Train - Epoch 2, Batch: 20, Loss: 1.099037
Train - Epoch 2, Batch: 30, Loss: 1.098979
Train - Epoch 3, Batch: 0, Loss: 1.099151
Train - Epoch 3, Batch: 10, Loss: 1.080318
Train - Epoch 3, Batch: 20, Loss: 1.070475
Train - Epoch 3, Batch: 30, Loss: 1.063301
Train - Epoch 4, Batch: 0, Loss: 1.063129
Train - Epoch 4, Batch: 10, Loss: 1.064717
Train - Epoch 4, Batch: 20, Loss: 1.049947
Train - Epoch 4, Batch: 30, Loss: 1.061525
Train - Epoch 5, Batch: 0, Loss: 1.048848
Train - Epoch 5, Batch: 10, Loss: 1.045041
Train - Epoch 5, Batch: 20, Loss: 1.046942
Train - Epoch 5, Batch: 30, Loss: 1.036025
Train - Epoch 6, Batch: 0, Loss: 1.024482
Train - Epoch 6, Batch: 10, Loss: 1.033427
Train - Epoch 6, Batch: 20, Loss: 1.022023
Train - Epoch 6, Batch: 30, Loss: 1.022707
Train - Epoch 7, Batch: 0, Loss: 1.019476
Train - Epoch 7, Batch: 10, Loss: 1.002629
Train - Epoch 7, Batch: 20, Loss: 0.997744
Train - Epoch 7, Batch: 30, Loss: 1.003355
Train - Epoch 8, Batch: 0, Loss: 1.001997
Train - Epoch 8, Batch: 10, Loss: 1.003034
Train - Epoch 8, Batch: 20, Loss: 1.006782
Train - Epoch 8, Batch: 30, Loss: 0.996718
Train - Epoch 9, Batch: 0, Loss: 0.998445
Train - Epoch 9, Batch: 10, Loss: 0.992791
Train - Epoch 9, Batch: 20, Loss: 0.998026
Train - Epoch 9, Batch: 30, Loss: 0.989272
Train - Epoch 10, Batch: 0, Loss: 0.989813
Train - Epoch 10, Batch: 10, Loss: 0.980966
Train - Epoch 10, Batch: 20, Loss: 0.979901
Train - Epoch 10, Batch: 30, Loss: 0.975065
Train - Epoch 11, Batch: 0, Loss: 0.966081
Train - Epoch 11, Batch: 10, Loss: 0.975401
Train - Epoch 11, Batch: 20, Loss: 0.970580
Train - Epoch 11, Batch: 30, Loss: 0.965823
Train - Epoch 12, Batch: 0, Loss: 0.967075
Train - Epoch 12, Batch: 10, Loss: 0.964186
Train - Epoch 12, Batch: 20, Loss: 0.956695
Train - Epoch 12, Batch: 30, Loss: 0.966796
Train - Epoch 13, Batch: 0, Loss: 0.967845
Train - Epoch 13, Batch: 10, Loss: 0.963759
Train - Epoch 13, Batch: 20, Loss: 0.961017
Train - Epoch 13, Batch: 30, Loss: 0.951103
Train - Epoch 14, Batch: 0, Loss: 0.963688
Train - Epoch 14, Batch: 10, Loss: 0.952506
Train - Epoch 14, Batch: 20, Loss: 0.958117
Train - Epoch 14, Batch: 30, Loss: 0.944075
Train - Epoch 15, Batch: 0, Loss: 0.957377
Train - Epoch 15, Batch: 10, Loss: 0.954919
Train - Epoch 15, Batch: 20, Loss: 0.941651
Train - Epoch 15, Batch: 30, Loss: 0.946494
Train - Epoch 16, Batch: 0, Loss: 0.939845
Train - Epoch 16, Batch: 10, Loss: 0.944061
Train - Epoch 16, Batch: 20, Loss: 0.953039
Train - Epoch 16, Batch: 30, Loss: 0.941248
Train - Epoch 17, Batch: 0, Loss: 0.944675
Train - Epoch 17, Batch: 10, Loss: 0.940968
Train - Epoch 17, Batch: 20, Loss: 0.930994
Train - Epoch 17, Batch: 30, Loss: 0.939070
Train - Epoch 18, Batch: 0, Loss: 0.927717
Train - Epoch 18, Batch: 10, Loss: 0.931361
Train - Epoch 18, Batch: 20, Loss: 0.930404
Train - Epoch 18, Batch: 30, Loss: 0.925062
Train - Epoch 19, Batch: 0, Loss: 0.926448
Train - Epoch 19, Batch: 10, Loss: 0.934212
Train - Epoch 19, Batch: 20, Loss: 0.927670
Train - Epoch 19, Batch: 30, Loss: 0.923999
Train - Epoch 20, Batch: 0, Loss: 0.915404
Train - Epoch 20, Batch: 10, Loss: 0.917963
Train - Epoch 20, Batch: 20, Loss: 0.922478
Train - Epoch 20, Batch: 30, Loss: 0.929293
Train - Epoch 21, Batch: 0, Loss: 0.921067
Train - Epoch 21, Batch: 10, Loss: 0.931968
Train - Epoch 21, Batch: 20, Loss: 0.928824
Train - Epoch 21, Batch: 30, Loss: 0.913516
Train - Epoch 22, Batch: 0, Loss: 0.922375
Train - Epoch 22, Batch: 10, Loss: 0.913726
Train - Epoch 22, Batch: 20, Loss: 0.927056
Train - Epoch 22, Batch: 30, Loss: 0.920261
Train - Epoch 23, Batch: 0, Loss: 0.914323
Train - Epoch 23, Batch: 10, Loss: 0.912892
Train - Epoch 23, Batch: 20, Loss: 0.923414
Train - Epoch 23, Batch: 30, Loss: 0.916554
Train - Epoch 24, Batch: 0, Loss: 0.917172
Train - Epoch 24, Batch: 10, Loss: 0.908210
Train - Epoch 24, Batch: 20, Loss: 0.906017
Train - Epoch 24, Batch: 30, Loss: 0.905834
Train - Epoch 25, Batch: 0, Loss: 0.914219
Train - Epoch 25, Batch: 10, Loss: 0.910578
Train - Epoch 25, Batch: 20, Loss: 0.904487
Train - Epoch 25, Batch: 30, Loss: 0.907707
Train - Epoch 26, Batch: 0, Loss: 0.911679
Train - Epoch 26, Batch: 10, Loss: 0.905042
Train - Epoch 26, Batch: 20, Loss: 0.900220
Train - Epoch 26, Batch: 30, Loss: 0.897351
Train - Epoch 27, Batch: 0, Loss: 0.904335
Train - Epoch 27, Batch: 10, Loss: 0.905860
Train - Epoch 27, Batch: 20, Loss: 0.910370
Train - Epoch 27, Batch: 30, Loss: 0.908071
Train - Epoch 28, Batch: 0, Loss: 0.901850
Train - Epoch 28, Batch: 10, Loss: 0.893643
Train - Epoch 28, Batch: 20, Loss: 0.906813
Train - Epoch 28, Batch: 30, Loss: 0.898646
Train - Epoch 29, Batch: 0, Loss: 0.896509
Train - Epoch 29, Batch: 10, Loss: 0.901050
Train - Epoch 29, Batch: 20, Loss: 0.896659
Train - Epoch 29, Batch: 30, Loss: 0.897790
Train - Epoch 30, Batch: 0, Loss: 0.914868
Train - Epoch 30, Batch: 10, Loss: 0.895925
Train - Epoch 30, Batch: 20, Loss: 0.898760
Train - Epoch 30, Batch: 30, Loss: 0.902535
Train - Epoch 31, Batch: 0, Loss: 0.896309
Train - Epoch 31, Batch: 10, Loss: 0.900630
Train - Epoch 31, Batch: 20, Loss: 0.894563
Train - Epoch 31, Batch: 30, Loss: 0.887868
Test Avg. Loss: 0.000070, Accuracy: 0.629718
training_time:: 3.5769693851470947
training time full:: 3.577047824859619
provenance prepare time:: 7.867813110351562e-06
Test Avg. Loss: 0.000070, Accuracy: 0.629718
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
batch_size:: 16384
epoch:: 0
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 2.3555023670196533
overhead:: 0
overhead2:: 0.4938931465148926
overhead3:: 0
time_baseline:: 2.35602068901062
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629718
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.004965543746948242
overhead3:: 0.049634456634521484
overhead4:: 0.15960288047790527
overhead5:: 0
memory usage:: 3763625984
time_provenance:: 0.764620304107666
curr_diff: 0 tensor(2.8026e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.8026e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629718
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.006680488586425781
overhead3:: 0.05210518836975098
overhead4:: 0.17536640167236328
overhead5:: 0
memory usage:: 3798929408
time_provenance:: 0.7805991172790527
curr_diff: 0 tensor(2.4703e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.4703e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629718
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.006594181060791016
overhead3:: 0.052054405212402344
overhead4:: 0.20543766021728516
overhead5:: 0
memory usage:: 3766636544
time_provenance:: 0.8253829479217529
curr_diff: 0 tensor(2.8565e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.8565e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629718
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.007040262222290039
overhead3:: 0.05958700180053711
overhead4:: 0.20725369453430176
overhead5:: 0
memory usage:: 3770101760
time_provenance:: 0.8363010883331299
curr_diff: 0 tensor(2.4823e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.4823e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629718
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.00890350341796875
overhead3:: 0.05951881408691406
overhead4:: 0.26956725120544434
overhead5:: 0
memory usage:: 3763875840
time_provenance:: 0.9122371673583984
curr_diff: 0 tensor(2.8556e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.8556e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629718
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.008745908737182617
overhead3:: 0.0672750473022461
overhead4:: 0.29201292991638184
overhead5:: 0
memory usage:: 3791409152
time_provenance:: 1.032519817352295
curr_diff: 0 tensor(1.5825e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5825e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629718
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.00991201400756836
overhead3:: 0.06314277648925781
overhead4:: 0.3113369941711426
overhead5:: 0
memory usage:: 3774857216
time_provenance:: 0.9670584201812744
curr_diff: 0 tensor(1.5942e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5942e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629718
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.010286092758178711
overhead3:: 0.06526708602905273
overhead4:: 0.28963279724121094
overhead5:: 0
memory usage:: 3763384320
time_provenance:: 0.9274141788482666
curr_diff: 0 tensor(1.5960e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5960e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629718
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.010561227798461914
overhead3:: 0.07351517677307129
overhead4:: 0.321624755859375
overhead5:: 0
memory usage:: 3785416704
time_provenance:: 0.989422082901001
curr_diff: 0 tensor(1.5948e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5948e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629718
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.011863946914672852
overhead3:: 0.07491636276245117
overhead4:: 0.34584522247314453
overhead5:: 0
memory usage:: 3785039872
time_provenance:: 1.011493444442749
curr_diff: 0 tensor(1.5969e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5969e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629718
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.01670527458190918
overhead3:: 0.10395288467407227
overhead4:: 0.5578091144561768
overhead5:: 0
memory usage:: 3780022272
time_provenance:: 1.29364013671875
curr_diff: 0 tensor(6.6063e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.6063e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629718
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.02070140838623047
overhead3:: 0.10426640510559082
overhead4:: 0.4945859909057617
overhead5:: 0
memory usage:: 3782250496
time_provenance:: 1.2332513332366943
curr_diff: 0 tensor(6.7447e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.7447e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629718
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.019533157348632812
overhead3:: 0.12044072151184082
overhead4:: 0.5073709487915039
overhead5:: 0
memory usage:: 3770769408
time_provenance:: 1.2473769187927246
curr_diff: 0 tensor(6.7496e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.7496e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629718
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.018703699111938477
overhead3:: 0.1182403564453125
overhead4:: 0.5832266807556152
overhead5:: 0
memory usage:: 3785216000
time_provenance:: 1.3323767185211182
curr_diff: 0 tensor(6.7610e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.7610e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629718
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.026135683059692383
overhead3:: 0.14689373970031738
overhead4:: 0.7534165382385254
overhead5:: 0
memory usage:: 3762135040
time_provenance:: 1.713249921798706
curr_diff: 0 tensor(6.7542e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.7542e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629718
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.039354801177978516
overhead3:: 0.21347308158874512
overhead4:: 1.1988122463226318
overhead5:: 0
memory usage:: 3769540608
time_provenance:: 2.117121696472168
curr_diff: 0 tensor(1.5441e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5441e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629718
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.05199027061462402
overhead3:: 0.27303028106689453
overhead4:: 1.5809733867645264
overhead5:: 0
memory usage:: 3770056704
time_provenance:: 2.7801527976989746
curr_diff: 0 tensor(1.5828e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5828e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629718
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.05146908760070801
overhead3:: 0.27042698860168457
overhead4:: 1.6180596351623535
overhead5:: 0
memory usage:: 3788996608
time_provenance:: 2.8062877655029297
curr_diff: 0 tensor(1.5834e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5834e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629718
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.041805267333984375
overhead3:: 0.23367571830749512
overhead4:: 1.2621746063232422
overhead5:: 0
memory usage:: 3761528832
time_provenance:: 2.1882383823394775
curr_diff: 0 tensor(1.5847e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5847e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629718
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.05227327346801758
overhead3:: 0.29340386390686035
overhead4:: 1.5897557735443115
overhead5:: 0
memory usage:: 3776184320
time_provenance:: 2.790473699569702
curr_diff: 0 tensor(1.5821e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5821e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629718
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 3 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.08251500129699707
overhead3:: 0.43334031105041504
overhead4:: 2.0726726055145264
overhead5:: 0
memory usage:: 3797438464
time_provenance:: 2.821810245513916
curr_diff: 0 tensor(1.0024e-13, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0024e-13, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629718
repetition 4
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 4 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 1.944206
Train - Epoch 0, Batch: 10, Loss: 1.381659
Train - Epoch 0, Batch: 20, Loss: 1.238947
Train - Epoch 0, Batch: 30, Loss: 1.185036
Train - Epoch 1, Batch: 0, Loss: 1.177655
Train - Epoch 1, Batch: 10, Loss: 1.164586
Train - Epoch 1, Batch: 20, Loss: 1.138621
Train - Epoch 1, Batch: 30, Loss: 1.132211
Train - Epoch 2, Batch: 0, Loss: 1.140895
Train - Epoch 2, Batch: 10, Loss: 1.123028
Train - Epoch 2, Batch: 20, Loss: 1.109880
Train - Epoch 2, Batch: 30, Loss: 1.105623
Train - Epoch 3, Batch: 0, Loss: 1.090585
Train - Epoch 3, Batch: 10, Loss: 1.083790
Train - Epoch 3, Batch: 20, Loss: 1.083371
Train - Epoch 3, Batch: 30, Loss: 1.082532
Train - Epoch 4, Batch: 0, Loss: 1.086108
Train - Epoch 4, Batch: 10, Loss: 1.066548
Train - Epoch 4, Batch: 20, Loss: 1.055411
Train - Epoch 4, Batch: 30, Loss: 1.052392
Train - Epoch 5, Batch: 0, Loss: 1.052378
Train - Epoch 5, Batch: 10, Loss: 1.043797
Train - Epoch 5, Batch: 20, Loss: 1.050792
Train - Epoch 5, Batch: 30, Loss: 1.035393
Train - Epoch 6, Batch: 0, Loss: 1.035364
Train - Epoch 6, Batch: 10, Loss: 1.034877
Train - Epoch 6, Batch: 20, Loss: 1.026730
Train - Epoch 6, Batch: 30, Loss: 1.024825
Train - Epoch 7, Batch: 0, Loss: 1.021031
Train - Epoch 7, Batch: 10, Loss: 1.015589
Train - Epoch 7, Batch: 20, Loss: 1.012724
Train - Epoch 7, Batch: 30, Loss: 1.002306
Train - Epoch 8, Batch: 0, Loss: 1.008113
Train - Epoch 8, Batch: 10, Loss: 1.004833
Train - Epoch 8, Batch: 20, Loss: 1.000934
Train - Epoch 8, Batch: 30, Loss: 0.998085
Train - Epoch 9, Batch: 0, Loss: 0.992685
Train - Epoch 9, Batch: 10, Loss: 0.990202
Train - Epoch 9, Batch: 20, Loss: 0.992541
Train - Epoch 9, Batch: 30, Loss: 0.987355
Train - Epoch 10, Batch: 0, Loss: 0.998499
Train - Epoch 10, Batch: 10, Loss: 0.986981
Train - Epoch 10, Batch: 20, Loss: 0.981752
Train - Epoch 10, Batch: 30, Loss: 0.983593
Train - Epoch 11, Batch: 0, Loss: 0.976958
Train - Epoch 11, Batch: 10, Loss: 0.968677
Train - Epoch 11, Batch: 20, Loss: 0.979134
Train - Epoch 11, Batch: 30, Loss: 0.973882
Train - Epoch 12, Batch: 0, Loss: 0.979597
Train - Epoch 12, Batch: 10, Loss: 0.973562
Train - Epoch 12, Batch: 20, Loss: 0.969921
Train - Epoch 12, Batch: 30, Loss: 0.958397
Train - Epoch 13, Batch: 0, Loss: 0.963614
Train - Epoch 13, Batch: 10, Loss: 0.958642
Train - Epoch 13, Batch: 20, Loss: 0.959817
Train - Epoch 13, Batch: 30, Loss: 0.961351
Train - Epoch 14, Batch: 0, Loss: 0.962000
Train - Epoch 14, Batch: 10, Loss: 0.956115
Train - Epoch 14, Batch: 20, Loss: 0.955648
Train - Epoch 14, Batch: 30, Loss: 0.950954
Train - Epoch 15, Batch: 0, Loss: 0.955644
Train - Epoch 15, Batch: 10, Loss: 0.953341
Train - Epoch 15, Batch: 20, Loss: 0.948104
Train - Epoch 15, Batch: 30, Loss: 0.949661
Train - Epoch 16, Batch: 0, Loss: 0.939666
Train - Epoch 16, Batch: 10, Loss: 0.940173
Train - Epoch 16, Batch: 20, Loss: 0.950895
Train - Epoch 16, Batch: 30, Loss: 0.947771
Train - Epoch 17, Batch: 0, Loss: 0.941843
Train - Epoch 17, Batch: 10, Loss: 0.944400
Train - Epoch 17, Batch: 20, Loss: 0.947425
Train - Epoch 17, Batch: 30, Loss: 0.938769
Train - Epoch 18, Batch: 0, Loss: 0.939977
Train - Epoch 18, Batch: 10, Loss: 0.936440
Train - Epoch 18, Batch: 20, Loss: 0.932196
Train - Epoch 18, Batch: 30, Loss: 0.936039
Train - Epoch 19, Batch: 0, Loss: 0.931483
Train - Epoch 19, Batch: 10, Loss: 0.938384
Train - Epoch 19, Batch: 20, Loss: 0.923043
Train - Epoch 19, Batch: 30, Loss: 0.923531
Train - Epoch 20, Batch: 0, Loss: 0.931256
Train - Epoch 20, Batch: 10, Loss: 0.921066
Train - Epoch 20, Batch: 20, Loss: 0.924910
Train - Epoch 20, Batch: 30, Loss: 0.915098
Train - Epoch 21, Batch: 0, Loss: 0.927757
Train - Epoch 21, Batch: 10, Loss: 0.933003
Train - Epoch 21, Batch: 20, Loss: 0.923870
Train - Epoch 21, Batch: 30, Loss: 0.916820
Train - Epoch 22, Batch: 0, Loss: 0.918699
Train - Epoch 22, Batch: 10, Loss: 0.921598
Train - Epoch 22, Batch: 20, Loss: 0.913071
Train - Epoch 22, Batch: 30, Loss: 0.920943
Train - Epoch 23, Batch: 0, Loss: 0.913471
Train - Epoch 23, Batch: 10, Loss: 0.917926
Train - Epoch 23, Batch: 20, Loss: 0.917337
Train - Epoch 23, Batch: 30, Loss: 0.911339
Train - Epoch 24, Batch: 0, Loss: 0.926139
Train - Epoch 24, Batch: 10, Loss: 0.899596
Train - Epoch 24, Batch: 20, Loss: 0.913416
Train - Epoch 24, Batch: 30, Loss: 0.910329
Train - Epoch 25, Batch: 0, Loss: 0.904582
Train - Epoch 25, Batch: 10, Loss: 0.899614
Train - Epoch 25, Batch: 20, Loss: 0.910198
Train - Epoch 25, Batch: 30, Loss: 0.912642
Train - Epoch 26, Batch: 0, Loss: 0.897480
Train - Epoch 26, Batch: 10, Loss: 0.915881
Train - Epoch 26, Batch: 20, Loss: 0.904756
Train - Epoch 26, Batch: 30, Loss: 0.908445
Train - Epoch 27, Batch: 0, Loss: 0.896023
Train - Epoch 27, Batch: 10, Loss: 0.901216
Train - Epoch 27, Batch: 20, Loss: 0.903591
Train - Epoch 27, Batch: 30, Loss: 0.908576
Train - Epoch 28, Batch: 0, Loss: 0.901047
Train - Epoch 28, Batch: 10, Loss: 0.899915
Train - Epoch 28, Batch: 20, Loss: 0.900652
Train - Epoch 28, Batch: 30, Loss: 0.902979
Train - Epoch 29, Batch: 0, Loss: 0.902835
Train - Epoch 29, Batch: 10, Loss: 0.904906
Train - Epoch 29, Batch: 20, Loss: 0.905785
Train - Epoch 29, Batch: 30, Loss: 0.898092
Train - Epoch 30, Batch: 0, Loss: 0.896093
Train - Epoch 30, Batch: 10, Loss: 0.898765
Train - Epoch 30, Batch: 20, Loss: 0.906100
Train - Epoch 30, Batch: 30, Loss: 0.905572
Train - Epoch 31, Batch: 0, Loss: 0.895657
Train - Epoch 31, Batch: 10, Loss: 0.891585
Train - Epoch 31, Batch: 20, Loss: 0.894857
Train - Epoch 31, Batch: 30, Loss: 0.901089
Test Avg. Loss: 0.000070, Accuracy: 0.628789
training_time:: 3.5900628566741943
training time full:: 3.590128183364868
provenance prepare time:: 6.9141387939453125e-06
Test Avg. Loss: 0.000070, Accuracy: 0.628789
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
batch_size:: 16384
epoch:: 0
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 2.341127872467041
overhead:: 0
overhead2:: 0.47473931312561035
overhead3:: 0
time_baseline:: 2.3416318893432617
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628806
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.005801200866699219
overhead3:: 0.0502324104309082
overhead4:: 0.1419506072998047
overhead5:: 0
memory usage:: 3783028736
time_provenance:: 0.7561378479003906
curr_diff: 0 tensor(2.1332e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1332e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628806
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.006258487701416016
overhead3:: 0.04924201965332031
overhead4:: 0.15726208686828613
overhead5:: 0
memory usage:: 3789209600
time_provenance:: 0.7662372589111328
curr_diff: 0 tensor(2.7189e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.7189e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628806
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.007750511169433594
overhead3:: 0.057318687438964844
overhead4:: 0.19591426849365234
overhead5:: 0
memory usage:: 3762671616
time_provenance:: 0.8545994758605957
curr_diff: 0 tensor(2.1242e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1242e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628806
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.007360935211181641
overhead3:: 0.06671595573425293
overhead4:: 0.18975043296813965
overhead5:: 0
memory usage:: 3769053184
time_provenance:: 0.7990233898162842
curr_diff: 0 tensor(2.8485e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.8485e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628806
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.010779619216918945
overhead3:: 0.07950019836425781
overhead4:: 0.25537538528442383
overhead5:: 0
memory usage:: 3794436096
time_provenance:: 0.9168124198913574
curr_diff: 0 tensor(2.2364e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.2364e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628806
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.009774208068847656
overhead3:: 0.06530928611755371
overhead4:: 0.3043708801269531
overhead5:: 0
memory usage:: 3765342208
time_provenance:: 0.9577503204345703
curr_diff: 0 tensor(1.4647e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4647e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628806
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.010506153106689453
overhead3:: 0.06287527084350586
overhead4:: 0.32439184188842773
overhead5:: 0
memory usage:: 3812171776
time_provenance:: 0.9802563190460205
curr_diff: 0 tensor(1.4737e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4737e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628806
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.012391090393066406
overhead3:: 0.06607532501220703
overhead4:: 0.30732250213623047
overhead5:: 0
memory usage:: 3775070208
time_provenance:: 0.9603204727172852
curr_diff: 0 tensor(1.4711e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4711e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628806
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.012542009353637695
overhead3:: 0.07725739479064941
overhead4:: 0.34082746505737305
overhead5:: 0
memory usage:: 3770191872
time_provenance:: 1.047278881072998
curr_diff: 0 tensor(1.5289e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5289e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628806
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.012517213821411133
overhead3:: 0.06851696968078613
overhead4:: 0.3309905529022217
overhead5:: 0
memory usage:: 3799355392
time_provenance:: 0.9535620212554932
curr_diff: 0 tensor(1.5199e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5199e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628806
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.017740726470947266
overhead3:: 0.11736440658569336
overhead4:: 0.542029619216919
overhead5:: 0
memory usage:: 3791085568
time_provenance:: 1.295419692993164
curr_diff: 0 tensor(5.6994e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.6994e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628806
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.01925492286682129
overhead3:: 0.11636495590209961
overhead4:: 0.5714514255523682
overhead5:: 0
memory usage:: 3767386112
time_provenance:: 1.3535256385803223
curr_diff: 0 tensor(5.7124e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.7124e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628806
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.018347501754760742
overhead3:: 0.1045222282409668
overhead4:: 0.5019962787628174
overhead5:: 0
memory usage:: 3782709248
time_provenance:: 1.2270336151123047
curr_diff: 0 tensor(5.7343e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.7343e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628806
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.018157958984375
overhead3:: 0.10365986824035645
overhead4:: 0.5595529079437256
overhead5:: 0
memory usage:: 3762876416
time_provenance:: 1.2919042110443115
curr_diff: 0 tensor(6.0338e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.0338e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628806
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.025264263153076172
overhead3:: 0.13410663604736328
overhead4:: 0.7106521129608154
overhead5:: 0
memory usage:: 3762438144
time_provenance:: 1.642477035522461
curr_diff: 0 tensor(6.0241e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.0241e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628806
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.04196739196777344
overhead3:: 0.2238023281097412
overhead4:: 1.284419059753418
overhead5:: 0
memory usage:: 3775369216
time_provenance:: 2.2443950176239014
curr_diff: 0 tensor(1.3113e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3113e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628806
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.04225802421569824
overhead3:: 0.23074817657470703
overhead4:: 1.2779748439788818
overhead5:: 0
memory usage:: 3769741312
time_provenance:: 2.255669355392456
curr_diff: 0 tensor(1.3265e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3265e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628806
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.06089043617248535
overhead3:: 0.28698158264160156
overhead4:: 1.6062936782836914
overhead5:: 0
memory usage:: 3762393088
time_provenance:: 2.8146369457244873
curr_diff: 0 tensor(1.3678e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3678e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628806
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.06055307388305664
overhead3:: 0.2854502201080322
overhead4:: 1.6767852306365967
overhead5:: 0
memory usage:: 3762630656
time_provenance:: 2.870802402496338
curr_diff: 0 tensor(1.4219e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4219e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628806
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.04294848442077637
overhead3:: 0.2498610019683838
overhead4:: 1.303636074066162
overhead5:: 0
memory usage:: 3777208320
time_provenance:: 2.2923901081085205
curr_diff: 0 tensor(1.4217e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4217e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628806
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 4 0.00002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 10
max_epoch:: 32
overhead:: 0
overhead2:: 0.0752253532409668
overhead3:: 0.3898465633392334
overhead4:: 2.0702621936798096
overhead5:: 0
memory usage:: 3781623808
time_provenance:: 2.7649664878845215
curr_diff: 0 tensor(1.0066e-13, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0066e-13, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628806
deletion rate:: 0.00005
python3 generate_rand_ids 0.00005  covtype 0
tensor([469568, 431936,  51651, 388422,   7430, 171591, 266441, 456650, 479180,
        117964,  35345,  65041, 233113, 134079, 507038, 509736,   2665, 445546,
         23210, 260149, 103094, 364534, 333306, 136699, 349692, 137087])
batch size:: 16384
repetition 0
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 0 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 1.997871
Train - Epoch 0, Batch: 10, Loss: 1.399305
Train - Epoch 0, Batch: 20, Loss: 1.254391
Train - Epoch 0, Batch: 30, Loss: 1.202092
Train - Epoch 1, Batch: 0, Loss: 1.184995
Train - Epoch 1, Batch: 10, Loss: 1.166699
Train - Epoch 1, Batch: 20, Loss: 1.138832
Train - Epoch 1, Batch: 30, Loss: 1.127095
Train - Epoch 2, Batch: 0, Loss: 1.140271
Train - Epoch 2, Batch: 10, Loss: 1.108420
Train - Epoch 2, Batch: 20, Loss: 1.119243
Train - Epoch 2, Batch: 30, Loss: 1.095908
Train - Epoch 3, Batch: 0, Loss: 1.098521
Train - Epoch 3, Batch: 10, Loss: 1.088131
Train - Epoch 3, Batch: 20, Loss: 1.084826
Train - Epoch 3, Batch: 30, Loss: 1.067113
Train - Epoch 4, Batch: 0, Loss: 1.074498
Train - Epoch 4, Batch: 10, Loss: 1.069854
Train - Epoch 4, Batch: 20, Loss: 1.062102
Train - Epoch 4, Batch: 30, Loss: 1.047903
Train - Epoch 5, Batch: 0, Loss: 1.059249
Train - Epoch 5, Batch: 10, Loss: 1.043143
Train - Epoch 5, Batch: 20, Loss: 1.041964
Train - Epoch 5, Batch: 30, Loss: 1.031764
Train - Epoch 6, Batch: 0, Loss: 1.033741
Train - Epoch 6, Batch: 10, Loss: 1.021611
Train - Epoch 6, Batch: 20, Loss: 1.016248
Train - Epoch 6, Batch: 30, Loss: 1.013822
Train - Epoch 7, Batch: 0, Loss: 1.018147
Train - Epoch 7, Batch: 10, Loss: 1.014525
Train - Epoch 7, Batch: 20, Loss: 1.018684
Train - Epoch 7, Batch: 30, Loss: 1.002191
Train - Epoch 8, Batch: 0, Loss: 1.005937
Train - Epoch 8, Batch: 10, Loss: 0.997549
Train - Epoch 8, Batch: 20, Loss: 1.007446
Train - Epoch 8, Batch: 30, Loss: 0.991572
Train - Epoch 9, Batch: 0, Loss: 0.997123
Train - Epoch 9, Batch: 10, Loss: 1.001840
Train - Epoch 9, Batch: 20, Loss: 0.992999
Train - Epoch 9, Batch: 30, Loss: 0.982844
Train - Epoch 10, Batch: 0, Loss: 0.976016
Train - Epoch 10, Batch: 10, Loss: 0.976779
Train - Epoch 10, Batch: 20, Loss: 0.992766
Train - Epoch 10, Batch: 30, Loss: 0.990885
Train - Epoch 11, Batch: 0, Loss: 0.970583
Train - Epoch 11, Batch: 10, Loss: 0.978956
Train - Epoch 11, Batch: 20, Loss: 0.970112
Train - Epoch 11, Batch: 30, Loss: 0.970886
Train - Epoch 12, Batch: 0, Loss: 0.959099
Train - Epoch 12, Batch: 10, Loss: 0.958917
Train - Epoch 12, Batch: 20, Loss: 0.962119
Train - Epoch 12, Batch: 30, Loss: 0.961017
Train - Epoch 13, Batch: 0, Loss: 0.955411
Train - Epoch 13, Batch: 10, Loss: 0.968481
Train - Epoch 13, Batch: 20, Loss: 0.956746
Train - Epoch 13, Batch: 30, Loss: 0.958851
Train - Epoch 14, Batch: 0, Loss: 0.955879
Train - Epoch 14, Batch: 10, Loss: 0.961547
Train - Epoch 14, Batch: 20, Loss: 0.946226
Train - Epoch 14, Batch: 30, Loss: 0.951428
Train - Epoch 15, Batch: 0, Loss: 0.950423
Train - Epoch 15, Batch: 10, Loss: 0.945971
Train - Epoch 15, Batch: 20, Loss: 0.940506
Train - Epoch 15, Batch: 30, Loss: 0.951661
Train - Epoch 16, Batch: 0, Loss: 0.938635
Train - Epoch 16, Batch: 10, Loss: 0.944864
Train - Epoch 16, Batch: 20, Loss: 0.943278
Train - Epoch 16, Batch: 30, Loss: 0.935965
Train - Epoch 17, Batch: 0, Loss: 0.944400
Train - Epoch 17, Batch: 10, Loss: 0.929690
Train - Epoch 17, Batch: 20, Loss: 0.939547
Train - Epoch 17, Batch: 30, Loss: 0.933394
Train - Epoch 18, Batch: 0, Loss: 0.927240
Train - Epoch 18, Batch: 10, Loss: 0.934097
Train - Epoch 18, Batch: 20, Loss: 0.933410
Train - Epoch 18, Batch: 30, Loss: 0.929174
Train - Epoch 19, Batch: 0, Loss: 0.933771
Train - Epoch 19, Batch: 10, Loss: 0.928354
Train - Epoch 19, Batch: 20, Loss: 0.923052
Train - Epoch 19, Batch: 30, Loss: 0.928207
Train - Epoch 20, Batch: 0, Loss: 0.924002
Train - Epoch 20, Batch: 10, Loss: 0.930544
Train - Epoch 20, Batch: 20, Loss: 0.919100
Train - Epoch 20, Batch: 30, Loss: 0.921499
Train - Epoch 21, Batch: 0, Loss: 0.919380
Train - Epoch 21, Batch: 10, Loss: 0.924687
Train - Epoch 21, Batch: 20, Loss: 0.919726
Train - Epoch 21, Batch: 30, Loss: 0.923546
Train - Epoch 22, Batch: 0, Loss: 0.920665
Train - Epoch 22, Batch: 10, Loss: 0.915586
Train - Epoch 22, Batch: 20, Loss: 0.924108
Train - Epoch 22, Batch: 30, Loss: 0.920092
Train - Epoch 23, Batch: 0, Loss: 0.911887
Train - Epoch 23, Batch: 10, Loss: 0.922568
Train - Epoch 23, Batch: 20, Loss: 0.919611
Train - Epoch 23, Batch: 30, Loss: 0.907612
Train - Epoch 24, Batch: 0, Loss: 0.914368
Train - Epoch 24, Batch: 10, Loss: 0.909149
Train - Epoch 24, Batch: 20, Loss: 0.915087
Train - Epoch 24, Batch: 30, Loss: 0.916764
Train - Epoch 25, Batch: 0, Loss: 0.911452
Train - Epoch 25, Batch: 10, Loss: 0.909235
Train - Epoch 25, Batch: 20, Loss: 0.898745
Train - Epoch 25, Batch: 30, Loss: 0.912377
Train - Epoch 26, Batch: 0, Loss: 0.909939
Train - Epoch 26, Batch: 10, Loss: 0.903981
Train - Epoch 26, Batch: 20, Loss: 0.913171
Train - Epoch 26, Batch: 30, Loss: 0.904855
Train - Epoch 27, Batch: 0, Loss: 0.896232
Train - Epoch 27, Batch: 10, Loss: 0.911604
Train - Epoch 27, Batch: 20, Loss: 0.900290
Train - Epoch 27, Batch: 30, Loss: 0.900582
Train - Epoch 28, Batch: 0, Loss: 0.894633
Train - Epoch 28, Batch: 10, Loss: 0.896844
Train - Epoch 28, Batch: 20, Loss: 0.895990
Train - Epoch 28, Batch: 30, Loss: 0.900701
Train - Epoch 29, Batch: 0, Loss: 0.899384
Train - Epoch 29, Batch: 10, Loss: 0.901269
Train - Epoch 29, Batch: 20, Loss: 0.897822
Train - Epoch 29, Batch: 30, Loss: 0.892251
Train - Epoch 30, Batch: 0, Loss: 0.886199
Train - Epoch 30, Batch: 10, Loss: 0.892093
Train - Epoch 30, Batch: 20, Loss: 0.888484
Train - Epoch 30, Batch: 30, Loss: 0.900987
Train - Epoch 31, Batch: 0, Loss: 0.884152
Train - Epoch 31, Batch: 10, Loss: 0.899005
Train - Epoch 31, Batch: 20, Loss: 0.885461
Train - Epoch 31, Batch: 30, Loss: 0.890198
Test Avg. Loss: 0.000070, Accuracy: 0.629150
training_time:: 3.4866435527801514
training time full:: 3.486707925796509
provenance prepare time:: 6.9141387939453125e-06
Test Avg. Loss: 0.000070, Accuracy: 0.629150
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
batch_size:: 16384
epoch:: 0
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 2.9681828022003174
overhead:: 0
overhead2:: 0.9998605251312256
overhead3:: 0
time_baseline:: 2.9689455032348633
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629150
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.010630369186401367
overhead3:: 0.05295705795288086
overhead4:: 0.1806774139404297
overhead5:: 0
memory usage:: 3762425856
time_provenance:: 0.9230451583862305
curr_diff: 0 tensor(3.9298e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.9298e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629150
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.010806560516357422
overhead3:: 0.053379058837890625
overhead4:: 0.20001745223999023
overhead5:: 0
memory usage:: 3763228672
time_provenance:: 0.954840898513794
curr_diff: 0 tensor(5.3546e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.3546e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629167
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.013281583786010742
overhead3:: 0.05655980110168457
overhead4:: 0.23360490798950195
overhead5:: 0
memory usage:: 3800166400
time_provenance:: 0.9622786045074463
curr_diff: 0 tensor(4.4240e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.4240e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629150
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.014478683471679688
overhead3:: 0.06582951545715332
overhead4:: 0.2423264980316162
overhead5:: 0
memory usage:: 3769270272
time_provenance:: 1.015188217163086
curr_diff: 0 tensor(5.3895e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.3895e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629167
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.015298843383789062
overhead3:: 0.06242680549621582
overhead4:: 0.255932092666626
overhead5:: 0
memory usage:: 3780927488
time_provenance:: 1.021618366241455
curr_diff: 0 tensor(4.4495e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.4495e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629150
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.016971111297607422
overhead3:: 0.060076236724853516
overhead4:: 0.3425450325012207
overhead5:: 0
memory usage:: 3769798656
time_provenance:: 1.128861427307129
curr_diff: 0 tensor(2.5677e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.5677e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629167
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.018454551696777344
overhead3:: 0.06306862831115723
overhead4:: 0.3555612564086914
overhead5:: 0
memory usage:: 3795812352
time_provenance:: 1.1403117179870605
curr_diff: 0 tensor(2.6620e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.6620e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629167
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.019947290420532227
overhead3:: 0.06469845771789551
overhead4:: 0.3453042507171631
overhead5:: 0
memory usage:: 3764682752
time_provenance:: 1.1232714653015137
curr_diff: 0 tensor(2.6636e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.6636e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629167
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.020631074905395508
overhead3:: 0.07086467742919922
overhead4:: 0.388974666595459
overhead5:: 0
memory usage:: 3799523328
time_provenance:: 1.174342155456543
curr_diff: 0 tensor(2.6725e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.6725e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629167
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.023038387298583984
overhead3:: 0.09030556678771973
overhead4:: 0.4094204902648926
overhead5:: 0
memory usage:: 3766558720
time_provenance:: 1.2407989501953125
curr_diff: 0 tensor(2.6876e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.6876e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629167
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.03376483917236328
overhead3:: 0.10566043853759766
overhead4:: 0.606581449508667
overhead5:: 0
memory usage:: 3770933248
time_provenance:: 1.449948787689209
curr_diff: 0 tensor(7.6330e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.6330e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629150
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.03340792655944824
overhead3:: 0.1002047061920166
overhead4:: 0.6277401447296143
overhead5:: 0
memory usage:: 3822759936
time_provenance:: 1.4794185161590576
curr_diff: 0 tensor(7.9304e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.9304e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629150
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.03853654861450195
overhead3:: 0.11089706420898438
overhead4:: 0.7217073440551758
overhead5:: 0
memory usage:: 3766620160
time_provenance:: 1.6570250988006592
curr_diff: 0 tensor(8.0509e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.0509e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629150
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.039520978927612305
overhead3:: 0.11327910423278809
overhead4:: 0.7161989212036133
overhead5:: 0
memory usage:: 3770081280
time_provenance:: 1.626969814300537
curr_diff: 0 tensor(8.1053e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.1053e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629150
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.03937029838562012
overhead3:: 0.10787630081176758
overhead4:: 0.6502530574798584
overhead5:: 0
memory usage:: 3804532736
time_provenance:: 1.4895753860473633
curr_diff: 0 tensor(8.1338e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.1338e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629150
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.08373498916625977
overhead3:: 0.21273231506347656
overhead4:: 1.4404261112213135
overhead5:: 0
memory usage:: 3764260864
time_provenance:: 2.50627064704895
curr_diff: 0 tensor(1.5317e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5317e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629150
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.08131599426269531
overhead3:: 0.22178387641906738
overhead4:: 1.5104060173034668
overhead5:: 0
memory usage:: 3774287872
time_provenance:: 2.585792064666748
curr_diff: 0 tensor(1.5688e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5688e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629150
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.08106875419616699
overhead3:: 0.22212743759155273
overhead4:: 1.5837466716766357
overhead5:: 0
memory usage:: 3799572480
time_provenance:: 2.6677660942077637
curr_diff: 0 tensor(1.5850e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5850e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629150
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.11162757873535156
overhead3:: 0.2929356098175049
overhead4:: 2.076486110687256
overhead5:: 0
memory usage:: 3764682752
time_provenance:: 3.458331346511841
curr_diff: 0 tensor(1.5937e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5937e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629150
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.08385586738586426
overhead3:: 0.24128413200378418
overhead4:: 1.5088343620300293
overhead5:: 0
memory usage:: 3798503424
time_provenance:: 2.589782476425171
curr_diff: 0 tensor(1.6014e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6014e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629150
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.17589974403381348
overhead3:: 0.44416260719299316
overhead4:: 2.6242940425872803
overhead5:: 0
memory usage:: 3759366144
time_provenance:: 3.5150463581085205
curr_diff: 0 tensor(9.9948e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.9948e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629150
repetition 1
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 1 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 2.076528
Train - Epoch 0, Batch: 10, Loss: 1.412047
Train - Epoch 0, Batch: 20, Loss: 1.240354
Train - Epoch 0, Batch: 30, Loss: 1.192576
Train - Epoch 1, Batch: 0, Loss: 1.174340
Train - Epoch 1, Batch: 10, Loss: 1.143467
Train - Epoch 1, Batch: 20, Loss: 1.145288
Train - Epoch 1, Batch: 30, Loss: 1.118050
Train - Epoch 2, Batch: 0, Loss: 1.112849
Train - Epoch 2, Batch: 10, Loss: 1.115934
Train - Epoch 2, Batch: 20, Loss: 1.104127
Train - Epoch 2, Batch: 30, Loss: 1.092245
Train - Epoch 3, Batch: 0, Loss: 1.104072
Train - Epoch 3, Batch: 10, Loss: 1.081955
Train - Epoch 3, Batch: 20, Loss: 1.077717
Train - Epoch 3, Batch: 30, Loss: 1.069581
Train - Epoch 4, Batch: 0, Loss: 1.074433
Train - Epoch 4, Batch: 10, Loss: 1.061850
Train - Epoch 4, Batch: 20, Loss: 1.054580
Train - Epoch 4, Batch: 30, Loss: 1.048420
Train - Epoch 5, Batch: 0, Loss: 1.041032
Train - Epoch 5, Batch: 10, Loss: 1.041484
Train - Epoch 5, Batch: 20, Loss: 1.037586
Train - Epoch 5, Batch: 30, Loss: 1.035015
Train - Epoch 6, Batch: 0, Loss: 1.043723
Train - Epoch 6, Batch: 10, Loss: 1.025570
Train - Epoch 6, Batch: 20, Loss: 1.025566
Train - Epoch 6, Batch: 30, Loss: 1.018827
Train - Epoch 7, Batch: 0, Loss: 1.015367
Train - Epoch 7, Batch: 10, Loss: 1.008652
Train - Epoch 7, Batch: 20, Loss: 1.012755
Train - Epoch 7, Batch: 30, Loss: 0.999323
Train - Epoch 8, Batch: 0, Loss: 1.008482
Train - Epoch 8, Batch: 10, Loss: 0.993765
Train - Epoch 8, Batch: 20, Loss: 1.007596
Train - Epoch 8, Batch: 30, Loss: 0.987540
Train - Epoch 9, Batch: 0, Loss: 0.989720
Train - Epoch 9, Batch: 10, Loss: 0.989472
Train - Epoch 9, Batch: 20, Loss: 0.979823
Train - Epoch 9, Batch: 30, Loss: 0.987618
Train - Epoch 10, Batch: 0, Loss: 0.986630
Train - Epoch 10, Batch: 10, Loss: 0.977482
Train - Epoch 10, Batch: 20, Loss: 0.975433
Train - Epoch 10, Batch: 30, Loss: 0.967383
Train - Epoch 11, Batch: 0, Loss: 0.976549
Train - Epoch 11, Batch: 10, Loss: 0.975509
Train - Epoch 11, Batch: 20, Loss: 0.972312
Train - Epoch 11, Batch: 30, Loss: 0.962157
Train - Epoch 12, Batch: 0, Loss: 0.975490
Train - Epoch 12, Batch: 10, Loss: 0.961860
Train - Epoch 12, Batch: 20, Loss: 0.960981
Train - Epoch 12, Batch: 30, Loss: 0.950410
Train - Epoch 13, Batch: 0, Loss: 0.958091
Train - Epoch 13, Batch: 10, Loss: 0.961168
Train - Epoch 13, Batch: 20, Loss: 0.964954
Train - Epoch 13, Batch: 30, Loss: 0.956716
Train - Epoch 14, Batch: 0, Loss: 0.950159
Train - Epoch 14, Batch: 10, Loss: 0.953699
Train - Epoch 14, Batch: 20, Loss: 0.956705
Train - Epoch 14, Batch: 30, Loss: 0.954746
Train - Epoch 15, Batch: 0, Loss: 0.949979
Train - Epoch 15, Batch: 10, Loss: 0.951820
Train - Epoch 15, Batch: 20, Loss: 0.945216
Train - Epoch 15, Batch: 30, Loss: 0.939955
Train - Epoch 16, Batch: 0, Loss: 0.947260
Train - Epoch 16, Batch: 10, Loss: 0.944931
Train - Epoch 16, Batch: 20, Loss: 0.943099
Train - Epoch 16, Batch: 30, Loss: 0.937253
Train - Epoch 17, Batch: 0, Loss: 0.938111
Train - Epoch 17, Batch: 10, Loss: 0.936961
Train - Epoch 17, Batch: 20, Loss: 0.937904
Train - Epoch 17, Batch: 30, Loss: 0.935292
Train - Epoch 18, Batch: 0, Loss: 0.935291
Train - Epoch 18, Batch: 10, Loss: 0.939319
Train - Epoch 18, Batch: 20, Loss: 0.918918
Train - Epoch 18, Batch: 30, Loss: 0.929686
Train - Epoch 19, Batch: 0, Loss: 0.934494
Train - Epoch 19, Batch: 10, Loss: 0.927273
Train - Epoch 19, Batch: 20, Loss: 0.917612
Train - Epoch 19, Batch: 30, Loss: 0.919200
Train - Epoch 20, Batch: 0, Loss: 0.928082
Train - Epoch 20, Batch: 10, Loss: 0.922444
Train - Epoch 20, Batch: 20, Loss: 0.929567
Train - Epoch 20, Batch: 30, Loss: 0.915760
Train - Epoch 21, Batch: 0, Loss: 0.923449
Train - Epoch 21, Batch: 10, Loss: 0.920306
Train - Epoch 21, Batch: 20, Loss: 0.920603
Train - Epoch 21, Batch: 30, Loss: 0.922015
Train - Epoch 22, Batch: 0, Loss: 0.911251
Train - Epoch 22, Batch: 10, Loss: 0.920256
Train - Epoch 22, Batch: 20, Loss: 0.913668
Train - Epoch 22, Batch: 30, Loss: 0.920587
Train - Epoch 23, Batch: 0, Loss: 0.924322
Train - Epoch 23, Batch: 10, Loss: 0.918209
Train - Epoch 23, Batch: 20, Loss: 0.919581
Train - Epoch 23, Batch: 30, Loss: 0.899687
Train - Epoch 24, Batch: 0, Loss: 0.910443
Train - Epoch 24, Batch: 10, Loss: 0.905694
Train - Epoch 24, Batch: 20, Loss: 0.910617
Train - Epoch 24, Batch: 30, Loss: 0.911921
Train - Epoch 25, Batch: 0, Loss: 0.910781
Train - Epoch 25, Batch: 10, Loss: 0.917774
Train - Epoch 25, Batch: 20, Loss: 0.911598
Train - Epoch 25, Batch: 30, Loss: 0.905958
Train - Epoch 26, Batch: 0, Loss: 0.912928
Train - Epoch 26, Batch: 10, Loss: 0.900182
Train - Epoch 26, Batch: 20, Loss: 0.908075
Train - Epoch 26, Batch: 30, Loss: 0.904024
Train - Epoch 27, Batch: 0, Loss: 0.905957
Train - Epoch 27, Batch: 10, Loss: 0.907217
Train - Epoch 27, Batch: 20, Loss: 0.901641
Train - Epoch 27, Batch: 30, Loss: 0.897373
Train - Epoch 28, Batch: 0, Loss: 0.907637
Train - Epoch 28, Batch: 10, Loss: 0.893085
Train - Epoch 28, Batch: 20, Loss: 0.903493
Train - Epoch 28, Batch: 30, Loss: 0.893971
Train - Epoch 29, Batch: 0, Loss: 0.901184
Train - Epoch 29, Batch: 10, Loss: 0.917455
Train - Epoch 29, Batch: 20, Loss: 0.890543
Train - Epoch 29, Batch: 30, Loss: 0.891243
Train - Epoch 30, Batch: 0, Loss: 0.901385
Train - Epoch 30, Batch: 10, Loss: 0.890808
Train - Epoch 30, Batch: 20, Loss: 0.889262
Train - Epoch 30, Batch: 30, Loss: 0.893098
Train - Epoch 31, Batch: 0, Loss: 0.887338
Train - Epoch 31, Batch: 10, Loss: 0.894160
Train - Epoch 31, Batch: 20, Loss: 0.903984
Train - Epoch 31, Batch: 30, Loss: 0.891347
Test Avg. Loss: 0.000070, Accuracy: 0.630062
training_time:: 3.296449899673462
training time full:: 3.2965147495269775
provenance prepare time:: 6.198883056640625e-06
Test Avg. Loss: 0.000070, Accuracy: 0.630062
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
batch_size:: 16384
epoch:: 0
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 3.107536554336548
overhead:: 0
overhead2:: 0.9749326705932617
overhead3:: 0
time_baseline:: 3.108247756958008
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630062
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.010518789291381836
overhead3:: 0.06093001365661621
overhead4:: 0.16963815689086914
overhead5:: 0
memory usage:: 3805556736
time_provenance:: 0.9315011501312256
curr_diff: 0 tensor(5.1851e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.1851e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630062
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.013953208923339844
overhead3:: 0.054048776626586914
overhead4:: 0.22077393531799316
overhead5:: 0
memory usage:: 3849428992
time_provenance:: 1.0358870029449463
curr_diff: 0 tensor(5.9941e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.9941e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630062
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.012648344039916992
overhead3:: 0.057323455810546875
overhead4:: 0.22323250770568848
overhead5:: 0
memory usage:: 3800379392
time_provenance:: 0.9839844703674316
curr_diff: 0 tensor(5.3855e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.3855e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630062
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.016527652740478516
overhead3:: 0.05309271812438965
overhead4:: 0.25693607330322266
overhead5:: 0
memory usage:: 3762929664
time_provenance:: 1.0614280700683594
curr_diff: 0 tensor(6.0407e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.0407e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630062
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.018582582473754883
overhead3:: 0.0732121467590332
overhead4:: 0.288240909576416
overhead5:: 0
memory usage:: 3788034048
time_provenance:: 1.0689313411712646
curr_diff: 0 tensor(5.3327e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.3327e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630062
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.0199432373046875
overhead3:: 0.06577181816101074
overhead4:: 0.3334028720855713
overhead5:: 0
memory usage:: 3763654656
time_provenance:: 1.1169493198394775
curr_diff: 0 tensor(2.8598e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.8598e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630062
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.020987510681152344
overhead3:: 0.06590867042541504
overhead4:: 0.3677830696105957
overhead5:: 0
memory usage:: 3776327680
time_provenance:: 1.1620190143585205
curr_diff: 0 tensor(2.8929e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.8929e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630062
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.02335071563720703
overhead3:: 0.08962011337280273
overhead4:: 0.3558032512664795
overhead5:: 0
memory usage:: 3766513664
time_provenance:: 1.1607813835144043
curr_diff: 0 tensor(2.9033e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.9033e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630062
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.02126622200012207
overhead3:: 0.07573223114013672
overhead4:: 0.3800036907196045
overhead5:: 0
memory usage:: 3782725632
time_provenance:: 1.1327228546142578
curr_diff: 0 tensor(2.9027e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.9027e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630062
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.025669574737548828
overhead3:: 0.09096527099609375
overhead4:: 0.4674239158630371
overhead5:: 0
memory usage:: 3767152640
time_provenance:: 1.3553798198699951
curr_diff: 0 tensor(2.9084e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.9084e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630062
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.037204742431640625
overhead3:: 0.12906527519226074
overhead4:: 0.6268863677978516
overhead5:: 0
memory usage:: 3762606080
time_provenance:: 1.516343355178833
curr_diff: 0 tensor(1.0102e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0102e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630062
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.03666806221008301
overhead3:: 0.09998893737792969
overhead4:: 0.6160953044891357
overhead5:: 0
memory usage:: 3799597056
time_provenance:: 1.445293664932251
curr_diff: 0 tensor(1.0169e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0169e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630062
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.0407099723815918
overhead3:: 0.1279442310333252
overhead4:: 0.6817641258239746
overhead5:: 0
memory usage:: 3764109312
time_provenance:: 1.6205155849456787
curr_diff: 0 tensor(1.0166e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0166e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630062
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.0377802848815918
overhead3:: 0.10724854469299316
overhead4:: 0.6674954891204834
overhead5:: 0
memory usage:: 3799773184
time_provenance:: 1.5156638622283936
curr_diff: 0 tensor(1.0153e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0153e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630062
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.038149356842041016
overhead3:: 0.1260848045349121
overhead4:: 0.7369811534881592
overhead5:: 0
memory usage:: 3790864384
time_provenance:: 1.6194877624511719
curr_diff: 0 tensor(1.0144e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0144e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630062
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.08720040321350098
overhead3:: 0.23041057586669922
overhead4:: 1.4075732231140137
overhead5:: 0
memory usage:: 3766571008
time_provenance:: 2.4786276817321777
curr_diff: 0 tensor(1.5748e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5748e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630062
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.08886933326721191
overhead3:: 0.2322230339050293
overhead4:: 1.532045841217041
overhead5:: 0
memory usage:: 3780169728
time_provenance:: 2.607600450515747
curr_diff: 0 tensor(1.6051e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6051e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630062
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.08294439315795898
overhead3:: 0.21986842155456543
overhead4:: 1.560546636581421
overhead5:: 0
memory usage:: 3766198272
time_provenance:: 2.623717784881592
curr_diff: 0 tensor(1.6069e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6069e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630062
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.10465145111083984
overhead3:: 0.27086973190307617
overhead4:: 1.6068830490112305
overhead5:: 0
memory usage:: 3774939136
time_provenance:: 2.7718865871429443
curr_diff: 0 tensor(1.6058e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6058e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630062
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.08650827407836914
overhead3:: 0.22801446914672852
overhead4:: 1.5937836170196533
overhead5:: 0
memory usage:: 3804282880
time_provenance:: 2.658823013305664
curr_diff: 0 tensor(1.6038e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6038e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630062
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.2075343132019043
overhead3:: 0.56522536277771
overhead4:: 2.8247432708740234
overhead5:: 0
memory usage:: 3754668032
time_provenance:: 3.8580408096313477
curr_diff: 0 tensor(1.0025e-13, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0025e-13, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630062
repetition 2
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 2 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 2.028841
Train - Epoch 0, Batch: 10, Loss: 1.405326
Train - Epoch 0, Batch: 20, Loss: 1.259847
Train - Epoch 0, Batch: 30, Loss: 1.198003
Train - Epoch 1, Batch: 0, Loss: 1.185816
Train - Epoch 1, Batch: 10, Loss: 1.162579
Train - Epoch 1, Batch: 20, Loss: 1.146079
Train - Epoch 1, Batch: 30, Loss: 1.133419
Train - Epoch 2, Batch: 0, Loss: 1.134032
Train - Epoch 2, Batch: 10, Loss: 1.122705
Train - Epoch 2, Batch: 20, Loss: 1.101433
Train - Epoch 2, Batch: 30, Loss: 1.117018
Train - Epoch 3, Batch: 0, Loss: 1.102053
Train - Epoch 3, Batch: 10, Loss: 1.102740
Train - Epoch 3, Batch: 20, Loss: 1.082222
Train - Epoch 3, Batch: 30, Loss: 1.087136
Train - Epoch 4, Batch: 0, Loss: 1.076729
Train - Epoch 4, Batch: 10, Loss: 1.061812
Train - Epoch 4, Batch: 20, Loss: 1.061062
Train - Epoch 4, Batch: 30, Loss: 1.051272
Train - Epoch 5, Batch: 0, Loss: 1.055332
Train - Epoch 5, Batch: 10, Loss: 1.055200
Train - Epoch 5, Batch: 20, Loss: 1.038630
Train - Epoch 5, Batch: 30, Loss: 1.040712
Train - Epoch 6, Batch: 0, Loss: 1.042449
Train - Epoch 6, Batch: 10, Loss: 1.025864
Train - Epoch 6, Batch: 20, Loss: 1.033736
Train - Epoch 6, Batch: 30, Loss: 1.017594
Train - Epoch 7, Batch: 0, Loss: 1.018035
Train - Epoch 7, Batch: 10, Loss: 1.026067
Train - Epoch 7, Batch: 20, Loss: 1.014600
Train - Epoch 7, Batch: 30, Loss: 1.014490
Train - Epoch 8, Batch: 0, Loss: 1.005708
Train - Epoch 8, Batch: 10, Loss: 1.005634
Train - Epoch 8, Batch: 20, Loss: 1.002968
Train - Epoch 8, Batch: 30, Loss: 1.002792
Train - Epoch 9, Batch: 0, Loss: 0.999155
Train - Epoch 9, Batch: 10, Loss: 0.998977
Train - Epoch 9, Batch: 20, Loss: 0.999135
Train - Epoch 9, Batch: 30, Loss: 0.984956
Train - Epoch 10, Batch: 0, Loss: 0.990601
Train - Epoch 10, Batch: 10, Loss: 0.986978
Train - Epoch 10, Batch: 20, Loss: 0.973403
Train - Epoch 10, Batch: 30, Loss: 0.985888
Train - Epoch 11, Batch: 0, Loss: 0.980645
Train - Epoch 11, Batch: 10, Loss: 0.971452
Train - Epoch 11, Batch: 20, Loss: 0.974356
Train - Epoch 11, Batch: 30, Loss: 0.971223
Train - Epoch 12, Batch: 0, Loss: 0.969616
Train - Epoch 12, Batch: 10, Loss: 0.975316
Train - Epoch 12, Batch: 20, Loss: 0.965635
Train - Epoch 12, Batch: 30, Loss: 0.973458
Train - Epoch 13, Batch: 0, Loss: 0.967366
Train - Epoch 13, Batch: 10, Loss: 0.955720
Train - Epoch 13, Batch: 20, Loss: 0.959560
Train - Epoch 13, Batch: 30, Loss: 0.973598
Train - Epoch 14, Batch: 0, Loss: 0.956938
Train - Epoch 14, Batch: 10, Loss: 0.963620
Train - Epoch 14, Batch: 20, Loss: 0.956038
Train - Epoch 14, Batch: 30, Loss: 0.961160
Train - Epoch 15, Batch: 0, Loss: 0.955471
Train - Epoch 15, Batch: 10, Loss: 0.947908
Train - Epoch 15, Batch: 20, Loss: 0.942785
Train - Epoch 15, Batch: 30, Loss: 0.944459
Train - Epoch 16, Batch: 0, Loss: 0.953959
Train - Epoch 16, Batch: 10, Loss: 0.942776
Train - Epoch 16, Batch: 20, Loss: 0.934937
Train - Epoch 16, Batch: 30, Loss: 0.947524
Train - Epoch 17, Batch: 0, Loss: 0.949022
Train - Epoch 17, Batch: 10, Loss: 0.944637
Train - Epoch 17, Batch: 20, Loss: 0.923899
Train - Epoch 17, Batch: 30, Loss: 0.937639
Train - Epoch 18, Batch: 0, Loss: 0.932843
Train - Epoch 18, Batch: 10, Loss: 0.932141
Train - Epoch 18, Batch: 20, Loss: 0.932222
Train - Epoch 18, Batch: 30, Loss: 0.934452
Train - Epoch 19, Batch: 0, Loss: 0.929881
Train - Epoch 19, Batch: 10, Loss: 0.922330
Train - Epoch 19, Batch: 20, Loss: 0.934018
Train - Epoch 19, Batch: 30, Loss: 0.922006
Train - Epoch 20, Batch: 0, Loss: 0.926118
Train - Epoch 20, Batch: 10, Loss: 0.922377
Train - Epoch 20, Batch: 20, Loss: 0.919556
Train - Epoch 20, Batch: 30, Loss: 0.922662
Train - Epoch 21, Batch: 0, Loss: 0.926877
Train - Epoch 21, Batch: 10, Loss: 0.919380
Train - Epoch 21, Batch: 20, Loss: 0.916340
Train - Epoch 21, Batch: 30, Loss: 0.915751
Train - Epoch 22, Batch: 0, Loss: 0.920959
Train - Epoch 22, Batch: 10, Loss: 0.924008
Train - Epoch 22, Batch: 20, Loss: 0.921938
Train - Epoch 22, Batch: 30, Loss: 0.915056
Train - Epoch 23, Batch: 0, Loss: 0.917657
Train - Epoch 23, Batch: 10, Loss: 0.915038
Train - Epoch 23, Batch: 20, Loss: 0.910252
Train - Epoch 23, Batch: 30, Loss: 0.919174
Train - Epoch 24, Batch: 0, Loss: 0.909582
Train - Epoch 24, Batch: 10, Loss: 0.919929
Train - Epoch 24, Batch: 20, Loss: 0.906518
Train - Epoch 24, Batch: 30, Loss: 0.909600
Train - Epoch 25, Batch: 0, Loss: 0.904605
Train - Epoch 25, Batch: 10, Loss: 0.916926
Train - Epoch 25, Batch: 20, Loss: 0.915954
Train - Epoch 25, Batch: 30, Loss: 0.918973
Train - Epoch 26, Batch: 0, Loss: 0.905998
Train - Epoch 26, Batch: 10, Loss: 0.906138
Train - Epoch 26, Batch: 20, Loss: 0.899922
Train - Epoch 26, Batch: 30, Loss: 0.909017
Train - Epoch 27, Batch: 0, Loss: 0.909836
Train - Epoch 27, Batch: 10, Loss: 0.901338
Train - Epoch 27, Batch: 20, Loss: 0.917151
Train - Epoch 27, Batch: 30, Loss: 0.897648
Train - Epoch 28, Batch: 0, Loss: 0.906285
Train - Epoch 28, Batch: 10, Loss: 0.903335
Train - Epoch 28, Batch: 20, Loss: 0.909375
Train - Epoch 28, Batch: 30, Loss: 0.904216
Train - Epoch 29, Batch: 0, Loss: 0.900415
Train - Epoch 29, Batch: 10, Loss: 0.893250
Train - Epoch 29, Batch: 20, Loss: 0.900755
Train - Epoch 29, Batch: 30, Loss: 0.898736
Train - Epoch 30, Batch: 0, Loss: 0.897844
Train - Epoch 30, Batch: 10, Loss: 0.899854
Train - Epoch 30, Batch: 20, Loss: 0.902587
Train - Epoch 30, Batch: 30, Loss: 0.889781
Train - Epoch 31, Batch: 0, Loss: 0.890192
Train - Epoch 31, Batch: 10, Loss: 0.891847
Train - Epoch 31, Batch: 20, Loss: 0.894681
Train - Epoch 31, Batch: 30, Loss: 0.882251
Test Avg. Loss: 0.000070, Accuracy: 0.629632
training_time:: 3.498175621032715
training time full:: 3.498239755630493
provenance prepare time:: 7.3909759521484375e-06
Test Avg. Loss: 0.000070, Accuracy: 0.629632
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
batch_size:: 16384
epoch:: 0
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 2.7663567066192627
overhead:: 0
overhead2:: 0.972604513168335
overhead3:: 0
time_baseline:: 2.7670071125030518
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629632
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.009204387664794922
overhead3:: 0.04905247688293457
overhead4:: 0.16934847831726074
overhead5:: 0
memory usage:: 3770589184
time_provenance:: 0.8926990032196045
curr_diff: 0 tensor(4.8332e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.8332e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629632
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.012091398239135742
overhead3:: 0.045317888259887695
overhead4:: 0.22122883796691895
overhead5:: 0
memory usage:: 3762618368
time_provenance:: 0.9756565093994141
curr_diff: 0 tensor(6.1137e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.1137e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629649
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.012324094772338867
overhead3:: 0.053854942321777344
overhead4:: 0.21685576438903809
overhead5:: 0
memory usage:: 3791110144
time_provenance:: 0.9379355907440186
curr_diff: 0 tensor(4.8437e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.8437e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629632
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.0162351131439209
overhead3:: 0.07624053955078125
overhead4:: 0.26793599128723145
overhead5:: 0
memory usage:: 3769430016
time_provenance:: 1.058819055557251
curr_diff: 0 tensor(6.2065e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.2065e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629649
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.015269756317138672
overhead3:: 0.06588244438171387
overhead4:: 0.2632124423980713
overhead5:: 0
memory usage:: 3766747136
time_provenance:: 1.0869293212890625
curr_diff: 0 tensor(4.8421e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.8421e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629632
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.018445968627929688
overhead3:: 0.06041145324707031
overhead4:: 0.3048262596130371
overhead5:: 0
memory usage:: 3765428224
time_provenance:: 1.0777587890625
curr_diff: 0 tensor(2.4522e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.4522e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629632
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.017797231674194336
overhead3:: 0.06107473373413086
overhead4:: 0.31764960289001465
overhead5:: 0
memory usage:: 3790331904
time_provenance:: 1.0846455097198486
curr_diff: 0 tensor(2.4712e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.4712e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629632
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.021153926849365234
overhead3:: 0.06600737571716309
overhead4:: 0.33356475830078125
overhead5:: 0
memory usage:: 3766394880
time_provenance:: 1.1384191513061523
curr_diff: 0 tensor(2.4862e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.4862e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629632
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.021289825439453125
overhead3:: 0.06919455528259277
overhead4:: 0.38831663131713867
overhead5:: 0
memory usage:: 3769786368
time_provenance:: 1.1910014152526855
curr_diff: 0 tensor(2.4752e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.4752e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629632
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.022927522659301758
overhead3:: 0.07860112190246582
overhead4:: 0.3868560791015625
overhead5:: 0
memory usage:: 3763761152
time_provenance:: 1.1423962116241455
curr_diff: 0 tensor(2.4667e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.4667e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629632
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.03617262840270996
overhead3:: 0.10926246643066406
overhead4:: 0.6562759876251221
overhead5:: 0
memory usage:: 3764957184
time_provenance:: 1.5188839435577393
curr_diff: 0 tensor(9.0965e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.0965e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629632
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.03956413269042969
overhead3:: 0.11182045936584473
overhead4:: 0.7377481460571289
overhead5:: 0
memory usage:: 3776311296
time_provenance:: 1.6957812309265137
curr_diff: 0 tensor(9.1760e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.1760e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629632
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.03989076614379883
overhead3:: 0.11005330085754395
overhead4:: 0.7391326427459717
overhead5:: 0
memory usage:: 3789938688
time_provenance:: 1.6763877868652344
curr_diff: 0 tensor(9.2847e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.2847e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629632
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.04413270950317383
overhead3:: 0.12413287162780762
overhead4:: 0.8537805080413818
overhead5:: 0
memory usage:: 3782983680
time_provenance:: 1.8822228908538818
curr_diff: 0 tensor(9.3067e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.3067e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629632
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.044997215270996094
overhead3:: 0.12199664115905762
overhead4:: 0.7834532260894775
overhead5:: 0
memory usage:: 3780829184
time_provenance:: 1.7660539150238037
curr_diff: 0 tensor(9.3027e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.3027e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629632
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.0903923511505127
overhead3:: 0.23932170867919922
overhead4:: 1.6350419521331787
overhead5:: 0
memory usage:: 3771473920
time_provenance:: 2.7849395275115967
curr_diff: 0 tensor(2.1430e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1430e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629632
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.1160430908203125
overhead3:: 0.2900104522705078
overhead4:: 2.0061049461364746
overhead5:: 0
memory usage:: 3765915648
time_provenance:: 3.387080192565918
curr_diff: 0 tensor(2.1463e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1463e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629632
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.07815051078796387
overhead3:: 0.23699045181274414
overhead4:: 1.5333070755004883
overhead5:: 0
memory usage:: 3773890560
time_provenance:: 2.611943483352661
curr_diff: 0 tensor(2.1733e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1733e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629632
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.08690166473388672
overhead3:: 0.2400801181793213
overhead4:: 1.4504082202911377
overhead5:: 0
memory usage:: 3764711424
time_provenance:: 2.532991647720337
curr_diff: 0 tensor(2.1749e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1749e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629632
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.11810040473937988
overhead3:: 0.33771419525146484
overhead4:: 1.8517889976501465
overhead5:: 0
memory usage:: 3789893632
time_provenance:: 3.2003068923950195
curr_diff: 0 tensor(2.1742e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1742e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629632
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.18793964385986328
overhead3:: 0.49537134170532227
overhead4:: 2.6565144062042236
overhead5:: 0
memory usage:: 3785703424
time_provenance:: 3.6076390743255615
curr_diff: 0 tensor(1.0223e-13, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0223e-13, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629632
repetition 3
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 3 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 2.059664
Train - Epoch 0, Batch: 10, Loss: 1.392394
Train - Epoch 0, Batch: 20, Loss: 1.230701
Train - Epoch 0, Batch: 30, Loss: 1.180867
Train - Epoch 1, Batch: 0, Loss: 1.163659
Train - Epoch 1, Batch: 10, Loss: 1.147760
Train - Epoch 1, Batch: 20, Loss: 1.118169
Train - Epoch 1, Batch: 30, Loss: 1.118453
Train - Epoch 2, Batch: 0, Loss: 1.119227
Train - Epoch 2, Batch: 10, Loss: 1.103024
Train - Epoch 2, Batch: 20, Loss: 1.095515
Train - Epoch 2, Batch: 30, Loss: 1.094563
Train - Epoch 3, Batch: 0, Loss: 1.094065
Train - Epoch 3, Batch: 10, Loss: 1.075866
Train - Epoch 3, Batch: 20, Loss: 1.067503
Train - Epoch 3, Batch: 30, Loss: 1.059924
Train - Epoch 4, Batch: 0, Loss: 1.060021
Train - Epoch 4, Batch: 10, Loss: 1.061754
Train - Epoch 4, Batch: 20, Loss: 1.047130
Train - Epoch 4, Batch: 30, Loss: 1.058252
Train - Epoch 5, Batch: 0, Loss: 1.045956
Train - Epoch 5, Batch: 10, Loss: 1.042121
Train - Epoch 5, Batch: 20, Loss: 1.043988
Train - Epoch 5, Batch: 30, Loss: 1.033179
Train - Epoch 6, Batch: 0, Loss: 1.021270
Train - Epoch 6, Batch: 10, Loss: 1.031179
Train - Epoch 6, Batch: 20, Loss: 1.020118
Train - Epoch 6, Batch: 30, Loss: 1.020897
Train - Epoch 7, Batch: 0, Loss: 1.017844
Train - Epoch 7, Batch: 10, Loss: 1.000289
Train - Epoch 7, Batch: 20, Loss: 0.995432
Train - Epoch 7, Batch: 30, Loss: 1.001357
Train - Epoch 8, Batch: 0, Loss: 0.999200
Train - Epoch 8, Batch: 10, Loss: 1.001250
Train - Epoch 8, Batch: 20, Loss: 1.004461
Train - Epoch 8, Batch: 30, Loss: 0.994883
Train - Epoch 9, Batch: 0, Loss: 0.996849
Train - Epoch 9, Batch: 10, Loss: 0.991073
Train - Epoch 9, Batch: 20, Loss: 0.996061
Train - Epoch 9, Batch: 30, Loss: 0.987351
Train - Epoch 10, Batch: 0, Loss: 0.987834
Train - Epoch 10, Batch: 10, Loss: 0.978490
Train - Epoch 10, Batch: 20, Loss: 0.978265
Train - Epoch 10, Batch: 30, Loss: 0.973050
Train - Epoch 11, Batch: 0, Loss: 0.964300
Train - Epoch 11, Batch: 10, Loss: 0.973347
Train - Epoch 11, Batch: 20, Loss: 0.968473
Train - Epoch 11, Batch: 30, Loss: 0.963852
Train - Epoch 12, Batch: 0, Loss: 0.965270
Train - Epoch 12, Batch: 10, Loss: 0.962762
Train - Epoch 12, Batch: 20, Loss: 0.955290
Train - Epoch 12, Batch: 30, Loss: 0.965113
Train - Epoch 13, Batch: 0, Loss: 0.966885
Train - Epoch 13, Batch: 10, Loss: 0.962240
Train - Epoch 13, Batch: 20, Loss: 0.959766
Train - Epoch 13, Batch: 30, Loss: 0.949166
Train - Epoch 14, Batch: 0, Loss: 0.962081
Train - Epoch 14, Batch: 10, Loss: 0.951121
Train - Epoch 14, Batch: 20, Loss: 0.957008
Train - Epoch 14, Batch: 30, Loss: 0.942724
Train - Epoch 15, Batch: 0, Loss: 0.955476
Train - Epoch 15, Batch: 10, Loss: 0.953638
Train - Epoch 15, Batch: 20, Loss: 0.940835
Train - Epoch 15, Batch: 30, Loss: 0.945173
Train - Epoch 16, Batch: 0, Loss: 0.938556
Train - Epoch 16, Batch: 10, Loss: 0.942563
Train - Epoch 16, Batch: 20, Loss: 0.951399
Train - Epoch 16, Batch: 30, Loss: 0.939836
Train - Epoch 17, Batch: 0, Loss: 0.942985
Train - Epoch 17, Batch: 10, Loss: 0.940058
Train - Epoch 17, Batch: 20, Loss: 0.929542
Train - Epoch 17, Batch: 30, Loss: 0.937596
Train - Epoch 18, Batch: 0, Loss: 0.926511
Train - Epoch 18, Batch: 10, Loss: 0.929569
Train - Epoch 18, Batch: 20, Loss: 0.928987
Train - Epoch 18, Batch: 30, Loss: 0.923920
Train - Epoch 19, Batch: 0, Loss: 0.924819
Train - Epoch 19, Batch: 10, Loss: 0.932659
Train - Epoch 19, Batch: 20, Loss: 0.925943
Train - Epoch 19, Batch: 30, Loss: 0.922566
Train - Epoch 20, Batch: 0, Loss: 0.914262
Train - Epoch 20, Batch: 10, Loss: 0.916637
Train - Epoch 20, Batch: 20, Loss: 0.921206
Train - Epoch 20, Batch: 30, Loss: 0.928308
Train - Epoch 21, Batch: 0, Loss: 0.919693
Train - Epoch 21, Batch: 10, Loss: 0.930781
Train - Epoch 21, Batch: 20, Loss: 0.927810
Train - Epoch 21, Batch: 30, Loss: 0.912012
Train - Epoch 22, Batch: 0, Loss: 0.921499
Train - Epoch 22, Batch: 10, Loss: 0.912588
Train - Epoch 22, Batch: 20, Loss: 0.925820
Train - Epoch 22, Batch: 30, Loss: 0.919463
Train - Epoch 23, Batch: 0, Loss: 0.913426
Train - Epoch 23, Batch: 10, Loss: 0.911477
Train - Epoch 23, Batch: 20, Loss: 0.922177
Train - Epoch 23, Batch: 30, Loss: 0.915523
Train - Epoch 24, Batch: 0, Loss: 0.916101
Train - Epoch 24, Batch: 10, Loss: 0.907025
Train - Epoch 24, Batch: 20, Loss: 0.905016
Train - Epoch 24, Batch: 30, Loss: 0.905000
Train - Epoch 25, Batch: 0, Loss: 0.912658
Train - Epoch 25, Batch: 10, Loss: 0.909343
Train - Epoch 25, Batch: 20, Loss: 0.903608
Train - Epoch 25, Batch: 30, Loss: 0.906200
Train - Epoch 26, Batch: 0, Loss: 0.910860
Train - Epoch 26, Batch: 10, Loss: 0.903989
Train - Epoch 26, Batch: 20, Loss: 0.898799
Train - Epoch 26, Batch: 30, Loss: 0.896353
Train - Epoch 27, Batch: 0, Loss: 0.903271
Train - Epoch 27, Batch: 10, Loss: 0.904747
Train - Epoch 27, Batch: 20, Loss: 0.909337
Train - Epoch 27, Batch: 30, Loss: 0.907159
Train - Epoch 28, Batch: 0, Loss: 0.900779
Train - Epoch 28, Batch: 10, Loss: 0.892607
Train - Epoch 28, Batch: 20, Loss: 0.905955
Train - Epoch 28, Batch: 30, Loss: 0.897619
Train - Epoch 29, Batch: 0, Loss: 0.895174
Train - Epoch 29, Batch: 10, Loss: 0.900008
Train - Epoch 29, Batch: 20, Loss: 0.895521
Train - Epoch 29, Batch: 30, Loss: 0.897378
Train - Epoch 30, Batch: 0, Loss: 0.914287
Train - Epoch 30, Batch: 10, Loss: 0.895021
Train - Epoch 30, Batch: 20, Loss: 0.897604
Train - Epoch 30, Batch: 30, Loss: 0.901653
Train - Epoch 31, Batch: 0, Loss: 0.895300
Train - Epoch 31, Batch: 10, Loss: 0.899931
Train - Epoch 31, Batch: 20, Loss: 0.893726
Train - Epoch 31, Batch: 30, Loss: 0.887254
Test Avg. Loss: 0.000070, Accuracy: 0.628858
training_time:: 3.440378427505493
training time full:: 3.440441370010376
provenance prepare time:: 6.9141387939453125e-06
Test Avg. Loss: 0.000070, Accuracy: 0.628858
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
batch_size:: 16384
epoch:: 0
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 3.1351282596588135
overhead:: 0
overhead2:: 1.0125629901885986
overhead3:: 0
time_baseline:: 3.135789394378662
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628858
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.01020193099975586
overhead3:: 0.053244829177856445
overhead4:: 0.17165517807006836
overhead5:: 0
memory usage:: 3787243520
time_provenance:: 0.9511122703552246
curr_diff: 0 tensor(4.0700e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.0700e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628858
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.01232290267944336
overhead3:: 0.054326534271240234
overhead4:: 0.22258758544921875
overhead5:: 0
memory usage:: 3779125248
time_provenance:: 0.9585175514221191
curr_diff: 0 tensor(3.6234e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.6234e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628858
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.013596296310424805
overhead3:: 0.05836653709411621
overhead4:: 0.2563166618347168
overhead5:: 0
memory usage:: 3812913152
time_provenance:: 1.0228509902954102
curr_diff: 0 tensor(4.7630e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.7630e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628858
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.014876604080200195
overhead3:: 0.061177968978881836
overhead4:: 0.25292348861694336
overhead5:: 0
memory usage:: 3773837312
time_provenance:: 1.0159876346588135
curr_diff: 0 tensor(3.6998e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.6998e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628858
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.015589237213134766
overhead3:: 0.07030749320983887
overhead4:: 0.26236748695373535
overhead5:: 0
memory usage:: 3769962496
time_provenance:: 0.9982819557189941
curr_diff: 0 tensor(4.8197e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.8197e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628858
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.01726675033569336
overhead3:: 0.07836532592773438
overhead4:: 0.3180389404296875
overhead5:: 0
memory usage:: 3775500288
time_provenance:: 1.1016390323638916
curr_diff: 0 tensor(2.4095e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.4095e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628858
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.020894765853881836
overhead3:: 0.0777583122253418
overhead4:: 0.36788439750671387
overhead5:: 0
memory usage:: 3775262720
time_provenance:: 1.2112915515899658
curr_diff: 0 tensor(2.4839e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.4839e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628858
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.020793914794921875
overhead3:: 0.07036757469177246
overhead4:: 0.35758280754089355
overhead5:: 0
memory usage:: 3809996800
time_provenance:: 1.1217906475067139
curr_diff: 0 tensor(2.5295e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.5295e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628858
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.02117323875427246
overhead3:: 0.06998610496520996
overhead4:: 0.3983922004699707
overhead5:: 0
memory usage:: 3769724928
time_provenance:: 1.1917006969451904
curr_diff: 0 tensor(2.5344e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.5344e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628858
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.023374557495117188
overhead3:: 0.07488155364990234
overhead4:: 0.43494701385498047
overhead5:: 0
memory usage:: 3782311936
time_provenance:: 1.2238352298736572
curr_diff: 0 tensor(2.5310e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.5310e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628858
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.03247976303100586
overhead3:: 0.09783411026000977
overhead4:: 0.6443166732788086
overhead5:: 0
memory usage:: 3798163456
time_provenance:: 1.5075578689575195
curr_diff: 0 tensor(8.6493e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.6493e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628858
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.03377032279968262
overhead3:: 0.1046288013458252
overhead4:: 0.6508083343505859
overhead5:: 0
memory usage:: 3768774656
time_provenance:: 1.5015902519226074
curr_diff: 0 tensor(8.7971e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.7971e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628858
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.04601287841796875
overhead3:: 0.12781071662902832
overhead4:: 0.8497521877288818
overhead5:: 0
memory usage:: 3764543488
time_provenance:: 1.9634501934051514
curr_diff: 0 tensor(8.8771e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.8771e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628858
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.04275798797607422
overhead3:: 0.12321233749389648
overhead4:: 0.8066949844360352
overhead5:: 0
memory usage:: 3777777664
time_provenance:: 1.8435256481170654
curr_diff: 0 tensor(8.9274e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.9274e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628858
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.0440678596496582
overhead3:: 0.12730646133422852
overhead4:: 0.8055951595306396
overhead5:: 0
memory usage:: 3767578624
time_provenance:: 1.7811040878295898
curr_diff: 0 tensor(8.9449e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.9449e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628858
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.07958197593688965
overhead3:: 0.21557140350341797
overhead4:: 1.51125168800354
overhead5:: 0
memory usage:: 3795075072
time_provenance:: 2.5701067447662354
curr_diff: 0 tensor(1.7971e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.7971e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628858
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.12075042724609375
overhead3:: 0.2897043228149414
overhead4:: 2.0414605140686035
overhead5:: 0
memory usage:: 3790282752
time_provenance:: 3.4311118125915527
curr_diff: 0 tensor(1.8391e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.8391e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628858
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.08031249046325684
overhead3:: 0.2169179916381836
overhead4:: 1.5623202323913574
overhead5:: 0
memory usage:: 3763638272
time_provenance:: 2.6252522468566895
curr_diff: 0 tensor(1.8496e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.8496e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628858
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.1172182559967041
overhead3:: 0.32541918754577637
overhead4:: 1.8034961223602295
overhead5:: 0
memory usage:: 3790340096
time_provenance:: 3.127244472503662
curr_diff: 0 tensor(1.8493e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.8493e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628858
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.10653090476989746
overhead3:: 0.2861006259918213
overhead4:: 1.8546757698059082
overhead5:: 0
memory usage:: 3789766656
time_provenance:: 3.135162115097046
curr_diff: 0 tensor(1.8530e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.8530e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628858
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.17961978912353516
overhead3:: 0.44382309913635254
overhead4:: 2.61860728263855
overhead5:: 0
memory usage:: 3789729792
time_provenance:: 3.5239696502685547
curr_diff: 0 tensor(9.9821e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.9821e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628858
repetition 4
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 4 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 1.817893
Train - Epoch 0, Batch: 10, Loss: 1.357703
Train - Epoch 0, Batch: 20, Loss: 1.236486
Train - Epoch 0, Batch: 30, Loss: 1.188958
Train - Epoch 1, Batch: 0, Loss: 1.183208
Train - Epoch 1, Batch: 10, Loss: 1.170280
Train - Epoch 1, Batch: 20, Loss: 1.145057
Train - Epoch 1, Batch: 30, Loss: 1.138579
Train - Epoch 2, Batch: 0, Loss: 1.146191
Train - Epoch 2, Batch: 10, Loss: 1.128402
Train - Epoch 2, Batch: 20, Loss: 1.116684
Train - Epoch 2, Batch: 30, Loss: 1.110647
Train - Epoch 3, Batch: 0, Loss: 1.096299
Train - Epoch 3, Batch: 10, Loss: 1.089129
Train - Epoch 3, Batch: 20, Loss: 1.088643
Train - Epoch 3, Batch: 30, Loss: 1.085922
Train - Epoch 4, Batch: 0, Loss: 1.090592
Train - Epoch 4, Batch: 10, Loss: 1.070345
Train - Epoch 4, Batch: 20, Loss: 1.058932
Train - Epoch 4, Batch: 30, Loss: 1.055955
Train - Epoch 5, Batch: 0, Loss: 1.054655
Train - Epoch 5, Batch: 10, Loss: 1.047014
Train - Epoch 5, Batch: 20, Loss: 1.053874
Train - Epoch 5, Batch: 30, Loss: 1.038833
Train - Epoch 6, Batch: 0, Loss: 1.038982
Train - Epoch 6, Batch: 10, Loss: 1.038198
Train - Epoch 6, Batch: 20, Loss: 1.029878
Train - Epoch 6, Batch: 30, Loss: 1.028073
Train - Epoch 7, Batch: 0, Loss: 1.023410
Train - Epoch 7, Batch: 10, Loss: 1.018061
Train - Epoch 7, Batch: 20, Loss: 1.015577
Train - Epoch 7, Batch: 30, Loss: 1.003816
Train - Epoch 8, Batch: 0, Loss: 1.009583
Train - Epoch 8, Batch: 10, Loss: 1.007281
Train - Epoch 8, Batch: 20, Loss: 1.003001
Train - Epoch 8, Batch: 30, Loss: 0.999767
Train - Epoch 9, Batch: 0, Loss: 0.993979
Train - Epoch 9, Batch: 10, Loss: 0.991803
Train - Epoch 9, Batch: 20, Loss: 0.993505
Train - Epoch 9, Batch: 30, Loss: 0.989224
Train - Epoch 10, Batch: 0, Loss: 1.000541
Train - Epoch 10, Batch: 10, Loss: 0.988453
Train - Epoch 10, Batch: 20, Loss: 0.983077
Train - Epoch 10, Batch: 30, Loss: 0.985527
Train - Epoch 11, Batch: 0, Loss: 0.978383
Train - Epoch 11, Batch: 10, Loss: 0.970285
Train - Epoch 11, Batch: 20, Loss: 0.980422
Train - Epoch 11, Batch: 30, Loss: 0.975545
Train - Epoch 12, Batch: 0, Loss: 0.980597
Train - Epoch 12, Batch: 10, Loss: 0.974314
Train - Epoch 12, Batch: 20, Loss: 0.970214
Train - Epoch 12, Batch: 30, Loss: 0.959241
Train - Epoch 13, Batch: 0, Loss: 0.964369
Train - Epoch 13, Batch: 10, Loss: 0.959822
Train - Epoch 13, Batch: 20, Loss: 0.960659
Train - Epoch 13, Batch: 30, Loss: 0.962080
Train - Epoch 14, Batch: 0, Loss: 0.963069
Train - Epoch 14, Batch: 10, Loss: 0.956797
Train - Epoch 14, Batch: 20, Loss: 0.955831
Train - Epoch 14, Batch: 30, Loss: 0.951483
Train - Epoch 15, Batch: 0, Loss: 0.956260
Train - Epoch 15, Batch: 10, Loss: 0.954184
Train - Epoch 15, Batch: 20, Loss: 0.948748
Train - Epoch 15, Batch: 30, Loss: 0.949725
Train - Epoch 16, Batch: 0, Loss: 0.939891
Train - Epoch 16, Batch: 10, Loss: 0.940069
Train - Epoch 16, Batch: 20, Loss: 0.951568
Train - Epoch 16, Batch: 30, Loss: 0.948438
Train - Epoch 17, Batch: 0, Loss: 0.942141
Train - Epoch 17, Batch: 10, Loss: 0.944941
Train - Epoch 17, Batch: 20, Loss: 0.947668
Train - Epoch 17, Batch: 30, Loss: 0.939202
Train - Epoch 18, Batch: 0, Loss: 0.940337
Train - Epoch 18, Batch: 10, Loss: 0.937100
Train - Epoch 18, Batch: 20, Loss: 0.932573
Train - Epoch 18, Batch: 30, Loss: 0.936668
Train - Epoch 19, Batch: 0, Loss: 0.931682
Train - Epoch 19, Batch: 10, Loss: 0.939235
Train - Epoch 19, Batch: 20, Loss: 0.923594
Train - Epoch 19, Batch: 30, Loss: 0.924173
Train - Epoch 20, Batch: 0, Loss: 0.931546
Train - Epoch 20, Batch: 10, Loss: 0.921406
Train - Epoch 20, Batch: 20, Loss: 0.925487
Train - Epoch 20, Batch: 30, Loss: 0.915472
Train - Epoch 21, Batch: 0, Loss: 0.928001
Train - Epoch 21, Batch: 10, Loss: 0.933311
Train - Epoch 21, Batch: 20, Loss: 0.924092
Train - Epoch 21, Batch: 30, Loss: 0.917123
Train - Epoch 22, Batch: 0, Loss: 0.918610
Train - Epoch 22, Batch: 10, Loss: 0.921922
Train - Epoch 22, Batch: 20, Loss: 0.913166
Train - Epoch 22, Batch: 30, Loss: 0.921654
Train - Epoch 23, Batch: 0, Loss: 0.913766
Train - Epoch 23, Batch: 10, Loss: 0.918305
Train - Epoch 23, Batch: 20, Loss: 0.917209
Train - Epoch 23, Batch: 30, Loss: 0.911626
Train - Epoch 24, Batch: 0, Loss: 0.926684
Train - Epoch 24, Batch: 10, Loss: 0.900049
Train - Epoch 24, Batch: 20, Loss: 0.913323
Train - Epoch 24, Batch: 30, Loss: 0.910935
Train - Epoch 25, Batch: 0, Loss: 0.904784
Train - Epoch 25, Batch: 10, Loss: 0.899819
Train - Epoch 25, Batch: 20, Loss: 0.910491
Train - Epoch 25, Batch: 30, Loss: 0.913125
Train - Epoch 26, Batch: 0, Loss: 0.897532
Train - Epoch 26, Batch: 10, Loss: 0.916308
Train - Epoch 26, Batch: 20, Loss: 0.904821
Train - Epoch 26, Batch: 30, Loss: 0.908876
Train - Epoch 27, Batch: 0, Loss: 0.896582
Train - Epoch 27, Batch: 10, Loss: 0.901388
Train - Epoch 27, Batch: 20, Loss: 0.903591
Train - Epoch 27, Batch: 30, Loss: 0.908622
Train - Epoch 28, Batch: 0, Loss: 0.901526
Train - Epoch 28, Batch: 10, Loss: 0.900418
Train - Epoch 28, Batch: 20, Loss: 0.901179
Train - Epoch 28, Batch: 30, Loss: 0.903460
Train - Epoch 29, Batch: 0, Loss: 0.902806
Train - Epoch 29, Batch: 10, Loss: 0.905130
Train - Epoch 29, Batch: 20, Loss: 0.905963
Train - Epoch 29, Batch: 30, Loss: 0.897998
Train - Epoch 30, Batch: 0, Loss: 0.896504
Train - Epoch 30, Batch: 10, Loss: 0.898648
Train - Epoch 30, Batch: 20, Loss: 0.906277
Train - Epoch 30, Batch: 30, Loss: 0.905750
Train - Epoch 31, Batch: 0, Loss: 0.895761
Train - Epoch 31, Batch: 10, Loss: 0.891851
Train - Epoch 31, Batch: 20, Loss: 0.894970
Train - Epoch 31, Batch: 30, Loss: 0.900993
Test Avg. Loss: 0.000070, Accuracy: 0.630562
training_time:: 3.4859371185302734
training time full:: 3.4860005378723145
provenance prepare time:: 6.4373016357421875e-06
Test Avg. Loss: 0.000070, Accuracy: 0.630562
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
batch_size:: 16384
epoch:: 0
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 2.7613348960876465
overhead:: 0
overhead2:: 0.9609251022338867
overhead3:: 0
time_baseline:: 2.7619998455047607
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630613
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.013061761856079102
overhead3:: 0.04341769218444824
overhead4:: 0.20775103569030762
overhead5:: 0
memory usage:: 3766292480
time_provenance:: 0.9696145057678223
curr_diff: 0 tensor(4.1885e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.1885e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630596
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.014238119125366211
overhead3:: 0.04990220069885254
overhead4:: 0.2189037799835205
overhead5:: 0
memory usage:: 3848966144
time_provenance:: 1.0308494567871094
curr_diff: 0 tensor(5.3504e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.3504e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630596
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.014853477478027344
overhead3:: 0.046936988830566406
overhead4:: 0.23386716842651367
overhead5:: 0
memory usage:: 3770376192
time_provenance:: 0.9870285987854004
curr_diff: 0 tensor(4.1361e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.1361e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630596
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.01607489585876465
overhead3:: 0.06329512596130371
overhead4:: 0.254838228225708
overhead5:: 0
memory usage:: 3779190784
time_provenance:: 1.0436897277832031
curr_diff: 0 tensor(5.4055e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.4055e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630596
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.017325639724731445
overhead3:: 0.054563045501708984
overhead4:: 0.27535486221313477
overhead5:: 0
memory usage:: 3780362240
time_provenance:: 1.0413954257965088
curr_diff: 0 tensor(4.4148e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.4148e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630596
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.021242141723632812
overhead3:: 0.0599362850189209
overhead4:: 0.3148517608642578
overhead5:: 0
memory usage:: 3791077376
time_provenance:: 1.102858543395996
curr_diff: 0 tensor(2.3080e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.3080e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630596
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.02198934555053711
overhead3:: 0.06165337562561035
overhead4:: 0.3478388786315918
overhead5:: 0
memory usage:: 3765600256
time_provenance:: 1.1172091960906982
curr_diff: 0 tensor(2.3178e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.3178e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630596
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.02496933937072754
overhead3:: 0.07034945487976074
overhead4:: 0.38434481620788574
overhead5:: 0
memory usage:: 3764350976
time_provenance:: 1.150953769683838
curr_diff: 0 tensor(2.2571e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.2571e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630596
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.023613452911376953
overhead3:: 0.07170462608337402
overhead4:: 0.42714571952819824
overhead5:: 0
memory usage:: 3766161408
time_provenance:: 1.2019975185394287
curr_diff: 0 tensor(2.3549e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.3549e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630596
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.026273250579833984
overhead3:: 0.07100176811218262
overhead4:: 0.41167473793029785
overhead5:: 0
memory usage:: 3775582208
time_provenance:: 1.1791274547576904
curr_diff: 0 tensor(2.3548e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.3548e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630596
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.0361936092376709
overhead3:: 0.09647035598754883
overhead4:: 0.6417281627655029
overhead5:: 0
memory usage:: 3786985472
time_provenance:: 1.461097002029419
curr_diff: 0 tensor(8.0469e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.0469e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630613
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.046521902084350586
overhead3:: 0.11410856246948242
overhead4:: 0.7574567794799805
overhead5:: 0
memory usage:: 3789426688
time_provenance:: 1.7161438465118408
curr_diff: 0 tensor(8.1717e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.1717e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630613
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.0397639274597168
overhead3:: 0.1099553108215332
overhead4:: 0.6762945652008057
overhead5:: 0
memory usage:: 3768893440
time_provenance:: 1.510502815246582
curr_diff: 0 tensor(7.9808e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.9808e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630613
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.05031752586364746
overhead3:: 0.14037728309631348
overhead4:: 0.913855791091919
overhead5:: 0
memory usage:: 3775377408
time_provenance:: 2.0413007736206055
curr_diff: 0 tensor(8.3747e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.3747e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630613
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.042844295501708984
overhead3:: 0.11755228042602539
overhead4:: 0.6770949363708496
overhead5:: 0
memory usage:: 3770986496
time_provenance:: 1.5460610389709473
curr_diff: 0 tensor(8.4342e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.4342e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630613
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.07927656173706055
overhead3:: 0.21901392936706543
overhead4:: 1.5304298400878906
overhead5:: 0
memory usage:: 3771731968
time_provenance:: 2.607145071029663
curr_diff: 0 tensor(1.3348e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3348e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630613
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.08156418800354004
overhead3:: 0.22219204902648926
overhead4:: 1.5004656314849854
overhead5:: 0
memory usage:: 3765985280
time_provenance:: 2.569265127182007
curr_diff: 0 tensor(1.3476e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3476e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630613
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.08232712745666504
overhead3:: 0.23750042915344238
overhead4:: 1.4826929569244385
overhead5:: 0
memory usage:: 3763785728
time_provenance:: 2.560883045196533
curr_diff: 0 tensor(1.3469e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3469e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630613
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.08245635032653809
overhead3:: 0.22589755058288574
overhead4:: 1.5090899467468262
overhead5:: 0
memory usage:: 3775365120
time_provenance:: 2.572110176086426
curr_diff: 0 tensor(1.3682e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3682e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630613
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.08995723724365234
overhead3:: 0.24247217178344727
overhead4:: 1.431854248046875
overhead5:: 0
memory usage:: 3763609600
time_provenance:: 2.5240719318389893
curr_diff: 0 tensor(1.3740e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3740e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630613
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 26
max_epoch:: 32
overhead:: 0
overhead2:: 0.21329975128173828
overhead3:: 0.6328039169311523
overhead4:: 2.893768310546875
overhead5:: 0
memory usage:: 3753381888
time_provenance:: 3.9964609146118164
curr_diff: 0 tensor(1.0101e-13, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0101e-13, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630613
deletion rate:: 0.0001
python3 generate_rand_ids 0.0001  covtype 0
tensor([274562,  84483,   7430, 498696,  35345,  65041, 233113, 298139,  74397,
        507038, 236447, 158239,  32934, 102183, 509736,  23210, 147370, 260149,
        103094, 292405, 393909,  29496, 163897, 162872, 457404, 134079, 469568,
        431936, 255167,  51651,  49983, 388422, 171591, 266441, 456650, 479180,
        117964, 207954, 375125, 156118, 491743, 328545,  84328,   2665, 445546,
         55534, 364534, 333306, 136699, 349692, 300286, 137087])
batch size:: 16384
repetition 0
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 0 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 2.003612
Train - Epoch 0, Batch: 10, Loss: 1.393994
Train - Epoch 0, Batch: 20, Loss: 1.248411
Train - Epoch 0, Batch: 30, Loss: 1.200490
Train - Epoch 1, Batch: 0, Loss: 1.181946
Train - Epoch 1, Batch: 10, Loss: 1.165412
Train - Epoch 1, Batch: 20, Loss: 1.138252
Train - Epoch 1, Batch: 30, Loss: 1.128030
Train - Epoch 2, Batch: 0, Loss: 1.140563
Train - Epoch 2, Batch: 10, Loss: 1.109184
Train - Epoch 2, Batch: 20, Loss: 1.120517
Train - Epoch 2, Batch: 30, Loss: 1.097186
Train - Epoch 3, Batch: 0, Loss: 1.101040
Train - Epoch 3, Batch: 10, Loss: 1.089991
Train - Epoch 3, Batch: 20, Loss: 1.087523
Train - Epoch 3, Batch: 30, Loss: 1.069151
Train - Epoch 4, Batch: 0, Loss: 1.076871
Train - Epoch 4, Batch: 10, Loss: 1.071808
Train - Epoch 4, Batch: 20, Loss: 1.064873
Train - Epoch 4, Batch: 30, Loss: 1.050920
Train - Epoch 5, Batch: 0, Loss: 1.063205
Train - Epoch 5, Batch: 10, Loss: 1.045836
Train - Epoch 5, Batch: 20, Loss: 1.045400
Train - Epoch 5, Batch: 30, Loss: 1.034617
Train - Epoch 6, Batch: 0, Loss: 1.037426
Train - Epoch 6, Batch: 10, Loss: 1.024874
Train - Epoch 6, Batch: 20, Loss: 1.020378
Train - Epoch 6, Batch: 30, Loss: 1.017761
Train - Epoch 7, Batch: 0, Loss: 1.021539
Train - Epoch 7, Batch: 10, Loss: 1.017859
Train - Epoch 7, Batch: 20, Loss: 1.022426
Train - Epoch 7, Batch: 30, Loss: 1.006196
Train - Epoch 8, Batch: 0, Loss: 1.009320
Train - Epoch 8, Batch: 10, Loss: 1.000778
Train - Epoch 8, Batch: 20, Loss: 1.011224
Train - Epoch 8, Batch: 30, Loss: 0.995446
Train - Epoch 9, Batch: 0, Loss: 1.000200
Train - Epoch 9, Batch: 10, Loss: 1.005951
Train - Epoch 9, Batch: 20, Loss: 0.996773
Train - Epoch 9, Batch: 30, Loss: 0.986380
Train - Epoch 10, Batch: 0, Loss: 0.980199
Train - Epoch 10, Batch: 10, Loss: 0.980415
Train - Epoch 10, Batch: 20, Loss: 0.995979
Train - Epoch 10, Batch: 30, Loss: 0.994936
Train - Epoch 11, Batch: 0, Loss: 0.974316
Train - Epoch 11, Batch: 10, Loss: 0.982886
Train - Epoch 11, Batch: 20, Loss: 0.973970
Train - Epoch 11, Batch: 30, Loss: 0.974601
Train - Epoch 12, Batch: 0, Loss: 0.963061
Train - Epoch 12, Batch: 10, Loss: 0.962535
Train - Epoch 12, Batch: 20, Loss: 0.965872
Train - Epoch 12, Batch: 30, Loss: 0.964511
Train - Epoch 13, Batch: 0, Loss: 0.959037
Train - Epoch 13, Batch: 10, Loss: 0.972191
Train - Epoch 13, Batch: 20, Loss: 0.959723
Train - Epoch 13, Batch: 30, Loss: 0.962017
Train - Epoch 14, Batch: 0, Loss: 0.958720
Train - Epoch 14, Batch: 10, Loss: 0.965177
Train - Epoch 14, Batch: 20, Loss: 0.948986
Train - Epoch 14, Batch: 30, Loss: 0.954342
Train - Epoch 15, Batch: 0, Loss: 0.953362
Train - Epoch 15, Batch: 10, Loss: 0.949114
Train - Epoch 15, Batch: 20, Loss: 0.943923
Train - Epoch 15, Batch: 30, Loss: 0.955227
Train - Epoch 16, Batch: 0, Loss: 0.941667
Train - Epoch 16, Batch: 10, Loss: 0.947952
Train - Epoch 16, Batch: 20, Loss: 0.946455
Train - Epoch 16, Batch: 30, Loss: 0.939530
Train - Epoch 17, Batch: 0, Loss: 0.948145
Train - Epoch 17, Batch: 10, Loss: 0.932931
Train - Epoch 17, Batch: 20, Loss: 0.943045
Train - Epoch 17, Batch: 30, Loss: 0.936129
Train - Epoch 18, Batch: 0, Loss: 0.930262
Train - Epoch 18, Batch: 10, Loss: 0.936803
Train - Epoch 18, Batch: 20, Loss: 0.936691
Train - Epoch 18, Batch: 30, Loss: 0.932093
Train - Epoch 19, Batch: 0, Loss: 0.937339
Train - Epoch 19, Batch: 10, Loss: 0.931084
Train - Epoch 19, Batch: 20, Loss: 0.926127
Train - Epoch 19, Batch: 30, Loss: 0.931093
Train - Epoch 20, Batch: 0, Loss: 0.927277
Train - Epoch 20, Batch: 10, Loss: 0.933514
Train - Epoch 20, Batch: 20, Loss: 0.921541
Train - Epoch 20, Batch: 30, Loss: 0.924869
Train - Epoch 21, Batch: 0, Loss: 0.922235
Train - Epoch 21, Batch: 10, Loss: 0.927379
Train - Epoch 21, Batch: 20, Loss: 0.922280
Train - Epoch 21, Batch: 30, Loss: 0.926312
Train - Epoch 22, Batch: 0, Loss: 0.923285
Train - Epoch 22, Batch: 10, Loss: 0.918507
Train - Epoch 22, Batch: 20, Loss: 0.926947
Train - Epoch 22, Batch: 30, Loss: 0.922405
Train - Epoch 23, Batch: 0, Loss: 0.914129
Train - Epoch 23, Batch: 10, Loss: 0.925084
Train - Epoch 23, Batch: 20, Loss: 0.921954
Train - Epoch 23, Batch: 30, Loss: 0.910475
Train - Epoch 24, Batch: 0, Loss: 0.916921
Train - Epoch 24, Batch: 10, Loss: 0.911848
Train - Epoch 24, Batch: 20, Loss: 0.917289
Train - Epoch 24, Batch: 30, Loss: 0.919312
Train - Epoch 25, Batch: 0, Loss: 0.913921
Train - Epoch 25, Batch: 10, Loss: 0.911765
Train - Epoch 25, Batch: 20, Loss: 0.901009
Train - Epoch 25, Batch: 30, Loss: 0.914904
Train - Epoch 26, Batch: 0, Loss: 0.912255
Train - Epoch 26, Batch: 10, Loss: 0.906157
Train - Epoch 26, Batch: 20, Loss: 0.915820
Train - Epoch 26, Batch: 30, Loss: 0.907201
Train - Epoch 27, Batch: 0, Loss: 0.898547
Train - Epoch 27, Batch: 10, Loss: 0.913897
Train - Epoch 27, Batch: 20, Loss: 0.902276
Train - Epoch 27, Batch: 30, Loss: 0.902884
Train - Epoch 28, Batch: 0, Loss: 0.897168
Train - Epoch 28, Batch: 10, Loss: 0.899489
Train - Epoch 28, Batch: 20, Loss: 0.898336
Train - Epoch 28, Batch: 30, Loss: 0.902675
Train - Epoch 29, Batch: 0, Loss: 0.901619
Train - Epoch 29, Batch: 10, Loss: 0.903352
Train - Epoch 29, Batch: 20, Loss: 0.899786
Train - Epoch 29, Batch: 30, Loss: 0.894424
Train - Epoch 30, Batch: 0, Loss: 0.888743
Train - Epoch 30, Batch: 10, Loss: 0.894024
Train - Epoch 30, Batch: 20, Loss: 0.890414
Train - Epoch 30, Batch: 30, Loss: 0.902898
Train - Epoch 31, Batch: 0, Loss: 0.886465
Train - Epoch 31, Batch: 10, Loss: 0.900936
Train - Epoch 31, Batch: 20, Loss: 0.887545
Train - Epoch 31, Batch: 30, Loss: 0.892212
Test Avg. Loss: 0.000070, Accuracy: 0.629460
training_time:: 3.5245814323425293
training time full:: 3.5246458053588867
provenance prepare time:: 7.3909759521484375e-06
Test Avg. Loss: 0.000070, Accuracy: 0.629460
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
batch_size:: 16384
epoch:: 0
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 3.460577964782715
overhead:: 0
overhead2:: 1.394413709640503
overhead3:: 0
time_baseline:: 3.4614131450653076
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629460
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.014927387237548828
overhead3:: 0.05212521553039551
overhead4:: 0.21125149726867676
overhead5:: 0
memory usage:: 3798573056
time_provenance:: 1.1140644550323486
curr_diff: 0 tensor(4.9720e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.9720e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629460
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.016723155975341797
overhead3:: 0.0557253360748291
overhead4:: 0.22277092933654785
overhead5:: 0
memory usage:: 3776233472
time_provenance:: 1.1000704765319824
curr_diff: 0 tensor(5.6433e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.6433e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629460
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.021479368209838867
overhead3:: 0.06527042388916016
overhead4:: 0.2751448154449463
overhead5:: 0
memory usage:: 3799416832
time_provenance:: 1.2888014316558838
curr_diff: 0 tensor(5.4099e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.4099e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629460
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.022814035415649414
overhead3:: 0.06848931312561035
overhead4:: 0.28305578231811523
overhead5:: 0
memory usage:: 3787653120
time_provenance:: 1.2477288246154785
curr_diff: 0 tensor(5.5351e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.5351e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629460
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.022296428680419922
overhead3:: 0.055075883865356445
overhead4:: 0.3041396141052246
overhead5:: 0
memory usage:: 3776454656
time_provenance:: 1.1743388175964355
curr_diff: 0 tensor(5.4623e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.4623e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629460
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.025957584381103516
overhead3:: 0.060488224029541016
overhead4:: 0.3522160053253174
overhead5:: 0
memory usage:: 3789791232
time_provenance:: 1.2245349884033203
curr_diff: 0 tensor(3.2818e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.2818e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629460
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.02755570411682129
overhead3:: 0.08122634887695312
overhead4:: 0.38542652130126953
overhead5:: 0
memory usage:: 3790864384
time_provenance:: 1.2796335220336914
curr_diff: 0 tensor(3.4579e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.4579e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629460
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.03140687942504883
overhead3:: 0.08295607566833496
overhead4:: 0.4482417106628418
overhead5:: 0
memory usage:: 3763757056
time_provenance:: 1.415276288986206
curr_diff: 0 tensor(3.4685e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.4685e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629460
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.03344106674194336
overhead3:: 0.08571696281433105
overhead4:: 0.46091794967651367
overhead5:: 0
memory usage:: 3763429376
time_provenance:: 1.3857033252716064
curr_diff: 0 tensor(3.4623e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.4623e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629460
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.03619956970214844
overhead3:: 0.0846552848815918
overhead4:: 0.47194933891296387
overhead5:: 0
memory usage:: 3775995904
time_provenance:: 1.3976128101348877
curr_diff: 0 tensor(3.4921e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.4921e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629460
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.04613542556762695
overhead3:: 0.1110842227935791
overhead4:: 0.6897048950195312
overhead5:: 0
memory usage:: 3775418368
time_provenance:: 1.6175291538238525
curr_diff: 0 tensor(1.1590e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1590e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629460
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.051116228103637695
overhead3:: 0.09630060195922852
overhead4:: 0.6622037887573242
overhead5:: 0
memory usage:: 3767496704
time_provenance:: 1.5746517181396484
curr_diff: 0 tensor(1.2291e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2291e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629460
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.05573463439941406
overhead3:: 0.1149299144744873
overhead4:: 0.7342588901519775
overhead5:: 0
memory usage:: 3765350400
time_provenance:: 1.690718173980713
curr_diff: 0 tensor(1.2338e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2338e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629460
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.05751919746398926
overhead3:: 0.11691975593566895
overhead4:: 0.8601276874542236
overhead5:: 0
memory usage:: 3804651520
time_provenance:: 1.8632714748382568
curr_diff: 0 tensor(1.2335e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2335e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629460
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.05652022361755371
overhead3:: 0.11754250526428223
overhead4:: 0.8230681419372559
overhead5:: 0
memory usage:: 3771113472
time_provenance:: 1.7897522449493408
curr_diff: 0 tensor(1.2406e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2406e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629460
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.1299304962158203
overhead3:: 0.23994851112365723
overhead4:: 1.8188002109527588
overhead5:: 0
memory usage:: 3780939776
time_provenance:: 3.042949914932251
curr_diff: 0 tensor(2.5753e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.5753e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629460
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.15136456489562988
overhead3:: 0.26978516578674316
overhead4:: 2.318147897720337
overhead5:: 0
memory usage:: 3819298816
time_provenance:: 3.733942747116089
curr_diff: 0 tensor(2.7033e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.7033e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629460
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.13052821159362793
overhead3:: 0.2435595989227295
overhead4:: 1.9988739490509033
overhead5:: 0
memory usage:: 3766525952
time_provenance:: 3.2555429935455322
curr_diff: 0 tensor(2.7002e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.7002e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629460
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.11734890937805176
overhead3:: 0.23024344444274902
overhead4:: 1.7951898574829102
overhead5:: 0
memory usage:: 3800731648
time_provenance:: 2.9530582427978516
curr_diff: 0 tensor(2.7084e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.7084e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629460
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.11606383323669434
overhead3:: 0.22599482536315918
overhead4:: 1.7866830825805664
overhead5:: 0
memory usage:: 3789541376
time_provenance:: 2.937697410583496
curr_diff: 0 tensor(2.7136e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.7136e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629460
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.21854376792907715
overhead3:: 0.3867015838623047
overhead4:: 2.9027557373046875
overhead5:: 0
memory usage:: 3759865856
time_provenance:: 3.859281539916992
curr_diff: 0 tensor(9.8648e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.8648e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629460
repetition 1
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 1 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 2.116590
Train - Epoch 0, Batch: 10, Loss: 1.418956
Train - Epoch 0, Batch: 20, Loss: 1.242888
Train - Epoch 0, Batch: 30, Loss: 1.193366
Train - Epoch 1, Batch: 0, Loss: 1.175520
Train - Epoch 1, Batch: 10, Loss: 1.145097
Train - Epoch 1, Batch: 20, Loss: 1.145582
Train - Epoch 1, Batch: 30, Loss: 1.117941
Train - Epoch 2, Batch: 0, Loss: 1.113069
Train - Epoch 2, Batch: 10, Loss: 1.116903
Train - Epoch 2, Batch: 20, Loss: 1.103851
Train - Epoch 2, Batch: 30, Loss: 1.092603
Train - Epoch 3, Batch: 0, Loss: 1.104890
Train - Epoch 3, Batch: 10, Loss: 1.081747
Train - Epoch 3, Batch: 20, Loss: 1.077387
Train - Epoch 3, Batch: 30, Loss: 1.069544
Train - Epoch 4, Batch: 0, Loss: 1.074982
Train - Epoch 4, Batch: 10, Loss: 1.061217
Train - Epoch 4, Batch: 20, Loss: 1.054767
Train - Epoch 4, Batch: 30, Loss: 1.048658
Train - Epoch 5, Batch: 0, Loss: 1.040931
Train - Epoch 5, Batch: 10, Loss: 1.041436
Train - Epoch 5, Batch: 20, Loss: 1.037314
Train - Epoch 5, Batch: 30, Loss: 1.035280
Train - Epoch 6, Batch: 0, Loss: 1.042860
Train - Epoch 6, Batch: 10, Loss: 1.026250
Train - Epoch 6, Batch: 20, Loss: 1.025555
Train - Epoch 6, Batch: 30, Loss: 1.019439
Train - Epoch 7, Batch: 0, Loss: 1.015295
Train - Epoch 7, Batch: 10, Loss: 1.009081
Train - Epoch 7, Batch: 20, Loss: 1.012946
Train - Epoch 7, Batch: 30, Loss: 1.000144
Train - Epoch 8, Batch: 0, Loss: 1.008197
Train - Epoch 8, Batch: 10, Loss: 0.993970
Train - Epoch 8, Batch: 20, Loss: 1.007340
Train - Epoch 8, Batch: 30, Loss: 0.988090
Train - Epoch 9, Batch: 0, Loss: 0.989782
Train - Epoch 9, Batch: 10, Loss: 0.989067
Train - Epoch 9, Batch: 20, Loss: 0.980460
Train - Epoch 9, Batch: 30, Loss: 0.988209
Train - Epoch 10, Batch: 0, Loss: 0.987075
Train - Epoch 10, Batch: 10, Loss: 0.978336
Train - Epoch 10, Batch: 20, Loss: 0.976190
Train - Epoch 10, Batch: 30, Loss: 0.968047
Train - Epoch 11, Batch: 0, Loss: 0.976802
Train - Epoch 11, Batch: 10, Loss: 0.976447
Train - Epoch 11, Batch: 20, Loss: 0.973006
Train - Epoch 11, Batch: 30, Loss: 0.962573
Train - Epoch 12, Batch: 0, Loss: 0.976004
Train - Epoch 12, Batch: 10, Loss: 0.962866
Train - Epoch 12, Batch: 20, Loss: 0.961607
Train - Epoch 12, Batch: 30, Loss: 0.951462
Train - Epoch 13, Batch: 0, Loss: 0.958483
Train - Epoch 13, Batch: 10, Loss: 0.961749
Train - Epoch 13, Batch: 20, Loss: 0.965640
Train - Epoch 13, Batch: 30, Loss: 0.957248
Train - Epoch 14, Batch: 0, Loss: 0.950411
Train - Epoch 14, Batch: 10, Loss: 0.954708
Train - Epoch 14, Batch: 20, Loss: 0.957286
Train - Epoch 14, Batch: 30, Loss: 0.955261
Train - Epoch 15, Batch: 0, Loss: 0.950639
Train - Epoch 15, Batch: 10, Loss: 0.951945
Train - Epoch 15, Batch: 20, Loss: 0.946079
Train - Epoch 15, Batch: 30, Loss: 0.940754
Train - Epoch 16, Batch: 0, Loss: 0.947874
Train - Epoch 16, Batch: 10, Loss: 0.945555
Train - Epoch 16, Batch: 20, Loss: 0.943539
Train - Epoch 16, Batch: 30, Loss: 0.938179
Train - Epoch 17, Batch: 0, Loss: 0.938636
Train - Epoch 17, Batch: 10, Loss: 0.937947
Train - Epoch 17, Batch: 20, Loss: 0.938723
Train - Epoch 17, Batch: 30, Loss: 0.935554
Train - Epoch 18, Batch: 0, Loss: 0.936108
Train - Epoch 18, Batch: 10, Loss: 0.940173
Train - Epoch 18, Batch: 20, Loss: 0.919390
Train - Epoch 18, Batch: 30, Loss: 0.930306
Train - Epoch 19, Batch: 0, Loss: 0.935417
Train - Epoch 19, Batch: 10, Loss: 0.927932
Train - Epoch 19, Batch: 20, Loss: 0.918421
Train - Epoch 19, Batch: 30, Loss: 0.919924
Train - Epoch 20, Batch: 0, Loss: 0.928581
Train - Epoch 20, Batch: 10, Loss: 0.923028
Train - Epoch 20, Batch: 20, Loss: 0.930496
Train - Epoch 20, Batch: 30, Loss: 0.916041
Train - Epoch 21, Batch: 0, Loss: 0.924118
Train - Epoch 21, Batch: 10, Loss: 0.920799
Train - Epoch 21, Batch: 20, Loss: 0.921039
Train - Epoch 21, Batch: 30, Loss: 0.922670
Train - Epoch 22, Batch: 0, Loss: 0.911975
Train - Epoch 22, Batch: 10, Loss: 0.920715
Train - Epoch 22, Batch: 20, Loss: 0.914344
Train - Epoch 22, Batch: 30, Loss: 0.921017
Train - Epoch 23, Batch: 0, Loss: 0.925185
Train - Epoch 23, Batch: 10, Loss: 0.918589
Train - Epoch 23, Batch: 20, Loss: 0.920219
Train - Epoch 23, Batch: 30, Loss: 0.900557
Train - Epoch 24, Batch: 0, Loss: 0.911085
Train - Epoch 24, Batch: 10, Loss: 0.906456
Train - Epoch 24, Batch: 20, Loss: 0.911312
Train - Epoch 24, Batch: 30, Loss: 0.912796
Train - Epoch 25, Batch: 0, Loss: 0.911338
Train - Epoch 25, Batch: 10, Loss: 0.918442
Train - Epoch 25, Batch: 20, Loss: 0.911969
Train - Epoch 25, Batch: 30, Loss: 0.906328
Train - Epoch 26, Batch: 0, Loss: 0.913341
Train - Epoch 26, Batch: 10, Loss: 0.900873
Train - Epoch 26, Batch: 20, Loss: 0.908550
Train - Epoch 26, Batch: 30, Loss: 0.904879
Train - Epoch 27, Batch: 0, Loss: 0.906611
Train - Epoch 27, Batch: 10, Loss: 0.907614
Train - Epoch 27, Batch: 20, Loss: 0.902088
Train - Epoch 27, Batch: 30, Loss: 0.898139
Train - Epoch 28, Batch: 0, Loss: 0.908253
Train - Epoch 28, Batch: 10, Loss: 0.893631
Train - Epoch 28, Batch: 20, Loss: 0.903976
Train - Epoch 28, Batch: 30, Loss: 0.894731
Train - Epoch 29, Batch: 0, Loss: 0.901593
Train - Epoch 29, Batch: 10, Loss: 0.917926
Train - Epoch 29, Batch: 20, Loss: 0.891019
Train - Epoch 29, Batch: 30, Loss: 0.891646
Train - Epoch 30, Batch: 0, Loss: 0.902088
Train - Epoch 30, Batch: 10, Loss: 0.890990
Train - Epoch 30, Batch: 20, Loss: 0.889756
Train - Epoch 30, Batch: 30, Loss: 0.893716
Train - Epoch 31, Batch: 0, Loss: 0.887902
Train - Epoch 31, Batch: 10, Loss: 0.894897
Train - Epoch 31, Batch: 20, Loss: 0.904327
Train - Epoch 31, Batch: 30, Loss: 0.891865
Test Avg. Loss: 0.000070, Accuracy: 0.630906
training_time:: 3.424665927886963
training time full:: 3.424731969833374
provenance prepare time:: 6.67572021484375e-06
Test Avg. Loss: 0.000070, Accuracy: 0.630906
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
batch_size:: 16384
epoch:: 0
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 3.6014130115509033
overhead:: 0
overhead2:: 1.4589636325836182
overhead3:: 0
time_baseline:: 3.602221965789795
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630906
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.014010429382324219
overhead3:: 0.049936771392822266
overhead4:: 0.1896045207977295
overhead5:: 0
memory usage:: 3774771200
time_provenance:: 1.012296199798584
curr_diff: 0 tensor(5.8117e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.8117e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630906
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.018405675888061523
overhead3:: 0.05559420585632324
overhead4:: 0.23188066482543945
overhead5:: 0
memory usage:: 3769372672
time_provenance:: 1.0908212661743164
curr_diff: 0 tensor(5.5452e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.5452e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630906
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.01878952980041504
overhead3:: 0.05910038948059082
overhead4:: 0.26593899726867676
overhead5:: 0
memory usage:: 3766603776
time_provenance:: 1.129645586013794
curr_diff: 0 tensor(6.8370e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.8370e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630906
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.022179365158081055
overhead3:: 0.05739402770996094
overhead4:: 0.2802245616912842
overhead5:: 0
memory usage:: 3762774016
time_provenance:: 1.1212117671966553
curr_diff: 0 tensor(5.6859e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.6859e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630906
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.022716522216796875
overhead3:: 0.06886768341064453
overhead4:: 0.31990885734558105
overhead5:: 0
memory usage:: 3802398720
time_provenance:: 1.200782060623169
curr_diff: 0 tensor(6.8186e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.8186e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630906
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.027507543563842773
overhead3:: 0.06485939025878906
overhead4:: 0.37423181533813477
overhead5:: 0
memory usage:: 3769810944
time_provenance:: 1.2771449089050293
curr_diff: 0 tensor(3.6732e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.6732e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630906
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.029350996017456055
overhead3:: 0.08960390090942383
overhead4:: 0.3904862403869629
overhead5:: 0
memory usage:: 3775463424
time_provenance:: 1.306736946105957
curr_diff: 0 tensor(3.8085e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.8085e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630906
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.037389278411865234
overhead3:: 0.08256793022155762
overhead4:: 0.4469473361968994
overhead5:: 0
memory usage:: 3799801856
time_provenance:: 1.3471159934997559
curr_diff: 0 tensor(3.8196e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.8196e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630906
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.0317988395690918
overhead3:: 0.07792353630065918
overhead4:: 0.44661736488342285
overhead5:: 0
memory usage:: 3762614272
time_provenance:: 1.3016300201416016
curr_diff: 0 tensor(3.8306e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.8306e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630906
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.03650951385498047
overhead3:: 0.07621645927429199
overhead4:: 0.5047974586486816
overhead5:: 0
memory usage:: 3788009472
time_provenance:: 1.4533448219299316
curr_diff: 0 tensor(3.8562e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.8562e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630906
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.05015444755554199
overhead3:: 0.09994912147521973
overhead4:: 0.7314097881317139
overhead5:: 0
memory usage:: 3791929344
time_provenance:: 1.6934435367584229
curr_diff: 0 tensor(1.4277e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4277e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630906
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.05467844009399414
overhead3:: 0.09886503219604492
overhead4:: 0.7043890953063965
overhead5:: 0
memory usage:: 3762720768
time_provenance:: 1.6494364738464355
curr_diff: 0 tensor(1.4528e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4528e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630906
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.05659985542297363
overhead3:: 0.11263465881347656
overhead4:: 0.7745294570922852
overhead5:: 0
memory usage:: 3764826112
time_provenance:: 1.779134750366211
curr_diff: 0 tensor(1.4501e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4501e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630906
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.05396604537963867
overhead3:: 0.10341262817382812
overhead4:: 0.7687969207763672
overhead5:: 0
memory usage:: 3799756800
time_provenance:: 1.697190761566162
curr_diff: 0 tensor(1.4481e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4481e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630906
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.05742597579956055
overhead3:: 0.11679792404174805
overhead4:: 0.8233644962310791
overhead5:: 0
memory usage:: 3782762496
time_provenance:: 1.7853827476501465
curr_diff: 0 tensor(1.4552e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4552e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630906
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.11904644966125488
overhead3:: 0.22263336181640625
overhead4:: 1.7479262351989746
overhead5:: 0
memory usage:: 3791462400
time_provenance:: 2.9014711380004883
curr_diff: 0 tensor(2.3413e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.3413e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630906
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.17532753944396973
overhead3:: 0.2934284210205078
overhead4:: 2.4553489685058594
overhead5:: 0
memory usage:: 3779121152
time_provenance:: 3.9857280254364014
curr_diff: 0 tensor(2.3934e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.3934e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630906
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.13110828399658203
overhead3:: 0.24216151237487793
overhead4:: 1.7958805561065674
overhead5:: 0
memory usage:: 3768840192
time_provenance:: 2.969444751739502
curr_diff: 0 tensor(2.3816e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.3816e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630906
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.12553095817565918
overhead3:: 0.23584389686584473
overhead4:: 1.7863128185272217
overhead5:: 0
memory usage:: 3760746496
time_provenance:: 2.9576244354248047
curr_diff: 0 tensor(2.3926e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.3926e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630906
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.12068939208984375
overhead3:: 0.2265465259552002
overhead4:: 1.8006925582885742
overhead5:: 0
memory usage:: 3804696576
time_provenance:: 2.9409687519073486
curr_diff: 0 tensor(2.4267e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.4267e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630906
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.26210737228393555
overhead3:: 0.4932394027709961
overhead4:: 3.1987228393554688
overhead5:: 0
memory usage:: 3760259072
time_provenance:: 4.308488607406616
curr_diff: 0 tensor(9.9952e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.9952e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630906
repetition 2
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 2 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 2.001896
Train - Epoch 0, Batch: 10, Loss: 1.399878
Train - Epoch 0, Batch: 20, Loss: 1.259308
Train - Epoch 0, Batch: 30, Loss: 1.197346
Train - Epoch 1, Batch: 0, Loss: 1.184246
Train - Epoch 1, Batch: 10, Loss: 1.161701
Train - Epoch 1, Batch: 20, Loss: 1.145060
Train - Epoch 1, Batch: 30, Loss: 1.131123
Train - Epoch 2, Batch: 0, Loss: 1.132979
Train - Epoch 2, Batch: 10, Loss: 1.121853
Train - Epoch 2, Batch: 20, Loss: 1.100013
Train - Epoch 2, Batch: 30, Loss: 1.115714
Train - Epoch 3, Batch: 0, Loss: 1.101830
Train - Epoch 3, Batch: 10, Loss: 1.101419
Train - Epoch 3, Batch: 20, Loss: 1.081496
Train - Epoch 3, Batch: 30, Loss: 1.087591
Train - Epoch 4, Batch: 0, Loss: 1.075275
Train - Epoch 4, Batch: 10, Loss: 1.061687
Train - Epoch 4, Batch: 20, Loss: 1.059898
Train - Epoch 4, Batch: 30, Loss: 1.050451
Train - Epoch 5, Batch: 0, Loss: 1.054075
Train - Epoch 5, Batch: 10, Loss: 1.054682
Train - Epoch 5, Batch: 20, Loss: 1.037611
Train - Epoch 5, Batch: 30, Loss: 1.039937
Train - Epoch 6, Batch: 0, Loss: 1.041708
Train - Epoch 6, Batch: 10, Loss: 1.025103
Train - Epoch 6, Batch: 20, Loss: 1.032489
Train - Epoch 6, Batch: 30, Loss: 1.017364
Train - Epoch 7, Batch: 0, Loss: 1.017421
Train - Epoch 7, Batch: 10, Loss: 1.025840
Train - Epoch 7, Batch: 20, Loss: 1.013747
Train - Epoch 7, Batch: 30, Loss: 1.013926
Train - Epoch 8, Batch: 0, Loss: 1.005038
Train - Epoch 8, Batch: 10, Loss: 1.004535
Train - Epoch 8, Batch: 20, Loss: 1.002697
Train - Epoch 8, Batch: 30, Loss: 1.002553
Train - Epoch 9, Batch: 0, Loss: 0.998908
Train - Epoch 9, Batch: 10, Loss: 0.998886
Train - Epoch 9, Batch: 20, Loss: 0.998346
Train - Epoch 9, Batch: 30, Loss: 0.983731
Train - Epoch 10, Batch: 0, Loss: 0.989951
Train - Epoch 10, Batch: 10, Loss: 0.987180
Train - Epoch 10, Batch: 20, Loss: 0.972990
Train - Epoch 10, Batch: 30, Loss: 0.985917
Train - Epoch 11, Batch: 0, Loss: 0.980724
Train - Epoch 11, Batch: 10, Loss: 0.971254
Train - Epoch 11, Batch: 20, Loss: 0.974063
Train - Epoch 11, Batch: 30, Loss: 0.970711
Train - Epoch 12, Batch: 0, Loss: 0.969604
Train - Epoch 12, Batch: 10, Loss: 0.975339
Train - Epoch 12, Batch: 20, Loss: 0.964328
Train - Epoch 12, Batch: 30, Loss: 0.972520
Train - Epoch 13, Batch: 0, Loss: 0.967068
Train - Epoch 13, Batch: 10, Loss: 0.955294
Train - Epoch 13, Batch: 20, Loss: 0.958958
Train - Epoch 13, Batch: 30, Loss: 0.972711
Train - Epoch 14, Batch: 0, Loss: 0.956721
Train - Epoch 14, Batch: 10, Loss: 0.963449
Train - Epoch 14, Batch: 20, Loss: 0.955668
Train - Epoch 14, Batch: 30, Loss: 0.960222
Train - Epoch 15, Batch: 0, Loss: 0.954765
Train - Epoch 15, Batch: 10, Loss: 0.947868
Train - Epoch 15, Batch: 20, Loss: 0.942554
Train - Epoch 15, Batch: 30, Loss: 0.943932
Train - Epoch 16, Batch: 0, Loss: 0.953180
Train - Epoch 16, Batch: 10, Loss: 0.942466
Train - Epoch 16, Batch: 20, Loss: 0.934639
Train - Epoch 16, Batch: 30, Loss: 0.947553
Train - Epoch 17, Batch: 0, Loss: 0.948577
Train - Epoch 17, Batch: 10, Loss: 0.943796
Train - Epoch 17, Batch: 20, Loss: 0.923532
Train - Epoch 17, Batch: 30, Loss: 0.936989
Train - Epoch 18, Batch: 0, Loss: 0.932434
Train - Epoch 18, Batch: 10, Loss: 0.931988
Train - Epoch 18, Batch: 20, Loss: 0.931713
Train - Epoch 18, Batch: 30, Loss: 0.934179
Train - Epoch 19, Batch: 0, Loss: 0.929727
Train - Epoch 19, Batch: 10, Loss: 0.921622
Train - Epoch 19, Batch: 20, Loss: 0.933078
Train - Epoch 19, Batch: 30, Loss: 0.921208
Train - Epoch 20, Batch: 0, Loss: 0.925805
Train - Epoch 20, Batch: 10, Loss: 0.921545
Train - Epoch 20, Batch: 20, Loss: 0.919304
Train - Epoch 20, Batch: 30, Loss: 0.921882
Train - Epoch 21, Batch: 0, Loss: 0.926614
Train - Epoch 21, Batch: 10, Loss: 0.918909
Train - Epoch 21, Batch: 20, Loss: 0.916157
Train - Epoch 21, Batch: 30, Loss: 0.915415
Train - Epoch 22, Batch: 0, Loss: 0.920424
Train - Epoch 22, Batch: 10, Loss: 0.923344
Train - Epoch 22, Batch: 20, Loss: 0.921340
Train - Epoch 22, Batch: 30, Loss: 0.914661
Train - Epoch 23, Batch: 0, Loss: 0.916940
Train - Epoch 23, Batch: 10, Loss: 0.914346
Train - Epoch 23, Batch: 20, Loss: 0.909982
Train - Epoch 23, Batch: 30, Loss: 0.918233
Train - Epoch 24, Batch: 0, Loss: 0.908996
Train - Epoch 24, Batch: 10, Loss: 0.919382
Train - Epoch 24, Batch: 20, Loss: 0.905987
Train - Epoch 24, Batch: 30, Loss: 0.909312
Train - Epoch 25, Batch: 0, Loss: 0.903915
Train - Epoch 25, Batch: 10, Loss: 0.916126
Train - Epoch 25, Batch: 20, Loss: 0.915213
Train - Epoch 25, Batch: 30, Loss: 0.918268
Train - Epoch 26, Batch: 0, Loss: 0.904990
Train - Epoch 26, Batch: 10, Loss: 0.905474
Train - Epoch 26, Batch: 20, Loss: 0.899156
Train - Epoch 26, Batch: 30, Loss: 0.908048
Train - Epoch 27, Batch: 0, Loss: 0.909231
Train - Epoch 27, Batch: 10, Loss: 0.900680
Train - Epoch 27, Batch: 20, Loss: 0.916423
Train - Epoch 27, Batch: 30, Loss: 0.896687
Train - Epoch 28, Batch: 0, Loss: 0.905976
Train - Epoch 28, Batch: 10, Loss: 0.902548
Train - Epoch 28, Batch: 20, Loss: 0.908889
Train - Epoch 28, Batch: 30, Loss: 0.903387
Train - Epoch 29, Batch: 0, Loss: 0.899828
Train - Epoch 29, Batch: 10, Loss: 0.892904
Train - Epoch 29, Batch: 20, Loss: 0.900201
Train - Epoch 29, Batch: 30, Loss: 0.898182
Train - Epoch 30, Batch: 0, Loss: 0.897262
Train - Epoch 30, Batch: 10, Loss: 0.899404
Train - Epoch 30, Batch: 20, Loss: 0.901964
Train - Epoch 30, Batch: 30, Loss: 0.889095
Train - Epoch 31, Batch: 0, Loss: 0.889805
Train - Epoch 31, Batch: 10, Loss: 0.891146
Train - Epoch 31, Batch: 20, Loss: 0.894050
Train - Epoch 31, Batch: 30, Loss: 0.881592
Test Avg. Loss: 0.000070, Accuracy: 0.628772
training_time:: 3.4836459159851074
training time full:: 3.4837238788604736
provenance prepare time:: 7.152557373046875e-06
Test Avg. Loss: 0.000070, Accuracy: 0.628772
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
batch_size:: 16384
epoch:: 0
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 3.256038188934326
overhead:: 0
overhead2:: 1.3918707370758057
overhead3:: 0
time_baseline:: 3.256760835647583
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628754
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.016263723373413086
overhead3:: 0.05055665969848633
overhead4:: 0.19928264617919922
overhead5:: 0
memory usage:: 3776315392
time_provenance:: 1.055967092514038
curr_diff: 0 tensor(5.6409e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.6409e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628720
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.018919706344604492
overhead3:: 0.057488203048706055
overhead4:: 0.23837041854858398
overhead5:: 0
memory usage:: 3772465152
time_provenance:: 1.189117193222046
curr_diff: 0 tensor(7.4840e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.4840e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628737
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.020270109176635742
overhead3:: 0.050710439682006836
overhead4:: 0.24942994117736816
overhead5:: 0
memory usage:: 3761672192
time_provenance:: 1.121227502822876
curr_diff: 0 tensor(5.8029e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.8029e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628720
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.021581411361694336
overhead3:: 0.06569790840148926
overhead4:: 0.2879202365875244
overhead5:: 0
memory usage:: 3778588672
time_provenance:: 1.2145867347717285
curr_diff: 0 tensor(7.6760e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.6760e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628737
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.026528120040893555
overhead3:: 0.06618618965148926
overhead4:: 0.3115072250366211
overhead5:: 0
memory usage:: 3766329344
time_provenance:: 1.1540822982788086
curr_diff: 0 tensor(5.7428e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.7428e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628720
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.028414487838745117
overhead3:: 0.07275772094726562
overhead4:: 0.3964240550994873
overhead5:: 0
memory usage:: 3764375552
time_provenance:: 1.3279564380645752
curr_diff: 0 tensor(3.7205e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.7205e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628754
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.03058600425720215
overhead3:: 0.06836128234863281
overhead4:: 0.422060489654541
overhead5:: 0
memory usage:: 3790381056
time_provenance:: 1.3184030055999756
curr_diff: 0 tensor(3.7369e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.7369e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628754
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.0313258171081543
overhead3:: 0.06977438926696777
overhead4:: 0.46688318252563477
overhead5:: 0
memory usage:: 3761655808
time_provenance:: 1.3555822372436523
curr_diff: 0 tensor(3.7388e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.7388e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628754
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.03312492370605469
overhead3:: 0.08542990684509277
overhead4:: 0.44400501251220703
overhead5:: 0
memory usage:: 3770843136
time_provenance:: 1.3365895748138428
curr_diff: 0 tensor(3.7377e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.7377e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628754
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.037164926528930664
overhead3:: 0.09504461288452148
overhead4:: 0.5175094604492188
overhead5:: 0
memory usage:: 3767480320
time_provenance:: 1.4613981246948242
curr_diff: 0 tensor(3.7201e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.7201e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628754
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.05134129524230957
overhead3:: 0.1011660099029541
overhead4:: 0.7611815929412842
overhead5:: 0
memory usage:: 3775639552
time_provenance:: 1.7034862041473389
curr_diff: 0 tensor(1.1465e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1465e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628754
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.0514674186706543
overhead3:: 0.11274123191833496
overhead4:: 0.7783777713775635
overhead5:: 0
memory usage:: 3790925824
time_provenance:: 1.7373273372650146
curr_diff: 0 tensor(1.1463e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1463e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628754
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.06077075004577637
overhead3:: 0.11403203010559082
overhead4:: 0.8067328929901123
overhead5:: 0
memory usage:: 3789508608
time_provenance:: 1.8029427528381348
curr_diff: 0 tensor(1.1625e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1625e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628754
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.059476375579833984
overhead3:: 0.11226892471313477
overhead4:: 0.7848420143127441
overhead5:: 0
memory usage:: 3790405632
time_provenance:: 1.7523529529571533
curr_diff: 0 tensor(1.1639e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1639e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628754
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.060125112533569336
overhead3:: 0.11469888687133789
overhead4:: 0.8744666576385498
overhead5:: 0
memory usage:: 3782819840
time_provenance:: 1.8361141681671143
curr_diff: 0 tensor(1.1687e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1687e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628754
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.10997247695922852
overhead3:: 0.2169947624206543
overhead4:: 1.6863596439361572
overhead5:: 0
memory usage:: 3770515456
time_provenance:: 2.82315993309021
curr_diff: 0 tensor(2.9125e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.9125e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628754
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.11502861976623535
overhead3:: 0.21907520294189453
overhead4:: 1.7483232021331787
overhead5:: 0
memory usage:: 3781570560
time_provenance:: 2.894601345062256
curr_diff: 0 tensor(2.9158e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.9158e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628754
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.14896130561828613
overhead3:: 0.2895474433898926
overhead4:: 2.3862411975860596
overhead5:: 0
memory usage:: 3776475136
time_provenance:: 3.866633892059326
curr_diff: 0 tensor(2.9584e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.9584e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628754
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.11310696601867676
overhead3:: 0.23185443878173828
overhead4:: 1.7838120460510254
overhead5:: 0
memory usage:: 3791486976
time_provenance:: 2.9303250312805176
curr_diff: 0 tensor(2.9658e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.9658e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628754
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.1541578769683838
overhead3:: 0.3312032222747803
overhead4:: 1.763359546661377
overhead5:: 0
memory usage:: 3798700032
time_provenance:: 3.039254903793335
curr_diff: 0 tensor(2.9613e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.9613e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628754
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.24632501602172852
overhead3:: 0.4768853187561035
overhead4:: 3.0637764930725098
overhead5:: 0
memory usage:: 3758235648
time_provenance:: 4.131169319152832
curr_diff: 0 tensor(9.8535e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.8535e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628754
repetition 3
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 3 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 2.044861
Train - Epoch 0, Batch: 10, Loss: 1.400114
Train - Epoch 0, Batch: 20, Loss: 1.245931
Train - Epoch 0, Batch: 30, Loss: 1.197101
Train - Epoch 1, Batch: 0, Loss: 1.177246
Train - Epoch 1, Batch: 10, Loss: 1.160472
Train - Epoch 1, Batch: 20, Loss: 1.132117
Train - Epoch 1, Batch: 30, Loss: 1.131046
Train - Epoch 2, Batch: 0, Loss: 1.134236
Train - Epoch 2, Batch: 10, Loss: 1.116154
Train - Epoch 2, Batch: 20, Loss: 1.106838
Train - Epoch 2, Batch: 30, Loss: 1.106098
Train - Epoch 3, Batch: 0, Loss: 1.105820
Train - Epoch 3, Batch: 10, Loss: 1.086737
Train - Epoch 3, Batch: 20, Loss: 1.078055
Train - Epoch 3, Batch: 30, Loss: 1.069079
Train - Epoch 4, Batch: 0, Loss: 1.069553
Train - Epoch 4, Batch: 10, Loss: 1.070994
Train - Epoch 4, Batch: 20, Loss: 1.056416
Train - Epoch 4, Batch: 30, Loss: 1.066537
Train - Epoch 5, Batch: 0, Loss: 1.055361
Train - Epoch 5, Batch: 10, Loss: 1.051402
Train - Epoch 5, Batch: 20, Loss: 1.052659
Train - Epoch 5, Batch: 30, Loss: 1.041536
Train - Epoch 6, Batch: 0, Loss: 1.029376
Train - Epoch 6, Batch: 10, Loss: 1.039077
Train - Epoch 6, Batch: 20, Loss: 1.028031
Train - Epoch 6, Batch: 30, Loss: 1.028180
Train - Epoch 7, Batch: 0, Loss: 1.024873
Train - Epoch 7, Batch: 10, Loss: 1.007385
Train - Epoch 7, Batch: 20, Loss: 1.002612
Train - Epoch 7, Batch: 30, Loss: 1.008495
Train - Epoch 8, Batch: 0, Loss: 1.007095
Train - Epoch 8, Batch: 10, Loss: 1.007845
Train - Epoch 8, Batch: 20, Loss: 1.011731
Train - Epoch 8, Batch: 30, Loss: 1.001460
Train - Epoch 9, Batch: 0, Loss: 1.003364
Train - Epoch 9, Batch: 10, Loss: 0.997654
Train - Epoch 9, Batch: 20, Loss: 1.003019
Train - Epoch 9, Batch: 30, Loss: 0.994143
Train - Epoch 10, Batch: 0, Loss: 0.994620
Train - Epoch 10, Batch: 10, Loss: 0.984721
Train - Epoch 10, Batch: 20, Loss: 0.984487
Train - Epoch 10, Batch: 30, Loss: 0.978983
Train - Epoch 11, Batch: 0, Loss: 0.969724
Train - Epoch 11, Batch: 10, Loss: 0.979260
Train - Epoch 11, Batch: 20, Loss: 0.973731
Train - Epoch 11, Batch: 30, Loss: 0.969590
Train - Epoch 12, Batch: 0, Loss: 0.971270
Train - Epoch 12, Batch: 10, Loss: 0.968343
Train - Epoch 12, Batch: 20, Loss: 0.960274
Train - Epoch 12, Batch: 30, Loss: 0.971078
Train - Epoch 13, Batch: 0, Loss: 0.972077
Train - Epoch 13, Batch: 10, Loss: 0.967965
Train - Epoch 13, Batch: 20, Loss: 0.964635
Train - Epoch 13, Batch: 30, Loss: 0.953892
Train - Epoch 14, Batch: 0, Loss: 0.967220
Train - Epoch 14, Batch: 10, Loss: 0.955955
Train - Epoch 14, Batch: 20, Loss: 0.961669
Train - Epoch 14, Batch: 30, Loss: 0.947232
Train - Epoch 15, Batch: 0, Loss: 0.960087
Train - Epoch 15, Batch: 10, Loss: 0.958258
Train - Epoch 15, Batch: 20, Loss: 0.945211
Train - Epoch 15, Batch: 30, Loss: 0.949465
Train - Epoch 16, Batch: 0, Loss: 0.942429
Train - Epoch 16, Batch: 10, Loss: 0.946840
Train - Epoch 16, Batch: 20, Loss: 0.955734
Train - Epoch 16, Batch: 30, Loss: 0.944317
Train - Epoch 17, Batch: 0, Loss: 0.947548
Train - Epoch 17, Batch: 10, Loss: 0.943654
Train - Epoch 17, Batch: 20, Loss: 0.934074
Train - Epoch 17, Batch: 30, Loss: 0.941291
Train - Epoch 18, Batch: 0, Loss: 0.930480
Train - Epoch 18, Batch: 10, Loss: 0.934145
Train - Epoch 18, Batch: 20, Loss: 0.933275
Train - Epoch 18, Batch: 30, Loss: 0.927440
Train - Epoch 19, Batch: 0, Loss: 0.928424
Train - Epoch 19, Batch: 10, Loss: 0.936605
Train - Epoch 19, Batch: 20, Loss: 0.929642
Train - Epoch 19, Batch: 30, Loss: 0.926394
Train - Epoch 20, Batch: 0, Loss: 0.918093
Train - Epoch 20, Batch: 10, Loss: 0.920184
Train - Epoch 20, Batch: 20, Loss: 0.925002
Train - Epoch 20, Batch: 30, Loss: 0.931524
Train - Epoch 21, Batch: 0, Loss: 0.923315
Train - Epoch 21, Batch: 10, Loss: 0.934286
Train - Epoch 21, Batch: 20, Loss: 0.931254
Train - Epoch 21, Batch: 30, Loss: 0.915729
Train - Epoch 22, Batch: 0, Loss: 0.924618
Train - Epoch 22, Batch: 10, Loss: 0.915959
Train - Epoch 22, Batch: 20, Loss: 0.929123
Train - Epoch 22, Batch: 30, Loss: 0.922626
Train - Epoch 23, Batch: 0, Loss: 0.916526
Train - Epoch 23, Batch: 10, Loss: 0.915093
Train - Epoch 23, Batch: 20, Loss: 0.925107
Train - Epoch 23, Batch: 30, Loss: 0.918373
Train - Epoch 24, Batch: 0, Loss: 0.919130
Train - Epoch 24, Batch: 10, Loss: 0.910443
Train - Epoch 24, Batch: 20, Loss: 0.908205
Train - Epoch 24, Batch: 30, Loss: 0.907625
Train - Epoch 25, Batch: 0, Loss: 0.915743
Train - Epoch 25, Batch: 10, Loss: 0.912608
Train - Epoch 25, Batch: 20, Loss: 0.906475
Train - Epoch 25, Batch: 30, Loss: 0.909072
Train - Epoch 26, Batch: 0, Loss: 0.913884
Train - Epoch 26, Batch: 10, Loss: 0.906722
Train - Epoch 26, Batch: 20, Loss: 0.901590
Train - Epoch 26, Batch: 30, Loss: 0.899016
Train - Epoch 27, Batch: 0, Loss: 0.906211
Train - Epoch 27, Batch: 10, Loss: 0.907243
Train - Epoch 27, Batch: 20, Loss: 0.911935
Train - Epoch 27, Batch: 30, Loss: 0.909755
Train - Epoch 28, Batch: 0, Loss: 0.903403
Train - Epoch 28, Batch: 10, Loss: 0.895478
Train - Epoch 28, Batch: 20, Loss: 0.908785
Train - Epoch 28, Batch: 30, Loss: 0.900204
Train - Epoch 29, Batch: 0, Loss: 0.897803
Train - Epoch 29, Batch: 10, Loss: 0.902733
Train - Epoch 29, Batch: 20, Loss: 0.898007
Train - Epoch 29, Batch: 30, Loss: 0.899823
Train - Epoch 30, Batch: 0, Loss: 0.916553
Train - Epoch 30, Batch: 10, Loss: 0.897437
Train - Epoch 30, Batch: 20, Loss: 0.900012
Train - Epoch 30, Batch: 30, Loss: 0.903892
Train - Epoch 31, Batch: 0, Loss: 0.897889
Train - Epoch 31, Batch: 10, Loss: 0.902299
Train - Epoch 31, Batch: 20, Loss: 0.895792
Train - Epoch 31, Batch: 30, Loss: 0.889662
Test Avg. Loss: 0.000071, Accuracy: 0.628823
training_time:: 3.5130372047424316
training time full:: 3.513104200363159
provenance prepare time:: 6.67572021484375e-06
Test Avg. Loss: 0.000071, Accuracy: 0.628823
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
batch_size:: 16384
epoch:: 0
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 3.301919937133789
overhead:: 0
overhead2:: 1.3919131755828857
overhead3:: 0
time_baseline:: 3.302656412124634
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.628840
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.01461482048034668
overhead3:: 0.042168378829956055
overhead4:: 0.19602441787719727
overhead5:: 0
memory usage:: 3764670464
time_provenance:: 1.0678973197937012
curr_diff: 0 tensor(5.1418e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.1418e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.628840
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.017075538635253906
overhead3:: 0.04610586166381836
overhead4:: 0.22874140739440918
overhead5:: 0
memory usage:: 3763654656
time_provenance:: 1.1460363864898682
curr_diff: 0 tensor(6.1940e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.1940e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.628840
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.023954153060913086
overhead3:: 0.056926727294921875
overhead4:: 0.250746488571167
overhead5:: 0
memory usage:: 3771912192
time_provenance:: 1.0956134796142578
curr_diff: 0 tensor(5.3576e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.3576e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.628840
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.025243043899536133
overhead3:: 0.06309795379638672
overhead4:: 0.29886293411254883
overhead5:: 0
memory usage:: 3833876480
time_provenance:: 1.1768510341644287
curr_diff: 0 tensor(6.3985e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.3985e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.628840
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.02362346649169922
overhead3:: 0.05443692207336426
overhead4:: 0.30844807624816895
overhead5:: 0
memory usage:: 3781644288
time_provenance:: 1.182934284210205
curr_diff: 0 tensor(5.4471e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.4471e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.628840
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.024929523468017578
overhead3:: 0.06172037124633789
overhead4:: 0.354722261428833
overhead5:: 0
memory usage:: 3764604928
time_provenance:: 1.2181429862976074
curr_diff: 0 tensor(3.1275e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.1275e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.628840
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.028493642807006836
overhead3:: 0.07254672050476074
overhead4:: 0.377551794052124
overhead5:: 0
memory usage:: 3761967104
time_provenance:: 1.265977144241333
curr_diff: 0 tensor(3.2245e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.2245e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.628840
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.038268089294433594
overhead3:: 0.07926154136657715
overhead4:: 0.4202756881713867
overhead5:: 0
memory usage:: 3763978240
time_provenance:: 1.3265807628631592
curr_diff: 0 tensor(3.2926e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.2926e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.628840
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.03236746788024902
overhead3:: 0.07934975624084473
overhead4:: 0.44922375679016113
overhead5:: 0
memory usage:: 3789037568
time_provenance:: 1.3558557033538818
curr_diff: 0 tensor(3.2938e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.2938e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.628840
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.033791303634643555
overhead3:: 0.08365559577941895
overhead4:: 0.45609092712402344
overhead5:: 0
memory usage:: 3764285440
time_provenance:: 1.3481495380401611
curr_diff: 0 tensor(3.3178e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.3178e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.628840
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.050795555114746094
overhead3:: 0.09721732139587402
overhead4:: 0.6750040054321289
overhead5:: 0
memory usage:: 3793948672
time_provenance:: 1.6144740581512451
curr_diff: 0 tensor(1.2708e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2708e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.628840
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.05040884017944336
overhead3:: 0.12538909912109375
overhead4:: 0.7678813934326172
overhead5:: 0
memory usage:: 3781926912
time_provenance:: 1.7236673831939697
curr_diff: 0 tensor(1.2871e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2871e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.628840
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.05532526969909668
overhead3:: 0.11499786376953125
overhead4:: 0.8030974864959717
overhead5:: 0
memory usage:: 3791056896
time_provenance:: 1.8015391826629639
curr_diff: 0 tensor(1.3235e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3235e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.628840
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.056860923767089844
overhead3:: 0.11283755302429199
overhead4:: 0.8496963977813721
overhead5:: 0
memory usage:: 3764281344
time_provenance:: 1.8720171451568604
curr_diff: 0 tensor(1.3318e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3318e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.628840
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.055298566818237305
overhead3:: 0.11075878143310547
overhead4:: 0.8134384155273438
overhead5:: 0
memory usage:: 3763400704
time_provenance:: 1.7567973136901855
curr_diff: 0 tensor(1.3396e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3396e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.628840
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.15331315994262695
overhead3:: 0.28161191940307617
overhead4:: 2.4388859272003174
overhead5:: 0
memory usage:: 3783929856
time_provenance:: 3.937440872192383
curr_diff: 0 tensor(2.9038e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.9038e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.628840
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.11333107948303223
overhead3:: 0.21666765213012695
overhead4:: 1.746856689453125
overhead5:: 0
memory usage:: 3770368000
time_provenance:: 2.886180877685547
curr_diff: 0 tensor(2.9627e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.9627e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.628840
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.13348007202148438
overhead3:: 0.27206897735595703
overhead4:: 1.816274642944336
overhead5:: 0
memory usage:: 3779735552
time_provenance:: 3.0743284225463867
curr_diff: 0 tensor(3.0257e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.0257e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.628840
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.12321591377258301
overhead3:: 0.22822880744934082
overhead4:: 1.6504480838775635
overhead5:: 0
memory usage:: 3790065664
time_provenance:: 2.806162118911743
curr_diff: 0 tensor(3.0226e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.0226e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.628840
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.12661266326904297
overhead3:: 0.2540106773376465
overhead4:: 1.823909044265747
overhead5:: 0
memory usage:: 3782561792
time_provenance:: 2.9931812286376953
curr_diff: 0 tensor(3.0365e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.0365e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.628840
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.2429964542388916
overhead3:: 0.44857287406921387
overhead4:: 3.014549970626831
overhead5:: 0
memory usage:: 3765788672
time_provenance:: 4.03810453414917
curr_diff: 0 tensor(9.8607e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.8607e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.628840
repetition 4
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 4 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 1.978958
Train - Epoch 0, Batch: 10, Loss: 1.392522
Train - Epoch 0, Batch: 20, Loss: 1.246411
Train - Epoch 0, Batch: 30, Loss: 1.192327
Train - Epoch 1, Batch: 0, Loss: 1.185012
Train - Epoch 1, Batch: 10, Loss: 1.171186
Train - Epoch 1, Batch: 20, Loss: 1.146344
Train - Epoch 1, Batch: 30, Loss: 1.138916
Train - Epoch 2, Batch: 0, Loss: 1.146772
Train - Epoch 2, Batch: 10, Loss: 1.128839
Train - Epoch 2, Batch: 20, Loss: 1.116946
Train - Epoch 2, Batch: 30, Loss: 1.111926
Train - Epoch 3, Batch: 0, Loss: 1.096268
Train - Epoch 3, Batch: 10, Loss: 1.089813
Train - Epoch 3, Batch: 20, Loss: 1.089435
Train - Epoch 3, Batch: 30, Loss: 1.087034
Train - Epoch 4, Batch: 0, Loss: 1.091341
Train - Epoch 4, Batch: 10, Loss: 1.072025
Train - Epoch 4, Batch: 20, Loss: 1.060185
Train - Epoch 4, Batch: 30, Loss: 1.056468
Train - Epoch 5, Batch: 0, Loss: 1.056015
Train - Epoch 5, Batch: 10, Loss: 1.048093
Train - Epoch 5, Batch: 20, Loss: 1.055606
Train - Epoch 5, Batch: 30, Loss: 1.039860
Train - Epoch 6, Batch: 0, Loss: 1.039834
Train - Epoch 6, Batch: 10, Loss: 1.038957
Train - Epoch 6, Batch: 20, Loss: 1.031306
Train - Epoch 6, Batch: 30, Loss: 1.028738
Train - Epoch 7, Batch: 0, Loss: 1.024735
Train - Epoch 7, Batch: 10, Loss: 1.019143
Train - Epoch 7, Batch: 20, Loss: 1.017063
Train - Epoch 7, Batch: 30, Loss: 1.006316
Train - Epoch 8, Batch: 0, Loss: 1.012005
Train - Epoch 8, Batch: 10, Loss: 1.008770
Train - Epoch 8, Batch: 20, Loss: 1.005006
Train - Epoch 8, Batch: 30, Loss: 1.001224
Train - Epoch 9, Batch: 0, Loss: 0.995851
Train - Epoch 9, Batch: 10, Loss: 0.993473
Train - Epoch 9, Batch: 20, Loss: 0.996259
Train - Epoch 9, Batch: 30, Loss: 0.990378
Train - Epoch 10, Batch: 0, Loss: 1.002046
Train - Epoch 10, Batch: 10, Loss: 0.989929
Train - Epoch 10, Batch: 20, Loss: 0.984301
Train - Epoch 10, Batch: 30, Loss: 0.987182
Train - Epoch 11, Batch: 0, Loss: 0.980106
Train - Epoch 11, Batch: 10, Loss: 0.972172
Train - Epoch 11, Batch: 20, Loss: 0.981656
Train - Epoch 11, Batch: 30, Loss: 0.977333
Train - Epoch 12, Batch: 0, Loss: 0.982078
Train - Epoch 12, Batch: 10, Loss: 0.976175
Train - Epoch 12, Batch: 20, Loss: 0.972123
Train - Epoch 12, Batch: 30, Loss: 0.960673
Train - Epoch 13, Batch: 0, Loss: 0.966049
Train - Epoch 13, Batch: 10, Loss: 0.961338
Train - Epoch 13, Batch: 20, Loss: 0.962252
Train - Epoch 13, Batch: 30, Loss: 0.963751
Train - Epoch 14, Batch: 0, Loss: 0.964517
Train - Epoch 14, Batch: 10, Loss: 0.958391
Train - Epoch 14, Batch: 20, Loss: 0.957475
Train - Epoch 14, Batch: 30, Loss: 0.952874
Train - Epoch 15, Batch: 0, Loss: 0.958112
Train - Epoch 15, Batch: 10, Loss: 0.956197
Train - Epoch 15, Batch: 20, Loss: 0.951046
Train - Epoch 15, Batch: 30, Loss: 0.951294
Train - Epoch 16, Batch: 0, Loss: 0.941049
Train - Epoch 16, Batch: 10, Loss: 0.941872
Train - Epoch 16, Batch: 20, Loss: 0.952862
Train - Epoch 16, Batch: 30, Loss: 0.950209
Train - Epoch 17, Batch: 0, Loss: 0.943600
Train - Epoch 17, Batch: 10, Loss: 0.946390
Train - Epoch 17, Batch: 20, Loss: 0.948748
Train - Epoch 17, Batch: 30, Loss: 0.940865
Train - Epoch 18, Batch: 0, Loss: 0.941616
Train - Epoch 18, Batch: 10, Loss: 0.938091
Train - Epoch 18, Batch: 20, Loss: 0.933590
Train - Epoch 18, Batch: 30, Loss: 0.938090
Train - Epoch 19, Batch: 0, Loss: 0.932967
Train - Epoch 19, Batch: 10, Loss: 0.940590
Train - Epoch 19, Batch: 20, Loss: 0.925222
Train - Epoch 19, Batch: 30, Loss: 0.925425
Train - Epoch 20, Batch: 0, Loss: 0.932940
Train - Epoch 20, Batch: 10, Loss: 0.922387
Train - Epoch 20, Batch: 20, Loss: 0.926498
Train - Epoch 20, Batch: 30, Loss: 0.917025
Train - Epoch 21, Batch: 0, Loss: 0.929662
Train - Epoch 21, Batch: 10, Loss: 0.934565
Train - Epoch 21, Batch: 20, Loss: 0.925256
Train - Epoch 21, Batch: 30, Loss: 0.918213
Train - Epoch 22, Batch: 0, Loss: 0.920369
Train - Epoch 22, Batch: 10, Loss: 0.923151
Train - Epoch 22, Batch: 20, Loss: 0.914341
Train - Epoch 22, Batch: 30, Loss: 0.922998
Train - Epoch 23, Batch: 0, Loss: 0.914763
Train - Epoch 23, Batch: 10, Loss: 0.919559
Train - Epoch 23, Batch: 20, Loss: 0.918857
Train - Epoch 23, Batch: 30, Loss: 0.912845
Train - Epoch 24, Batch: 0, Loss: 0.927643
Train - Epoch 24, Batch: 10, Loss: 0.901204
Train - Epoch 24, Batch: 20, Loss: 0.914335
Train - Epoch 24, Batch: 30, Loss: 0.912082
Train - Epoch 25, Batch: 0, Loss: 0.905896
Train - Epoch 25, Batch: 10, Loss: 0.900918
Train - Epoch 25, Batch: 20, Loss: 0.911392
Train - Epoch 25, Batch: 30, Loss: 0.914347
Train - Epoch 26, Batch: 0, Loss: 0.898672
Train - Epoch 26, Batch: 10, Loss: 0.917362
Train - Epoch 26, Batch: 20, Loss: 0.905748
Train - Epoch 26, Batch: 30, Loss: 0.909734
Train - Epoch 27, Batch: 0, Loss: 0.897283
Train - Epoch 27, Batch: 10, Loss: 0.902411
Train - Epoch 27, Batch: 20, Loss: 0.904441
Train - Epoch 27, Batch: 30, Loss: 0.909662
Train - Epoch 28, Batch: 0, Loss: 0.902445
Train - Epoch 28, Batch: 10, Loss: 0.901551
Train - Epoch 28, Batch: 20, Loss: 0.901998
Train - Epoch 28, Batch: 30, Loss: 0.904459
Train - Epoch 29, Batch: 0, Loss: 0.903606
Train - Epoch 29, Batch: 10, Loss: 0.905819
Train - Epoch 29, Batch: 20, Loss: 0.907130
Train - Epoch 29, Batch: 30, Loss: 0.899510
Train - Epoch 30, Batch: 0, Loss: 0.897203
Train - Epoch 30, Batch: 10, Loss: 0.899423
Train - Epoch 30, Batch: 20, Loss: 0.907218
Train - Epoch 30, Batch: 30, Loss: 0.906567
Train - Epoch 31, Batch: 0, Loss: 0.896695
Train - Epoch 31, Batch: 10, Loss: 0.892616
Train - Epoch 31, Batch: 20, Loss: 0.895630
Train - Epoch 31, Batch: 30, Loss: 0.901727
Test Avg. Loss: 0.000071, Accuracy: 0.629529
training_time:: 3.5186822414398193
training time full:: 3.5187435150146484
provenance prepare time:: 7.62939453125e-06
Test Avg. Loss: 0.000071, Accuracy: 0.629529
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
batch_size:: 16384
epoch:: 0
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 3.483933448791504
overhead:: 0
overhead2:: 1.369511604309082
overhead3:: 0
time_baseline:: 3.4847309589385986
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.629529
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.014937162399291992
overhead3:: 0.04528045654296875
overhead4:: 0.20008563995361328
overhead5:: 0
memory usage:: 3770114048
time_provenance:: 1.0828709602355957
curr_diff: 0 tensor(3.7846e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.7846e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.629529
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.023012161254882812
overhead3:: 0.06410765647888184
overhead4:: 0.24420571327209473
overhead5:: 0
memory usage:: 3791777792
time_provenance:: 1.1443359851837158
curr_diff: 0 tensor(7.3001e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.3001e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.629546
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.01947021484375
overhead3:: 0.05849432945251465
overhead4:: 0.26003050804138184
overhead5:: 0
memory usage:: 3779112960
time_provenance:: 1.135575294494629
curr_diff: 0 tensor(3.9339e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.9339e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.629529
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.022326946258544922
overhead3:: 0.06670928001403809
overhead4:: 0.30263590812683105
overhead5:: 0
memory usage:: 3767844864
time_provenance:: 1.2148849964141846
curr_diff: 0 tensor(7.5588e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.5588e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.629546
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.022289037704467773
overhead3:: 0.06817317008972168
overhead4:: 0.3097829818725586
overhead5:: 0
memory usage:: 3771281408
time_provenance:: 1.1478829383850098
curr_diff: 0 tensor(4.3733e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.3733e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.629529
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.0257413387298584
overhead3:: 0.07152748107910156
overhead4:: 0.34811925888061523
overhead5:: 0
memory usage:: 3790004224
time_provenance:: 1.1972708702087402
curr_diff: 0 tensor(2.9042e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.9042e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.629546
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.02916240692138672
overhead3:: 0.06668949127197266
overhead4:: 0.3821370601654053
overhead5:: 0
memory usage:: 3764559872
time_provenance:: 1.259514570236206
curr_diff: 0 tensor(2.9560e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.9560e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.629546
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.03429222106933594
overhead3:: 0.06983113288879395
overhead4:: 0.4566059112548828
overhead5:: 0
memory usage:: 3769413632
time_provenance:: 1.3297786712646484
curr_diff: 0 tensor(2.9262e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.9262e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.629546
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.03244614601135254
overhead3:: 0.06952714920043945
overhead4:: 0.42701053619384766
overhead5:: 0
memory usage:: 3765600256
time_provenance:: 1.3005938529968262
curr_diff: 0 tensor(3.1227e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.1227e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.629546
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.03266644477844238
overhead3:: 0.07992339134216309
overhead4:: 0.4487009048461914
overhead5:: 0
memory usage:: 3792961536
time_provenance:: 1.297832727432251
curr_diff: 0 tensor(3.1350e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.1350e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.629546
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.04711794853210449
overhead3:: 0.09692502021789551
overhead4:: 0.7252216339111328
overhead5:: 0
memory usage:: 3775016960
time_provenance:: 1.6573593616485596
curr_diff: 0 tensor(9.6593e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.6593e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.629529
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.048775434494018555
overhead3:: 0.10046935081481934
overhead4:: 0.7574822902679443
overhead5:: 0
memory usage:: 3762896896
time_provenance:: 1.708589792251587
curr_diff: 0 tensor(9.8439e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.8439e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.629529
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.05384683609008789
overhead3:: 0.10563874244689941
overhead4:: 0.7442829608917236
overhead5:: 0
memory usage:: 3766603776
time_provenance:: 1.7077243328094482
curr_diff: 0 tensor(9.6725e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.6725e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.629529
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.06066465377807617
overhead3:: 0.13315629959106445
overhead4:: 0.8420960903167725
overhead5:: 0
memory usage:: 3782463488
time_provenance:: 1.9105305671691895
curr_diff: 0 tensor(1.0484e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0484e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.629529
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.05585741996765137
overhead3:: 0.10471892356872559
overhead4:: 0.744962215423584
overhead5:: 0
memory usage:: 3764473856
time_provenance:: 1.6642982959747314
curr_diff: 0 tensor(1.0494e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0494e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.629529
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.11779546737670898
overhead3:: 0.21845746040344238
overhead4:: 1.7515919208526611
overhead5:: 0
memory usage:: 3762884608
time_provenance:: 2.9027633666992188
curr_diff: 0 tensor(2.7700e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.7700e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.629529
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.11214184761047363
overhead3:: 0.22810673713684082
overhead4:: 1.7350316047668457
overhead5:: 0
memory usage:: 3765538816
time_provenance:: 2.889460802078247
curr_diff: 0 tensor(2.7652e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.7652e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.629529
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.14207029342651367
overhead3:: 0.2602534294128418
overhead4:: 2.164638042449951
overhead5:: 0
memory usage:: 3761864704
time_provenance:: 3.522653818130493
curr_diff: 0 tensor(2.7439e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.7439e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.629529
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.11403918266296387
overhead3:: 0.22246766090393066
overhead4:: 1.7103707790374756
overhead5:: 0
memory usage:: 3814350848
time_provenance:: 2.838834762573242
curr_diff: 0 tensor(2.9504e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.9504e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.629529
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.11546683311462402
overhead3:: 0.22697210311889648
overhead4:: 1.7823374271392822
overhead5:: 0
memory usage:: 3807117312
time_provenance:: 2.914515972137451
curr_diff: 0 tensor(2.9382e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.9382e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.629529
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 52
max_epoch:: 32
overhead:: 0
overhead2:: 0.27581071853637695
overhead3:: 0.5286891460418701
overhead4:: 3.1832540035247803
overhead5:: 0
memory usage:: 3790872576
time_provenance:: 4.302364826202393
curr_diff: 0 tensor(9.9561e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.9561e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.629529
deletion rate:: 0.0002
python3 generate_rand_ids 0.0002  covtype 0
tensor([ 84483,  12035, 270083,   7430, 149255, 498696,  35345,  65041,  45331,
         33300, 207894,  12824, 301339, 158239, 102183, 509736, 260149, 292405,
        170805,  29496, 163897, 162872, 169272,  80957, 483390,  49983, 469568,
        431936, 388422, 171591,  74313, 207954, 375125, 241749, 137048, 171104,
        328545,  84328,   2665, 445546,  85102,   3955, 379510, 127608, 277884,
        137087, 274562, 480388, 307341, 312723, 131990, 233113, 298139, 203675,
         74397, 507038, 236447,  91551, 185504, 157088, 162466,  32934,  23210,
        147370, 144558, 141747, 393909, 103094, 457404, 134079, 255167,    961,
         51651, 126661, 266441, 456650, 479180, 117964, 474319, 177362, 307925,
        156118, 261337,  26844, 491743, 168415, 477156, 150247,  72681, 279019,
        303340,  55534, 260590, 505584, 460017, 301554, 355570, 364534, 363256,
        333306, 136699, 349692, 493053, 300286])
batch size:: 16384
repetition 0
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 0 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 1.950978
Train - Epoch 0, Batch: 10, Loss: 1.391974
Train - Epoch 0, Batch: 20, Loss: 1.255119
Train - Epoch 0, Batch: 30, Loss: 1.208506
Train - Epoch 1, Batch: 0, Loss: 1.187740
Train - Epoch 1, Batch: 10, Loss: 1.171430
Train - Epoch 1, Batch: 20, Loss: 1.144648
Train - Epoch 1, Batch: 30, Loss: 1.134368
Train - Epoch 2, Batch: 0, Loss: 1.146396
Train - Epoch 2, Batch: 10, Loss: 1.114195
Train - Epoch 2, Batch: 20, Loss: 1.126798
Train - Epoch 2, Batch: 30, Loss: 1.103861
Train - Epoch 3, Batch: 0, Loss: 1.106887
Train - Epoch 3, Batch: 10, Loss: 1.096294
Train - Epoch 3, Batch: 20, Loss: 1.093842
Train - Epoch 3, Batch: 30, Loss: 1.075551
Train - Epoch 4, Batch: 0, Loss: 1.083127
Train - Epoch 4, Batch: 10, Loss: 1.077934
Train - Epoch 4, Batch: 20, Loss: 1.070757
Train - Epoch 4, Batch: 30, Loss: 1.056279
Train - Epoch 5, Batch: 0, Loss: 1.068931
Train - Epoch 5, Batch: 10, Loss: 1.051786
Train - Epoch 5, Batch: 20, Loss: 1.051703
Train - Epoch 5, Batch: 30, Loss: 1.039826
Train - Epoch 6, Batch: 0, Loss: 1.042313
Train - Epoch 6, Batch: 10, Loss: 1.030365
Train - Epoch 6, Batch: 20, Loss: 1.026350
Train - Epoch 6, Batch: 30, Loss: 1.023248
Train - Epoch 7, Batch: 0, Loss: 1.026779
Train - Epoch 7, Batch: 10, Loss: 1.023347
Train - Epoch 7, Batch: 20, Loss: 1.027690
Train - Epoch 7, Batch: 30, Loss: 1.011839
Train - Epoch 8, Batch: 0, Loss: 1.015145
Train - Epoch 8, Batch: 10, Loss: 1.006215
Train - Epoch 8, Batch: 20, Loss: 1.016132
Train - Epoch 8, Batch: 30, Loss: 1.000163
Train - Epoch 9, Batch: 0, Loss: 1.005753
Train - Epoch 9, Batch: 10, Loss: 1.010593
Train - Epoch 9, Batch: 20, Loss: 1.002009
Train - Epoch 9, Batch: 30, Loss: 0.990679
Train - Epoch 10, Batch: 0, Loss: 0.984947
Train - Epoch 10, Batch: 10, Loss: 0.984586
Train - Epoch 10, Batch: 20, Loss: 1.000928
Train - Epoch 10, Batch: 30, Loss: 0.999589
Train - Epoch 11, Batch: 0, Loss: 0.979162
Train - Epoch 11, Batch: 10, Loss: 0.987081
Train - Epoch 11, Batch: 20, Loss: 0.978488
Train - Epoch 11, Batch: 30, Loss: 0.978914
Train - Epoch 12, Batch: 0, Loss: 0.967497
Train - Epoch 12, Batch: 10, Loss: 0.966911
Train - Epoch 12, Batch: 20, Loss: 0.969845
Train - Epoch 12, Batch: 30, Loss: 0.968938
Train - Epoch 13, Batch: 0, Loss: 0.963353
Train - Epoch 13, Batch: 10, Loss: 0.976454
Train - Epoch 13, Batch: 20, Loss: 0.963753
Train - Epoch 13, Batch: 30, Loss: 0.966778
Train - Epoch 14, Batch: 0, Loss: 0.963363
Train - Epoch 14, Batch: 10, Loss: 0.969439
Train - Epoch 14, Batch: 20, Loss: 0.952991
Train - Epoch 14, Batch: 30, Loss: 0.958553
Train - Epoch 15, Batch: 0, Loss: 0.957357
Train - Epoch 15, Batch: 10, Loss: 0.952898
Train - Epoch 15, Batch: 20, Loss: 0.947666
Train - Epoch 15, Batch: 30, Loss: 0.959085
Train - Epoch 16, Batch: 0, Loss: 0.945817
Train - Epoch 16, Batch: 10, Loss: 0.951549
Train - Epoch 16, Batch: 20, Loss: 0.950041
Train - Epoch 16, Batch: 30, Loss: 0.943072
Train - Epoch 17, Batch: 0, Loss: 0.951613
Train - Epoch 17, Batch: 10, Loss: 0.936468
Train - Epoch 17, Batch: 20, Loss: 0.946029
Train - Epoch 17, Batch: 30, Loss: 0.940080
Train - Epoch 18, Batch: 0, Loss: 0.933956
Train - Epoch 18, Batch: 10, Loss: 0.939966
Train - Epoch 18, Batch: 20, Loss: 0.940103
Train - Epoch 18, Batch: 30, Loss: 0.935234
Train - Epoch 19, Batch: 0, Loss: 0.940444
Train - Epoch 19, Batch: 10, Loss: 0.934675
Train - Epoch 19, Batch: 20, Loss: 0.929208
Train - Epoch 19, Batch: 30, Loss: 0.934718
Train - Epoch 20, Batch: 0, Loss: 0.930241
Train - Epoch 20, Batch: 10, Loss: 0.936837
Train - Epoch 20, Batch: 20, Loss: 0.924425
Train - Epoch 20, Batch: 30, Loss: 0.927491
Train - Epoch 21, Batch: 0, Loss: 0.925410
Train - Epoch 21, Batch: 10, Loss: 0.929966
Train - Epoch 21, Batch: 20, Loss: 0.925648
Train - Epoch 21, Batch: 30, Loss: 0.929601
Train - Epoch 22, Batch: 0, Loss: 0.925930
Train - Epoch 22, Batch: 10, Loss: 0.921737
Train - Epoch 22, Batch: 20, Loss: 0.929655
Train - Epoch 22, Batch: 30, Loss: 0.925440
Train - Epoch 23, Batch: 0, Loss: 0.916960
Train - Epoch 23, Batch: 10, Loss: 0.927861
Train - Epoch 23, Batch: 20, Loss: 0.924742
Train - Epoch 23, Batch: 30, Loss: 0.913082
Train - Epoch 24, Batch: 0, Loss: 0.919734
Train - Epoch 24, Batch: 10, Loss: 0.914344
Train - Epoch 24, Batch: 20, Loss: 0.920272
Train - Epoch 24, Batch: 30, Loss: 0.921885
Train - Epoch 25, Batch: 0, Loss: 0.916525
Train - Epoch 25, Batch: 10, Loss: 0.914612
Train - Epoch 25, Batch: 20, Loss: 0.903589
Train - Epoch 25, Batch: 30, Loss: 0.917496
Train - Epoch 26, Batch: 0, Loss: 0.914671
Train - Epoch 26, Batch: 10, Loss: 0.908673
Train - Epoch 26, Batch: 20, Loss: 0.918413
Train - Epoch 26, Batch: 30, Loss: 0.909563
Train - Epoch 27, Batch: 0, Loss: 0.900875
Train - Epoch 27, Batch: 10, Loss: 0.916596
Train - Epoch 27, Batch: 20, Loss: 0.904789
Train - Epoch 27, Batch: 30, Loss: 0.905210
Train - Epoch 28, Batch: 0, Loss: 0.899405
Train - Epoch 28, Batch: 10, Loss: 0.901385
Train - Epoch 28, Batch: 20, Loss: 0.900536
Train - Epoch 28, Batch: 30, Loss: 0.904625
Train - Epoch 29, Batch: 0, Loss: 0.903832
Train - Epoch 29, Batch: 10, Loss: 0.905757
Train - Epoch 29, Batch: 20, Loss: 0.902137
Train - Epoch 29, Batch: 30, Loss: 0.896567
Train - Epoch 30, Batch: 0, Loss: 0.890609
Train - Epoch 30, Batch: 10, Loss: 0.896209
Train - Epoch 30, Batch: 20, Loss: 0.892300
Train - Epoch 30, Batch: 30, Loss: 0.904812
Train - Epoch 31, Batch: 0, Loss: 0.888345
Train - Epoch 31, Batch: 10, Loss: 0.902855
Train - Epoch 31, Batch: 20, Loss: 0.889332
Train - Epoch 31, Batch: 30, Loss: 0.894466
Test Avg. Loss: 0.000071, Accuracy: 0.628634
training_time:: 3.4616281986236572
training time full:: 3.461693048477173
provenance prepare time:: 7.3909759521484375e-06
Test Avg. Loss: 0.000071, Accuracy: 0.628634
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
batch_size:: 16384
epoch:: 0
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 3.770477294921875
overhead:: 0
overhead2:: 1.6983287334442139
overhead3:: 0
time_baseline:: 3.7714033126831055
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.628668
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.016865015029907227
overhead3:: 0.04387331008911133
overhead4:: 0.20648598670959473
overhead5:: 0
memory usage:: 3766280192
time_provenance:: 1.1274998188018799
curr_diff: 0 tensor(8.3563e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.3563e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.628651
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.019812583923339844
overhead3:: 0.0547182559967041
overhead4:: 0.24482250213623047
overhead5:: 0
memory usage:: 3770253312
time_provenance:: 1.1539342403411865
curr_diff: 0 tensor(9.6490e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.6490e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.628668
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.021627426147460938
overhead3:: 0.0525209903717041
overhead4:: 0.26749348640441895
overhead5:: 0
memory usage:: 3765882880
time_provenance:: 1.1647329330444336
curr_diff: 0 tensor(8.6283e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.6283e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.628651
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.025805234909057617
overhead3:: 0.05744123458862305
overhead4:: 0.30355286598205566
overhead5:: 0
memory usage:: 3802791936
time_provenance:: 1.2366421222686768
curr_diff: 0 tensor(9.8500e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.8500e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.628668
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.027055740356445312
overhead3:: 0.05617403984069824
overhead4:: 0.3263382911682129
overhead5:: 0
memory usage:: 3766362112
time_provenance:: 1.289738416671753
curr_diff: 0 tensor(8.7293e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.7293e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.628651
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.03090214729309082
overhead3:: 0.07597947120666504
overhead4:: 0.373598575592041
overhead5:: 0
memory usage:: 3767242752
time_provenance:: 1.3062543869018555
curr_diff: 0 tensor(3.2633e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.2633e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.628668
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.045175790786743164
overhead3:: 0.06994271278381348
overhead4:: 0.4200625419616699
overhead5:: 0
memory usage:: 3770314752
time_provenance:: 1.4032673835754395
curr_diff: 0 tensor(3.3152e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.3152e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.628668
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.03728795051574707
overhead3:: 0.06944537162780762
overhead4:: 0.4647529125213623
overhead5:: 0
memory usage:: 3769839616
time_provenance:: 1.4379966259002686
curr_diff: 0 tensor(3.3464e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.3464e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.628668
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.041791439056396484
overhead3:: 0.09729194641113281
overhead4:: 0.47468113899230957
overhead5:: 0
memory usage:: 3765145600
time_provenance:: 1.5000205039978027
curr_diff: 0 tensor(3.3497e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.3497e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.628668
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.040421485900878906
overhead3:: 0.07630586624145508
overhead4:: 0.5154623985290527
overhead5:: 0
memory usage:: 3786608640
time_provenance:: 1.5272400379180908
curr_diff: 0 tensor(3.3549e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.3549e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.628668
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.056224822998046875
overhead3:: 0.09847307205200195
overhead4:: 0.8074989318847656
overhead5:: 0
memory usage:: 3812556800
time_provenance:: 1.8116278648376465
curr_diff: 0 tensor(1.4183e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4183e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.628668
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.061397552490234375
overhead3:: 0.10905027389526367
overhead4:: 0.8082261085510254
overhead5:: 0
memory usage:: 3801214976
time_provenance:: 1.856086015701294
curr_diff: 0 tensor(1.4279e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4279e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.628668
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.0627751350402832
overhead3:: 0.10664629936218262
overhead4:: 0.8390629291534424
overhead5:: 0
memory usage:: 3772293120
time_provenance:: 1.8722882270812988
curr_diff: 0 tensor(1.4488e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4488e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.628668
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.06314277648925781
overhead3:: 0.10777497291564941
overhead4:: 0.8568394184112549
overhead5:: 0
memory usage:: 3775475712
time_provenance:: 1.8752415180206299
curr_diff: 0 tensor(1.4458e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4458e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.628668
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.07366538047790527
overhead3:: 0.12451481819152832
overhead4:: 0.9246361255645752
overhead5:: 0
memory usage:: 3781922816
time_provenance:: 2.048435688018799
curr_diff: 0 tensor(1.4485e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4485e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.628668
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.14657902717590332
overhead3:: 0.21979832649230957
overhead4:: 1.8443171977996826
overhead5:: 0
memory usage:: 3774320640
time_provenance:: 3.042520523071289
curr_diff: 0 tensor(3.0038e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.0038e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.628668
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.13677668571472168
overhead3:: 0.2198953628540039
overhead4:: 1.9117131233215332
overhead5:: 0
memory usage:: 3764858880
time_provenance:: 3.1081790924072266
curr_diff: 0 tensor(3.0628e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.0628e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.628668
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.14822864532470703
overhead3:: 0.22567200660705566
overhead4:: 1.8523595333099365
overhead5:: 0
memory usage:: 3767205888
time_provenance:: 3.0586209297180176
curr_diff: 0 tensor(3.0738e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.0738e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.628668
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.1875293254852295
overhead3:: 0.29279136657714844
overhead4:: 2.6517069339752197
overhead5:: 0
memory usage:: 3772379136
time_provenance:: 4.210302352905273
curr_diff: 0 tensor(3.1012e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.1012e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.628668
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.14617180824279785
overhead3:: 0.23632192611694336
overhead4:: 1.9380862712860107
overhead5:: 0
memory usage:: 3775426560
time_provenance:: 3.145940065383911
curr_diff: 0 tensor(3.1398e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.1398e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.628668
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 0 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.2882063388824463
overhead3:: 0.44965291023254395
overhead4:: 3.2564361095428467
overhead5:: 0
memory usage:: 3766935552
time_provenance:: 4.391141891479492
curr_diff: 0 tensor(9.8149e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.8149e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.628668
repetition 1
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 1 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 2.002556
Train - Epoch 0, Batch: 10, Loss: 1.414632
Train - Epoch 0, Batch: 20, Loss: 1.253909
Train - Epoch 0, Batch: 30, Loss: 1.206828
Train - Epoch 1, Batch: 0, Loss: 1.188375
Train - Epoch 1, Batch: 10, Loss: 1.157754
Train - Epoch 1, Batch: 20, Loss: 1.158823
Train - Epoch 1, Batch: 30, Loss: 1.132590
Train - Epoch 2, Batch: 0, Loss: 1.126874
Train - Epoch 2, Batch: 10, Loss: 1.130026
Train - Epoch 2, Batch: 20, Loss: 1.117025
Train - Epoch 2, Batch: 30, Loss: 1.104366
Train - Epoch 3, Batch: 0, Loss: 1.116945
Train - Epoch 3, Batch: 10, Loss: 1.093898
Train - Epoch 3, Batch: 20, Loss: 1.088504
Train - Epoch 3, Batch: 30, Loss: 1.080347
Train - Epoch 4, Batch: 0, Loss: 1.087077
Train - Epoch 4, Batch: 10, Loss: 1.071904
Train - Epoch 4, Batch: 20, Loss: 1.064992
Train - Epoch 4, Batch: 30, Loss: 1.058866
Train - Epoch 5, Batch: 0, Loss: 1.050208
Train - Epoch 5, Batch: 10, Loss: 1.051064
Train - Epoch 5, Batch: 20, Loss: 1.046669
Train - Epoch 5, Batch: 30, Loss: 1.044131
Train - Epoch 6, Batch: 0, Loss: 1.051762
Train - Epoch 6, Batch: 10, Loss: 1.035113
Train - Epoch 6, Batch: 20, Loss: 1.034092
Train - Epoch 6, Batch: 30, Loss: 1.027239
Train - Epoch 7, Batch: 0, Loss: 1.023070
Train - Epoch 7, Batch: 10, Loss: 1.016043
Train - Epoch 7, Batch: 20, Loss: 1.020322
Train - Epoch 7, Batch: 30, Loss: 1.006705
Train - Epoch 8, Batch: 0, Loss: 1.014686
Train - Epoch 8, Batch: 10, Loss: 1.000916
Train - Epoch 8, Batch: 20, Loss: 1.013829
Train - Epoch 8, Batch: 30, Loss: 0.994452
Train - Epoch 9, Batch: 0, Loss: 0.995565
Train - Epoch 9, Batch: 10, Loss: 0.995177
Train - Epoch 9, Batch: 20, Loss: 0.986002
Train - Epoch 9, Batch: 30, Loss: 0.993885
Train - Epoch 10, Batch: 0, Loss: 0.992859
Train - Epoch 10, Batch: 10, Loss: 0.983275
Train - Epoch 10, Batch: 20, Loss: 0.981468
Train - Epoch 10, Batch: 30, Loss: 0.972747
Train - Epoch 11, Batch: 0, Loss: 0.981719
Train - Epoch 11, Batch: 10, Loss: 0.981040
Train - Epoch 11, Batch: 20, Loss: 0.977439
Train - Epoch 11, Batch: 30, Loss: 0.966859
Train - Epoch 12, Batch: 0, Loss: 0.980792
Train - Epoch 12, Batch: 10, Loss: 0.966433
Train - Epoch 12, Batch: 20, Loss: 0.965679
Train - Epoch 12, Batch: 30, Loss: 0.955660
Train - Epoch 13, Batch: 0, Loss: 0.962608
Train - Epoch 13, Batch: 10, Loss: 0.965357
Train - Epoch 13, Batch: 20, Loss: 0.969251
Train - Epoch 13, Batch: 30, Loss: 0.961362
Train - Epoch 14, Batch: 0, Loss: 0.954235
Train - Epoch 14, Batch: 10, Loss: 0.957922
Train - Epoch 14, Batch: 20, Loss: 0.960800
Train - Epoch 14, Batch: 30, Loss: 0.958535
Train - Epoch 15, Batch: 0, Loss: 0.953640
Train - Epoch 15, Batch: 10, Loss: 0.955636
Train - Epoch 15, Batch: 20, Loss: 0.948845
Train - Epoch 15, Batch: 30, Loss: 0.943836
Train - Epoch 16, Batch: 0, Loss: 0.950964
Train - Epoch 16, Batch: 10, Loss: 0.948557
Train - Epoch 16, Batch: 20, Loss: 0.946550
Train - Epoch 16, Batch: 30, Loss: 0.940797
Train - Epoch 17, Batch: 0, Loss: 0.941580
Train - Epoch 17, Batch: 10, Loss: 0.940112
Train - Epoch 17, Batch: 20, Loss: 0.941666
Train - Epoch 17, Batch: 30, Loss: 0.938842
Train - Epoch 18, Batch: 0, Loss: 0.938511
Train - Epoch 18, Batch: 10, Loss: 0.942285
Train - Epoch 18, Batch: 20, Loss: 0.921889
Train - Epoch 18, Batch: 30, Loss: 0.932741
Train - Epoch 19, Batch: 0, Loss: 0.937443
Train - Epoch 19, Batch: 10, Loss: 0.929590
Train - Epoch 19, Batch: 20, Loss: 0.920713
Train - Epoch 19, Batch: 30, Loss: 0.922216
Train - Epoch 20, Batch: 0, Loss: 0.930692
Train - Epoch 20, Batch: 10, Loss: 0.925025
Train - Epoch 20, Batch: 20, Loss: 0.932671
Train - Epoch 20, Batch: 30, Loss: 0.918002
Train - Epoch 21, Batch: 0, Loss: 0.925882
Train - Epoch 21, Batch: 10, Loss: 0.922353
Train - Epoch 21, Batch: 20, Loss: 0.922998
Train - Epoch 21, Batch: 30, Loss: 0.924186
Train - Epoch 22, Batch: 0, Loss: 0.913375
Train - Epoch 22, Batch: 10, Loss: 0.922592
Train - Epoch 22, Batch: 20, Loss: 0.916056
Train - Epoch 22, Batch: 30, Loss: 0.922475
Train - Epoch 23, Batch: 0, Loss: 0.926599
Train - Epoch 23, Batch: 10, Loss: 0.920203
Train - Epoch 23, Batch: 20, Loss: 0.921533
Train - Epoch 23, Batch: 30, Loss: 0.902364
Train - Epoch 24, Batch: 0, Loss: 0.912406
Train - Epoch 24, Batch: 10, Loss: 0.907680
Train - Epoch 24, Batch: 20, Loss: 0.912375
Train - Epoch 24, Batch: 30, Loss: 0.913644
Train - Epoch 25, Batch: 0, Loss: 0.912665
Train - Epoch 25, Batch: 10, Loss: 0.919513
Train - Epoch 25, Batch: 20, Loss: 0.913304
Train - Epoch 25, Batch: 30, Loss: 0.907788
Train - Epoch 26, Batch: 0, Loss: 0.914315
Train - Epoch 26, Batch: 10, Loss: 0.901848
Train - Epoch 26, Batch: 20, Loss: 0.909463
Train - Epoch 26, Batch: 30, Loss: 0.905967
Train - Epoch 27, Batch: 0, Loss: 0.907606
Train - Epoch 27, Batch: 10, Loss: 0.908671
Train - Epoch 27, Batch: 20, Loss: 0.902752
Train - Epoch 27, Batch: 30, Loss: 0.899154
Train - Epoch 28, Batch: 0, Loss: 0.909429
Train - Epoch 28, Batch: 10, Loss: 0.894544
Train - Epoch 28, Batch: 20, Loss: 0.904878
Train - Epoch 28, Batch: 30, Loss: 0.895356
Train - Epoch 29, Batch: 0, Loss: 0.902022
Train - Epoch 29, Batch: 10, Loss: 0.918827
Train - Epoch 29, Batch: 20, Loss: 0.892037
Train - Epoch 29, Batch: 30, Loss: 0.892178
Train - Epoch 30, Batch: 0, Loss: 0.902752
Train - Epoch 30, Batch: 10, Loss: 0.891649
Train - Epoch 30, Batch: 20, Loss: 0.890498
Train - Epoch 30, Batch: 30, Loss: 0.894418
Train - Epoch 31, Batch: 0, Loss: 0.888830
Train - Epoch 31, Batch: 10, Loss: 0.895090
Train - Epoch 31, Batch: 20, Loss: 0.904993
Train - Epoch 31, Batch: 30, Loss: 0.892634
Test Avg. Loss: 0.000070, Accuracy: 0.630372
training_time:: 3.3751704692840576
training time full:: 3.3752336502075195
provenance prepare time:: 6.9141387939453125e-06
Test Avg. Loss: 0.000070, Accuracy: 0.630372
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
batch_size:: 16384
epoch:: 0
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 3.7541422843933105
overhead:: 0
overhead2:: 1.6743180751800537
overhead3:: 0
time_baseline:: 3.755110740661621
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630338
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.02013230323791504
overhead3:: 0.04786515235900879
overhead4:: 0.2475275993347168
overhead5:: 0
memory usage:: 3762552832
time_provenance:: 1.2947442531585693
curr_diff: 0 tensor(9.1237e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.1237e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630338
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.02027606964111328
overhead3:: 0.04440784454345703
overhead4:: 0.2470686435699463
overhead5:: 0
memory usage:: 3764867072
time_provenance:: 1.2073814868927002
curr_diff: 0 tensor(8.3716e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.3716e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630338
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.02225971221923828
overhead3:: 0.05167555809020996
overhead4:: 0.26889920234680176
overhead5:: 0
memory usage:: 3770339328
time_provenance:: 1.197192668914795
curr_diff: 0 tensor(8.5208e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.5208e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630321
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.025006532669067383
overhead3:: 0.05307292938232422
overhead4:: 0.30558013916015625
overhead5:: 0
memory usage:: 3771678720
time_provenance:: 1.267453908920288
curr_diff: 0 tensor(7.5795e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.5795e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630338
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.026117563247680664
overhead3:: 0.05615687370300293
overhead4:: 0.3197479248046875
overhead5:: 0
memory usage:: 3775410176
time_provenance:: 1.2213144302368164
curr_diff: 0 tensor(9.2824e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.2824e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630321
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.04195094108581543
overhead3:: 0.06726598739624023
overhead4:: 0.37741756439208984
overhead5:: 0
memory usage:: 3770241024
time_provenance:: 1.332991600036621
curr_diff: 0 tensor(3.6003e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.6003e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630338
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.03244614601135254
overhead3:: 0.06424903869628906
overhead4:: 0.41512441635131836
overhead5:: 0
memory usage:: 3798982656
time_provenance:: 1.3736841678619385
curr_diff: 0 tensor(3.6550e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.6550e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630338
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.03315424919128418
overhead3:: 0.07836294174194336
overhead4:: 0.45477890968322754
overhead5:: 0
memory usage:: 3766894592
time_provenance:: 1.374678134918213
curr_diff: 0 tensor(3.7615e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.7615e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630338
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.03617143630981445
overhead3:: 0.08859777450561523
overhead4:: 0.4534893035888672
overhead5:: 0
memory usage:: 3774873600
time_provenance:: 1.3932888507843018
curr_diff: 0 tensor(3.8207e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.8207e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630338
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.04334449768066406
overhead3:: 0.07518959045410156
overhead4:: 0.4879302978515625
overhead5:: 0
memory usage:: 3767291904
time_provenance:: 1.4843573570251465
curr_diff: 0 tensor(3.9063e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.9063e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630338
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.05739402770996094
overhead3:: 0.10407042503356934
overhead4:: 0.7286524772644043
overhead5:: 0
memory usage:: 3784892416
time_provenance:: 1.7167277336120605
curr_diff: 0 tensor(1.5767e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5767e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630338
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.06846117973327637
overhead3:: 0.1134941577911377
overhead4:: 0.9391837120056152
overhead5:: 0
memory usage:: 3773538304
time_provenance:: 2.106889486312866
curr_diff: 0 tensor(1.5826e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.5826e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630338
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.06378698348999023
overhead3:: 0.11083006858825684
overhead4:: 0.8216383457183838
overhead5:: 0
memory usage:: 3766636544
time_provenance:: 1.8533828258514404
curr_diff: 0 tensor(1.6031e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6031e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630338
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.06153750419616699
overhead3:: 0.10708212852478027
overhead4:: 0.8067469596862793
overhead5:: 0
memory usage:: 3782623232
time_provenance:: 1.77592134475708
curr_diff: 0 tensor(1.6338e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6338e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630338
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.06789088249206543
overhead3:: 0.11428976058959961
overhead4:: 0.9306628704071045
overhead5:: 0
memory usage:: 3783524352
time_provenance:: 2.0027191638946533
curr_diff: 0 tensor(1.6505e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6505e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630338
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.14153051376342773
overhead3:: 0.22518467903137207
overhead4:: 1.8671009540557861
overhead5:: 0
memory usage:: 3761541120
time_provenance:: 3.0929176807403564
curr_diff: 0 tensor(3.9683e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.9683e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630355
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.18244552612304688
overhead3:: 0.2841982841491699
overhead4:: 2.541513442993164
overhead5:: 0
memory usage:: 3798331392
time_provenance:: 4.053087472915649
curr_diff: 0 tensor(4.0098e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.0098e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630355
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.17510056495666504
overhead3:: 0.278393030166626
overhead4:: 2.4576313495635986
overhead5:: 0
memory usage:: 3774545920
time_provenance:: 3.921539545059204
curr_diff: 0 tensor(4.0238e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.0238e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630355
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.1898481845855713
overhead3:: 0.31557583808898926
overhead4:: 2.153871774673462
overhead5:: 0
memory usage:: 3789971456
time_provenance:: 3.6281678676605225
curr_diff: 0 tensor(4.0875e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.0875e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630355
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.18997716903686523
overhead3:: 0.28871726989746094
overhead4:: 2.6941094398498535
overhead5:: 0
memory usage:: 3762749440
time_provenance:: 4.2409021854400635
curr_diff: 0 tensor(4.1335e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.1335e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630355
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 1 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.28940486907958984
overhead3:: 0.4578680992126465
overhead4:: 3.306579828262329
overhead5:: 0
memory usage:: 3755712512
time_provenance:: 4.448299407958984
curr_diff: 0 tensor(9.9632e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.9632e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630338
repetition 2
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 2 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 1.861612
Train - Epoch 0, Batch: 10, Loss: 1.361933
Train - Epoch 0, Batch: 20, Loss: 1.238830
Train - Epoch 0, Batch: 30, Loss: 1.182771
Train - Epoch 1, Batch: 0, Loss: 1.171445
Train - Epoch 1, Batch: 10, Loss: 1.148211
Train - Epoch 1, Batch: 20, Loss: 1.133960
Train - Epoch 1, Batch: 30, Loss: 1.121675
Train - Epoch 2, Batch: 0, Loss: 1.122034
Train - Epoch 2, Batch: 10, Loss: 1.112072
Train - Epoch 2, Batch: 20, Loss: 1.090068
Train - Epoch 2, Batch: 30, Loss: 1.105913
Train - Epoch 3, Batch: 0, Loss: 1.091060
Train - Epoch 3, Batch: 10, Loss: 1.092103
Train - Epoch 3, Batch: 20, Loss: 1.072488
Train - Epoch 3, Batch: 30, Loss: 1.077469
Train - Epoch 4, Batch: 0, Loss: 1.067657
Train - Epoch 4, Batch: 10, Loss: 1.052963
Train - Epoch 4, Batch: 20, Loss: 1.052542
Train - Epoch 4, Batch: 30, Loss: 1.042634
Train - Epoch 5, Batch: 0, Loss: 1.046121
Train - Epoch 5, Batch: 10, Loss: 1.047003
Train - Epoch 5, Batch: 20, Loss: 1.030642
Train - Epoch 5, Batch: 30, Loss: 1.033643
Train - Epoch 6, Batch: 0, Loss: 1.033914
Train - Epoch 6, Batch: 10, Loss: 1.018077
Train - Epoch 6, Batch: 20, Loss: 1.026646
Train - Epoch 6, Batch: 30, Loss: 1.010745
Train - Epoch 7, Batch: 0, Loss: 1.010736
Train - Epoch 7, Batch: 10, Loss: 1.018765
Train - Epoch 7, Batch: 20, Loss: 1.008822
Train - Epoch 7, Batch: 30, Loss: 1.008433
Train - Epoch 8, Batch: 0, Loss: 0.998983
Train - Epoch 8, Batch: 10, Loss: 0.999798
Train - Epoch 8, Batch: 20, Loss: 0.997240
Train - Epoch 8, Batch: 30, Loss: 0.996862
Train - Epoch 9, Batch: 0, Loss: 0.993675
Train - Epoch 9, Batch: 10, Loss: 0.993631
Train - Epoch 9, Batch: 20, Loss: 0.994041
Train - Epoch 9, Batch: 30, Loss: 0.979629
Train - Epoch 10, Batch: 0, Loss: 0.985207
Train - Epoch 10, Batch: 10, Loss: 0.982306
Train - Epoch 10, Batch: 20, Loss: 0.968783
Train - Epoch 10, Batch: 30, Loss: 0.981421
Train - Epoch 11, Batch: 0, Loss: 0.976467
Train - Epoch 11, Batch: 10, Loss: 0.966798
Train - Epoch 11, Batch: 20, Loss: 0.969999
Train - Epoch 11, Batch: 30, Loss: 0.966753
Train - Epoch 12, Batch: 0, Loss: 0.966059
Train - Epoch 12, Batch: 10, Loss: 0.971437
Train - Epoch 12, Batch: 20, Loss: 0.961703
Train - Epoch 12, Batch: 30, Loss: 0.969375
Train - Epoch 13, Batch: 0, Loss: 0.963552
Train - Epoch 13, Batch: 10, Loss: 0.952068
Train - Epoch 13, Batch: 20, Loss: 0.955875
Train - Epoch 13, Batch: 30, Loss: 0.969441
Train - Epoch 14, Batch: 0, Loss: 0.953170
Train - Epoch 14, Batch: 10, Loss: 0.960473
Train - Epoch 14, Batch: 20, Loss: 0.953127
Train - Epoch 14, Batch: 30, Loss: 0.957572
Train - Epoch 15, Batch: 0, Loss: 0.951899
Train - Epoch 15, Batch: 10, Loss: 0.944720
Train - Epoch 15, Batch: 20, Loss: 0.939847
Train - Epoch 15, Batch: 30, Loss: 0.941044
Train - Epoch 16, Batch: 0, Loss: 0.950752
Train - Epoch 16, Batch: 10, Loss: 0.939491
Train - Epoch 16, Batch: 20, Loss: 0.932143
Train - Epoch 16, Batch: 30, Loss: 0.944640
Train - Epoch 17, Batch: 0, Loss: 0.946509
Train - Epoch 17, Batch: 10, Loss: 0.941365
Train - Epoch 17, Batch: 20, Loss: 0.921200
Train - Epoch 17, Batch: 30, Loss: 0.934557
Train - Epoch 18, Batch: 0, Loss: 0.929808
Train - Epoch 18, Batch: 10, Loss: 0.929551
Train - Epoch 18, Batch: 20, Loss: 0.929994
Train - Epoch 18, Batch: 30, Loss: 0.931699
Train - Epoch 19, Batch: 0, Loss: 0.927509
Train - Epoch 19, Batch: 10, Loss: 0.919498
Train - Epoch 19, Batch: 20, Loss: 0.931325
Train - Epoch 19, Batch: 30, Loss: 0.919700
Train - Epoch 20, Batch: 0, Loss: 0.924018
Train - Epoch 20, Batch: 10, Loss: 0.919726
Train - Epoch 20, Batch: 20, Loss: 0.917105
Train - Epoch 20, Batch: 30, Loss: 0.920304
Train - Epoch 21, Batch: 0, Loss: 0.924529
Train - Epoch 21, Batch: 10, Loss: 0.916998
Train - Epoch 21, Batch: 20, Loss: 0.914096
Train - Epoch 21, Batch: 30, Loss: 0.913759
Train - Epoch 22, Batch: 0, Loss: 0.918461
Train - Epoch 22, Batch: 10, Loss: 0.922110
Train - Epoch 22, Batch: 20, Loss: 0.919627
Train - Epoch 22, Batch: 30, Loss: 0.913131
Train - Epoch 23, Batch: 0, Loss: 0.915234
Train - Epoch 23, Batch: 10, Loss: 0.912701
Train - Epoch 23, Batch: 20, Loss: 0.907952
Train - Epoch 23, Batch: 30, Loss: 0.916936
Train - Epoch 24, Batch: 0, Loss: 0.907692
Train - Epoch 24, Batch: 10, Loss: 0.917746
Train - Epoch 24, Batch: 20, Loss: 0.904632
Train - Epoch 24, Batch: 30, Loss: 0.907720
Train - Epoch 25, Batch: 0, Loss: 0.902720
Train - Epoch 25, Batch: 10, Loss: 0.915063
Train - Epoch 25, Batch: 20, Loss: 0.914231
Train - Epoch 25, Batch: 30, Loss: 0.916997
Train - Epoch 26, Batch: 0, Loss: 0.903913
Train - Epoch 26, Batch: 10, Loss: 0.904412
Train - Epoch 26, Batch: 20, Loss: 0.898147
Train - Epoch 26, Batch: 30, Loss: 0.907328
Train - Epoch 27, Batch: 0, Loss: 0.907772
Train - Epoch 27, Batch: 10, Loss: 0.899487
Train - Epoch 27, Batch: 20, Loss: 0.915825
Train - Epoch 27, Batch: 30, Loss: 0.895625
Train - Epoch 28, Batch: 0, Loss: 0.904559
Train - Epoch 28, Batch: 10, Loss: 0.901817
Train - Epoch 28, Batch: 20, Loss: 0.907686
Train - Epoch 28, Batch: 30, Loss: 0.902312
Train - Epoch 29, Batch: 0, Loss: 0.898653
Train - Epoch 29, Batch: 10, Loss: 0.891454
Train - Epoch 29, Batch: 20, Loss: 0.899088
Train - Epoch 29, Batch: 30, Loss: 0.896924
Train - Epoch 30, Batch: 0, Loss: 0.896383
Train - Epoch 30, Batch: 10, Loss: 0.897964
Train - Epoch 30, Batch: 20, Loss: 0.901156
Train - Epoch 30, Batch: 30, Loss: 0.888221
Train - Epoch 31, Batch: 0, Loss: 0.888950
Train - Epoch 31, Batch: 10, Loss: 0.890090
Train - Epoch 31, Batch: 20, Loss: 0.893404
Train - Epoch 31, Batch: 30, Loss: 0.880824
Test Avg. Loss: 0.000070, Accuracy: 0.630269
training_time:: 3.5461699962615967
training time full:: 3.546232223510742
provenance prepare time:: 9.059906005859375e-06
Test Avg. Loss: 0.000070, Accuracy: 0.630269
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
batch_size:: 16384
epoch:: 0
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 3.592289924621582
overhead:: 0
overhead2:: 1.7088422775268555
overhead3:: 0
time_baseline:: 3.593074321746826
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630269
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.016952037811279297
overhead3:: 0.0437619686126709
overhead4:: 0.20719432830810547
overhead5:: 0
memory usage:: 3761848320
time_provenance:: 1.1158370971679688
curr_diff: 0 tensor(7.6427e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.6427e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630269
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.020467758178710938
overhead3:: 0.05279421806335449
overhead4:: 0.24785208702087402
overhead5:: 0
memory usage:: 3769114624
time_provenance:: 1.1784825325012207
curr_diff: 0 tensor(6.9628e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.9628e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630269
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.021691083908081055
overhead3:: 0.04880857467651367
overhead4:: 0.26955437660217285
overhead5:: 0
memory usage:: 3764420608
time_provenance:: 1.1819329261779785
curr_diff: 0 tensor(7.9928e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.9928e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630269
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.025693893432617188
overhead3:: 0.05360865592956543
overhead4:: 0.30889225006103516
overhead5:: 0
memory usage:: 3788910592
time_provenance:: 1.293365240097046
curr_diff: 0 tensor(7.1055e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.1055e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630269
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.02877497673034668
overhead3:: 0.059816837310791016
overhead4:: 0.32918810844421387
overhead5:: 0
memory usage:: 3800694784
time_provenance:: 1.2503623962402344
curr_diff: 0 tensor(8.1120e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.1120e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630269
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.03145313262939453
overhead3:: 0.07875227928161621
overhead4:: 0.3972296714782715
overhead5:: 0
memory usage:: 3764744192
time_provenance:: 1.3625552654266357
curr_diff: 0 tensor(3.6246e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.6246e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630269
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.03634810447692871
overhead3:: 0.06751608848571777
overhead4:: 0.4520854949951172
overhead5:: 0
memory usage:: 3762843648
time_provenance:: 1.4311985969543457
curr_diff: 0 tensor(3.6475e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.6475e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630269
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.03612804412841797
overhead3:: 0.07983016967773438
overhead4:: 0.48418688774108887
overhead5:: 0
memory usage:: 3769520128
time_provenance:: 1.4873359203338623
curr_diff: 0 tensor(3.6725e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.6725e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630269
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.041901350021362305
overhead3:: 0.09216928482055664
overhead4:: 0.5099625587463379
overhead5:: 0
memory usage:: 3789893632
time_provenance:: 1.5774874687194824
curr_diff: 0 tensor(3.7264e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.7264e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630269
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.04269528388977051
overhead3:: 0.07583165168762207
overhead4:: 0.4987051486968994
overhead5:: 0
memory usage:: 3764203520
time_provenance:: 1.4926092624664307
curr_diff: 0 tensor(3.7279e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.7279e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630269
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.06013798713684082
overhead3:: 0.10538005828857422
overhead4:: 0.8387706279754639
overhead5:: 0
memory usage:: 3764387840
time_provenance:: 1.901510238647461
curr_diff: 0 tensor(1.6774e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6774e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630269
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.05635499954223633
overhead3:: 0.0985879898071289
overhead4:: 0.7566537857055664
overhead5:: 0
memory usage:: 3789918208
time_provenance:: 1.712233066558838
curr_diff: 0 tensor(1.6988e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6988e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630269
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.06575822830200195
overhead3:: 0.11099553108215332
overhead4:: 0.8408613204956055
overhead5:: 0
memory usage:: 3770056704
time_provenance:: 1.8894529342651367
curr_diff: 0 tensor(1.7101e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.7101e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630269
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.06170797348022461
overhead3:: 0.10617733001708984
overhead4:: 0.8651318550109863
overhead5:: 0
memory usage:: 3790028800
time_provenance:: 1.8787875175476074
curr_diff: 0 tensor(1.7272e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.7272e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630269
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.06719088554382324
overhead3:: 0.12427163124084473
overhead4:: 0.8365092277526855
overhead5:: 0
memory usage:: 3786420224
time_provenance:: 1.8642237186431885
curr_diff: 0 tensor(1.7382e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.7382e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630269
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.14355182647705078
overhead3:: 0.23707127571105957
overhead4:: 1.8664977550506592
overhead5:: 0
memory usage:: 3766468608
time_provenance:: 3.084078311920166
curr_diff: 0 tensor(3.3209e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.3209e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630269
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.2045726776123047
overhead3:: 0.29274964332580566
overhead4:: 2.656804323196411
overhead5:: 0
memory usage:: 3769663488
time_provenance:: 4.254213333129883
curr_diff: 0 tensor(3.3312e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.3312e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630269
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.1544628143310547
overhead3:: 0.24038386344909668
overhead4:: 1.8652660846710205
overhead5:: 0
memory usage:: 3774197760
time_provenance:: 3.129819393157959
curr_diff: 0 tensor(3.3675e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.3675e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630269
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.17670631408691406
overhead3:: 0.27643251419067383
overhead4:: 2.4995853900909424
overhead5:: 0
memory usage:: 3790438400
time_provenance:: 3.9633748531341553
curr_diff: 0 tensor(3.4231e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.4231e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630269
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.19127559661865234
overhead3:: 0.295271635055542
overhead4:: 2.6533963680267334
overhead5:: 0
memory usage:: 3763716096
time_provenance:: 4.209577798843384
curr_diff: 0 tensor(3.4317e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.4317e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630269
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 2 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.3206295967102051
overhead3:: 0.522899866104126
overhead4:: 3.479006052017212
overhead5:: 0
memory usage:: 3772076032
time_provenance:: 4.680332899093628
curr_diff: 0 tensor(1.0310e-13, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0310e-13, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630269
repetition 3
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 3 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 1.886814
Train - Epoch 0, Batch: 10, Loss: 1.375339
Train - Epoch 0, Batch: 20, Loss: 1.247059
Train - Epoch 0, Batch: 30, Loss: 1.202766
Train - Epoch 1, Batch: 0, Loss: 1.183584
Train - Epoch 1, Batch: 10, Loss: 1.167919
Train - Epoch 1, Batch: 20, Loss: 1.138605
Train - Epoch 1, Batch: 30, Loss: 1.136246
Train - Epoch 2, Batch: 0, Loss: 1.139457
Train - Epoch 2, Batch: 10, Loss: 1.121728
Train - Epoch 2, Batch: 20, Loss: 1.111635
Train - Epoch 2, Batch: 30, Loss: 1.110901
Train - Epoch 3, Batch: 0, Loss: 1.109783
Train - Epoch 3, Batch: 10, Loss: 1.090198
Train - Epoch 3, Batch: 20, Loss: 1.082060
Train - Epoch 3, Batch: 30, Loss: 1.072630
Train - Epoch 4, Batch: 0, Loss: 1.072574
Train - Epoch 4, Batch: 10, Loss: 1.074486
Train - Epoch 4, Batch: 20, Loss: 1.058709
Train - Epoch 4, Batch: 30, Loss: 1.069486
Train - Epoch 5, Batch: 0, Loss: 1.057553
Train - Epoch 5, Batch: 10, Loss: 1.053689
Train - Epoch 5, Batch: 20, Loss: 1.054847
Train - Epoch 5, Batch: 30, Loss: 1.044412
Train - Epoch 6, Batch: 0, Loss: 1.032150
Train - Epoch 6, Batch: 10, Loss: 1.041358
Train - Epoch 6, Batch: 20, Loss: 1.030419
Train - Epoch 6, Batch: 30, Loss: 1.030413
Train - Epoch 7, Batch: 0, Loss: 1.026893
Train - Epoch 7, Batch: 10, Loss: 1.010095
Train - Epoch 7, Batch: 20, Loss: 1.004598
Train - Epoch 7, Batch: 30, Loss: 1.010379
Train - Epoch 8, Batch: 0, Loss: 1.008567
Train - Epoch 8, Batch: 10, Loss: 1.010196
Train - Epoch 8, Batch: 20, Loss: 1.013897
Train - Epoch 8, Batch: 30, Loss: 1.003018
Train - Epoch 9, Batch: 0, Loss: 1.005217
Train - Epoch 9, Batch: 10, Loss: 0.999067
Train - Epoch 9, Batch: 20, Loss: 1.004883
Train - Epoch 9, Batch: 30, Loss: 0.995599
Train - Epoch 10, Batch: 0, Loss: 0.995956
Train - Epoch 10, Batch: 10, Loss: 0.986962
Train - Epoch 10, Batch: 20, Loss: 0.986231
Train - Epoch 10, Batch: 30, Loss: 0.980615
Train - Epoch 11, Batch: 0, Loss: 0.971629
Train - Epoch 11, Batch: 10, Loss: 0.980345
Train - Epoch 11, Batch: 20, Loss: 0.975664
Train - Epoch 11, Batch: 30, Loss: 0.970767
Train - Epoch 12, Batch: 0, Loss: 0.972449
Train - Epoch 12, Batch: 10, Loss: 0.970173
Train - Epoch 12, Batch: 20, Loss: 0.961791
Train - Epoch 12, Batch: 30, Loss: 0.972093
Train - Epoch 13, Batch: 0, Loss: 0.973161
Train - Epoch 13, Batch: 10, Loss: 0.968875
Train - Epoch 13, Batch: 20, Loss: 0.965938
Train - Epoch 13, Batch: 30, Loss: 0.955391
Train - Epoch 14, Batch: 0, Loss: 0.968227
Train - Epoch 14, Batch: 10, Loss: 0.957118
Train - Epoch 14, Batch: 20, Loss: 0.962522
Train - Epoch 14, Batch: 30, Loss: 0.948386
Train - Epoch 15, Batch: 0, Loss: 0.961386
Train - Epoch 15, Batch: 10, Loss: 0.959370
Train - Epoch 15, Batch: 20, Loss: 0.946137
Train - Epoch 15, Batch: 30, Loss: 0.950367
Train - Epoch 16, Batch: 0, Loss: 0.943501
Train - Epoch 16, Batch: 10, Loss: 0.947802
Train - Epoch 16, Batch: 20, Loss: 0.956956
Train - Epoch 16, Batch: 30, Loss: 0.945633
Train - Epoch 17, Batch: 0, Loss: 0.948410
Train - Epoch 17, Batch: 10, Loss: 0.944655
Train - Epoch 17, Batch: 20, Loss: 0.934414
Train - Epoch 17, Batch: 30, Loss: 0.942648
Train - Epoch 18, Batch: 0, Loss: 0.931728
Train - Epoch 18, Batch: 10, Loss: 0.934997
Train - Epoch 18, Batch: 20, Loss: 0.934024
Train - Epoch 18, Batch: 30, Loss: 0.928295
Train - Epoch 19, Batch: 0, Loss: 0.929417
Train - Epoch 19, Batch: 10, Loss: 0.937529
Train - Epoch 19, Batch: 20, Loss: 0.930601
Train - Epoch 19, Batch: 30, Loss: 0.927159
Train - Epoch 20, Batch: 0, Loss: 0.918631
Train - Epoch 20, Batch: 10, Loss: 0.921152
Train - Epoch 20, Batch: 20, Loss: 0.925493
Train - Epoch 20, Batch: 30, Loss: 0.932157
Train - Epoch 21, Batch: 0, Loss: 0.923594
Train - Epoch 21, Batch: 10, Loss: 0.934879
Train - Epoch 21, Batch: 20, Loss: 0.932051
Train - Epoch 21, Batch: 30, Loss: 0.916105
Train - Epoch 22, Batch: 0, Loss: 0.924797
Train - Epoch 22, Batch: 10, Loss: 0.916722
Train - Epoch 22, Batch: 20, Loss: 0.929695
Train - Epoch 22, Batch: 30, Loss: 0.923041
Train - Epoch 23, Batch: 0, Loss: 0.917571
Train - Epoch 23, Batch: 10, Loss: 0.915308
Train - Epoch 23, Batch: 20, Loss: 0.925883
Train - Epoch 23, Batch: 30, Loss: 0.918910
Train - Epoch 24, Batch: 0, Loss: 0.919584
Train - Epoch 24, Batch: 10, Loss: 0.910630
Train - Epoch 24, Batch: 20, Loss: 0.908678
Train - Epoch 24, Batch: 30, Loss: 0.907694
Train - Epoch 25, Batch: 0, Loss: 0.916621
Train - Epoch 25, Batch: 10, Loss: 0.912842
Train - Epoch 25, Batch: 20, Loss: 0.906701
Train - Epoch 25, Batch: 30, Loss: 0.909877
Train - Epoch 26, Batch: 0, Loss: 0.914332
Train - Epoch 26, Batch: 10, Loss: 0.907478
Train - Epoch 26, Batch: 20, Loss: 0.902249
Train - Epoch 26, Batch: 30, Loss: 0.899757
Train - Epoch 27, Batch: 0, Loss: 0.906348
Train - Epoch 27, Batch: 10, Loss: 0.907912
Train - Epoch 27, Batch: 20, Loss: 0.912608
Train - Epoch 27, Batch: 30, Loss: 0.910221
Train - Epoch 28, Batch: 0, Loss: 0.903952
Train - Epoch 28, Batch: 10, Loss: 0.895777
Train - Epoch 28, Batch: 20, Loss: 0.908808
Train - Epoch 28, Batch: 30, Loss: 0.900319
Train - Epoch 29, Batch: 0, Loss: 0.897975
Train - Epoch 29, Batch: 10, Loss: 0.903053
Train - Epoch 29, Batch: 20, Loss: 0.898258
Train - Epoch 29, Batch: 30, Loss: 0.899814
Train - Epoch 30, Batch: 0, Loss: 0.916659
Train - Epoch 30, Batch: 10, Loss: 0.897889
Train - Epoch 30, Batch: 20, Loss: 0.900391
Train - Epoch 30, Batch: 30, Loss: 0.904299
Train - Epoch 31, Batch: 0, Loss: 0.897844
Train - Epoch 31, Batch: 10, Loss: 0.902410
Train - Epoch 31, Batch: 20, Loss: 0.896081
Train - Epoch 31, Batch: 30, Loss: 0.889644
Test Avg. Loss: 0.000071, Accuracy: 0.629288
training_time:: 3.4649338722229004
training time full:: 3.4649980068206787
provenance prepare time:: 6.4373016357421875e-06
Test Avg. Loss: 0.000071, Accuracy: 0.629288
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
batch_size:: 16384
epoch:: 0
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 3.5564229488372803
overhead:: 0
overhead2:: 1.6736648082733154
overhead3:: 0
time_baseline:: 3.557368755340576
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.629305
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.016930341720581055
overhead3:: 0.05061197280883789
overhead4:: 0.21092462539672852
overhead5:: 0
memory usage:: 3800076288
time_provenance:: 1.1044597625732422
curr_diff: 0 tensor(9.8035e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.8035e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.629305
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.019131183624267578
overhead3:: 0.05262279510498047
overhead4:: 0.2336418628692627
overhead5:: 0
memory usage:: 3763699712
time_provenance:: 1.1284904479980469
curr_diff: 0 tensor(8.0498e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.0498e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.629305
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.021494150161743164
overhead3:: 0.05879378318786621
overhead4:: 0.26435351371765137
overhead5:: 0
memory usage:: 3791818752
time_provenance:: 1.16951584815979
curr_diff: 0 tensor(9.9312e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.9312e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.629305
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.026497364044189453
overhead3:: 0.05949521064758301
overhead4:: 0.3196721076965332
overhead5:: 0
memory usage:: 3767136256
time_provenance:: 1.2799577713012695
curr_diff: 0 tensor(8.2218e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.2218e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.629305
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.028732776641845703
overhead3:: 0.06547784805297852
overhead4:: 0.32454609870910645
overhead5:: 0
memory usage:: 3797762048
time_provenance:: 1.230198860168457
curr_diff: 0 tensor(9.9231e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.9231e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.629305
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.03367185592651367
overhead3:: 0.06459379196166992
overhead4:: 0.4227330684661865
overhead5:: 0
memory usage:: 3764670464
time_provenance:: 1.4340646266937256
curr_diff: 0 tensor(5.2125e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.2125e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.629305
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.033853769302368164
overhead3:: 0.07147550582885742
overhead4:: 0.40249204635620117
overhead5:: 0
memory usage:: 3788541952
time_provenance:: 1.3485147953033447
curr_diff: 0 tensor(5.2367e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.2367e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.629305
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.03689289093017578
overhead3:: 0.07699918746948242
overhead4:: 0.46584033966064453
overhead5:: 0
memory usage:: 3773886464
time_provenance:: 1.4515612125396729
curr_diff: 0 tensor(5.3530e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.3530e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.629305
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.03809857368469238
overhead3:: 0.0727834701538086
overhead4:: 0.4588460922241211
overhead5:: 0
memory usage:: 3762020352
time_provenance:: 1.415328025817871
curr_diff: 0 tensor(5.3379e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.3379e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.629305
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.040039777755737305
overhead3:: 0.07811951637268066
overhead4:: 0.5172154903411865
overhead5:: 0
memory usage:: 3789897728
time_provenance:: 1.5116147994995117
curr_diff: 0 tensor(5.3392e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.3392e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.629305
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.05669593811035156
overhead3:: 0.09915590286254883
overhead4:: 0.7953176498413086
overhead5:: 0
memory usage:: 3766120448
time_provenance:: 1.7928056716918945
curr_diff: 0 tensor(1.9202e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.9202e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.629305
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.05911421775817871
overhead3:: 0.10147595405578613
overhead4:: 0.8336961269378662
overhead5:: 0
memory usage:: 3799703552
time_provenance:: 1.8427023887634277
curr_diff: 0 tensor(1.9430e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.9430e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.629305
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.06098151206970215
overhead3:: 0.10114192962646484
overhead4:: 0.7998743057250977
overhead5:: 0
memory usage:: 3767476224
time_provenance:: 1.7687764167785645
curr_diff: 0 tensor(1.9718e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.9718e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.629305
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.07232284545898438
overhead3:: 0.1185760498046875
overhead4:: 0.8835878372192383
overhead5:: 0
memory usage:: 3778088960
time_provenance:: 1.9740242958068848
curr_diff: 0 tensor(1.9684e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.9684e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.629305
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.07647562026977539
overhead3:: 0.13711214065551758
overhead4:: 0.8907825946807861
overhead5:: 0
memory usage:: 3767349248
time_provenance:: 1.9780468940734863
curr_diff: 0 tensor(1.9612e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.9612e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.629305
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.13388848304748535
overhead3:: 0.23447608947753906
overhead4:: 1.9270145893096924
overhead5:: 0
memory usage:: 3764764672
time_provenance:: 3.1422805786132812
curr_diff: 0 tensor(3.2777e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.2777e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.629305
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.14262652397155762
overhead3:: 0.21776199340820312
overhead4:: 1.951960563659668
overhead5:: 0
memory usage:: 3770626048
time_provenance:: 3.156921863555908
curr_diff: 0 tensor(3.3639e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.3639e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.629305
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.1505286693572998
overhead3:: 0.25777196884155273
overhead4:: 2.0498626232147217
overhead5:: 0
memory usage:: 3811635200
time_provenance:: 3.3501224517822266
curr_diff: 0 tensor(3.5323e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.5323e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.629305
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.17468929290771484
overhead3:: 0.2819039821624756
overhead4:: 1.9747998714447021
overhead5:: 0
memory usage:: 3763720192
time_provenance:: 3.3167166709899902
curr_diff: 0 tensor(3.5162e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.5162e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.629305
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.1429767608642578
overhead3:: 0.22540998458862305
overhead4:: 1.9232122898101807
overhead5:: 0
memory usage:: 3774304256
time_provenance:: 3.114961624145508
curr_diff: 0 tensor(3.5302e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.5302e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.629305
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 3 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.3266012668609619
overhead3:: 0.5488259792327881
overhead4:: 3.4825937747955322
overhead5:: 0
memory usage:: 3765919744
time_provenance:: 4.704956293106079
curr_diff: 0 tensor(9.8055e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.8055e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000071, Accuracy: 0.629305
repetition 4
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 4 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 1.868731
Train - Epoch 0, Batch: 10, Loss: 1.367249
Train - Epoch 0, Batch: 20, Loss: 1.236103
Train - Epoch 0, Batch: 30, Loss: 1.185005
Train - Epoch 1, Batch: 0, Loss: 1.176921
Train - Epoch 1, Batch: 10, Loss: 1.164460
Train - Epoch 1, Batch: 20, Loss: 1.138836
Train - Epoch 1, Batch: 30, Loss: 1.131869
Train - Epoch 2, Batch: 0, Loss: 1.140293
Train - Epoch 2, Batch: 10, Loss: 1.121753
Train - Epoch 2, Batch: 20, Loss: 1.108947
Train - Epoch 2, Batch: 30, Loss: 1.103890
Train - Epoch 3, Batch: 0, Loss: 1.088520
Train - Epoch 3, Batch: 10, Loss: 1.082600
Train - Epoch 3, Batch: 20, Loss: 1.082315
Train - Epoch 3, Batch: 30, Loss: 1.079264
Train - Epoch 4, Batch: 0, Loss: 1.083517
Train - Epoch 4, Batch: 10, Loss: 1.063870
Train - Epoch 4, Batch: 20, Loss: 1.053017
Train - Epoch 4, Batch: 30, Loss: 1.049857
Train - Epoch 5, Batch: 0, Loss: 1.049124
Train - Epoch 5, Batch: 10, Loss: 1.040844
Train - Epoch 5, Batch: 20, Loss: 1.048267
Train - Epoch 5, Batch: 30, Loss: 1.033005
Train - Epoch 6, Batch: 0, Loss: 1.033198
Train - Epoch 6, Batch: 10, Loss: 1.032653
Train - Epoch 6, Batch: 20, Loss: 1.024877
Train - Epoch 6, Batch: 30, Loss: 1.021980
Train - Epoch 7, Batch: 0, Loss: 1.018536
Train - Epoch 7, Batch: 10, Loss: 1.013499
Train - Epoch 7, Batch: 20, Loss: 1.010770
Train - Epoch 7, Batch: 30, Loss: 0.999351
Train - Epoch 8, Batch: 0, Loss: 1.005465
Train - Epoch 8, Batch: 10, Loss: 1.002635
Train - Epoch 8, Batch: 20, Loss: 0.998626
Train - Epoch 8, Batch: 30, Loss: 0.995973
Train - Epoch 9, Batch: 0, Loss: 0.989725
Train - Epoch 9, Batch: 10, Loss: 0.987878
Train - Epoch 9, Batch: 20, Loss: 0.990206
Train - Epoch 9, Batch: 30, Loss: 0.985554
Train - Epoch 10, Batch: 0, Loss: 0.995948
Train - Epoch 10, Batch: 10, Loss: 0.984478
Train - Epoch 10, Batch: 20, Loss: 0.979402
Train - Epoch 10, Batch: 30, Loss: 0.982031
Train - Epoch 11, Batch: 0, Loss: 0.975230
Train - Epoch 11, Batch: 10, Loss: 0.966750
Train - Epoch 11, Batch: 20, Loss: 0.976927
Train - Epoch 11, Batch: 30, Loss: 0.972604
Train - Epoch 12, Batch: 0, Loss: 0.977775
Train - Epoch 12, Batch: 10, Loss: 0.971493
Train - Epoch 12, Batch: 20, Loss: 0.967699
Train - Epoch 12, Batch: 30, Loss: 0.956874
Train - Epoch 13, Batch: 0, Loss: 0.961825
Train - Epoch 13, Batch: 10, Loss: 0.957235
Train - Epoch 13, Batch: 20, Loss: 0.958290
Train - Epoch 13, Batch: 30, Loss: 0.959764
Train - Epoch 14, Batch: 0, Loss: 0.960177
Train - Epoch 14, Batch: 10, Loss: 0.954213
Train - Epoch 14, Batch: 20, Loss: 0.953474
Train - Epoch 14, Batch: 30, Loss: 0.949259
Train - Epoch 15, Batch: 0, Loss: 0.953873
Train - Epoch 15, Batch: 10, Loss: 0.951918
Train - Epoch 15, Batch: 20, Loss: 0.947138
Train - Epoch 15, Batch: 30, Loss: 0.947791
Train - Epoch 16, Batch: 0, Loss: 0.937828
Train - Epoch 16, Batch: 10, Loss: 0.938137
Train - Epoch 16, Batch: 20, Loss: 0.949247
Train - Epoch 16, Batch: 30, Loss: 0.946489
Train - Epoch 17, Batch: 0, Loss: 0.940398
Train - Epoch 17, Batch: 10, Loss: 0.943285
Train - Epoch 17, Batch: 20, Loss: 0.945747
Train - Epoch 17, Batch: 30, Loss: 0.937392
Train - Epoch 18, Batch: 0, Loss: 0.938234
Train - Epoch 18, Batch: 10, Loss: 0.935074
Train - Epoch 18, Batch: 20, Loss: 0.930569
Train - Epoch 18, Batch: 30, Loss: 0.935248
Train - Epoch 19, Batch: 0, Loss: 0.929833
Train - Epoch 19, Batch: 10, Loss: 0.937087
Train - Epoch 19, Batch: 20, Loss: 0.921814
Train - Epoch 19, Batch: 30, Loss: 0.922719
Train - Epoch 20, Batch: 0, Loss: 0.930139
Train - Epoch 20, Batch: 10, Loss: 0.919523
Train - Epoch 20, Batch: 20, Loss: 0.923772
Train - Epoch 20, Batch: 30, Loss: 0.914141
Train - Epoch 21, Batch: 0, Loss: 0.926941
Train - Epoch 21, Batch: 10, Loss: 0.931915
Train - Epoch 21, Batch: 20, Loss: 0.922440
Train - Epoch 21, Batch: 30, Loss: 0.915746
Train - Epoch 22, Batch: 0, Loss: 0.917633
Train - Epoch 22, Batch: 10, Loss: 0.920568
Train - Epoch 22, Batch: 20, Loss: 0.911947
Train - Epoch 22, Batch: 30, Loss: 0.920077
Train - Epoch 23, Batch: 0, Loss: 0.912215
Train - Epoch 23, Batch: 10, Loss: 0.917145
Train - Epoch 23, Batch: 20, Loss: 0.916462
Train - Epoch 23, Batch: 30, Loss: 0.910634
Train - Epoch 24, Batch: 0, Loss: 0.925200
Train - Epoch 24, Batch: 10, Loss: 0.898966
Train - Epoch 24, Batch: 20, Loss: 0.912161
Train - Epoch 24, Batch: 30, Loss: 0.909765
Train - Epoch 25, Batch: 0, Loss: 0.903728
Train - Epoch 25, Batch: 10, Loss: 0.898700
Train - Epoch 25, Batch: 20, Loss: 0.909356
Train - Epoch 25, Batch: 30, Loss: 0.912084
Train - Epoch 26, Batch: 0, Loss: 0.896696
Train - Epoch 26, Batch: 10, Loss: 0.915171
Train - Epoch 26, Batch: 20, Loss: 0.903871
Train - Epoch 26, Batch: 30, Loss: 0.907808
Train - Epoch 27, Batch: 0, Loss: 0.895201
Train - Epoch 27, Batch: 10, Loss: 0.900293
Train - Epoch 27, Batch: 20, Loss: 0.903013
Train - Epoch 27, Batch: 30, Loss: 0.907605
Train - Epoch 28, Batch: 0, Loss: 0.900402
Train - Epoch 28, Batch: 10, Loss: 0.899708
Train - Epoch 28, Batch: 20, Loss: 0.900301
Train - Epoch 28, Batch: 30, Loss: 0.902439
Train - Epoch 29, Batch: 0, Loss: 0.901715
Train - Epoch 29, Batch: 10, Loss: 0.904215
Train - Epoch 29, Batch: 20, Loss: 0.905178
Train - Epoch 29, Batch: 30, Loss: 0.897831
Train - Epoch 30, Batch: 0, Loss: 0.895587
Train - Epoch 30, Batch: 10, Loss: 0.897954
Train - Epoch 30, Batch: 20, Loss: 0.905292
Train - Epoch 30, Batch: 30, Loss: 0.904840
Train - Epoch 31, Batch: 0, Loss: 0.895000
Train - Epoch 31, Batch: 10, Loss: 0.890925
Train - Epoch 31, Batch: 20, Loss: 0.894100
Train - Epoch 31, Batch: 30, Loss: 0.900260
Test Avg. Loss: 0.000070, Accuracy: 0.630200
training_time:: 3.4802000522613525
training time full:: 3.4802770614624023
provenance prepare time:: 7.152557373046875e-06
Test Avg. Loss: 0.000070, Accuracy: 0.630200
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
batch_size:: 16384
epoch:: 0
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 3.596118927001953
overhead:: 0
overhead2:: 1.669524908065796
overhead3:: 0
time_baseline:: 3.596938371658325
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630200
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.01781177520751953
overhead3:: 0.04068112373352051
overhead4:: 0.2146775722503662
overhead5:: 0
memory usage:: 3782500352
time_provenance:: 1.1545677185058594
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630235
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.02108168601989746
overhead3:: 0.04653429985046387
overhead4:: 0.254594087600708
overhead5:: 0
memory usage:: 3774550016
time_provenance:: 1.1989030838012695
curr_diff: 0 tensor(9.9378e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.9378e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630200
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.022626876831054688
overhead3:: 0.04763031005859375
overhead4:: 0.2726268768310547
overhead5:: 0
memory usage:: 3788333056
time_provenance:: 1.2063348293304443
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630235
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.02482891082763672
overhead3:: 0.06035804748535156
overhead4:: 0.3052961826324463
overhead5:: 0
memory usage:: 3774824448
time_provenance:: 1.2071359157562256
curr_diff: 0 tensor(9.8175e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.8175e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630200
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.026805877685546875
overhead3:: 0.05785417556762695
overhead4:: 0.33042097091674805
overhead5:: 0
memory usage:: 3766841344
time_provenance:: 1.2588615417480469
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630235
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.03336691856384277
overhead3:: 0.07410645484924316
overhead4:: 0.438248872756958
overhead5:: 0
memory usage:: 3825893376
time_provenance:: 1.4726316928863525
curr_diff: 0 tensor(5.0488e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.0488e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630200
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.03253483772277832
overhead3:: 0.08173537254333496
overhead4:: 0.4277839660644531
overhead5:: 0
memory usage:: 3782733824
time_provenance:: 1.3683257102966309
curr_diff: 0 tensor(5.1155e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.1155e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630200
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.03768634796142578
overhead3:: 0.07073736190795898
overhead4:: 0.4932289123535156
overhead5:: 0
memory usage:: 3770077184
time_provenance:: 1.4993095397949219
curr_diff: 0 tensor(5.1701e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.1701e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630200
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.04017472267150879
overhead3:: 0.07687830924987793
overhead4:: 0.48873138427734375
overhead5:: 0
memory usage:: 3762618368
time_provenance:: 1.5045487880706787
curr_diff: 0 tensor(5.2045e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.2045e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630200
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.04549145698547363
overhead3:: 0.08947348594665527
overhead4:: 0.52675461769104
overhead5:: 0
memory usage:: 3774308352
time_provenance:: 1.501065969467163
curr_diff: 0 tensor(5.2343e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.2343e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630200
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.0636141300201416
overhead3:: 0.10545492172241211
overhead4:: 0.8300502300262451
overhead5:: 0
memory usage:: 3765751808
time_provenance:: 1.913414478302002
curr_diff: 0 tensor(1.6563e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6563e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630200
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.06593108177185059
overhead3:: 0.1067209243774414
overhead4:: 0.8369641304016113
overhead5:: 0
memory usage:: 3806363648
time_provenance:: 1.9005370140075684
curr_diff: 0 tensor(1.6775e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6775e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630200
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.06379055976867676
overhead3:: 0.11650204658508301
overhead4:: 0.7796642780303955
overhead5:: 0
memory usage:: 3799691264
time_provenance:: 1.7624526023864746
curr_diff: 0 tensor(1.6911e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6911e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630200
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.06944131851196289
overhead3:: 0.11706304550170898
overhead4:: 0.8934791088104248
overhead5:: 0
memory usage:: 3776065536
time_provenance:: 1.9765326976776123
curr_diff: 0 tensor(1.7254e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.7254e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630200
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.06511569023132324
overhead3:: 0.11078310012817383
overhead4:: 0.9066226482391357
overhead5:: 0
memory usage:: 3764580352
time_provenance:: 1.911870002746582
curr_diff: 0 tensor(1.7355e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.7355e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630200
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.18259000778198242
overhead3:: 0.27617764472961426
overhead4:: 2.6235263347625732
overhead5:: 0
memory usage:: 3775528960
time_provenance:: 4.153674125671387
curr_diff: 0 tensor(3.8361e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.8361e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630200
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.13969922065734863
overhead3:: 0.2234046459197998
overhead4:: 1.9866721630096436
overhead5:: 0
memory usage:: 3763605504
time_provenance:: 3.1913981437683105
curr_diff: 0 tensor(3.8199e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.8199e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630200
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.18058371543884277
overhead3:: 0.2940354347229004
overhead4:: 2.4461889266967773
overhead5:: 0
memory usage:: 3799379968
time_provenance:: 3.9395947456359863
curr_diff: 0 tensor(3.8657e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.8657e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630200
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.13859272003173828
overhead3:: 0.23446321487426758
overhead4:: 1.9337437152862549
overhead5:: 0
memory usage:: 3800219648
time_provenance:: 3.1372153759002686
curr_diff: 0 tensor(3.9147e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.9147e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630200
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.13915657997131348
overhead3:: 0.2317485809326172
overhead4:: 1.9499106407165527
overhead5:: 0
memory usage:: 3789623296
time_provenance:: 3.138345241546631
curr_diff: 0 tensor(3.9288e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.9288e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630200
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 4 0.0002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 104
max_epoch:: 32
overhead:: 0
overhead2:: 0.3777754306793213
overhead3:: 0.6611435413360596
overhead4:: 3.745828866958618
overhead5:: 0
memory usage:: 3752603648
time_provenance:: 5.074650049209595
curr_diff: 0 tensor(1.0086e-13, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0086e-13, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630200
deletion rate:: 0.0005
python3 generate_rand_ids 0.0005  covtype 0
tensor([145415, 498696, 239624, 223244, 133140, 207894, 514071, 120863, 260149,
        201783, 162872, 163897,  80957, 483390, 317501, 280641,  15435, 158799,
        270417, 207954, 241749, 490585, 147548, 171104,  67680, 150632, 445546,
        412781,  85102, 343157, 161912, 274562, 480388, 141450, 307341, 223383,
        298139,  99483, 507038, 185504, 188580,  32934, 421030, 176294, 315562,
        144558, 108718, 364718, 296113,  27830, 187574, 242871, 328891, 255167,
        425157, 266441, 117964, 474319, 390351, 177362, 223444, 261337,  26844,
        491743, 119014, 303340, 212204,  55534,  73964, 460017, 355570, 207096,
        300286, 504062, 477442,   7430, 160007,  85260, 413970,  45331, 301339,
        409891,  65832, 418089, 164139, 161073, 218422, 169272, 478523, 388422,
        320843, 398676, 375125, 428372, 345434,   1373,  84328, 389480,  43370,
         40302, 310643, 195961, 277884, 459132,  23952, 312723,  91551, 157088,
         29088,  68003, 465317, 192934, 303526, 240046, 141747, 492988,  51651,
        273866, 156118, 452056, 168415, 279019, 260590, 301554, 303602,  81398,
        333306, 136699, 349692, 493053,  84483,  37382,    521,  35345,  65041,
        154130,  33300,  12824, 301595, 229917, 158239, 130602, 218675, 292405,
        128565, 514613, 469568, 366146,  52803, 204357, 171591,  74313, 446032,
        307798, 370270, 208481, 272996, 406118,   2665, 332403, 351861, 379510,
        301686, 127608, 223877,  40587,  13974, 233113, 263836,  74397, 473761,
        162466,  23210, 144044, 161455, 393909, 103094, 341691, 457404, 126661,
        114373, 496331, 113358, 307925, 160470, 341723, 150247, 272104, 488168,
        104175, 505584,  55024,  89841, 339700, 363256, 507641, 374524,  12035,
        270083, 211716, 149255, 432921, 205593, 226078, 346916, 102183, 509736,
        506665, 508719, 170805,  29496, 199482, 452411,  49983, 431936, 357187,
        130902, 329559, 137048, 447323, 477021, 328545, 132972,   3955, 316281,
         19325, 137087, 201599, 252801, 181137, 131990, 361370, 203675, 509852,
        163742, 236447, 379806, 233381, 171942, 446377, 147370, 456625, 507829,
        337845, 134079,  67520,    961, 278466, 484290, 456650, 479180, 466894,
        470992, 350161, 337877, 477156, 486373,  72681,  87021, 112629, 364534])
batch size:: 16384
repetition 0
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 0 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 1.824851
Train - Epoch 0, Batch: 10, Loss: 1.358929
Train - Epoch 0, Batch: 20, Loss: 1.239977
Train - Epoch 0, Batch: 30, Loss: 1.197063
Train - Epoch 1, Batch: 0, Loss: 1.177168
Train - Epoch 1, Batch: 10, Loss: 1.160615
Train - Epoch 1, Batch: 20, Loss: 1.135109
Train - Epoch 1, Batch: 30, Loss: 1.125705
Train - Epoch 2, Batch: 0, Loss: 1.137016
Train - Epoch 2, Batch: 10, Loss: 1.106208
Train - Epoch 2, Batch: 20, Loss: 1.117318
Train - Epoch 2, Batch: 30, Loss: 1.094551
Train - Epoch 3, Batch: 0, Loss: 1.097587
Train - Epoch 3, Batch: 10, Loss: 1.087985
Train - Epoch 3, Batch: 20, Loss: 1.085121
Train - Epoch 3, Batch: 30, Loss: 1.067296
Train - Epoch 4, Batch: 0, Loss: 1.074408
Train - Epoch 4, Batch: 10, Loss: 1.068331
Train - Epoch 4, Batch: 20, Loss: 1.062164
Train - Epoch 4, Batch: 30, Loss: 1.048612
Train - Epoch 5, Batch: 0, Loss: 1.060789
Train - Epoch 5, Batch: 10, Loss: 1.044176
Train - Epoch 5, Batch: 20, Loss: 1.043383
Train - Epoch 5, Batch: 30, Loss: 1.033210
Train - Epoch 6, Batch: 0, Loss: 1.035057
Train - Epoch 6, Batch: 10, Loss: 1.023986
Train - Epoch 6, Batch: 20, Loss: 1.018623
Train - Epoch 6, Batch: 30, Loss: 1.016341
Train - Epoch 7, Batch: 0, Loss: 1.020202
Train - Epoch 7, Batch: 10, Loss: 1.016787
Train - Epoch 7, Batch: 20, Loss: 1.021048
Train - Epoch 7, Batch: 30, Loss: 1.004939
Train - Epoch 8, Batch: 0, Loss: 1.007495
Train - Epoch 8, Batch: 10, Loss: 0.999486
Train - Epoch 8, Batch: 20, Loss: 1.009882
Train - Epoch 8, Batch: 30, Loss: 0.993851
Train - Epoch 9, Batch: 0, Loss: 0.999626
Train - Epoch 9, Batch: 10, Loss: 1.004506
Train - Epoch 9, Batch: 20, Loss: 0.995683
Train - Epoch 9, Batch: 30, Loss: 0.984824
Train - Epoch 10, Batch: 0, Loss: 0.979466
Train - Epoch 10, Batch: 10, Loss: 0.978791
Train - Epoch 10, Batch: 20, Loss: 0.995708
Train - Epoch 10, Batch: 30, Loss: 0.993593
Train - Epoch 11, Batch: 0, Loss: 0.973545
Train - Epoch 11, Batch: 10, Loss: 0.981192
Train - Epoch 11, Batch: 20, Loss: 0.973263
Train - Epoch 11, Batch: 30, Loss: 0.973830
Train - Epoch 12, Batch: 0, Loss: 0.962042
Train - Epoch 12, Batch: 10, Loss: 0.962290
Train - Epoch 12, Batch: 20, Loss: 0.964789
Train - Epoch 12, Batch: 30, Loss: 0.963894
Train - Epoch 13, Batch: 0, Loss: 0.958733
Train - Epoch 13, Batch: 10, Loss: 0.971510
Train - Epoch 13, Batch: 20, Loss: 0.959157
Train - Epoch 13, Batch: 30, Loss: 0.962066
Train - Epoch 14, Batch: 0, Loss: 0.958353
Train - Epoch 14, Batch: 10, Loss: 0.964385
Train - Epoch 14, Batch: 20, Loss: 0.948331
Train - Epoch 14, Batch: 30, Loss: 0.953884
Train - Epoch 15, Batch: 0, Loss: 0.953264
Train - Epoch 15, Batch: 10, Loss: 0.948363
Train - Epoch 15, Batch: 20, Loss: 0.943290
Train - Epoch 15, Batch: 30, Loss: 0.954475
Train - Epoch 16, Batch: 0, Loss: 0.941330
Train - Epoch 16, Batch: 10, Loss: 0.947692
Train - Epoch 16, Batch: 20, Loss: 0.945884
Train - Epoch 16, Batch: 30, Loss: 0.938755
Train - Epoch 17, Batch: 0, Loss: 0.947822
Train - Epoch 17, Batch: 10, Loss: 0.932684
Train - Epoch 17, Batch: 20, Loss: 0.942251
Train - Epoch 17, Batch: 30, Loss: 0.936214
Train - Epoch 18, Batch: 0, Loss: 0.930046
Train - Epoch 18, Batch: 10, Loss: 0.936646
Train - Epoch 18, Batch: 20, Loss: 0.935910
Train - Epoch 18, Batch: 30, Loss: 0.931506
Train - Epoch 19, Batch: 0, Loss: 0.936828
Train - Epoch 19, Batch: 10, Loss: 0.931065
Train - Epoch 19, Batch: 20, Loss: 0.925527
Train - Epoch 19, Batch: 30, Loss: 0.930903
Train - Epoch 20, Batch: 0, Loss: 0.926427
Train - Epoch 20, Batch: 10, Loss: 0.933469
Train - Epoch 20, Batch: 20, Loss: 0.920940
Train - Epoch 20, Batch: 30, Loss: 0.924685
Train - Epoch 21, Batch: 0, Loss: 0.921729
Train - Epoch 21, Batch: 10, Loss: 0.926739
Train - Epoch 21, Batch: 20, Loss: 0.922397
Train - Epoch 21, Batch: 30, Loss: 0.926192
Train - Epoch 22, Batch: 0, Loss: 0.923172
Train - Epoch 22, Batch: 10, Loss: 0.918776
Train - Epoch 22, Batch: 20, Loss: 0.927024
Train - Epoch 22, Batch: 30, Loss: 0.922493
Train - Epoch 23, Batch: 0, Loss: 0.914050
Train - Epoch 23, Batch: 10, Loss: 0.924685
Train - Epoch 23, Batch: 20, Loss: 0.921683
Train - Epoch 23, Batch: 30, Loss: 0.909956
Train - Epoch 24, Batch: 0, Loss: 0.916637
Train - Epoch 24, Batch: 10, Loss: 0.911537
Train - Epoch 24, Batch: 20, Loss: 0.917177
Train - Epoch 24, Batch: 30, Loss: 0.918807
Train - Epoch 25, Batch: 0, Loss: 0.913701
Train - Epoch 25, Batch: 10, Loss: 0.911714
Train - Epoch 25, Batch: 20, Loss: 0.900919
Train - Epoch 25, Batch: 30, Loss: 0.914890
Train - Epoch 26, Batch: 0, Loss: 0.912161
Train - Epoch 26, Batch: 10, Loss: 0.905815
Train - Epoch 26, Batch: 20, Loss: 0.915861
Train - Epoch 26, Batch: 30, Loss: 0.906926
Train - Epoch 27, Batch: 0, Loss: 0.898218
Train - Epoch 27, Batch: 10, Loss: 0.913844
Train - Epoch 27, Batch: 20, Loss: 0.902239
Train - Epoch 27, Batch: 30, Loss: 0.902441
Train - Epoch 28, Batch: 0, Loss: 0.897147
Train - Epoch 28, Batch: 10, Loss: 0.898811
Train - Epoch 28, Batch: 20, Loss: 0.898173
Train - Epoch 28, Batch: 30, Loss: 0.902292
Train - Epoch 29, Batch: 0, Loss: 0.901406
Train - Epoch 29, Batch: 10, Loss: 0.903335
Train - Epoch 29, Batch: 20, Loss: 0.899800
Train - Epoch 29, Batch: 30, Loss: 0.893943
Train - Epoch 30, Batch: 0, Loss: 0.888630
Train - Epoch 30, Batch: 10, Loss: 0.893922
Train - Epoch 30, Batch: 20, Loss: 0.890073
Train - Epoch 30, Batch: 30, Loss: 0.902544
Train - Epoch 31, Batch: 0, Loss: 0.886099
Train - Epoch 31, Batch: 10, Loss: 0.900566
Train - Epoch 31, Batch: 20, Loss: 0.887531
Train - Epoch 31, Batch: 30, Loss: 0.892401
Test Avg. Loss: 0.000070, Accuracy: 0.630768
training_time:: 3.17863392829895
training time full:: 3.1787002086639404
provenance prepare time:: 7.3909759521484375e-06
Test Avg. Loss: 0.000070, Accuracy: 0.630768
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
batch_size:: 16384
epoch:: 0
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 3.65915846824646
overhead:: 0
overhead2:: 1.7826933860778809
overhead3:: 0
time_baseline:: 3.6600770950317383
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630768
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.01789116859436035
overhead3:: 0.046892642974853516
overhead4:: 0.21736645698547363
overhead5:: 0
memory usage:: 3797807104
time_provenance:: 1.1448922157287598
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630803
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.020579099655151367
overhead3:: 0.04625558853149414
overhead4:: 0.23941898345947266
overhead5:: 0
memory usage:: 3799191552
time_provenance:: 1.1561570167541504
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630803
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.023215532302856445
overhead3:: 0.06136918067932129
overhead4:: 0.27400994300842285
overhead5:: 0
memory usage:: 3781361664
time_provenance:: 1.1938841342926025
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630803
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.025743961334228516
overhead3:: 0.0637819766998291
overhead4:: 0.30499839782714844
overhead5:: 0
memory usage:: 3796647936
time_provenance:: 1.247201681137085
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630803
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.027501344680786133
overhead3:: 0.055626869201660156
overhead4:: 0.32758092880249023
overhead5:: 0
memory usage:: 3776925696
time_provenance:: 1.2384898662567139
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630803
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.03316974639892578
overhead3:: 0.07536602020263672
overhead4:: 0.3967103958129883
overhead5:: 0
memory usage:: 3770171392
time_provenance:: 1.3904657363891602
curr_diff: 0 tensor(6.7147e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.7147e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630820
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.039279937744140625
overhead3:: 0.07635068893432617
overhead4:: 0.44958066940307617
overhead5:: 0
memory usage:: 3799642112
time_provenance:: 1.534719467163086
curr_diff: 0 tensor(6.7415e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.7415e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630820
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.0380091667175293
overhead3:: 0.07013726234436035
overhead4:: 0.4775362014770508
overhead5:: 0
memory usage:: 3764527104
time_provenance:: 1.452782392501831
curr_diff: 0 tensor(7.0383e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.0383e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630820
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.037308454513549805
overhead3:: 0.0708777904510498
overhead4:: 0.47470521926879883
overhead5:: 0
memory usage:: 3789893632
time_provenance:: 1.4456567764282227
curr_diff: 0 tensor(7.0092e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.0092e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630820
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.04352140426635742
overhead3:: 0.07580327987670898
overhead4:: 0.5017309188842773
overhead5:: 0
memory usage:: 3791601664
time_provenance:: 1.5156126022338867
curr_diff: 0 tensor(7.0406e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.0406e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630820
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.06084299087524414
overhead3:: 0.10279130935668945
overhead4:: 0.8298852443695068
overhead5:: 0
memory usage:: 3770183680
time_provenance:: 1.8800809383392334
curr_diff: 0 tensor(2.2554e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.2554e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630785
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.06469178199768066
overhead3:: 0.10589170455932617
overhead4:: 0.8109755516052246
overhead5:: 0
memory usage:: 3766628352
time_provenance:: 1.8649852275848389
curr_diff: 0 tensor(2.2706e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.2706e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630785
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.06809306144714355
overhead3:: 0.10711479187011719
overhead4:: 0.8185551166534424
overhead5:: 0
memory usage:: 3781849088
time_provenance:: 1.882037878036499
curr_diff: 0 tensor(2.3920e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.3920e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630785
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.06424570083618164
overhead3:: 0.10955333709716797
overhead4:: 0.8857855796813965
overhead5:: 0
memory usage:: 3769524224
time_provenance:: 1.9142577648162842
curr_diff: 0 tensor(2.3994e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.3994e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630785
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.06986069679260254
overhead3:: 0.11424922943115234
overhead4:: 0.9052839279174805
overhead5:: 0
memory usage:: 3763367936
time_provenance:: 1.9351158142089844
curr_diff: 0 tensor(2.4420e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.4420e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630785
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.14620614051818848
overhead3:: 0.22437477111816406
overhead4:: 1.8713979721069336
overhead5:: 0
memory usage:: 3802079232
time_provenance:: 3.0976243019104004
curr_diff: 0 tensor(5.1157e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.1157e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630768
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.18261289596557617
overhead3:: 0.2620997428894043
overhead4:: 2.3742682933807373
overhead5:: 0
memory usage:: 3788414976
time_provenance:: 3.8181278705596924
curr_diff: 0 tensor(5.1485e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.1485e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630768
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.14522409439086914
overhead3:: 0.23341107368469238
overhead4:: 1.9754714965820312
overhead5:: 0
memory usage:: 3767435264
time_provenance:: 3.198866367340088
curr_diff: 0 tensor(5.4139e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.4139e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630768
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.1540999412536621
overhead3:: 0.23261332511901855
overhead4:: 1.773216724395752
overhead5:: 0
memory usage:: 3806371840
time_provenance:: 2.964723587036133
curr_diff: 0 tensor(5.4144e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.4144e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630768
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.19410324096679688
overhead3:: 0.283552885055542
overhead4:: 2.7351491451263428
overhead5:: 0
memory usage:: 3774509056
time_provenance:: 4.293852806091309
curr_diff: 0 tensor(5.5062e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.5062e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630768
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.2784740924835205
overhead3:: 0.39990878105163574
overhead4:: 3.200927734375
overhead5:: 0
memory usage:: 3767992320
time_provenance:: 4.3044273853302
curr_diff: 0 tensor(9.9157e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.9157e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630768
repetition 1
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 1 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 1.933572
Train - Epoch 0, Batch: 10, Loss: 1.402051
Train - Epoch 0, Batch: 20, Loss: 1.248337
Train - Epoch 0, Batch: 30, Loss: 1.202609
Train - Epoch 1, Batch: 0, Loss: 1.183423
Train - Epoch 1, Batch: 10, Loss: 1.155100
Train - Epoch 1, Batch: 20, Loss: 1.155992
Train - Epoch 1, Batch: 30, Loss: 1.129640
Train - Epoch 2, Batch: 0, Loss: 1.124364
Train - Epoch 2, Batch: 10, Loss: 1.128140
Train - Epoch 2, Batch: 20, Loss: 1.115266
Train - Epoch 2, Batch: 30, Loss: 1.102828
Train - Epoch 3, Batch: 0, Loss: 1.114643
Train - Epoch 3, Batch: 10, Loss: 1.091587
Train - Epoch 3, Batch: 20, Loss: 1.087409
Train - Epoch 3, Batch: 30, Loss: 1.078774
Train - Epoch 4, Batch: 0, Loss: 1.085624
Train - Epoch 4, Batch: 10, Loss: 1.070397
Train - Epoch 4, Batch: 20, Loss: 1.063384
Train - Epoch 4, Batch: 30, Loss: 1.056258
Train - Epoch 5, Batch: 0, Loss: 1.049813
Train - Epoch 5, Batch: 10, Loss: 1.050020
Train - Epoch 5, Batch: 20, Loss: 1.045655
Train - Epoch 5, Batch: 30, Loss: 1.042802
Train - Epoch 6, Batch: 0, Loss: 1.050450
Train - Epoch 6, Batch: 10, Loss: 1.033515
Train - Epoch 6, Batch: 20, Loss: 1.032493
Train - Epoch 6, Batch: 30, Loss: 1.026034
Train - Epoch 7, Batch: 0, Loss: 1.021845
Train - Epoch 7, Batch: 10, Loss: 1.015043
Train - Epoch 7, Batch: 20, Loss: 1.019126
Train - Epoch 7, Batch: 30, Loss: 1.005820
Train - Epoch 8, Batch: 0, Loss: 1.013670
Train - Epoch 8, Batch: 10, Loss: 1.000014
Train - Epoch 8, Batch: 20, Loss: 1.013077
Train - Epoch 8, Batch: 30, Loss: 0.993383
Train - Epoch 9, Batch: 0, Loss: 0.994803
Train - Epoch 9, Batch: 10, Loss: 0.994443
Train - Epoch 9, Batch: 20, Loss: 0.985208
Train - Epoch 9, Batch: 30, Loss: 0.992811
Train - Epoch 10, Batch: 0, Loss: 0.991504
Train - Epoch 10, Batch: 10, Loss: 0.982611
Train - Epoch 10, Batch: 20, Loss: 0.980436
Train - Epoch 10, Batch: 30, Loss: 0.972528
Train - Epoch 11, Batch: 0, Loss: 0.981076
Train - Epoch 11, Batch: 10, Loss: 0.980364
Train - Epoch 11, Batch: 20, Loss: 0.976691
Train - Epoch 11, Batch: 30, Loss: 0.966481
Train - Epoch 12, Batch: 0, Loss: 0.979978
Train - Epoch 12, Batch: 10, Loss: 0.965659
Train - Epoch 12, Batch: 20, Loss: 0.965284
Train - Epoch 12, Batch: 30, Loss: 0.954065
Train - Epoch 13, Batch: 0, Loss: 0.961848
Train - Epoch 13, Batch: 10, Loss: 0.964601
Train - Epoch 13, Batch: 20, Loss: 0.968458
Train - Epoch 13, Batch: 30, Loss: 0.959980
Train - Epoch 14, Batch: 0, Loss: 0.953132
Train - Epoch 14, Batch: 10, Loss: 0.957134
Train - Epoch 14, Batch: 20, Loss: 0.960225
Train - Epoch 14, Batch: 30, Loss: 0.958154
Train - Epoch 15, Batch: 0, Loss: 0.953439
Train - Epoch 15, Batch: 10, Loss: 0.954997
Train - Epoch 15, Batch: 20, Loss: 0.948701
Train - Epoch 15, Batch: 30, Loss: 0.943108
Train - Epoch 16, Batch: 0, Loss: 0.950235
Train - Epoch 16, Batch: 10, Loss: 0.948233
Train - Epoch 16, Batch: 20, Loss: 0.945753
Train - Epoch 16, Batch: 30, Loss: 0.940896
Train - Epoch 17, Batch: 0, Loss: 0.941288
Train - Epoch 17, Batch: 10, Loss: 0.940130
Train - Epoch 17, Batch: 20, Loss: 0.941108
Train - Epoch 17, Batch: 30, Loss: 0.938165
Train - Epoch 18, Batch: 0, Loss: 0.938327
Train - Epoch 18, Batch: 10, Loss: 0.942360
Train - Epoch 18, Batch: 20, Loss: 0.921452
Train - Epoch 18, Batch: 30, Loss: 0.932574
Train - Epoch 19, Batch: 0, Loss: 0.936917
Train - Epoch 19, Batch: 10, Loss: 0.930100
Train - Epoch 19, Batch: 20, Loss: 0.920504
Train - Epoch 19, Batch: 30, Loss: 0.921743
Train - Epoch 20, Batch: 0, Loss: 0.930207
Train - Epoch 20, Batch: 10, Loss: 0.924626
Train - Epoch 20, Batch: 20, Loss: 0.932344
Train - Epoch 20, Batch: 30, Loss: 0.917686
Train - Epoch 21, Batch: 0, Loss: 0.925596
Train - Epoch 21, Batch: 10, Loss: 0.922102
Train - Epoch 21, Batch: 20, Loss: 0.922619
Train - Epoch 21, Batch: 30, Loss: 0.924143
Train - Epoch 22, Batch: 0, Loss: 0.913245
Train - Epoch 22, Batch: 10, Loss: 0.922092
Train - Epoch 22, Batch: 20, Loss: 0.915733
Train - Epoch 22, Batch: 30, Loss: 0.922428
Train - Epoch 23, Batch: 0, Loss: 0.926740
Train - Epoch 23, Batch: 10, Loss: 0.919953
Train - Epoch 23, Batch: 20, Loss: 0.921603
Train - Epoch 23, Batch: 30, Loss: 0.901729
Train - Epoch 24, Batch: 0, Loss: 0.912206
Train - Epoch 24, Batch: 10, Loss: 0.907393
Train - Epoch 24, Batch: 20, Loss: 0.912115
Train - Epoch 24, Batch: 30, Loss: 0.913757
Train - Epoch 25, Batch: 0, Loss: 0.912626
Train - Epoch 25, Batch: 10, Loss: 0.919283
Train - Epoch 25, Batch: 20, Loss: 0.913485
Train - Epoch 25, Batch: 30, Loss: 0.907433
Train - Epoch 26, Batch: 0, Loss: 0.914240
Train - Epoch 26, Batch: 10, Loss: 0.901825
Train - Epoch 26, Batch: 20, Loss: 0.909390
Train - Epoch 26, Batch: 30, Loss: 0.905802
Train - Epoch 27, Batch: 0, Loss: 0.907615
Train - Epoch 27, Batch: 10, Loss: 0.908503
Train - Epoch 27, Batch: 20, Loss: 0.902787
Train - Epoch 27, Batch: 30, Loss: 0.898894
Train - Epoch 28, Batch: 0, Loss: 0.909017
Train - Epoch 28, Batch: 10, Loss: 0.894281
Train - Epoch 28, Batch: 20, Loss: 0.904727
Train - Epoch 28, Batch: 30, Loss: 0.895583
Train - Epoch 29, Batch: 0, Loss: 0.902097
Train - Epoch 29, Batch: 10, Loss: 0.918740
Train - Epoch 29, Batch: 20, Loss: 0.892021
Train - Epoch 29, Batch: 30, Loss: 0.892384
Train - Epoch 30, Batch: 0, Loss: 0.902659
Train - Epoch 30, Batch: 10, Loss: 0.891871
Train - Epoch 30, Batch: 20, Loss: 0.890527
Train - Epoch 30, Batch: 30, Loss: 0.894511
Train - Epoch 31, Batch: 0, Loss: 0.888512
Train - Epoch 31, Batch: 10, Loss: 0.895607
Train - Epoch 31, Batch: 20, Loss: 0.905457
Train - Epoch 31, Batch: 30, Loss: 0.892433
Test Avg. Loss: 0.000070, Accuracy: 0.630183
training_time:: 3.492924928665161
training time full:: 3.4929909706115723
provenance prepare time:: 7.152557373046875e-06
Test Avg. Loss: 0.000070, Accuracy: 0.630183
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
batch_size:: 16384
epoch:: 0
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 3.7060585021972656
overhead:: 0
overhead2:: 1.8244011402130127
overhead3:: 0
time_baseline:: 3.7069528102874756
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630252
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.017600059509277344
overhead3:: 0.04302835464477539
overhead4:: 0.21601057052612305
overhead5:: 0
memory usage:: 3770552320
time_provenance:: 1.1372277736663818
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630252
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.02302837371826172
overhead3:: 0.060488224029541016
overhead4:: 0.2735450267791748
overhead5:: 0
memory usage:: 3792121856
time_provenance:: 1.3444674015045166
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630269
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.025019168853759766
overhead3:: 0.06266450881958008
overhead4:: 0.3129703998565674
overhead5:: 0
memory usage:: 3766484992
time_provenance:: 1.2821190357208252
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630252
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.025876998901367188
overhead3:: 0.05471038818359375
overhead4:: 0.31835365295410156
overhead5:: 0
memory usage:: 3797884928
time_provenance:: 1.3312923908233643
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630269
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.028326988220214844
overhead3:: 0.05578494071960449
overhead4:: 0.33063673973083496
overhead5:: 0
memory usage:: 3761549312
time_provenance:: 1.2875173091888428
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630252
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.03453326225280762
overhead3:: 0.07597184181213379
overhead4:: 0.45031166076660156
overhead5:: 0
memory usage:: 3761856512
time_provenance:: 1.5219144821166992
curr_diff: 0 tensor(7.3922e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.3922e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630252
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.0373687744140625
overhead3:: 0.08289408683776855
overhead4:: 0.4226224422454834
overhead5:: 0
memory usage:: 3767463936
time_provenance:: 1.4530069828033447
curr_diff: 0 tensor(7.6174e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.6174e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630252
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.03571605682373047
overhead3:: 0.0701138973236084
overhead4:: 0.47307372093200684
overhead5:: 0
memory usage:: 3799822336
time_provenance:: 1.430187463760376
curr_diff: 0 tensor(7.6840e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.6840e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630252
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.03888559341430664
overhead3:: 0.0719912052154541
overhead4:: 0.4684617519378662
overhead5:: 0
memory usage:: 3762655232
time_provenance:: 1.4432449340820312
curr_diff: 0 tensor(7.7061e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.7061e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630252
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.04211068153381348
overhead3:: 0.09121823310852051
overhead4:: 0.5092809200286865
overhead5:: 0
memory usage:: 3770933248
time_provenance:: 1.5001635551452637
curr_diff: 0 tensor(7.7521e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.7521e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630252
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.05987834930419922
overhead3:: 0.10168910026550293
overhead4:: 0.8131527900695801
overhead5:: 0
memory usage:: 3770388480
time_provenance:: 1.8414082527160645
curr_diff: 0 tensor(3.0791e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.0791e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630252
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.05964350700378418
overhead3:: 0.10197234153747559
overhead4:: 0.8202114105224609
overhead5:: 0
memory usage:: 3799261184
time_provenance:: 1.8335893154144287
curr_diff: 0 tensor(3.1750e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.1750e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630252
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.06280946731567383
overhead3:: 0.10587048530578613
overhead4:: 0.8085854053497314
overhead5:: 0
memory usage:: 3790098432
time_provenance:: 1.8209099769592285
curr_diff: 0 tensor(3.2015e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.2015e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630252
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.06411409378051758
overhead3:: 0.10872125625610352
overhead4:: 0.8778185844421387
overhead5:: 0
memory usage:: 3763163136
time_provenance:: 1.911149024963379
curr_diff: 0 tensor(3.2137e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.2137e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630252
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.07129573822021484
overhead3:: 0.1284036636352539
overhead4:: 0.909113883972168
overhead5:: 0
memory usage:: 3773382656
time_provenance:: 1.9797403812408447
curr_diff: 0 tensor(3.2317e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.2317e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630252
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.14179730415344238
overhead3:: 0.22167015075683594
overhead4:: 1.9226880073547363
overhead5:: 0
memory usage:: 3775135744
time_provenance:: 3.134042501449585
curr_diff: 0 tensor(5.6467e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.6467e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630252
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.19073224067687988
overhead3:: 0.2716400623321533
overhead4:: 2.496753454208374
overhead5:: 0
memory usage:: 3774595072
time_provenance:: 3.986902952194214
curr_diff: 0 tensor(5.7139e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.7139e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630252
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.19809603691101074
overhead3:: 0.30523061752319336
overhead4:: 2.7643628120422363
overhead5:: 0
memory usage:: 3799130112
time_provenance:: 4.372779846191406
curr_diff: 0 tensor(5.7177e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.7177e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630252
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.1584930419921875
overhead3:: 0.23447060585021973
overhead4:: 1.8248639106750488
overhead5:: 0
memory usage:: 3790938112
time_provenance:: 3.048466205596924
curr_diff: 0 tensor(5.8165e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.8165e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630252
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.18042373657226562
overhead3:: 0.27217674255371094
overhead4:: 2.3837227821350098
overhead5:: 0
memory usage:: 3791781888
time_provenance:: 3.8435606956481934
curr_diff: 0 tensor(6.0054e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.0054e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630252
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.32878780364990234
overhead3:: 0.49240541458129883
overhead4:: 3.47247314453125
overhead5:: 0
memory usage:: 3767816192
time_provenance:: 4.662301301956177
curr_diff: 0 tensor(1.0087e-13, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0087e-13, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630252
repetition 2
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 2 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 1.971513
Train - Epoch 0, Batch: 10, Loss: 1.389433
Train - Epoch 0, Batch: 20, Loss: 1.251559
Train - Epoch 0, Batch: 30, Loss: 1.193386
Train - Epoch 1, Batch: 0, Loss: 1.181637
Train - Epoch 1, Batch: 10, Loss: 1.158370
Train - Epoch 1, Batch: 20, Loss: 1.143138
Train - Epoch 1, Batch: 30, Loss: 1.130420
Train - Epoch 2, Batch: 0, Loss: 1.131182
Train - Epoch 2, Batch: 10, Loss: 1.121220
Train - Epoch 2, Batch: 20, Loss: 1.098972
Train - Epoch 2, Batch: 30, Loss: 1.115424
Train - Epoch 3, Batch: 0, Loss: 1.100486
Train - Epoch 3, Batch: 10, Loss: 1.100503
Train - Epoch 3, Batch: 20, Loss: 1.080825
Train - Epoch 3, Batch: 30, Loss: 1.085062
Train - Epoch 4, Batch: 0, Loss: 1.075070
Train - Epoch 4, Batch: 10, Loss: 1.061097
Train - Epoch 4, Batch: 20, Loss: 1.060588
Train - Epoch 4, Batch: 30, Loss: 1.049028
Train - Epoch 5, Batch: 0, Loss: 1.053371
Train - Epoch 5, Batch: 10, Loss: 1.054431
Train - Epoch 5, Batch: 20, Loss: 1.037420
Train - Epoch 5, Batch: 30, Loss: 1.039617
Train - Epoch 6, Batch: 0, Loss: 1.041606
Train - Epoch 6, Batch: 10, Loss: 1.023899
Train - Epoch 6, Batch: 20, Loss: 1.032582
Train - Epoch 6, Batch: 30, Loss: 1.017214
Train - Epoch 7, Batch: 0, Loss: 1.017151
Train - Epoch 7, Batch: 10, Loss: 1.024958
Train - Epoch 7, Batch: 20, Loss: 1.013669
Train - Epoch 7, Batch: 30, Loss: 1.013743
Train - Epoch 8, Batch: 0, Loss: 1.005266
Train - Epoch 8, Batch: 10, Loss: 1.004496
Train - Epoch 8, Batch: 20, Loss: 1.001889
Train - Epoch 8, Batch: 30, Loss: 1.001727
Train - Epoch 9, Batch: 0, Loss: 0.998356
Train - Epoch 9, Batch: 10, Loss: 0.998461
Train - Epoch 9, Batch: 20, Loss: 0.998102
Train - Epoch 9, Batch: 30, Loss: 0.983340
Train - Epoch 10, Batch: 0, Loss: 0.990040
Train - Epoch 10, Batch: 10, Loss: 0.986454
Train - Epoch 10, Batch: 20, Loss: 0.972406
Train - Epoch 10, Batch: 30, Loss: 0.984842
Train - Epoch 11, Batch: 0, Loss: 0.980328
Train - Epoch 11, Batch: 10, Loss: 0.971005
Train - Epoch 11, Batch: 20, Loss: 0.973753
Train - Epoch 11, Batch: 30, Loss: 0.970131
Train - Epoch 12, Batch: 0, Loss: 0.969034
Train - Epoch 12, Batch: 10, Loss: 0.975089
Train - Epoch 12, Batch: 20, Loss: 0.964868
Train - Epoch 12, Batch: 30, Loss: 0.972123
Train - Epoch 13, Batch: 0, Loss: 0.966304
Train - Epoch 13, Batch: 10, Loss: 0.954839
Train - Epoch 13, Batch: 20, Loss: 0.958812
Train - Epoch 13, Batch: 30, Loss: 0.972428
Train - Epoch 14, Batch: 0, Loss: 0.956605
Train - Epoch 14, Batch: 10, Loss: 0.963456
Train - Epoch 14, Batch: 20, Loss: 0.955363
Train - Epoch 14, Batch: 30, Loss: 0.960107
Train - Epoch 15, Batch: 0, Loss: 0.954929
Train - Epoch 15, Batch: 10, Loss: 0.947567
Train - Epoch 15, Batch: 20, Loss: 0.942686
Train - Epoch 15, Batch: 30, Loss: 0.944056
Train - Epoch 16, Batch: 0, Loss: 0.953078
Train - Epoch 16, Batch: 10, Loss: 0.942791
Train - Epoch 16, Batch: 20, Loss: 0.934756
Train - Epoch 16, Batch: 30, Loss: 0.946998
Train - Epoch 17, Batch: 0, Loss: 0.948739
Train - Epoch 17, Batch: 10, Loss: 0.943373
Train - Epoch 17, Batch: 20, Loss: 0.923797
Train - Epoch 17, Batch: 30, Loss: 0.937309
Train - Epoch 18, Batch: 0, Loss: 0.931840
Train - Epoch 18, Batch: 10, Loss: 0.932016
Train - Epoch 18, Batch: 20, Loss: 0.931468
Train - Epoch 18, Batch: 30, Loss: 0.934196
Train - Epoch 19, Batch: 0, Loss: 0.929805
Train - Epoch 19, Batch: 10, Loss: 0.921818
Train - Epoch 19, Batch: 20, Loss: 0.932938
Train - Epoch 19, Batch: 30, Loss: 0.920890
Train - Epoch 20, Batch: 0, Loss: 0.925390
Train - Epoch 20, Batch: 10, Loss: 0.921453
Train - Epoch 20, Batch: 20, Loss: 0.919200
Train - Epoch 20, Batch: 30, Loss: 0.922192
Train - Epoch 21, Batch: 0, Loss: 0.926702
Train - Epoch 21, Batch: 10, Loss: 0.918693
Train - Epoch 21, Batch: 20, Loss: 0.915566
Train - Epoch 21, Batch: 30, Loss: 0.915040
Train - Epoch 22, Batch: 0, Loss: 0.920276
Train - Epoch 22, Batch: 10, Loss: 0.923284
Train - Epoch 22, Batch: 20, Loss: 0.921282
Train - Epoch 22, Batch: 30, Loss: 0.914592
Train - Epoch 23, Batch: 0, Loss: 0.916609
Train - Epoch 23, Batch: 10, Loss: 0.914338
Train - Epoch 23, Batch: 20, Loss: 0.910064
Train - Epoch 23, Batch: 30, Loss: 0.918984
Train - Epoch 24, Batch: 0, Loss: 0.908663
Train - Epoch 24, Batch: 10, Loss: 0.919375
Train - Epoch 24, Batch: 20, Loss: 0.906031
Train - Epoch 24, Batch: 30, Loss: 0.909102
Train - Epoch 25, Batch: 0, Loss: 0.903735
Train - Epoch 25, Batch: 10, Loss: 0.916496
Train - Epoch 25, Batch: 20, Loss: 0.915415
Train - Epoch 25, Batch: 30, Loss: 0.917993
Train - Epoch 26, Batch: 0, Loss: 0.905225
Train - Epoch 26, Batch: 10, Loss: 0.905157
Train - Epoch 26, Batch: 20, Loss: 0.899428
Train - Epoch 26, Batch: 30, Loss: 0.908290
Train - Epoch 27, Batch: 0, Loss: 0.909491
Train - Epoch 27, Batch: 10, Loss: 0.900635
Train - Epoch 27, Batch: 20, Loss: 0.916643
Train - Epoch 27, Batch: 30, Loss: 0.897231
Train - Epoch 28, Batch: 0, Loss: 0.905927
Train - Epoch 28, Batch: 10, Loss: 0.902631
Train - Epoch 28, Batch: 20, Loss: 0.908836
Train - Epoch 28, Batch: 30, Loss: 0.903255
Train - Epoch 29, Batch: 0, Loss: 0.899680
Train - Epoch 29, Batch: 10, Loss: 0.892456
Train - Epoch 29, Batch: 20, Loss: 0.900177
Train - Epoch 29, Batch: 30, Loss: 0.898014
Train - Epoch 30, Batch: 0, Loss: 0.897054
Train - Epoch 30, Batch: 10, Loss: 0.899232
Train - Epoch 30, Batch: 20, Loss: 0.902043
Train - Epoch 30, Batch: 30, Loss: 0.889298
Train - Epoch 31, Batch: 0, Loss: 0.889588
Train - Epoch 31, Batch: 10, Loss: 0.890807
Train - Epoch 31, Batch: 20, Loss: 0.894079
Train - Epoch 31, Batch: 30, Loss: 0.881595
Test Avg. Loss: 0.000070, Accuracy: 0.630562
training_time:: 3.506913423538208
training time full:: 3.506978988647461
provenance prepare time:: 5.9604644775390625e-06
Test Avg. Loss: 0.000070, Accuracy: 0.630562
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
batch_size:: 16384
epoch:: 0
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 3.698659658432007
overhead:: 0
overhead2:: 1.747868537902832
overhead3:: 0
time_baseline:: 3.699592351913452
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630596
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.017659425735473633
overhead3:: 0.04298090934753418
overhead4:: 0.21174311637878418
overhead5:: 0
memory usage:: 3799183360
time_provenance:: 1.1256206035614014
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630596
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.020303726196289062
overhead3:: 0.055022239685058594
overhead4:: 0.2441556453704834
overhead5:: 0
memory usage:: 3774410752
time_provenance:: 1.1763949394226074
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630596
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.02254796028137207
overhead3:: 0.05931520462036133
overhead4:: 0.2752535343170166
overhead5:: 0
memory usage:: 3799797760
time_provenance:: 1.227292537689209
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630596
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.025748729705810547
overhead3:: 0.055145978927612305
overhead4:: 0.3436136245727539
overhead5:: 0
memory usage:: 3800301568
time_provenance:: 1.2721471786499023
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630596
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.027956485748291016
overhead3:: 0.0610198974609375
overhead4:: 0.3323707580566406
overhead5:: 0
memory usage:: 3799027712
time_provenance:: 1.2755649089813232
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630596
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.033495187759399414
overhead3:: 0.06316757202148438
overhead4:: 0.40748119354248047
overhead5:: 0
memory usage:: 3834867712
time_provenance:: 1.3688082695007324
curr_diff: 0 tensor(7.4888e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.4888e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630596
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.03362131118774414
overhead3:: 0.07396268844604492
overhead4:: 0.42958641052246094
overhead5:: 0
memory usage:: 3799744512
time_provenance:: 1.380286693572998
curr_diff: 0 tensor(7.9085e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.9085e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630596
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.03562331199645996
overhead3:: 0.07822728157043457
overhead4:: 0.4646949768066406
overhead5:: 0
memory usage:: 3779821568
time_provenance:: 1.4170658588409424
curr_diff: 0 tensor(8.0309e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.0309e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630596
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.043168067932128906
overhead3:: 0.07361006736755371
overhead4:: 0.5080044269561768
overhead5:: 0
memory usage:: 3762462720
time_provenance:: 1.471144199371338
curr_diff: 0 tensor(8.0183e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.0183e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630596
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.0418095588684082
overhead3:: 0.08520364761352539
overhead4:: 0.5359876155853271
overhead5:: 0
memory usage:: 3790364672
time_provenance:: 1.5332834720611572
curr_diff: 0 tensor(7.9955e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.9955e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630596
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.06493210792541504
overhead3:: 0.10451960563659668
overhead4:: 0.8014285564422607
overhead5:: 0
memory usage:: 3763585024
time_provenance:: 1.8506948947906494
curr_diff: 0 tensor(2.2577e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.2577e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630596
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.060373544692993164
overhead3:: 0.10422563552856445
overhead4:: 0.8373119831085205
overhead5:: 0
memory usage:: 3774914560
time_provenance:: 1.8746411800384521
curr_diff: 0 tensor(2.4034e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.4034e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630596
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.06709575653076172
overhead3:: 0.10364174842834473
overhead4:: 0.7743816375732422
overhead5:: 0
memory usage:: 3769282560
time_provenance:: 1.7825257778167725
curr_diff: 0 tensor(2.4347e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.4347e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630596
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.0754549503326416
overhead3:: 0.12045741081237793
overhead4:: 0.9820168018341064
overhead5:: 0
memory usage:: 3774521344
time_provenance:: 2.1527657508850098
curr_diff: 0 tensor(2.4627e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.4627e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630596
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.07004618644714355
overhead3:: 0.11926054954528809
overhead4:: 0.9241724014282227
overhead5:: 0
memory usage:: 3774869504
time_provenance:: 1.99210786819458
curr_diff: 0 tensor(2.4741e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.4741e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630596
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.14210724830627441
overhead3:: 0.22391557693481445
overhead4:: 1.9152600765228271
overhead5:: 0
memory usage:: 3774976000
time_provenance:: 3.138343334197998
curr_diff: 0 tensor(6.1378e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.1378e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630596
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.1915431022644043
overhead3:: 0.2813072204589844
overhead4:: 2.433412551879883
overhead5:: 0
memory usage:: 3788718080
time_provenance:: 3.9262712001800537
curr_diff: 0 tensor(6.5658e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.5658e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630596
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.16183757781982422
overhead3:: 0.24802756309509277
overhead4:: 2.135143995285034
overhead5:: 0
memory usage:: 3766591488
time_provenance:: 3.4618277549743652
curr_diff: 0 tensor(6.5861e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.5861e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630596
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.1500871181488037
overhead3:: 0.23063278198242188
overhead4:: 1.9975244998931885
overhead5:: 0
memory usage:: 3762544640
time_provenance:: 3.2085797786712646
curr_diff: 0 tensor(6.5998e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.5998e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630596
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.15570664405822754
overhead3:: 0.23581457138061523
overhead4:: 1.7511470317840576
overhead5:: 0
memory usage:: 3839070208
time_provenance:: 2.9537060260772705
curr_diff: 0 tensor(6.6359e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.6359e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630596
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.37285757064819336
overhead3:: 0.6168842315673828
overhead4:: 3.66316556930542
overhead5:: 0
memory usage:: 3789463552
time_provenance:: 4.9682676792144775
curr_diff: 0 tensor(1.0009e-13, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0009e-13, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630596
repetition 3
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 3 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 1.920403
Train - Epoch 0, Batch: 10, Loss: 1.374125
Train - Epoch 0, Batch: 20, Loss: 1.236077
Train - Epoch 0, Batch: 30, Loss: 1.192874
Train - Epoch 1, Batch: 0, Loss: 1.172321
Train - Epoch 1, Batch: 10, Loss: 1.157888
Train - Epoch 1, Batch: 20, Loss: 1.128942
Train - Epoch 1, Batch: 30, Loss: 1.129834
Train - Epoch 2, Batch: 0, Loss: 1.131126
Train - Epoch 2, Batch: 10, Loss: 1.112991
Train - Epoch 2, Batch: 20, Loss: 1.103466
Train - Epoch 2, Batch: 30, Loss: 1.102504
Train - Epoch 3, Batch: 0, Loss: 1.103552
Train - Epoch 3, Batch: 10, Loss: 1.084128
Train - Epoch 3, Batch: 20, Loss: 1.074453
Train - Epoch 3, Batch: 30, Loss: 1.066850
Train - Epoch 4, Batch: 0, Loss: 1.065692
Train - Epoch 4, Batch: 10, Loss: 1.067791
Train - Epoch 4, Batch: 20, Loss: 1.053779
Train - Epoch 4, Batch: 30, Loss: 1.063420
Train - Epoch 5, Batch: 0, Loss: 1.050949
Train - Epoch 5, Batch: 10, Loss: 1.048002
Train - Epoch 5, Batch: 20, Loss: 1.047873
Train - Epoch 5, Batch: 30, Loss: 1.037510
Train - Epoch 6, Batch: 0, Loss: 1.026386
Train - Epoch 6, Batch: 10, Loss: 1.035411
Train - Epoch 6, Batch: 20, Loss: 1.024116
Train - Epoch 6, Batch: 30, Loss: 1.024847
Train - Epoch 7, Batch: 0, Loss: 1.020843
Train - Epoch 7, Batch: 10, Loss: 1.004316
Train - Epoch 7, Batch: 20, Loss: 0.999943
Train - Epoch 7, Batch: 30, Loss: 1.004978
Train - Epoch 8, Batch: 0, Loss: 1.003217
Train - Epoch 8, Batch: 10, Loss: 1.003861
Train - Epoch 8, Batch: 20, Loss: 1.008024
Train - Epoch 8, Batch: 30, Loss: 0.997613
Train - Epoch 9, Batch: 0, Loss: 0.999410
Train - Epoch 9, Batch: 10, Loss: 0.994425
Train - Epoch 9, Batch: 20, Loss: 0.999348
Train - Epoch 9, Batch: 30, Loss: 0.990326
Train - Epoch 10, Batch: 0, Loss: 0.990204
Train - Epoch 10, Batch: 10, Loss: 0.981612
Train - Epoch 10, Batch: 20, Loss: 0.980942
Train - Epoch 10, Batch: 30, Loss: 0.975556
Train - Epoch 11, Batch: 0, Loss: 0.966572
Train - Epoch 11, Batch: 10, Loss: 0.975701
Train - Epoch 11, Batch: 20, Loss: 0.971036
Train - Epoch 11, Batch: 30, Loss: 0.966073
Train - Epoch 12, Batch: 0, Loss: 0.967865
Train - Epoch 12, Batch: 10, Loss: 0.964619
Train - Epoch 12, Batch: 20, Loss: 0.957395
Train - Epoch 12, Batch: 30, Loss: 0.966928
Train - Epoch 13, Batch: 0, Loss: 0.968624
Train - Epoch 13, Batch: 10, Loss: 0.963994
Train - Epoch 13, Batch: 20, Loss: 0.961207
Train - Epoch 13, Batch: 30, Loss: 0.950958
Train - Epoch 14, Batch: 0, Loss: 0.963491
Train - Epoch 14, Batch: 10, Loss: 0.952184
Train - Epoch 14, Batch: 20, Loss: 0.958743
Train - Epoch 14, Batch: 30, Loss: 0.944061
Train - Epoch 15, Batch: 0, Loss: 0.956856
Train - Epoch 15, Batch: 10, Loss: 0.955297
Train - Epoch 15, Batch: 20, Loss: 0.941764
Train - Epoch 15, Batch: 30, Loss: 0.945996
Train - Epoch 16, Batch: 0, Loss: 0.939103
Train - Epoch 16, Batch: 10, Loss: 0.944143
Train - Epoch 16, Batch: 20, Loss: 0.952784
Train - Epoch 16, Batch: 30, Loss: 0.941019
Train - Epoch 17, Batch: 0, Loss: 0.943990
Train - Epoch 17, Batch: 10, Loss: 0.941219
Train - Epoch 17, Batch: 20, Loss: 0.930148
Train - Epoch 17, Batch: 30, Loss: 0.938662
Train - Epoch 18, Batch: 0, Loss: 0.927502
Train - Epoch 18, Batch: 10, Loss: 0.930719
Train - Epoch 18, Batch: 20, Loss: 0.930119
Train - Epoch 18, Batch: 30, Loss: 0.924616
Train - Epoch 19, Batch: 0, Loss: 0.925741
Train - Epoch 19, Batch: 10, Loss: 0.933159
Train - Epoch 19, Batch: 20, Loss: 0.926437
Train - Epoch 19, Batch: 30, Loss: 0.923411
Train - Epoch 20, Batch: 0, Loss: 0.914675
Train - Epoch 20, Batch: 10, Loss: 0.917577
Train - Epoch 20, Batch: 20, Loss: 0.922028
Train - Epoch 20, Batch: 30, Loss: 0.928022
Train - Epoch 21, Batch: 0, Loss: 0.920243
Train - Epoch 21, Batch: 10, Loss: 0.930659
Train - Epoch 21, Batch: 20, Loss: 0.928554
Train - Epoch 21, Batch: 30, Loss: 0.912960
Train - Epoch 22, Batch: 0, Loss: 0.921914
Train - Epoch 22, Batch: 10, Loss: 0.912835
Train - Epoch 22, Batch: 20, Loss: 0.926254
Train - Epoch 22, Batch: 30, Loss: 0.919902
Train - Epoch 23, Batch: 0, Loss: 0.913933
Train - Epoch 23, Batch: 10, Loss: 0.912072
Train - Epoch 23, Batch: 20, Loss: 0.922541
Train - Epoch 23, Batch: 30, Loss: 0.915906
Train - Epoch 24, Batch: 0, Loss: 0.916619
Train - Epoch 24, Batch: 10, Loss: 0.907364
Train - Epoch 24, Batch: 20, Loss: 0.905513
Train - Epoch 24, Batch: 30, Loss: 0.905299
Train - Epoch 25, Batch: 0, Loss: 0.913611
Train - Epoch 25, Batch: 10, Loss: 0.909824
Train - Epoch 25, Batch: 20, Loss: 0.903848
Train - Epoch 25, Batch: 30, Loss: 0.906505
Train - Epoch 26, Batch: 0, Loss: 0.911449
Train - Epoch 26, Batch: 10, Loss: 0.904198
Train - Epoch 26, Batch: 20, Loss: 0.899104
Train - Epoch 26, Batch: 30, Loss: 0.896906
Train - Epoch 27, Batch: 0, Loss: 0.903628
Train - Epoch 27, Batch: 10, Loss: 0.904807
Train - Epoch 27, Batch: 20, Loss: 0.909792
Train - Epoch 27, Batch: 30, Loss: 0.907405
Train - Epoch 28, Batch: 0, Loss: 0.901228
Train - Epoch 28, Batch: 10, Loss: 0.892769
Train - Epoch 28, Batch: 20, Loss: 0.906106
Train - Epoch 28, Batch: 30, Loss: 0.897825
Train - Epoch 29, Batch: 0, Loss: 0.895319
Train - Epoch 29, Batch: 10, Loss: 0.900049
Train - Epoch 29, Batch: 20, Loss: 0.895764
Train - Epoch 29, Batch: 30, Loss: 0.897064
Train - Epoch 30, Batch: 0, Loss: 0.914087
Train - Epoch 30, Batch: 10, Loss: 0.895052
Train - Epoch 30, Batch: 20, Loss: 0.897725
Train - Epoch 30, Batch: 30, Loss: 0.901611
Train - Epoch 31, Batch: 0, Loss: 0.895377
Train - Epoch 31, Batch: 10, Loss: 0.900171
Train - Epoch 31, Batch: 20, Loss: 0.893746
Train - Epoch 31, Batch: 30, Loss: 0.887040
Test Avg. Loss: 0.000070, Accuracy: 0.630390
training_time:: 3.423680305480957
training time full:: 3.42374324798584
provenance prepare time:: 7.3909759521484375e-06
Test Avg. Loss: 0.000070, Accuracy: 0.630390
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
batch_size:: 16384
epoch:: 0
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 3.6264145374298096
overhead:: 0
overhead2:: 1.7518634796142578
overhead3:: 0
time_baseline:: 3.6272425651550293
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630441
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.017429828643798828
overhead3:: 0.05145716667175293
overhead4:: 0.2163529396057129
overhead5:: 0
memory usage:: 3766321152
time_provenance:: 1.133143424987793
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630441
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.020342588424682617
overhead3:: 0.05425620079040527
overhead4:: 0.24364972114562988
overhead5:: 0
memory usage:: 3767029760
time_provenance:: 1.1622633934020996
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630441
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.0230710506439209
overhead3:: 0.060486555099487305
overhead4:: 0.26654767990112305
overhead5:: 0
memory usage:: 3769737216
time_provenance:: 1.1936371326446533
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630441
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.02857375144958496
overhead3:: 0.0660254955291748
overhead4:: 0.30925559997558594
overhead5:: 0
memory usage:: 3766550528
time_provenance:: 1.3237650394439697
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630441
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.027773141860961914
overhead3:: 0.05785179138183594
overhead4:: 0.3289504051208496
overhead5:: 0
memory usage:: 3769032704
time_provenance:: 1.2449190616607666
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630441
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.03068399429321289
overhead3:: 0.060567378997802734
overhead4:: 0.39789867401123047
overhead5:: 0
memory usage:: 3799109632
time_provenance:: 1.344557762145996
curr_diff: 0 tensor(7.9338e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.9338e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630441
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.03456902503967285
overhead3:: 0.06950592994689941
overhead4:: 0.436511754989624
overhead5:: 0
memory usage:: 3775877120
time_provenance:: 1.3896167278289795
curr_diff: 0 tensor(8.2320e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.2320e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630441
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.03858470916748047
overhead3:: 0.07856512069702148
overhead4:: 0.4510526657104492
overhead5:: 0
memory usage:: 3772624896
time_provenance:: 1.4583795070648193
curr_diff: 0 tensor(8.5984e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.5984e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630441
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.04270362854003906
overhead3:: 0.07994937896728516
overhead4:: 0.536064863204956
overhead5:: 0
memory usage:: 3762708480
time_provenance:: 1.5316073894500732
curr_diff: 0 tensor(8.6308e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.6308e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630441
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.04064488410949707
overhead3:: 0.08572530746459961
overhead4:: 0.5254173278808594
overhead5:: 0
memory usage:: 3800657920
time_provenance:: 1.4938979148864746
curr_diff: 0 tensor(8.6099e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.6099e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630441
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.05896639823913574
overhead3:: 0.10033917427062988
overhead4:: 0.8046116828918457
overhead5:: 0
memory usage:: 3788435456
time_provenance:: 1.8116834163665771
curr_diff: 0 tensor(2.8670e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.8670e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630441
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.060575246810913086
overhead3:: 0.10334110260009766
overhead4:: 0.816925048828125
overhead5:: 0
memory usage:: 3763978240
time_provenance:: 1.828357219696045
curr_diff: 0 tensor(2.9890e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.9890e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630441
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.07158613204956055
overhead3:: 0.13115811347961426
overhead4:: 0.9584934711456299
overhead5:: 0
memory usage:: 3764707328
time_provenance:: 2.1414923667907715
curr_diff: 0 tensor(3.0590e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.0590e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630441
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.06491541862487793
overhead3:: 0.1094675064086914
overhead4:: 0.8733317852020264
overhead5:: 0
memory usage:: 3769683968
time_provenance:: 1.901498556137085
curr_diff: 0 tensor(3.0676e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.0676e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630441
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.07010889053344727
overhead3:: 0.11011934280395508
overhead4:: 0.8315019607543945
overhead5:: 0
memory usage:: 3784957952
time_provenance:: 1.8607983589172363
curr_diff: 0 tensor(3.0602e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.0602e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630441
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.14006257057189941
overhead3:: 0.21849536895751953
overhead4:: 1.901548147201538
overhead5:: 0
memory usage:: 3805159424
time_provenance:: 3.108928918838501
curr_diff: 0 tensor(6.4333e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.4333e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630441
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.18482255935668945
overhead3:: 0.2650785446166992
overhead4:: 2.4315249919891357
overhead5:: 0
memory usage:: 3769004032
time_provenance:: 3.890590190887451
curr_diff: 0 tensor(6.6718e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.6718e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630441
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.14580774307250977
overhead3:: 0.2206120491027832
overhead4:: 1.9351484775543213
overhead5:: 0
memory usage:: 3769413632
time_provenance:: 3.1543004512786865
curr_diff: 0 tensor(6.9470e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.9470e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630441
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.15266966819763184
overhead3:: 0.24721956253051758
overhead4:: 1.9791765213012695
overhead5:: 0
memory usage:: 3790315520
time_provenance:: 3.22269868850708
curr_diff: 0 tensor(6.9452e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.9452e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630441
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.15055060386657715
overhead3:: 0.2443079948425293
overhead4:: 1.8955585956573486
overhead5:: 0
memory usage:: 3782963200
time_provenance:: 3.1067116260528564
curr_diff: 0 tensor(6.9670e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.9670e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630441
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.2891409397125244
overhead3:: 0.3990051746368408
overhead4:: 3.2282004356384277
overhead5:: 0
memory usage:: 3755307008
time_provenance:: 4.336011171340942
curr_diff: 0 tensor(9.9462e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.9462e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630441
repetition 4
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 4 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 1.875167
Train - Epoch 0, Batch: 10, Loss: 1.372345
Train - Epoch 0, Batch: 20, Loss: 1.238046
Train - Epoch 0, Batch: 30, Loss: 1.185577
Train - Epoch 1, Batch: 0, Loss: 1.179276
Train - Epoch 1, Batch: 10, Loss: 1.165868
Train - Epoch 1, Batch: 20, Loss: 1.140655
Train - Epoch 1, Batch: 30, Loss: 1.132917
Train - Epoch 2, Batch: 0, Loss: 1.141654
Train - Epoch 2, Batch: 10, Loss: 1.123171
Train - Epoch 2, Batch: 20, Loss: 1.110011
Train - Epoch 2, Batch: 30, Loss: 1.105261
Train - Epoch 3, Batch: 0, Loss: 1.091360
Train - Epoch 3, Batch: 10, Loss: 1.083976
Train - Epoch 3, Batch: 20, Loss: 1.082878
Train - Epoch 3, Batch: 30, Loss: 1.080542
Train - Epoch 4, Batch: 0, Loss: 1.085337
Train - Epoch 4, Batch: 10, Loss: 1.066580
Train - Epoch 4, Batch: 20, Loss: 1.054689
Train - Epoch 4, Batch: 30, Loss: 1.051532
Train - Epoch 5, Batch: 0, Loss: 1.050739
Train - Epoch 5, Batch: 10, Loss: 1.041789
Train - Epoch 5, Batch: 20, Loss: 1.050022
Train - Epoch 5, Batch: 30, Loss: 1.034633
Train - Epoch 6, Batch: 0, Loss: 1.034377
Train - Epoch 6, Batch: 10, Loss: 1.033729
Train - Epoch 6, Batch: 20, Loss: 1.025241
Train - Epoch 6, Batch: 30, Loss: 1.023250
Train - Epoch 7, Batch: 0, Loss: 1.019820
Train - Epoch 7, Batch: 10, Loss: 1.013854
Train - Epoch 7, Batch: 20, Loss: 1.011716
Train - Epoch 7, Batch: 30, Loss: 1.001144
Train - Epoch 8, Batch: 0, Loss: 1.006997
Train - Epoch 8, Batch: 10, Loss: 1.003551
Train - Epoch 8, Batch: 20, Loss: 0.999633
Train - Epoch 8, Batch: 30, Loss: 0.995823
Train - Epoch 9, Batch: 0, Loss: 0.991185
Train - Epoch 9, Batch: 10, Loss: 0.989073
Train - Epoch 9, Batch: 20, Loss: 0.990629
Train - Epoch 9, Batch: 30, Loss: 0.985770
Train - Epoch 10, Batch: 0, Loss: 0.997467
Train - Epoch 10, Batch: 10, Loss: 0.985442
Train - Epoch 10, Batch: 20, Loss: 0.980344
Train - Epoch 10, Batch: 30, Loss: 0.981887
Train - Epoch 11, Batch: 0, Loss: 0.975499
Train - Epoch 11, Batch: 10, Loss: 0.967352
Train - Epoch 11, Batch: 20, Loss: 0.977424
Train - Epoch 11, Batch: 30, Loss: 0.973161
Train - Epoch 12, Batch: 0, Loss: 0.978100
Train - Epoch 12, Batch: 10, Loss: 0.972523
Train - Epoch 12, Batch: 20, Loss: 0.968071
Train - Epoch 12, Batch: 30, Loss: 0.957120
Train - Epoch 13, Batch: 0, Loss: 0.962156
Train - Epoch 13, Batch: 10, Loss: 0.957381
Train - Epoch 13, Batch: 20, Loss: 0.958438
Train - Epoch 13, Batch: 30, Loss: 0.959647
Train - Epoch 14, Batch: 0, Loss: 0.960626
Train - Epoch 14, Batch: 10, Loss: 0.954108
Train - Epoch 14, Batch: 20, Loss: 0.953632
Train - Epoch 14, Batch: 30, Loss: 0.949036
Train - Epoch 15, Batch: 0, Loss: 0.953829
Train - Epoch 15, Batch: 10, Loss: 0.952162
Train - Epoch 15, Batch: 20, Loss: 0.947007
Train - Epoch 15, Batch: 30, Loss: 0.947561
Train - Epoch 16, Batch: 0, Loss: 0.937943
Train - Epoch 16, Batch: 10, Loss: 0.938035
Train - Epoch 16, Batch: 20, Loss: 0.949245
Train - Epoch 16, Batch: 30, Loss: 0.946294
Train - Epoch 17, Batch: 0, Loss: 0.940557
Train - Epoch 17, Batch: 10, Loss: 0.942621
Train - Epoch 17, Batch: 20, Loss: 0.945891
Train - Epoch 17, Batch: 30, Loss: 0.937046
Train - Epoch 18, Batch: 0, Loss: 0.938241
Train - Epoch 18, Batch: 10, Loss: 0.935384
Train - Epoch 18, Batch: 20, Loss: 0.930795
Train - Epoch 18, Batch: 30, Loss: 0.934581
Train - Epoch 19, Batch: 0, Loss: 0.929871
Train - Epoch 19, Batch: 10, Loss: 0.936901
Train - Epoch 19, Batch: 20, Loss: 0.921793
Train - Epoch 19, Batch: 30, Loss: 0.922121
Train - Epoch 20, Batch: 0, Loss: 0.929787
Train - Epoch 20, Batch: 10, Loss: 0.919162
Train - Epoch 20, Batch: 20, Loss: 0.923573
Train - Epoch 20, Batch: 30, Loss: 0.913514
Train - Epoch 21, Batch: 0, Loss: 0.925853
Train - Epoch 21, Batch: 10, Loss: 0.931620
Train - Epoch 21, Batch: 20, Loss: 0.922244
Train - Epoch 21, Batch: 30, Loss: 0.915582
Train - Epoch 22, Batch: 0, Loss: 0.917206
Train - Epoch 22, Batch: 10, Loss: 0.920572
Train - Epoch 22, Batch: 20, Loss: 0.911553
Train - Epoch 22, Batch: 30, Loss: 0.919563
Train - Epoch 23, Batch: 0, Loss: 0.912369
Train - Epoch 23, Batch: 10, Loss: 0.917450
Train - Epoch 23, Batch: 20, Loss: 0.915545
Train - Epoch 23, Batch: 30, Loss: 0.910157
Train - Epoch 24, Batch: 0, Loss: 0.924861
Train - Epoch 24, Batch: 10, Loss: 0.898405
Train - Epoch 24, Batch: 20, Loss: 0.912127
Train - Epoch 24, Batch: 30, Loss: 0.909470
Train - Epoch 25, Batch: 0, Loss: 0.903341
Train - Epoch 25, Batch: 10, Loss: 0.898426
Train - Epoch 25, Batch: 20, Loss: 0.908522
Train - Epoch 25, Batch: 30, Loss: 0.911845
Train - Epoch 26, Batch: 0, Loss: 0.896372
Train - Epoch 26, Batch: 10, Loss: 0.914706
Train - Epoch 26, Batch: 20, Loss: 0.903628
Train - Epoch 26, Batch: 30, Loss: 0.907023
Train - Epoch 27, Batch: 0, Loss: 0.894713
Train - Epoch 27, Batch: 10, Loss: 0.899862
Train - Epoch 27, Batch: 20, Loss: 0.902135
Train - Epoch 27, Batch: 30, Loss: 0.907235
Train - Epoch 28, Batch: 0, Loss: 0.900022
Train - Epoch 28, Batch: 10, Loss: 0.898716
Train - Epoch 28, Batch: 20, Loss: 0.899443
Train - Epoch 28, Batch: 30, Loss: 0.901861
Train - Epoch 29, Batch: 0, Loss: 0.901730
Train - Epoch 29, Batch: 10, Loss: 0.903669
Train - Epoch 29, Batch: 20, Loss: 0.904860
Train - Epoch 29, Batch: 30, Loss: 0.896926
Train - Epoch 30, Batch: 0, Loss: 0.895163
Train - Epoch 30, Batch: 10, Loss: 0.897385
Train - Epoch 30, Batch: 20, Loss: 0.904779
Train - Epoch 30, Batch: 30, Loss: 0.904531
Train - Epoch 31, Batch: 0, Loss: 0.894688
Train - Epoch 31, Batch: 10, Loss: 0.890395
Train - Epoch 31, Batch: 20, Loss: 0.893729
Train - Epoch 31, Batch: 30, Loss: 0.900162
Test Avg. Loss: 0.000070, Accuracy: 0.629959
training_time:: 3.258486032485962
training time full:: 3.258552312850952
provenance prepare time:: 6.4373016357421875e-06
Test Avg. Loss: 0.000070, Accuracy: 0.629959
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
batch_size:: 16384
epoch:: 0
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 3.734335422515869
overhead:: 0
overhead2:: 1.7296912670135498
overhead3:: 0
time_baseline:: 3.7352395057678223
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630028
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.01806473731994629
overhead3:: 0.04693198204040527
overhead4:: 0.21888160705566406
overhead5:: 0
memory usage:: 3783131136
time_provenance:: 1.1342718601226807
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630028
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.03043222427368164
overhead3:: 0.044989585876464844
overhead4:: 0.24796533584594727
overhead5:: 0
memory usage:: 3799212032
time_provenance:: 1.1760354042053223
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630028
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.02237725257873535
overhead3:: 0.05896496772766113
overhead4:: 0.2706625461578369
overhead5:: 0
memory usage:: 3766603776
time_provenance:: 1.1982653141021729
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630028
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.03234267234802246
overhead3:: 0.05564141273498535
overhead4:: 0.3157963752746582
overhead5:: 0
memory usage:: 3764641792
time_provenance:: 1.275866985321045
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630028
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.028519868850708008
overhead3:: 0.055925846099853516
overhead4:: 0.3282642364501953
overhead5:: 0
memory usage:: 3791175680
time_provenance:: 1.2914845943450928
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630028
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.03281235694885254
overhead3:: 0.06522798538208008
overhead4:: 0.4208974838256836
overhead5:: 0
memory usage:: 3800793088
time_provenance:: 1.3980488777160645
curr_diff: 0 tensor(4.8663e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.8663e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630028
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.042580604553222656
overhead3:: 0.0766146183013916
overhead4:: 0.43076634407043457
overhead5:: 0
memory usage:: 3763847168
time_provenance:: 1.4392709732055664
curr_diff: 0 tensor(4.9335e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.9335e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630028
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.04226279258728027
overhead3:: 0.09208440780639648
overhead4:: 0.48409485816955566
overhead5:: 0
memory usage:: 3799162880
time_provenance:: 1.525604248046875
curr_diff: 0 tensor(4.9748e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.9748e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630028
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.03921985626220703
overhead3:: 0.0924525260925293
overhead4:: 0.5075242519378662
overhead5:: 0
memory usage:: 3769180160
time_provenance:: 1.5168836116790771
curr_diff: 0 tensor(4.9737e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.9737e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630028
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.04115700721740723
overhead3:: 0.09762120246887207
overhead4:: 0.5088942050933838
overhead5:: 0
memory usage:: 3776524288
time_provenance:: 1.4906673431396484
curr_diff: 0 tensor(4.9747e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.9747e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630028
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.058061838150024414
overhead3:: 0.09928441047668457
overhead4:: 0.7971289157867432
overhead5:: 0
memory usage:: 3784601600
time_provenance:: 1.8362948894500732
curr_diff: 0 tensor(2.0920e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.0920e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630028
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.05962061882019043
overhead3:: 0.11634016036987305
overhead4:: 0.8141188621520996
overhead5:: 0
memory usage:: 3765018624
time_provenance:: 1.8345184326171875
curr_diff: 0 tensor(2.1084e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1084e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630028
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.06339836120605469
overhead3:: 0.10831856727600098
overhead4:: 0.8531222343444824
overhead5:: 0
memory usage:: 3763367936
time_provenance:: 1.8758790493011475
curr_diff: 0 tensor(2.1316e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1316e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630028
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.0673987865447998
overhead3:: 0.1104118824005127
overhead4:: 0.8470432758331299
overhead5:: 0
memory usage:: 3770851328
time_provenance:: 1.8756628036499023
curr_diff: 0 tensor(2.1214e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1214e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630028
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.07226800918579102
overhead3:: 0.11650514602661133
overhead4:: 0.9119136333465576
overhead5:: 0
memory usage:: 3791142912
time_provenance:: 1.9771957397460938
curr_diff: 0 tensor(2.1366e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1366e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630028
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.16508769989013672
overhead3:: 0.25893139839172363
overhead4:: 1.7900164127349854
overhead5:: 0
memory usage:: 3769839616
time_provenance:: 3.0553040504455566
curr_diff: 0 tensor(5.5090e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.5090e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630028
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.20604848861694336
overhead3:: 0.2891123294830322
overhead4:: 2.705826997756958
overhead5:: 0
memory usage:: 3765280768
time_provenance:: 4.3127968311309814
curr_diff: 0 tensor(5.5257e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.5257e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630028
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.14252686500549316
overhead3:: 0.2199850082397461
overhead4:: 1.9300668239593506
overhead5:: 0
memory usage:: 3790426112
time_provenance:: 3.1393332481384277
curr_diff: 0 tensor(5.5670e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.5670e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630028
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.14719772338867188
overhead3:: 0.23173856735229492
overhead4:: 1.9768621921539307
overhead5:: 0
memory usage:: 3775848448
time_provenance:: 3.2002336978912354
curr_diff: 0 tensor(5.5475e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.5475e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630028
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.1492302417755127
overhead3:: 0.24026870727539062
overhead4:: 1.9675548076629639
overhead5:: 0
memory usage:: 3762651136
time_provenance:: 3.1888606548309326
curr_diff: 0 tensor(5.5867e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.5867e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630028
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 261
max_epoch:: 32
overhead:: 0
overhead2:: 0.3104722499847412
overhead3:: 0.46506261825561523
overhead4:: 3.4307854175567627
overhead5:: 0
memory usage:: 3754147840
time_provenance:: 4.594613790512085
curr_diff: 0 tensor(1.0073e-13, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0073e-13, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630028
deletion rate:: 0.001
python3 generate_rand_ids 0.001  covtype 0
tensor([145415, 498696, 239624, 510986, 223244, 249870, 330768, 466961, 417808,
        133140, 207894, 514071,  33819, 226332, 120863, 419871, 281642, 260149,
        201783, 162872, 163897, 317501,  80957, 483390, 386111, 280641, 442433,
        407618, 200776,  15435, 483403, 400459, 261198, 158799, 300111, 270417,
        207954, 241749, 161879, 427096, 490585, 384090, 147548, 151647, 171104,
         67680,  34914, 281703, 150632, 383081, 445546, 232553, 412781,  85102,
        343157, 161912, 361599, 274562, 480388, 126087, 141450, 307341, 316558,
        236686, 301201, 261267, 209046, 223383,  15512, 298139,  99483,   3228,
        507038, 175263, 185504, 188580, 272548,  32934, 421030, 176294, 315562,
        144558, 108718, 364718, 296113, 211120,  27830, 187574, 242871, 148662,
        328891, 255167, 280769, 470212, 425157, 266441, 424137, 117964, 463053,
        474319, 390351, 177362, 102610, 223444, 261337,  26844, 397534, 491743,
        127205, 119014, 314600, 388330, 303340, 212204,  55534,  73964, 260336,
        460017, 355570, 324851, 321780, 207096, 269560,  10492, 504062, 300286,
        152832, 477442, 227586, 250116, 277762,   7430, 160007, 177415,  85260,
        413970,  45331, 298266, 301339, 409891, 284965,  65832, 418089, 164139,
        287019, 411949, 161073, 386354, 218422, 312631, 169272,  79161, 478523,
        291131, 455996, 386369, 388422, 320843, 238923, 406860, 334159, 285010,
        398676, 375125, 428372, 345434,   1373, 372064,  13670,  84328, 389480,
         43370,  40302,  16754, 310643, 236915, 195961, 277884, 459132, 358787,
        136580, 393609,  22925,  23952,  76176, 312723, 416158,  91551, 157088,
         29088, 469407,  68003, 254368, 465317, 192934, 303526, 425381, 371109,
        240046, 500142, 464304, 133551, 141747, 492988,  36284, 483774,  51651,
        497092, 273866, 171468, 405965, 477649, 156118, 191959, 452056, 127448,
        273880, 288218, 168415, 264672,  88551, 279019, 398829, 260590,  69103,
        301554, 303602, 349683,  35316,  81398, 216566, 100857, 333306, 136699,
        349692, 493053, 202234, 121338, 459257, 497146, 112125,  84483,  89599,
         37382, 448007,    521,  81418, 361995, 392716, 246283,  35345,  65041,
        154130,  33300,  12824, 301595, 229917, 322077, 158239, 380445,  77344,
        138788, 243237, 218665, 130602,  89643, 218675, 412211, 292405, 128565,
        514613,  41526,  70201, 469568, 366146,  52803, 204357,  49734, 171591,
         60999,  74313,   3656, 311887, 446032,  14930,  74322, 332372,  46674,
        307798,  39506, 183898, 370270,  70240, 208481, 272996, 245348, 406118,
          2665, 487017, 204400, 332403, 266867, 351861, 379510, 301686, 127608,
         25206, 332410, 503424,  66177, 223877, 334471, 497287,  40587, 215691,
        140943, 312977,  42645,  13974, 216726, 327320, 233113, 386713, 263836,
         74397, 320156, 521887, 473761, 162466,  23210, 144044,  65196, 216748,
        161455, 144046, 408243, 393909, 103094,  89781, 138934, 210615, 114362,
        341691, 457404, 420538, 246465, 261827, 126661, 114373, 496331, 113358,
        307925, 160470, 298714, 341723, 398053, 150247, 272104, 488168, 509673,
        448235, 104175, 505584,  55024,  89841, 196339, 339700, 367347, 363256,
        507641, 426746, 505595, 374524, 103165,  40702, 485119, 119554,  12035,
        270083, 211716, 326404, 149255, 321282, 220933,  18183, 224014,  40719,
        196368,  99089,  18194, 214805, 364309, 475928, 432921, 205593, 309018,
        164636, 199453, 226078, 305950, 165663, 346916, 220966, 102183, 509736,
        506665, 471849, 419626, 365355, 381741, 193326, 508719, 157486, 438057,
        170805,  29496, 199482, 452411, 384827,  49983, 431936, 199487, 208705,
        357187, 121667,  55107, 188229, 208710, 365383, 154439, 158538, 505675,
        130902, 329559, 137048, 394070, 166743, 447323, 393048, 477021, 393047,
        252765, 328545, 291685, 289640, 132972,   3955, 316281, 263034, 244603,
        226170,  19325, 521083, 137087, 201599, 252801, 362377,  26507, 522123,
         25484, 509840, 181137, 508820, 131990, 354198, 250777, 361370, 203675,
        509852, 507804, 163742, 236447, 379806, 493473, 233381, 171942, 389029,
        160678, 446377, 147370,  98222, 338864, 456625, 149427, 507829, 337845,
         37814, 134079,  67520,    961, 278466, 484290, 204736, 431041, 330692,
        277444, 456650, 479180, 360397, 466894,  60367, 470992, 350161, 124880,
        329681, 337877, 255960, 173019, 380892, 255964,  93150, 324571, 411616,
        477156, 486373,  63461,  72681,  87021, 367601, 112629, 364534, 330745])
batch size:: 16384
repetition 0
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 0 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 2.074128
Train - Epoch 0, Batch: 10, Loss: 1.421559
Train - Epoch 0, Batch: 20, Loss: 1.268527
Train - Epoch 0, Batch: 30, Loss: 1.213768
Train - Epoch 1, Batch: 0, Loss: 1.196812
Train - Epoch 1, Batch: 10, Loss: 1.176625
Train - Epoch 1, Batch: 20, Loss: 1.147856
Train - Epoch 1, Batch: 30, Loss: 1.137506
Train - Epoch 2, Batch: 0, Loss: 1.149170
Train - Epoch 2, Batch: 10, Loss: 1.116914
Train - Epoch 2, Batch: 20, Loss: 1.128163
Train - Epoch 2, Batch: 30, Loss: 1.103974
Train - Epoch 3, Batch: 0, Loss: 1.106681
Train - Epoch 3, Batch: 10, Loss: 1.096683
Train - Epoch 3, Batch: 20, Loss: 1.092618
Train - Epoch 3, Batch: 30, Loss: 1.074226
Train - Epoch 4, Batch: 0, Loss: 1.082183
Train - Epoch 4, Batch: 10, Loss: 1.076126
Train - Epoch 4, Batch: 20, Loss: 1.069712
Train - Epoch 4, Batch: 30, Loss: 1.054762
Train - Epoch 5, Batch: 0, Loss: 1.066669
Train - Epoch 5, Batch: 10, Loss: 1.050020
Train - Epoch 5, Batch: 20, Loss: 1.048176
Train - Epoch 5, Batch: 30, Loss: 1.037286
Train - Epoch 6, Batch: 0, Loss: 1.040279
Train - Epoch 6, Batch: 10, Loss: 1.027945
Train - Epoch 6, Batch: 20, Loss: 1.022302
Train - Epoch 6, Batch: 30, Loss: 1.019958
Train - Epoch 7, Batch: 0, Loss: 1.023907
Train - Epoch 7, Batch: 10, Loss: 1.019737
Train - Epoch 7, Batch: 20, Loss: 1.024274
Train - Epoch 7, Batch: 30, Loss: 1.008002
Train - Epoch 8, Batch: 0, Loss: 1.010602
Train - Epoch 8, Batch: 10, Loss: 1.002237
Train - Epoch 8, Batch: 20, Loss: 1.012190
Train - Epoch 8, Batch: 30, Loss: 0.996082
Train - Epoch 9, Batch: 0, Loss: 1.002109
Train - Epoch 9, Batch: 10, Loss: 1.006794
Train - Epoch 9, Batch: 20, Loss: 0.997781
Train - Epoch 9, Batch: 30, Loss: 0.987335
Train - Epoch 10, Batch: 0, Loss: 0.981089
Train - Epoch 10, Batch: 10, Loss: 0.980345
Train - Epoch 10, Batch: 20, Loss: 0.996292
Train - Epoch 10, Batch: 30, Loss: 0.995578
Train - Epoch 11, Batch: 0, Loss: 0.974596
Train - Epoch 11, Batch: 10, Loss: 0.982768
Train - Epoch 11, Batch: 20, Loss: 0.974402
Train - Epoch 11, Batch: 30, Loss: 0.974740
Train - Epoch 12, Batch: 0, Loss: 0.963466
Train - Epoch 12, Batch: 10, Loss: 0.962545
Train - Epoch 12, Batch: 20, Loss: 0.965801
Train - Epoch 12, Batch: 30, Loss: 0.964434
Train - Epoch 13, Batch: 0, Loss: 0.959062
Train - Epoch 13, Batch: 10, Loss: 0.972119
Train - Epoch 13, Batch: 20, Loss: 0.959656
Train - Epoch 13, Batch: 30, Loss: 0.962815
Train - Epoch 14, Batch: 0, Loss: 0.959288
Train - Epoch 14, Batch: 10, Loss: 0.965050
Train - Epoch 14, Batch: 20, Loss: 0.948473
Train - Epoch 14, Batch: 30, Loss: 0.954498
Train - Epoch 15, Batch: 0, Loss: 0.953399
Train - Epoch 15, Batch: 10, Loss: 0.948941
Train - Epoch 15, Batch: 20, Loss: 0.943914
Train - Epoch 15, Batch: 30, Loss: 0.955087
Train - Epoch 16, Batch: 0, Loss: 0.942140
Train - Epoch 16, Batch: 10, Loss: 0.947498
Train - Epoch 16, Batch: 20, Loss: 0.946260
Train - Epoch 16, Batch: 30, Loss: 0.939213
Train - Epoch 17, Batch: 0, Loss: 0.947580
Train - Epoch 17, Batch: 10, Loss: 0.932464
Train - Epoch 17, Batch: 20, Loss: 0.942345
Train - Epoch 17, Batch: 30, Loss: 0.936052
Train - Epoch 18, Batch: 0, Loss: 0.930058
Train - Epoch 18, Batch: 10, Loss: 0.936430
Train - Epoch 18, Batch: 20, Loss: 0.935909
Train - Epoch 18, Batch: 30, Loss: 0.931730
Train - Epoch 19, Batch: 0, Loss: 0.936334
Train - Epoch 19, Batch: 10, Loss: 0.931267
Train - Epoch 19, Batch: 20, Loss: 0.925479
Train - Epoch 19, Batch: 30, Loss: 0.930631
Train - Epoch 20, Batch: 0, Loss: 0.926464
Train - Epoch 20, Batch: 10, Loss: 0.933441
Train - Epoch 20, Batch: 20, Loss: 0.921207
Train - Epoch 20, Batch: 30, Loss: 0.924287
Train - Epoch 21, Batch: 0, Loss: 0.921608
Train - Epoch 21, Batch: 10, Loss: 0.926879
Train - Epoch 21, Batch: 20, Loss: 0.922058
Train - Epoch 21, Batch: 30, Loss: 0.926047
Train - Epoch 22, Batch: 0, Loss: 0.922790
Train - Epoch 22, Batch: 10, Loss: 0.918231
Train - Epoch 22, Batch: 20, Loss: 0.926275
Train - Epoch 22, Batch: 30, Loss: 0.922244
Train - Epoch 23, Batch: 0, Loss: 0.913857
Train - Epoch 23, Batch: 10, Loss: 0.924521
Train - Epoch 23, Batch: 20, Loss: 0.921572
Train - Epoch 23, Batch: 30, Loss: 0.909883
Train - Epoch 24, Batch: 0, Loss: 0.916727
Train - Epoch 24, Batch: 10, Loss: 0.911580
Train - Epoch 24, Batch: 20, Loss: 0.916850
Train - Epoch 24, Batch: 30, Loss: 0.918678
Train - Epoch 25, Batch: 0, Loss: 0.913459
Train - Epoch 25, Batch: 10, Loss: 0.911203
Train - Epoch 25, Batch: 20, Loss: 0.900490
Train - Epoch 25, Batch: 30, Loss: 0.914378
Train - Epoch 26, Batch: 0, Loss: 0.911826
Train - Epoch 26, Batch: 10, Loss: 0.905544
Train - Epoch 26, Batch: 20, Loss: 0.915522
Train - Epoch 26, Batch: 30, Loss: 0.906613
Train - Epoch 27, Batch: 0, Loss: 0.897780
Train - Epoch 27, Batch: 10, Loss: 0.913478
Train - Epoch 27, Batch: 20, Loss: 0.901903
Train - Epoch 27, Batch: 30, Loss: 0.902494
Train - Epoch 28, Batch: 0, Loss: 0.896596
Train - Epoch 28, Batch: 10, Loss: 0.898529
Train - Epoch 28, Batch: 20, Loss: 0.897790
Train - Epoch 28, Batch: 30, Loss: 0.902318
Train - Epoch 29, Batch: 0, Loss: 0.901059
Train - Epoch 29, Batch: 10, Loss: 0.903053
Train - Epoch 29, Batch: 20, Loss: 0.899184
Train - Epoch 29, Batch: 30, Loss: 0.893703
Train - Epoch 30, Batch: 0, Loss: 0.887912
Train - Epoch 30, Batch: 10, Loss: 0.893611
Train - Epoch 30, Batch: 20, Loss: 0.890140
Train - Epoch 30, Batch: 30, Loss: 0.902072
Train - Epoch 31, Batch: 0, Loss: 0.885846
Train - Epoch 31, Batch: 10, Loss: 0.900241
Train - Epoch 31, Batch: 20, Loss: 0.886942
Train - Epoch 31, Batch: 30, Loss: 0.891850
Test Avg. Loss: 0.000070, Accuracy: 0.629305
training_time:: 3.63926100730896
training time full:: 3.639324188232422
provenance prepare time:: 6.198883056640625e-06
Test Avg. Loss: 0.000070, Accuracy: 0.629305
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
batch_size:: 16384
epoch:: 0
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 3.5478780269622803
overhead:: 0
overhead2:: 1.7534873485565186
overhead3:: 0
time_baseline:: 3.54868745803833
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629322
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.01771998405456543
overhead3:: 0.04437255859375
overhead4:: 0.209730863571167
overhead5:: 0
memory usage:: 3782041600
time_provenance:: 1.1256768703460693
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629322
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.023219585418701172
overhead3:: 0.05868172645568848
overhead4:: 0.2651815414428711
overhead5:: 0
memory usage:: 3770044416
time_provenance:: 1.3649015426635742
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629305
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.022726774215698242
overhead3:: 0.05658578872680664
overhead4:: 0.275662899017334
overhead5:: 0
memory usage:: 3764621312
time_provenance:: 1.184804916381836
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629322
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.02526712417602539
overhead3:: 0.06719613075256348
overhead4:: 0.3061501979827881
overhead5:: 0
memory usage:: 3780685824
time_provenance:: 1.2523627281188965
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629322
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.028471946716308594
overhead3:: 0.07363390922546387
overhead4:: 0.35155606269836426
overhead5:: 0
memory usage:: 3763695616
time_provenance:: 1.3430087566375732
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629322
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.03220844268798828
overhead3:: 0.07099556922912598
overhead4:: 0.39746594429016113
overhead5:: 0
memory usage:: 3762294784
time_provenance:: 1.3653724193572998
curr_diff: 0 tensor(8.7361e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.7361e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629322
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.04182553291320801
overhead3:: 0.07624006271362305
overhead4:: 0.4204981327056885
overhead5:: 0
memory usage:: 3782533120
time_provenance:: 1.3917582035064697
curr_diff: 0 tensor(8.9348e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.9348e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629322
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.036788225173950195
overhead3:: 0.0692746639251709
overhead4:: 0.460935115814209
overhead5:: 0
memory usage:: 3779264512
time_provenance:: 1.4143798351287842
curr_diff: 0 tensor(9.0452e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.0452e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629322
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.038674116134643555
overhead3:: 0.08551406860351562
overhead4:: 0.4879448413848877
overhead5:: 0
memory usage:: 3790921728
time_provenance:: 1.475038766860962
curr_diff: 0 tensor(9.3156e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.3156e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629322
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.042034149169921875
overhead3:: 0.08731722831726074
overhead4:: 0.5069336891174316
overhead5:: 0
memory usage:: 3765149696
time_provenance:: 1.4997904300689697
curr_diff: 0 tensor(9.3575e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.3575e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629322
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.06371140480041504
overhead3:: 0.09893631935119629
overhead4:: 0.7289702892303467
overhead5:: 0
memory usage:: 3769319424
time_provenance:: 1.741584300994873
curr_diff: 0 tensor(3.7979e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.7979e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629322
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.06548762321472168
overhead3:: 0.11233043670654297
overhead4:: 0.8894922733306885
overhead5:: 0
memory usage:: 3798986752
time_provenance:: 1.9936161041259766
curr_diff: 0 tensor(3.9234e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.9234e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629322
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.0636298656463623
overhead3:: 0.10907411575317383
overhead4:: 0.8491263389587402
overhead5:: 0
memory usage:: 3764948992
time_provenance:: 1.8864505290985107
curr_diff: 0 tensor(4.0634e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.0634e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629322
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.06865262985229492
overhead3:: 0.11130499839782715
overhead4:: 0.8020081520080566
overhead5:: 0
memory usage:: 3788169216
time_provenance:: 1.816096305847168
curr_diff: 0 tensor(4.1554e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.1554e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629322
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.07302737236022949
overhead3:: 0.11833739280700684
overhead4:: 0.8364238739013672
overhead5:: 0
memory usage:: 3800072192
time_provenance:: 1.8837780952453613
curr_diff: 0 tensor(4.1615e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.1615e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629322
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.14009642601013184
overhead3:: 0.22782111167907715
overhead4:: 1.9137578010559082
overhead5:: 0
memory usage:: 3805007872
time_provenance:: 3.1558916568756104
curr_diff: 0 tensor(5.9738e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.9738e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629322
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.15634369850158691
overhead3:: 0.22190093994140625
overhead4:: 1.745985984802246
overhead5:: 0
memory usage:: 3799691264
time_provenance:: 2.959169864654541
curr_diff: 0 tensor(6.0977e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.0977e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629322
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.1890714168548584
overhead3:: 0.298445463180542
overhead4:: 2.702871084213257
overhead5:: 0
memory usage:: 3781967872
time_provenance:: 4.266631126403809
curr_diff: 0 tensor(6.4728e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.4728e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629322
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.15904879570007324
overhead3:: 0.2322371006011963
overhead4:: 1.7621917724609375
overhead5:: 0
memory usage:: 3762925568
time_provenance:: 2.9873971939086914
curr_diff: 0 tensor(6.7092e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.7092e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629322
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.156602144241333
overhead3:: 0.24025964736938477
overhead4:: 1.8311026096343994
overhead5:: 0
memory usage:: 3799298048
time_provenance:: 3.062241554260254
curr_diff: 0 tensor(6.7806e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.7806e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629322
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.37503886222839355
overhead3:: 0.6246144771575928
overhead4:: 3.6976513862609863
overhead5:: 0
memory usage:: 3752570880
time_provenance:: 5.006683349609375
curr_diff: 0 tensor(9.9803e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.9803e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629322
repetition 1
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 1 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 1.911427
Train - Epoch 0, Batch: 10, Loss: 1.383267
Train - Epoch 0, Batch: 20, Loss: 1.239128
Train - Epoch 0, Batch: 30, Loss: 1.196565
Train - Epoch 1, Batch: 0, Loss: 1.179212
Train - Epoch 1, Batch: 10, Loss: 1.150133
Train - Epoch 1, Batch: 20, Loss: 1.151043
Train - Epoch 1, Batch: 30, Loss: 1.125055
Train - Epoch 2, Batch: 0, Loss: 1.119390
Train - Epoch 2, Batch: 10, Loss: 1.121424
Train - Epoch 2, Batch: 20, Loss: 1.109907
Train - Epoch 2, Batch: 30, Loss: 1.097344
Train - Epoch 3, Batch: 0, Loss: 1.109517
Train - Epoch 3, Batch: 10, Loss: 1.086682
Train - Epoch 3, Batch: 20, Loss: 1.082801
Train - Epoch 3, Batch: 30, Loss: 1.073836
Train - Epoch 4, Batch: 0, Loss: 1.079772
Train - Epoch 4, Batch: 10, Loss: 1.066635
Train - Epoch 4, Batch: 20, Loss: 1.058387
Train - Epoch 4, Batch: 30, Loss: 1.052949
Train - Epoch 5, Batch: 0, Loss: 1.045099
Train - Epoch 5, Batch: 10, Loss: 1.045937
Train - Epoch 5, Batch: 20, Loss: 1.040990
Train - Epoch 5, Batch: 30, Loss: 1.037924
Train - Epoch 6, Batch: 0, Loss: 1.046892
Train - Epoch 6, Batch: 10, Loss: 1.029373
Train - Epoch 6, Batch: 20, Loss: 1.028312
Train - Epoch 6, Batch: 30, Loss: 1.021730
Train - Epoch 7, Batch: 0, Loss: 1.017813
Train - Epoch 7, Batch: 10, Loss: 1.011517
Train - Epoch 7, Batch: 20, Loss: 1.014656
Train - Epoch 7, Batch: 30, Loss: 1.001227
Train - Epoch 8, Batch: 0, Loss: 1.010419
Train - Epoch 8, Batch: 10, Loss: 0.996319
Train - Epoch 8, Batch: 20, Loss: 1.009409
Train - Epoch 8, Batch: 30, Loss: 0.989681
Train - Epoch 9, Batch: 0, Loss: 0.991617
Train - Epoch 9, Batch: 10, Loss: 0.991421
Train - Epoch 9, Batch: 20, Loss: 0.981590
Train - Epoch 9, Batch: 30, Loss: 0.989088
Train - Epoch 10, Batch: 0, Loss: 0.988018
Train - Epoch 10, Batch: 10, Loss: 0.979782
Train - Epoch 10, Batch: 20, Loss: 0.976973
Train - Epoch 10, Batch: 30, Loss: 0.969208
Train - Epoch 11, Batch: 0, Loss: 0.978333
Train - Epoch 11, Batch: 10, Loss: 0.976783
Train - Epoch 11, Batch: 20, Loss: 0.973764
Train - Epoch 11, Batch: 30, Loss: 0.963550
Train - Epoch 12, Batch: 0, Loss: 0.976819
Train - Epoch 12, Batch: 10, Loss: 0.962928
Train - Epoch 12, Batch: 20, Loss: 0.962139
Train - Epoch 12, Batch: 30, Loss: 0.951791
Train - Epoch 13, Batch: 0, Loss: 0.959261
Train - Epoch 13, Batch: 10, Loss: 0.961931
Train - Epoch 13, Batch: 20, Loss: 0.965965
Train - Epoch 13, Batch: 30, Loss: 0.957880
Train - Epoch 14, Batch: 0, Loss: 0.950430
Train - Epoch 14, Batch: 10, Loss: 0.954622
Train - Epoch 14, Batch: 20, Loss: 0.957768
Train - Epoch 14, Batch: 30, Loss: 0.955840
Train - Epoch 15, Batch: 0, Loss: 0.950555
Train - Epoch 15, Batch: 10, Loss: 0.952653
Train - Epoch 15, Batch: 20, Loss: 0.945510
Train - Epoch 15, Batch: 30, Loss: 0.940765
Train - Epoch 16, Batch: 0, Loss: 0.947874
Train - Epoch 16, Batch: 10, Loss: 0.945835
Train - Epoch 16, Batch: 20, Loss: 0.943780
Train - Epoch 16, Batch: 30, Loss: 0.938198
Train - Epoch 17, Batch: 0, Loss: 0.938758
Train - Epoch 17, Batch: 10, Loss: 0.937304
Train - Epoch 17, Batch: 20, Loss: 0.938671
Train - Epoch 17, Batch: 30, Loss: 0.935549
Train - Epoch 18, Batch: 0, Loss: 0.935319
Train - Epoch 18, Batch: 10, Loss: 0.939728
Train - Epoch 18, Batch: 20, Loss: 0.919639
Train - Epoch 18, Batch: 30, Loss: 0.930463
Train - Epoch 19, Batch: 0, Loss: 0.934451
Train - Epoch 19, Batch: 10, Loss: 0.927512
Train - Epoch 19, Batch: 20, Loss: 0.917912
Train - Epoch 19, Batch: 30, Loss: 0.919546
Train - Epoch 20, Batch: 0, Loss: 0.928782
Train - Epoch 20, Batch: 10, Loss: 0.922486
Train - Epoch 20, Batch: 20, Loss: 0.929682
Train - Epoch 20, Batch: 30, Loss: 0.915843
Train - Epoch 21, Batch: 0, Loss: 0.923675
Train - Epoch 21, Batch: 10, Loss: 0.920665
Train - Epoch 21, Batch: 20, Loss: 0.920709
Train - Epoch 21, Batch: 30, Loss: 0.922058
Train - Epoch 22, Batch: 0, Loss: 0.911340
Train - Epoch 22, Batch: 10, Loss: 0.920544
Train - Epoch 22, Batch: 20, Loss: 0.914025
Train - Epoch 22, Batch: 30, Loss: 0.920268
Train - Epoch 23, Batch: 0, Loss: 0.924413
Train - Epoch 23, Batch: 10, Loss: 0.917859
Train - Epoch 23, Batch: 20, Loss: 0.919385
Train - Epoch 23, Batch: 30, Loss: 0.900251
Train - Epoch 24, Batch: 0, Loss: 0.910535
Train - Epoch 24, Batch: 10, Loss: 0.905572
Train - Epoch 24, Batch: 20, Loss: 0.910667
Train - Epoch 24, Batch: 30, Loss: 0.911793
Train - Epoch 25, Batch: 0, Loss: 0.911095
Train - Epoch 25, Batch: 10, Loss: 0.917696
Train - Epoch 25, Batch: 20, Loss: 0.911580
Train - Epoch 25, Batch: 30, Loss: 0.905950
Train - Epoch 26, Batch: 0, Loss: 0.912668
Train - Epoch 26, Batch: 10, Loss: 0.900004
Train - Epoch 26, Batch: 20, Loss: 0.907703
Train - Epoch 26, Batch: 30, Loss: 0.904120
Train - Epoch 27, Batch: 0, Loss: 0.905986
Train - Epoch 27, Batch: 10, Loss: 0.907160
Train - Epoch 27, Batch: 20, Loss: 0.901582
Train - Epoch 27, Batch: 30, Loss: 0.897315
Train - Epoch 28, Batch: 0, Loss: 0.907376
Train - Epoch 28, Batch: 10, Loss: 0.892937
Train - Epoch 28, Batch: 20, Loss: 0.903262
Train - Epoch 28, Batch: 30, Loss: 0.893863
Train - Epoch 29, Batch: 0, Loss: 0.901036
Train - Epoch 29, Batch: 10, Loss: 0.917164
Train - Epoch 29, Batch: 20, Loss: 0.890369
Train - Epoch 29, Batch: 30, Loss: 0.891171
Train - Epoch 30, Batch: 0, Loss: 0.900845
Train - Epoch 30, Batch: 10, Loss: 0.890832
Train - Epoch 30, Batch: 20, Loss: 0.889120
Train - Epoch 30, Batch: 30, Loss: 0.893217
Train - Epoch 31, Batch: 0, Loss: 0.887326
Train - Epoch 31, Batch: 10, Loss: 0.894179
Train - Epoch 31, Batch: 20, Loss: 0.903714
Train - Epoch 31, Batch: 30, Loss: 0.891211
Test Avg. Loss: 0.000070, Accuracy: 0.630269
training_time:: 3.558135509490967
training time full:: 3.558199882507324
provenance prepare time:: 6.9141387939453125e-06
Test Avg. Loss: 0.000070, Accuracy: 0.630269
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
batch_size:: 16384
epoch:: 0
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 3.741405487060547
overhead:: 0
overhead2:: 1.7583553791046143
overhead3:: 0
time_baseline:: 3.742217779159546
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630355
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.01792430877685547
overhead3:: 0.04376697540283203
overhead4:: 0.2142329216003418
overhead5:: 0
memory usage:: 3780075520
time_provenance:: 1.1630542278289795
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630338
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.020311594009399414
overhead3:: 0.048882246017456055
overhead4:: 0.24291515350341797
overhead5:: 0
memory usage:: 3799957504
time_provenance:: 1.1766440868377686
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630372
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.02311420440673828
overhead3:: 0.05234360694885254
overhead4:: 0.27818846702575684
overhead5:: 0
memory usage:: 3794833408
time_provenance:: 1.2091710567474365
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630338
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.02655029296875
overhead3:: 0.056497812271118164
overhead4:: 0.30829739570617676
overhead5:: 0
memory usage:: 3770396672
time_provenance:: 1.2866125106811523
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630372
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.027615785598754883
overhead3:: 0.054802656173706055
overhead4:: 0.33157873153686523
overhead5:: 0
memory usage:: 3804016640
time_provenance:: 1.256037712097168
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630338
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.03336739540100098
overhead3:: 0.08371090888977051
overhead4:: 0.39896345138549805
overhead5:: 0
memory usage:: 3766140928
time_provenance:: 1.4100525379180908
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630355
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.03435206413269043
overhead3:: 0.06697368621826172
overhead4:: 0.4225611686706543
overhead5:: 0
memory usage:: 3800371200
time_provenance:: 1.4124054908752441
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630355
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.03998613357543945
overhead3:: 0.07335352897644043
overhead4:: 0.49030280113220215
overhead5:: 0
memory usage:: 3799769088
time_provenance:: 1.5037386417388916
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630355
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.04738044738769531
overhead3:: 0.08353924751281738
overhead4:: 0.5176601409912109
overhead5:: 0
memory usage:: 3770175488
time_provenance:: 1.5539038181304932
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630355
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.0470423698425293
overhead3:: 0.08649420738220215
overhead4:: 0.5779380798339844
overhead5:: 0
memory usage:: 3783188480
time_provenance:: 1.672142505645752
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630355
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.059580087661743164
overhead3:: 0.10192656517028809
overhead4:: 0.7831659317016602
overhead5:: 0
memory usage:: 3779764224
time_provenance:: 1.822359323501587
curr_diff: 0 tensor(3.8500e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.8500e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630355
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.07006597518920898
overhead3:: 0.11243867874145508
overhead4:: 0.8748245239257812
overhead5:: 0
memory usage:: 3789783040
time_provenance:: 1.9915852546691895
curr_diff: 0 tensor(3.9456e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.9456e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630355
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.0655372142791748
overhead3:: 0.12516546249389648
overhead4:: 0.8676929473876953
overhead5:: 0
memory usage:: 3766755328
time_provenance:: 1.932816743850708
curr_diff: 0 tensor(3.9833e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.9833e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630355
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.06979751586914062
overhead3:: 0.11433148384094238
overhead4:: 0.8938016891479492
overhead5:: 0
memory usage:: 3766931456
time_provenance:: 1.9690876007080078
curr_diff: 0 tensor(4.0287e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.0287e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630355
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.0674126148223877
overhead3:: 0.12191295623779297
overhead4:: 0.8926584720611572
overhead5:: 0
memory usage:: 3766779904
time_provenance:: 1.9546637535095215
curr_diff: 0 tensor(4.1035e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.1035e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630355
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.14077472686767578
overhead3:: 0.22176480293273926
overhead4:: 1.8918561935424805
overhead5:: 0
memory usage:: 3798913024
time_provenance:: 3.1119284629821777
curr_diff: 0 tensor(7.1963e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.1963e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630355
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.14867115020751953
overhead3:: 0.23372220993041992
overhead4:: 1.947293996810913
overhead5:: 0
memory usage:: 3818029056
time_provenance:: 3.1783838272094727
curr_diff: 0 tensor(7.2169e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.2169e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630355
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.14542365074157715
overhead3:: 0.23086285591125488
overhead4:: 1.9579262733459473
overhead5:: 0
memory usage:: 3781640192
time_provenance:: 3.1866557598114014
curr_diff: 0 tensor(7.2288e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.2288e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630355
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.14468765258789062
overhead3:: 0.23024606704711914
overhead4:: 1.958777666091919
overhead5:: 0
memory usage:: 3798007808
time_provenance:: 3.1823606491088867
curr_diff: 0 tensor(7.2234e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.2234e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630355
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.15801143646240234
overhead3:: 0.23680496215820312
overhead4:: 1.946485996246338
overhead5:: 0
memory usage:: 3799883776
time_provenance:: 3.177431344985962
curr_diff: 0 tensor(7.5854e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.5854e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630355
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.30765771865844727
overhead3:: 0.45450830459594727
overhead4:: 3.347717523574829
overhead5:: 0
memory usage:: 3775934464
time_provenance:: 4.508498430252075
curr_diff: 0 tensor(1.0304e-13, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0304e-13, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1., dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630355
repetition 2
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 2 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 2.055456
Train - Epoch 0, Batch: 10, Loss: 1.410964
Train - Epoch 0, Batch: 20, Loss: 1.260399
Train - Epoch 0, Batch: 30, Loss: 1.197980
Train - Epoch 1, Batch: 0, Loss: 1.185527
Train - Epoch 1, Batch: 10, Loss: 1.163078
Train - Epoch 1, Batch: 20, Loss: 1.147256
Train - Epoch 1, Batch: 30, Loss: 1.134019
Train - Epoch 2, Batch: 0, Loss: 1.134537
Train - Epoch 2, Batch: 10, Loss: 1.123920
Train - Epoch 2, Batch: 20, Loss: 1.102258
Train - Epoch 2, Batch: 30, Loss: 1.118614
Train - Epoch 3, Batch: 0, Loss: 1.104106
Train - Epoch 3, Batch: 10, Loss: 1.103028
Train - Epoch 3, Batch: 20, Loss: 1.084824
Train - Epoch 3, Batch: 30, Loss: 1.088821
Train - Epoch 4, Batch: 0, Loss: 1.079071
Train - Epoch 4, Batch: 10, Loss: 1.064172
Train - Epoch 4, Batch: 20, Loss: 1.063667
Train - Epoch 4, Batch: 30, Loss: 1.052865
Train - Epoch 5, Batch: 0, Loss: 1.057149
Train - Epoch 5, Batch: 10, Loss: 1.057788
Train - Epoch 5, Batch: 20, Loss: 1.040704
Train - Epoch 5, Batch: 30, Loss: 1.043110
Train - Epoch 6, Batch: 0, Loss: 1.044414
Train - Epoch 6, Batch: 10, Loss: 1.027822
Train - Epoch 6, Batch: 20, Loss: 1.036261
Train - Epoch 6, Batch: 30, Loss: 1.020018
Train - Epoch 7, Batch: 0, Loss: 1.020594
Train - Epoch 7, Batch: 10, Loss: 1.028196
Train - Epoch 7, Batch: 20, Loss: 1.017016
Train - Epoch 7, Batch: 30, Loss: 1.017171
Train - Epoch 8, Batch: 0, Loss: 1.008182
Train - Epoch 8, Batch: 10, Loss: 1.007565
Train - Epoch 8, Batch: 20, Loss: 1.005612
Train - Epoch 8, Batch: 30, Loss: 1.004702
Train - Epoch 9, Batch: 0, Loss: 1.001642
Train - Epoch 9, Batch: 10, Loss: 1.001534
Train - Epoch 9, Batch: 20, Loss: 1.001440
Train - Epoch 9, Batch: 30, Loss: 0.986795
Train - Epoch 10, Batch: 0, Loss: 0.992919
Train - Epoch 10, Batch: 10, Loss: 0.989594
Train - Epoch 10, Batch: 20, Loss: 0.975480
Train - Epoch 10, Batch: 30, Loss: 0.988242
Train - Epoch 11, Batch: 0, Loss: 0.983508
Train - Epoch 11, Batch: 10, Loss: 0.973871
Train - Epoch 11, Batch: 20, Loss: 0.976109
Train - Epoch 11, Batch: 30, Loss: 0.973523
Train - Epoch 12, Batch: 0, Loss: 0.971970
Train - Epoch 12, Batch: 10, Loss: 0.977720
Train - Epoch 12, Batch: 20, Loss: 0.967758
Train - Epoch 12, Batch: 30, Loss: 0.974721
Train - Epoch 13, Batch: 0, Loss: 0.969583
Train - Epoch 13, Batch: 10, Loss: 0.957825
Train - Epoch 13, Batch: 20, Loss: 0.961110
Train - Epoch 13, Batch: 30, Loss: 0.974731
Train - Epoch 14, Batch: 0, Loss: 0.958931
Train - Epoch 14, Batch: 10, Loss: 0.965532
Train - Epoch 14, Batch: 20, Loss: 0.958045
Train - Epoch 14, Batch: 30, Loss: 0.962467
Train - Epoch 15, Batch: 0, Loss: 0.957079
Train - Epoch 15, Batch: 10, Loss: 0.950033
Train - Epoch 15, Batch: 20, Loss: 0.944902
Train - Epoch 15, Batch: 30, Loss: 0.945759
Train - Epoch 16, Batch: 0, Loss: 0.955811
Train - Epoch 16, Batch: 10, Loss: 0.944971
Train - Epoch 16, Batch: 20, Loss: 0.936862
Train - Epoch 16, Batch: 30, Loss: 0.949171
Train - Epoch 17, Batch: 0, Loss: 0.950929
Train - Epoch 17, Batch: 10, Loss: 0.945878
Train - Epoch 17, Batch: 20, Loss: 0.925785
Train - Epoch 17, Batch: 30, Loss: 0.939356
Train - Epoch 18, Batch: 0, Loss: 0.934220
Train - Epoch 18, Batch: 10, Loss: 0.933241
Train - Epoch 18, Batch: 20, Loss: 0.933900
Train - Epoch 18, Batch: 30, Loss: 0.936165
Train - Epoch 19, Batch: 0, Loss: 0.931637
Train - Epoch 19, Batch: 10, Loss: 0.923654
Train - Epoch 19, Batch: 20, Loss: 0.934961
Train - Epoch 19, Batch: 30, Loss: 0.923353
Train - Epoch 20, Batch: 0, Loss: 0.927702
Train - Epoch 20, Batch: 10, Loss: 0.923496
Train - Epoch 20, Batch: 20, Loss: 0.920946
Train - Epoch 20, Batch: 30, Loss: 0.923787
Train - Epoch 21, Batch: 0, Loss: 0.928301
Train - Epoch 21, Batch: 10, Loss: 0.920666
Train - Epoch 21, Batch: 20, Loss: 0.917546
Train - Epoch 21, Batch: 30, Loss: 0.917051
Train - Epoch 22, Batch: 0, Loss: 0.922279
Train - Epoch 22, Batch: 10, Loss: 0.925074
Train - Epoch 22, Batch: 20, Loss: 0.923087
Train - Epoch 22, Batch: 30, Loss: 0.916515
Train - Epoch 23, Batch: 0, Loss: 0.918558
Train - Epoch 23, Batch: 10, Loss: 0.915997
Train - Epoch 23, Batch: 20, Loss: 0.911365
Train - Epoch 23, Batch: 30, Loss: 0.920047
Train - Epoch 24, Batch: 0, Loss: 0.910374
Train - Epoch 24, Batch: 10, Loss: 0.921030
Train - Epoch 24, Batch: 20, Loss: 0.907747
Train - Epoch 24, Batch: 30, Loss: 0.910932
Train - Epoch 25, Batch: 0, Loss: 0.905516
Train - Epoch 25, Batch: 10, Loss: 0.917764
Train - Epoch 25, Batch: 20, Loss: 0.917208
Train - Epoch 25, Batch: 30, Loss: 0.919633
Train - Epoch 26, Batch: 0, Loss: 0.906612
Train - Epoch 26, Batch: 10, Loss: 0.906958
Train - Epoch 26, Batch: 20, Loss: 0.900812
Train - Epoch 26, Batch: 30, Loss: 0.910011
Train - Epoch 27, Batch: 0, Loss: 0.910941
Train - Epoch 27, Batch: 10, Loss: 0.902123
Train - Epoch 27, Batch: 20, Loss: 0.918227
Train - Epoch 27, Batch: 30, Loss: 0.898194
Train - Epoch 28, Batch: 0, Loss: 0.907314
Train - Epoch 28, Batch: 10, Loss: 0.904196
Train - Epoch 28, Batch: 20, Loss: 0.910147
Train - Epoch 28, Batch: 30, Loss: 0.904735
Train - Epoch 29, Batch: 0, Loss: 0.901079
Train - Epoch 29, Batch: 10, Loss: 0.893915
Train - Epoch 29, Batch: 20, Loss: 0.901392
Train - Epoch 29, Batch: 30, Loss: 0.899373
Train - Epoch 30, Batch: 0, Loss: 0.898396
Train - Epoch 30, Batch: 10, Loss: 0.900537
Train - Epoch 30, Batch: 20, Loss: 0.903141
Train - Epoch 30, Batch: 30, Loss: 0.890445
Train - Epoch 31, Batch: 0, Loss: 0.891157
Train - Epoch 31, Batch: 10, Loss: 0.892095
Train - Epoch 31, Batch: 20, Loss: 0.895563
Train - Epoch 31, Batch: 30, Loss: 0.882668
Test Avg. Loss: 0.000070, Accuracy: 0.629030
training_time:: 3.4731619358062744
training time full:: 3.4732277393341064
provenance prepare time:: 7.3909759521484375e-06
Test Avg. Loss: 0.000070, Accuracy: 0.629030
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
batch_size:: 16384
epoch:: 0
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 3.5861330032348633
overhead:: 0
overhead2:: 1.7445228099822998
overhead3:: 0
time_baseline:: 3.5870277881622314
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629116
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.018523693084716797
overhead3:: 0.04356050491333008
overhead4:: 0.21036720275878906
overhead5:: 0
memory usage:: 3774111744
time_provenance:: 1.158376932144165
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629150
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.02764129638671875
overhead3:: 0.047507286071777344
overhead4:: 0.24251723289489746
overhead5:: 0
memory usage:: 3782127616
time_provenance:: 1.1708581447601318
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629116
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.02366948127746582
overhead3:: 0.05142569541931152
overhead4:: 0.2802572250366211
overhead5:: 0
memory usage:: 3774685184
time_provenance:: 1.251349687576294
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629150
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.026283979415893555
overhead3:: 0.059885263442993164
overhead4:: 0.31745362281799316
overhead5:: 0
memory usage:: 3763171328
time_provenance:: 1.323838233947754
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629116
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.03144097328186035
overhead3:: 0.07158422470092773
overhead4:: 0.37914371490478516
overhead5:: 0
memory usage:: 3763109888
time_provenance:: 1.3635423183441162
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629150
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.03367781639099121
overhead3:: 0.06394338607788086
overhead4:: 0.42382049560546875
overhead5:: 0
memory usage:: 3774922752
time_provenance:: 1.4573142528533936
curr_diff: 0 tensor(7.3920e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.3920e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629133
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.03474688529968262
overhead3:: 0.08084559440612793
overhead4:: 0.43810200691223145
overhead5:: 0
memory usage:: 3803258880
time_provenance:: 1.426283359527588
curr_diff: 0 tensor(7.7330e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.7330e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629133
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.03963828086853027
overhead3:: 0.07609224319458008
overhead4:: 0.46250033378601074
overhead5:: 0
memory usage:: 3778514944
time_provenance:: 1.4865317344665527
curr_diff: 0 tensor(7.9043e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.9043e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629133
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.04455256462097168
overhead3:: 0.09773683547973633
overhead4:: 0.5313384532928467
overhead5:: 0
memory usage:: 3766390784
time_provenance:: 1.664729356765747
curr_diff: 0 tensor(7.9624e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.9624e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629133
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.04604387283325195
overhead3:: 0.08040571212768555
overhead4:: 0.5508449077606201
overhead5:: 0
memory usage:: 3784257536
time_provenance:: 1.5353853702545166
curr_diff: 0 tensor(7.9422e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.9422e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629133
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.06286501884460449
overhead3:: 0.11919856071472168
overhead4:: 0.8250012397766113
overhead5:: 0
memory usage:: 3800023040
time_provenance:: 1.88645339012146
curr_diff: 0 tensor(2.7433e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.7433e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629116
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.06130051612854004
overhead3:: 0.10414242744445801
overhead4:: 0.8202998638153076
overhead5:: 0
memory usage:: 3775221760
time_provenance:: 1.8670978546142578
curr_diff: 0 tensor(2.8417e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.8417e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629116
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.0619053840637207
overhead3:: 0.1067955493927002
overhead4:: 0.8824365139007568
overhead5:: 0
memory usage:: 3790114816
time_provenance:: 1.915900707244873
curr_diff: 0 tensor(2.9188e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.9188e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629116
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.0737607479095459
overhead3:: 0.11075758934020996
overhead4:: 0.895606279373169
overhead5:: 0
memory usage:: 3790372864
time_provenance:: 1.9358799457550049
curr_diff: 0 tensor(3.0795e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.0795e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629116
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.07076263427734375
overhead3:: 0.12045001983642578
overhead4:: 0.916926383972168
overhead5:: 0
memory usage:: 3774603264
time_provenance:: 1.979426622390747
curr_diff: 0 tensor(3.0557e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.0557e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629116
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.18745160102844238
overhead3:: 0.2845125198364258
overhead4:: 2.6647119522094727
overhead5:: 0
memory usage:: 3764596736
time_provenance:: 4.222874402999878
curr_diff: 0 tensor(7.4253e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.4253e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629116
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.1426992416381836
overhead3:: 0.23192667961120605
overhead4:: 1.9792356491088867
overhead5:: 0
memory usage:: 3793494016
time_provenance:: 3.2021729946136475
curr_diff: 0 tensor(7.8507e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.8507e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629116
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.18639874458312988
overhead3:: 0.29427194595336914
overhead4:: 2.5461416244506836
overhead5:: 0
memory usage:: 3783663616
time_provenance:: 4.051326274871826
curr_diff: 0 tensor(7.9584e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.9584e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629116
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.14363431930541992
overhead3:: 0.22594833374023438
overhead4:: 1.9870569705963135
overhead5:: 0
memory usage:: 3775094784
time_provenance:: 3.1980485916137695
curr_diff: 0 tensor(8.2070e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.2070e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629116
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.18719911575317383
overhead3:: 0.2778921127319336
overhead4:: 2.134817123413086
overhead5:: 0
memory usage:: 3801743360
time_provenance:: 3.5327248573303223
curr_diff: 0 tensor(8.2719e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.2719e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629116
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.341083288192749
overhead3:: 0.5460708141326904
overhead4:: 3.5840513706207275
overhead5:: 0
memory usage:: 3792670720
time_provenance:: 4.820526599884033
curr_diff: 0 tensor(9.8050e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.8050e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629116
repetition 3
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 3 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 1.988843
Train - Epoch 0, Batch: 10, Loss: 1.382296
Train - Epoch 0, Batch: 20, Loss: 1.233559
Train - Epoch 0, Batch: 30, Loss: 1.185152
Train - Epoch 1, Batch: 0, Loss: 1.166693
Train - Epoch 1, Batch: 10, Loss: 1.151530
Train - Epoch 1, Batch: 20, Loss: 1.120896
Train - Epoch 1, Batch: 30, Loss: 1.122182
Train - Epoch 2, Batch: 0, Loss: 1.123893
Train - Epoch 2, Batch: 10, Loss: 1.106356
Train - Epoch 2, Batch: 20, Loss: 1.097766
Train - Epoch 2, Batch: 30, Loss: 1.096529
Train - Epoch 3, Batch: 0, Loss: 1.096838
Train - Epoch 3, Batch: 10, Loss: 1.077580
Train - Epoch 3, Batch: 20, Loss: 1.069541
Train - Epoch 3, Batch: 30, Loss: 1.061856
Train - Epoch 4, Batch: 0, Loss: 1.061096
Train - Epoch 4, Batch: 10, Loss: 1.062980
Train - Epoch 4, Batch: 20, Loss: 1.047959
Train - Epoch 4, Batch: 30, Loss: 1.058990
Train - Epoch 5, Batch: 0, Loss: 1.047204
Train - Epoch 5, Batch: 10, Loss: 1.042762
Train - Epoch 5, Batch: 20, Loss: 1.044930
Train - Epoch 5, Batch: 30, Loss: 1.033725
Train - Epoch 6, Batch: 0, Loss: 1.022875
Train - Epoch 6, Batch: 10, Loss: 1.031517
Train - Epoch 6, Batch: 20, Loss: 1.020504
Train - Epoch 6, Batch: 30, Loss: 1.020953
Train - Epoch 7, Batch: 0, Loss: 1.017888
Train - Epoch 7, Batch: 10, Loss: 1.000011
Train - Epoch 7, Batch: 20, Loss: 0.995848
Train - Epoch 7, Batch: 30, Loss: 1.001957
Train - Epoch 8, Batch: 0, Loss: 0.999735
Train - Epoch 8, Batch: 10, Loss: 1.001456
Train - Epoch 8, Batch: 20, Loss: 1.005104
Train - Epoch 8, Batch: 30, Loss: 0.994252
Train - Epoch 9, Batch: 0, Loss: 0.996531
Train - Epoch 9, Batch: 10, Loss: 0.990380
Train - Epoch 9, Batch: 20, Loss: 0.996178
Train - Epoch 9, Batch: 30, Loss: 0.986997
Train - Epoch 10, Batch: 0, Loss: 0.987708
Train - Epoch 10, Batch: 10, Loss: 0.978461
Train - Epoch 10, Batch: 20, Loss: 0.977954
Train - Epoch 10, Batch: 30, Loss: 0.972376
Train - Epoch 11, Batch: 0, Loss: 0.963744
Train - Epoch 11, Batch: 10, Loss: 0.972708
Train - Epoch 11, Batch: 20, Loss: 0.967672
Train - Epoch 11, Batch: 30, Loss: 0.963285
Train - Epoch 12, Batch: 0, Loss: 0.965296
Train - Epoch 12, Batch: 10, Loss: 0.961891
Train - Epoch 12, Batch: 20, Loss: 0.954697
Train - Epoch 12, Batch: 30, Loss: 0.964929
Train - Epoch 13, Batch: 0, Loss: 0.966556
Train - Epoch 13, Batch: 10, Loss: 0.961859
Train - Epoch 13, Batch: 20, Loss: 0.958491
Train - Epoch 13, Batch: 30, Loss: 0.948542
Train - Epoch 14, Batch: 0, Loss: 0.961391
Train - Epoch 14, Batch: 10, Loss: 0.950329
Train - Epoch 14, Batch: 20, Loss: 0.956350
Train - Epoch 14, Batch: 30, Loss: 0.942093
Train - Epoch 15, Batch: 0, Loss: 0.954656
Train - Epoch 15, Batch: 10, Loss: 0.953265
Train - Epoch 15, Batch: 20, Loss: 0.939774
Train - Epoch 15, Batch: 30, Loss: 0.943834
Train - Epoch 16, Batch: 0, Loss: 0.937370
Train - Epoch 16, Batch: 10, Loss: 0.941788
Train - Epoch 16, Batch: 20, Loss: 0.950606
Train - Epoch 16, Batch: 30, Loss: 0.939812
Train - Epoch 17, Batch: 0, Loss: 0.942107
Train - Epoch 17, Batch: 10, Loss: 0.938542
Train - Epoch 17, Batch: 20, Loss: 0.928941
Train - Epoch 17, Batch: 30, Loss: 0.936919
Train - Epoch 18, Batch: 0, Loss: 0.925906
Train - Epoch 18, Batch: 10, Loss: 0.928520
Train - Epoch 18, Batch: 20, Loss: 0.927958
Train - Epoch 18, Batch: 30, Loss: 0.922943
Train - Epoch 19, Batch: 0, Loss: 0.923742
Train - Epoch 19, Batch: 10, Loss: 0.932199
Train - Epoch 19, Batch: 20, Loss: 0.925102
Train - Epoch 19, Batch: 30, Loss: 0.921620
Train - Epoch 20, Batch: 0, Loss: 0.912904
Train - Epoch 20, Batch: 10, Loss: 0.915934
Train - Epoch 20, Batch: 20, Loss: 0.920394
Train - Epoch 20, Batch: 30, Loss: 0.926769
Train - Epoch 21, Batch: 0, Loss: 0.918726
Train - Epoch 21, Batch: 10, Loss: 0.929613
Train - Epoch 21, Batch: 20, Loss: 0.926526
Train - Epoch 21, Batch: 30, Loss: 0.911111
Train - Epoch 22, Batch: 0, Loss: 0.920450
Train - Epoch 22, Batch: 10, Loss: 0.911509
Train - Epoch 22, Batch: 20, Loss: 0.924840
Train - Epoch 22, Batch: 30, Loss: 0.918456
Train - Epoch 23, Batch: 0, Loss: 0.912179
Train - Epoch 23, Batch: 10, Loss: 0.911003
Train - Epoch 23, Batch: 20, Loss: 0.921054
Train - Epoch 23, Batch: 30, Loss: 0.914437
Train - Epoch 24, Batch: 0, Loss: 0.915127
Train - Epoch 24, Batch: 10, Loss: 0.906011
Train - Epoch 24, Batch: 20, Loss: 0.904163
Train - Epoch 24, Batch: 30, Loss: 0.903488
Train - Epoch 25, Batch: 0, Loss: 0.911993
Train - Epoch 25, Batch: 10, Loss: 0.908264
Train - Epoch 25, Batch: 20, Loss: 0.902359
Train - Epoch 25, Batch: 30, Loss: 0.905333
Train - Epoch 26, Batch: 0, Loss: 0.909798
Train - Epoch 26, Batch: 10, Loss: 0.903097
Train - Epoch 26, Batch: 20, Loss: 0.897997
Train - Epoch 26, Batch: 30, Loss: 0.895743
Train - Epoch 27, Batch: 0, Loss: 0.902342
Train - Epoch 27, Batch: 10, Loss: 0.903794
Train - Epoch 27, Batch: 20, Loss: 0.907902
Train - Epoch 27, Batch: 30, Loss: 0.905858
Train - Epoch 28, Batch: 0, Loss: 0.899874
Train - Epoch 28, Batch: 10, Loss: 0.891806
Train - Epoch 28, Batch: 20, Loss: 0.905029
Train - Epoch 28, Batch: 30, Loss: 0.896635
Train - Epoch 29, Batch: 0, Loss: 0.894231
Train - Epoch 29, Batch: 10, Loss: 0.899175
Train - Epoch 29, Batch: 20, Loss: 0.894438
Train - Epoch 29, Batch: 30, Loss: 0.896224
Train - Epoch 30, Batch: 0, Loss: 0.913036
Train - Epoch 30, Batch: 10, Loss: 0.893649
Train - Epoch 30, Batch: 20, Loss: 0.896671
Train - Epoch 30, Batch: 30, Loss: 0.900536
Train - Epoch 31, Batch: 0, Loss: 0.894092
Train - Epoch 31, Batch: 10, Loss: 0.898947
Train - Epoch 31, Batch: 20, Loss: 0.892767
Train - Epoch 31, Batch: 30, Loss: 0.886156
Test Avg. Loss: 0.000070, Accuracy: 0.629322
training_time:: 3.3972582817077637
training time full:: 3.3973233699798584
provenance prepare time:: 7.3909759521484375e-06
Test Avg. Loss: 0.000070, Accuracy: 0.629322
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
batch_size:: 16384
epoch:: 0
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 3.7325658798217773
overhead:: 0
overhead2:: 1.737764596939087
overhead3:: 0
time_baseline:: 3.733462333679199
curr_diff: 0 tensor(0.0016, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0016, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629374
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.01857280731201172
overhead3:: 0.05013918876647949
overhead4:: 0.2161860466003418
overhead5:: 0
memory usage:: 3763712000
time_provenance:: 1.1843047142028809
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629374
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.02114558219909668
overhead3:: 0.05677986145019531
overhead4:: 0.24212932586669922
overhead5:: 0
memory usage:: 3767185408
time_provenance:: 1.1862037181854248
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629374
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.02225780487060547
overhead3:: 0.05982041358947754
overhead4:: 0.26909518241882324
overhead5:: 0
memory usage:: 3799814144
time_provenance:: 1.1849358081817627
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629374
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.0259859561920166
overhead3:: 0.06373047828674316
overhead4:: 0.3105781078338623
overhead5:: 0
memory usage:: 3800064000
time_provenance:: 1.285351276397705
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629391
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.03217887878417969
overhead3:: 0.061337947845458984
overhead4:: 0.37096405029296875
overhead5:: 0
memory usage:: 3778834432
time_provenance:: 1.348562479019165
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629374
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.03219747543334961
overhead3:: 0.06644821166992188
overhead4:: 0.45658397674560547
overhead5:: 0
memory usage:: 3774984192
time_provenance:: 1.430527687072754
curr_diff: 0 tensor(9.0949e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.0949e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629374
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.037230491638183594
overhead3:: 0.08928680419921875
overhead4:: 0.44343018531799316
overhead5:: 0
memory usage:: 3762614272
time_provenance:: 1.5023157596588135
curr_diff: 0 tensor(9.5720e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.5720e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629374
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.03695797920227051
overhead3:: 0.08541059494018555
overhead4:: 0.43727612495422363
overhead5:: 0
memory usage:: 3772071936
time_provenance:: 1.4064488410949707
curr_diff: 0 tensor(9.7826e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.7826e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629374
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.04006338119506836
overhead3:: 0.08098864555358887
overhead4:: 0.4768359661102295
overhead5:: 0
memory usage:: 3803770880
time_provenance:: 1.4564247131347656
curr_diff: 0 tensor(9.8287e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.8287e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629374
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.04439902305603027
overhead3:: 0.09676146507263184
overhead4:: 0.5243988037109375
overhead5:: 0
memory usage:: 3761299456
time_provenance:: 1.5494098663330078
curr_diff: 0 tensor(9.8419e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.8419e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629374
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.059760332107543945
overhead3:: 0.10376954078674316
overhead4:: 0.8091990947723389
overhead5:: 0
memory usage:: 3782250496
time_provenance:: 1.865868091583252
curr_diff: 0 tensor(3.5213e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.5213e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629374
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.06894040107727051
overhead3:: 0.11835289001464844
overhead4:: 0.9273874759674072
overhead5:: 0
memory usage:: 3782909952
time_provenance:: 2.0826592445373535
curr_diff: 0 tensor(3.6189e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.6189e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629374
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.06452012062072754
overhead3:: 0.11818647384643555
overhead4:: 0.8685178756713867
overhead5:: 0
memory usage:: 3761295360
time_provenance:: 1.9407997131347656
curr_diff: 0 tensor(3.6428e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.6428e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629374
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.06978011131286621
overhead3:: 0.11328625679016113
overhead4:: 0.8340544700622559
overhead5:: 0
memory usage:: 3770011648
time_provenance:: 1.88624906539917
curr_diff: 0 tensor(3.6318e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.6318e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629374
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.07005763053894043
overhead3:: 0.11883902549743652
overhead4:: 0.909503698348999
overhead5:: 0
memory usage:: 3762929664
time_provenance:: 1.9856088161468506
curr_diff: 0 tensor(3.6409e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.6409e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629374
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.14279818534851074
overhead3:: 0.21810436248779297
overhead4:: 1.907569408416748
overhead5:: 0
memory usage:: 3770109952
time_provenance:: 3.118553638458252
curr_diff: 0 tensor(9.6447e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.6447e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629374
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.14253544807434082
overhead3:: 0.22186899185180664
overhead4:: 1.962564468383789
overhead5:: 0
memory usage:: 3836440576
time_provenance:: 3.1795222759246826
curr_diff: 0 tensor(9.9422e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.9422e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629374
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.15016937255859375
overhead3:: 0.23819541931152344
overhead4:: 1.9237313270568848
overhead5:: 0
memory usage:: 3780812800
time_provenance:: 3.1553211212158203
curr_diff: 0 tensor(1.0009e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0009e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629374
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.19370388984680176
overhead3:: 0.3085346221923828
overhead4:: 2.1999034881591797
overhead5:: 0
memory usage:: 3769253888
time_provenance:: 3.6681365966796875
curr_diff: 0 tensor(1.0052e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0052e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629374
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.15792083740234375
overhead3:: 0.23283100128173828
overhead4:: 2.0079264640808105
overhead5:: 0
memory usage:: 3774312448
time_provenance:: 3.231396436691284
curr_diff: 0 tensor(1.0074e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0074e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629374
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.3319699764251709
overhead3:: 0.5031919479370117
overhead4:: 3.4729819297790527
overhead5:: 0
memory usage:: 3759345664
time_provenance:: 4.67367148399353
curr_diff: 0 tensor(1.0250e-13, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0250e-13, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0016, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0016, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629374
repetition 4
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 4 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 1.915323
Train - Epoch 0, Batch: 10, Loss: 1.380309
Train - Epoch 0, Batch: 20, Loss: 1.242198
Train - Epoch 0, Batch: 30, Loss: 1.190393
Train - Epoch 1, Batch: 0, Loss: 1.183671
Train - Epoch 1, Batch: 10, Loss: 1.171733
Train - Epoch 1, Batch: 20, Loss: 1.144262
Train - Epoch 1, Batch: 30, Loss: 1.137810
Train - Epoch 2, Batch: 0, Loss: 1.146556
Train - Epoch 2, Batch: 10, Loss: 1.127457
Train - Epoch 2, Batch: 20, Loss: 1.114481
Train - Epoch 2, Batch: 30, Loss: 1.109742
Train - Epoch 3, Batch: 0, Loss: 1.094029
Train - Epoch 3, Batch: 10, Loss: 1.087117
Train - Epoch 3, Batch: 20, Loss: 1.086810
Train - Epoch 3, Batch: 30, Loss: 1.084380
Train - Epoch 4, Batch: 0, Loss: 1.088702
Train - Epoch 4, Batch: 10, Loss: 1.069130
Train - Epoch 4, Batch: 20, Loss: 1.057883
Train - Epoch 4, Batch: 30, Loss: 1.053871
Train - Epoch 5, Batch: 0, Loss: 1.053009
Train - Epoch 5, Batch: 10, Loss: 1.045566
Train - Epoch 5, Batch: 20, Loss: 1.052368
Train - Epoch 5, Batch: 30, Loss: 1.036506
Train - Epoch 6, Batch: 0, Loss: 1.036322
Train - Epoch 6, Batch: 10, Loss: 1.036500
Train - Epoch 6, Batch: 20, Loss: 1.027737
Train - Epoch 6, Batch: 30, Loss: 1.025707
Train - Epoch 7, Batch: 0, Loss: 1.021852
Train - Epoch 7, Batch: 10, Loss: 1.016292
Train - Epoch 7, Batch: 20, Loss: 1.013120
Train - Epoch 7, Batch: 30, Loss: 1.003064
Train - Epoch 8, Batch: 0, Loss: 1.008221
Train - Epoch 8, Batch: 10, Loss: 1.004944
Train - Epoch 8, Batch: 20, Loss: 1.001319
Train - Epoch 8, Batch: 30, Loss: 0.997716
Train - Epoch 9, Batch: 0, Loss: 0.992643
Train - Epoch 9, Batch: 10, Loss: 0.990332
Train - Epoch 9, Batch: 20, Loss: 0.992399
Train - Epoch 9, Batch: 30, Loss: 0.986881
Train - Epoch 10, Batch: 0, Loss: 0.998221
Train - Epoch 10, Batch: 10, Loss: 0.986479
Train - Epoch 10, Batch: 20, Loss: 0.980940
Train - Epoch 10, Batch: 30, Loss: 0.982730
Train - Epoch 11, Batch: 0, Loss: 0.976240
Train - Epoch 11, Batch: 10, Loss: 0.968226
Train - Epoch 11, Batch: 20, Loss: 0.978222
Train - Epoch 11, Batch: 30, Loss: 0.973233
Train - Epoch 12, Batch: 0, Loss: 0.978648
Train - Epoch 12, Batch: 10, Loss: 0.972833
Train - Epoch 12, Batch: 20, Loss: 0.968819
Train - Epoch 12, Batch: 30, Loss: 0.957254
Train - Epoch 13, Batch: 0, Loss: 0.962877
Train - Epoch 13, Batch: 10, Loss: 0.957901
Train - Epoch 13, Batch: 20, Loss: 0.959252
Train - Epoch 13, Batch: 30, Loss: 0.959694
Train - Epoch 14, Batch: 0, Loss: 0.961020
Train - Epoch 14, Batch: 10, Loss: 0.955125
Train - Epoch 14, Batch: 20, Loss: 0.954093
Train - Epoch 14, Batch: 30, Loss: 0.949626
Train - Epoch 15, Batch: 0, Loss: 0.954689
Train - Epoch 15, Batch: 10, Loss: 0.952284
Train - Epoch 15, Batch: 20, Loss: 0.947279
Train - Epoch 15, Batch: 30, Loss: 0.947849
Train - Epoch 16, Batch: 0, Loss: 0.937820
Train - Epoch 16, Batch: 10, Loss: 0.938575
Train - Epoch 16, Batch: 20, Loss: 0.949317
Train - Epoch 16, Batch: 30, Loss: 0.946553
Train - Epoch 17, Batch: 0, Loss: 0.940350
Train - Epoch 17, Batch: 10, Loss: 0.942672
Train - Epoch 17, Batch: 20, Loss: 0.945709
Train - Epoch 17, Batch: 30, Loss: 0.937219
Train - Epoch 18, Batch: 0, Loss: 0.938165
Train - Epoch 18, Batch: 10, Loss: 0.935289
Train - Epoch 18, Batch: 20, Loss: 0.930851
Train - Epoch 18, Batch: 30, Loss: 0.934885
Train - Epoch 19, Batch: 0, Loss: 0.930021
Train - Epoch 19, Batch: 10, Loss: 0.937244
Train - Epoch 19, Batch: 20, Loss: 0.921934
Train - Epoch 19, Batch: 30, Loss: 0.922444
Train - Epoch 20, Batch: 0, Loss: 0.930035
Train - Epoch 20, Batch: 10, Loss: 0.919476
Train - Epoch 20, Batch: 20, Loss: 0.923392
Train - Epoch 20, Batch: 30, Loss: 0.913919
Train - Epoch 21, Batch: 0, Loss: 0.926600
Train - Epoch 21, Batch: 10, Loss: 0.931377
Train - Epoch 21, Batch: 20, Loss: 0.922073
Train - Epoch 21, Batch: 30, Loss: 0.915482
Train - Epoch 22, Batch: 0, Loss: 0.917365
Train - Epoch 22, Batch: 10, Loss: 0.920335
Train - Epoch 22, Batch: 20, Loss: 0.911585
Train - Epoch 22, Batch: 30, Loss: 0.919505
Train - Epoch 23, Batch: 0, Loss: 0.912271
Train - Epoch 23, Batch: 10, Loss: 0.917027
Train - Epoch 23, Batch: 20, Loss: 0.915823
Train - Epoch 23, Batch: 30, Loss: 0.910225
Train - Epoch 24, Batch: 0, Loss: 0.924821
Train - Epoch 24, Batch: 10, Loss: 0.898521
Train - Epoch 24, Batch: 20, Loss: 0.911640
Train - Epoch 24, Batch: 30, Loss: 0.909351
Train - Epoch 25, Batch: 0, Loss: 0.903073
Train - Epoch 25, Batch: 10, Loss: 0.898175
Train - Epoch 25, Batch: 20, Loss: 0.908659
Train - Epoch 25, Batch: 30, Loss: 0.911404
Train - Epoch 26, Batch: 0, Loss: 0.896343
Train - Epoch 26, Batch: 10, Loss: 0.914627
Train - Epoch 26, Batch: 20, Loss: 0.903435
Train - Epoch 26, Batch: 30, Loss: 0.907157
Train - Epoch 27, Batch: 0, Loss: 0.894743
Train - Epoch 27, Batch: 10, Loss: 0.899997
Train - Epoch 27, Batch: 20, Loss: 0.902079
Train - Epoch 27, Batch: 30, Loss: 0.907337
Train - Epoch 28, Batch: 0, Loss: 0.899991
Train - Epoch 28, Batch: 10, Loss: 0.898821
Train - Epoch 28, Batch: 20, Loss: 0.899621
Train - Epoch 28, Batch: 30, Loss: 0.901997
Train - Epoch 29, Batch: 0, Loss: 0.901616
Train - Epoch 29, Batch: 10, Loss: 0.903651
Train - Epoch 29, Batch: 20, Loss: 0.904924
Train - Epoch 29, Batch: 30, Loss: 0.897052
Train - Epoch 30, Batch: 0, Loss: 0.895144
Train - Epoch 30, Batch: 10, Loss: 0.897429
Train - Epoch 30, Batch: 20, Loss: 0.904785
Train - Epoch 30, Batch: 30, Loss: 0.904560
Train - Epoch 31, Batch: 0, Loss: 0.894656
Train - Epoch 31, Batch: 10, Loss: 0.890358
Train - Epoch 31, Batch: 20, Loss: 0.893500
Train - Epoch 31, Batch: 30, Loss: 0.899930
Test Avg. Loss: 0.000070, Accuracy: 0.630321
training_time:: 3.5156185626983643
training time full:: 3.51568603515625
provenance prepare time:: 5.9604644775390625e-06
Test Avg. Loss: 0.000070, Accuracy: 0.630321
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
batch_size:: 16384
epoch:: 0
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 3.708780527114868
overhead:: 0
overhead2:: 1.7530794143676758
overhead3:: 0
time_baseline:: 3.709590196609497
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630390
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.01805281639099121
overhead3:: 0.043091773986816406
overhead4:: 0.20769715309143066
overhead5:: 0
memory usage:: 3782111232
time_provenance:: 1.1287591457366943
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0019, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0019, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630407
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.02077794075012207
overhead3:: 0.0570981502532959
overhead4:: 0.2491443157196045
overhead5:: 0
memory usage:: 3765370880
time_provenance:: 1.191507339477539
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630407
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.023494482040405273
overhead3:: 0.05946922302246094
overhead4:: 0.2786068916320801
overhead5:: 0
memory usage:: 3762122752
time_provenance:: 1.2266490459442139
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0019, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0019, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630407
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.026580810546875
overhead3:: 0.05386686325073242
overhead4:: 0.30495309829711914
overhead5:: 0
memory usage:: 3782320128
time_provenance:: 1.2594189643859863
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630407
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.02969980239868164
overhead3:: 0.06079864501953125
overhead4:: 0.35167646408081055
overhead5:: 0
memory usage:: 3769905152
time_provenance:: 1.3376002311706543
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0019, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0019, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630407
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.03128838539123535
overhead3:: 0.07383465766906738
overhead4:: 0.42526817321777344
overhead5:: 0
memory usage:: 3764441088
time_provenance:: 1.4036846160888672
curr_diff: 0 tensor(9.1019e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.1019e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630390
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.03457045555114746
overhead3:: 0.06761813163757324
overhead4:: 0.44698500633239746
overhead5:: 0
memory usage:: 3841245184
time_provenance:: 1.4138896465301514
curr_diff: 0 tensor(9.2515e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.2515e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630390
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.03727269172668457
overhead3:: 0.08400583267211914
overhead4:: 0.45369648933410645
overhead5:: 0
memory usage:: 3767287808
time_provenance:: 1.4659175872802734
curr_diff: 0 tensor(9.3116e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.3116e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630390
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.0420534610748291
overhead3:: 0.07783818244934082
overhead4:: 0.5394124984741211
overhead5:: 0
memory usage:: 3766808576
time_provenance:: 1.6013219356536865
curr_diff: 0 tensor(9.3455e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.3455e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630390
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.044449806213378906
overhead3:: 0.08069586753845215
overhead4:: 0.5254561901092529
overhead5:: 0
memory usage:: 3789602816
time_provenance:: 1.5200178623199463
curr_diff: 0 tensor(9.3223e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.3223e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630390
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.06054806709289551
overhead3:: 0.1100306510925293
overhead4:: 0.7835748195648193
overhead5:: 0
memory usage:: 3793580032
time_provenance:: 1.8233842849731445
curr_diff: 0 tensor(3.8557e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.8557e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630390
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.06513214111328125
overhead3:: 0.10767722129821777
overhead4:: 0.8444418907165527
overhead5:: 0
memory usage:: 3769761792
time_provenance:: 1.894510269165039
curr_diff: 0 tensor(3.9150e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.9150e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630390
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.0647284984588623
overhead3:: 0.11293959617614746
overhead4:: 0.8816540241241455
overhead5:: 0
memory usage:: 3769757696
time_provenance:: 1.9196603298187256
curr_diff: 0 tensor(3.9658e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.9658e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630390
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.0688478946685791
overhead3:: 0.11469388008117676
overhead4:: 0.9290764331817627
overhead5:: 0
memory usage:: 3768094720
time_provenance:: 2.0037403106689453
curr_diff: 0 tensor(3.9693e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.9693e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630390
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.07126474380493164
overhead3:: 0.11059856414794922
overhead4:: 0.8398342132568359
overhead5:: 0
memory usage:: 3794284544
time_provenance:: 1.8796513080596924
curr_diff: 0 tensor(3.9591e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.9591e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630390
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.14198946952819824
overhead3:: 0.22316694259643555
overhead4:: 1.9379866123199463
overhead5:: 0
memory usage:: 3766276096
time_provenance:: 3.151926040649414
curr_diff: 0 tensor(8.5951e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.5951e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630390
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.14529633522033691
overhead3:: 0.22559189796447754
overhead4:: 1.965024709701538
overhead5:: 0
memory usage:: 3776790528
time_provenance:: 3.1902480125427246
curr_diff: 0 tensor(8.8318e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.8318e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630390
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.14744234085083008
overhead3:: 0.23333477973937988
overhead4:: 1.9889283180236816
overhead5:: 0
memory usage:: 3795623936
time_provenance:: 3.2445337772369385
curr_diff: 0 tensor(8.8496e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.8496e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630390
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.17305397987365723
overhead3:: 0.2554352283477783
overhead4:: 1.954993724822998
overhead5:: 0
memory usage:: 3769397248
time_provenance:: 3.2585601806640625
curr_diff: 0 tensor(8.9156e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.9156e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630390
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.18805909156799316
overhead3:: 0.2907865047454834
overhead4:: 2.5649139881134033
overhead5:: 0
memory usage:: 3770912768
time_provenance:: 4.0947349071502686
curr_diff: 0 tensor(8.9079e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.9079e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630390
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 522
max_epoch:: 32
overhead:: 0
overhead2:: 0.36248183250427246
overhead3:: 0.59867262840271
overhead4:: 3.6113438606262207
overhead5:: 0
memory usage:: 3752534016
time_provenance:: 4.900674819946289
curr_diff: 0 tensor(1.0144e-13, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0144e-13, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630390
deletion rate:: 0.002
python3 generate_rand_ids 0.002  covtype 0
tensor([ 51204, 145415, 239624,  ..., 364534, 356343,  71675])
batch size:: 16384
repetition 0
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 0 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 2.066860
Train - Epoch 0, Batch: 10, Loss: 1.424576
Train - Epoch 0, Batch: 20, Loss: 1.265083
Train - Epoch 0, Batch: 30, Loss: 1.211450
Train - Epoch 1, Batch: 0, Loss: 1.190823
Train - Epoch 1, Batch: 10, Loss: 1.171597
Train - Epoch 1, Batch: 20, Loss: 1.143865
Train - Epoch 1, Batch: 30, Loss: 1.132606
Train - Epoch 2, Batch: 0, Loss: 1.145044
Train - Epoch 2, Batch: 10, Loss: 1.112280
Train - Epoch 2, Batch: 20, Loss: 1.123481
Train - Epoch 2, Batch: 30, Loss: 1.099673
Train - Epoch 3, Batch: 0, Loss: 1.103135
Train - Epoch 3, Batch: 10, Loss: 1.092762
Train - Epoch 3, Batch: 20, Loss: 1.089641
Train - Epoch 3, Batch: 30, Loss: 1.071285
Train - Epoch 4, Batch: 0, Loss: 1.078856
Train - Epoch 4, Batch: 10, Loss: 1.074098
Train - Epoch 4, Batch: 20, Loss: 1.066512
Train - Epoch 4, Batch: 30, Loss: 1.053061
Train - Epoch 5, Batch: 0, Loss: 1.064554
Train - Epoch 5, Batch: 10, Loss: 1.047283
Train - Epoch 5, Batch: 20, Loss: 1.047120
Train - Epoch 5, Batch: 30, Loss: 1.035845
Train - Epoch 6, Batch: 0, Loss: 1.038180
Train - Epoch 6, Batch: 10, Loss: 1.026200
Train - Epoch 6, Batch: 20, Loss: 1.021447
Train - Epoch 6, Batch: 30, Loss: 1.018811
Train - Epoch 7, Batch: 0, Loss: 1.022982
Train - Epoch 7, Batch: 10, Loss: 1.018936
Train - Epoch 7, Batch: 20, Loss: 1.023363
Train - Epoch 7, Batch: 30, Loss: 1.007003
Train - Epoch 8, Batch: 0, Loss: 1.010614
Train - Epoch 8, Batch: 10, Loss: 1.002156
Train - Epoch 8, Batch: 20, Loss: 1.011835
Train - Epoch 8, Batch: 30, Loss: 0.996112
Train - Epoch 9, Batch: 0, Loss: 1.001525
Train - Epoch 9, Batch: 10, Loss: 1.006162
Train - Epoch 9, Batch: 20, Loss: 0.997768
Train - Epoch 9, Batch: 30, Loss: 0.986561
Train - Epoch 10, Batch: 0, Loss: 0.980821
Train - Epoch 10, Batch: 10, Loss: 0.980509
Train - Epoch 10, Batch: 20, Loss: 0.997554
Train - Epoch 10, Batch: 30, Loss: 0.995764
Train - Epoch 11, Batch: 0, Loss: 0.975256
Train - Epoch 11, Batch: 10, Loss: 0.983329
Train - Epoch 11, Batch: 20, Loss: 0.974738
Train - Epoch 11, Batch: 30, Loss: 0.975167
Train - Epoch 12, Batch: 0, Loss: 0.963350
Train - Epoch 12, Batch: 10, Loss: 0.962949
Train - Epoch 12, Batch: 20, Loss: 0.966269
Train - Epoch 12, Batch: 30, Loss: 0.965356
Train - Epoch 13, Batch: 0, Loss: 0.959645
Train - Epoch 13, Batch: 10, Loss: 0.972685
Train - Epoch 13, Batch: 20, Loss: 0.960453
Train - Epoch 13, Batch: 30, Loss: 0.962878
Train - Epoch 14, Batch: 0, Loss: 0.959692
Train - Epoch 14, Batch: 10, Loss: 0.965464
Train - Epoch 14, Batch: 20, Loss: 0.949644
Train - Epoch 14, Batch: 30, Loss: 0.954889
Train - Epoch 15, Batch: 0, Loss: 0.953589
Train - Epoch 15, Batch: 10, Loss: 0.949302
Train - Epoch 15, Batch: 20, Loss: 0.944174
Train - Epoch 15, Batch: 30, Loss: 0.955687
Train - Epoch 16, Batch: 0, Loss: 0.942102
Train - Epoch 16, Batch: 10, Loss: 0.948687
Train - Epoch 16, Batch: 20, Loss: 0.946504
Train - Epoch 16, Batch: 30, Loss: 0.939459
Train - Epoch 17, Batch: 0, Loss: 0.948122
Train - Epoch 17, Batch: 10, Loss: 0.933159
Train - Epoch 17, Batch: 20, Loss: 0.943037
Train - Epoch 17, Batch: 30, Loss: 0.936800
Train - Epoch 18, Batch: 0, Loss: 0.930726
Train - Epoch 18, Batch: 10, Loss: 0.937695
Train - Epoch 18, Batch: 20, Loss: 0.936535
Train - Epoch 18, Batch: 30, Loss: 0.932222
Train - Epoch 19, Batch: 0, Loss: 0.937374
Train - Epoch 19, Batch: 10, Loss: 0.931596
Train - Epoch 19, Batch: 20, Loss: 0.926526
Train - Epoch 19, Batch: 30, Loss: 0.931086
Train - Epoch 20, Batch: 0, Loss: 0.927159
Train - Epoch 20, Batch: 10, Loss: 0.933926
Train - Epoch 20, Batch: 20, Loss: 0.921762
Train - Epoch 20, Batch: 30, Loss: 0.924748
Train - Epoch 21, Batch: 0, Loss: 0.922201
Train - Epoch 21, Batch: 10, Loss: 0.927558
Train - Epoch 21, Batch: 20, Loss: 0.922986
Train - Epoch 21, Batch: 30, Loss: 0.926848
Train - Epoch 22, Batch: 0, Loss: 0.923800
Train - Epoch 22, Batch: 10, Loss: 0.918575
Train - Epoch 22, Batch: 20, Loss: 0.927205
Train - Epoch 22, Batch: 30, Loss: 0.922784
Train - Epoch 23, Batch: 0, Loss: 0.914408
Train - Epoch 23, Batch: 10, Loss: 0.925531
Train - Epoch 23, Batch: 20, Loss: 0.922098
Train - Epoch 23, Batch: 30, Loss: 0.910350
Train - Epoch 24, Batch: 0, Loss: 0.916983
Train - Epoch 24, Batch: 10, Loss: 0.911845
Train - Epoch 24, Batch: 20, Loss: 0.917717
Train - Epoch 24, Batch: 30, Loss: 0.919147
Train - Epoch 25, Batch: 0, Loss: 0.914056
Train - Epoch 25, Batch: 10, Loss: 0.912083
Train - Epoch 25, Batch: 20, Loss: 0.900969
Train - Epoch 25, Batch: 30, Loss: 0.915016
Train - Epoch 26, Batch: 0, Loss: 0.912482
Train - Epoch 26, Batch: 10, Loss: 0.906243
Train - Epoch 26, Batch: 20, Loss: 0.915909
Train - Epoch 26, Batch: 30, Loss: 0.907443
Train - Epoch 27, Batch: 0, Loss: 0.898624
Train - Epoch 27, Batch: 10, Loss: 0.914214
Train - Epoch 27, Batch: 20, Loss: 0.902581
Train - Epoch 27, Batch: 30, Loss: 0.902748
Train - Epoch 28, Batch: 0, Loss: 0.897175
Train - Epoch 28, Batch: 10, Loss: 0.899330
Train - Epoch 28, Batch: 20, Loss: 0.898426
Train - Epoch 28, Batch: 30, Loss: 0.902760
Train - Epoch 29, Batch: 0, Loss: 0.901909
Train - Epoch 29, Batch: 10, Loss: 0.903642
Train - Epoch 29, Batch: 20, Loss: 0.899612
Train - Epoch 29, Batch: 30, Loss: 0.894444
Train - Epoch 30, Batch: 0, Loss: 0.888592
Train - Epoch 30, Batch: 10, Loss: 0.894079
Train - Epoch 30, Batch: 20, Loss: 0.890315
Train - Epoch 30, Batch: 30, Loss: 0.903058
Train - Epoch 31, Batch: 0, Loss: 0.886189
Train - Epoch 31, Batch: 10, Loss: 0.900923
Train - Epoch 31, Batch: 20, Loss: 0.887680
Train - Epoch 31, Batch: 30, Loss: 0.892203
Test Avg. Loss: 0.000070, Accuracy: 0.629254
training_time:: 3.414482831954956
training time full:: 3.414547920227051
provenance prepare time:: 6.9141387939453125e-06
Test Avg. Loss: 0.000070, Accuracy: 0.629254
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
batch_size:: 16384
epoch:: 0
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 3.850735902786255
overhead:: 0
overhead2:: 1.7669780254364014
overhead3:: 0
time_baseline:: 3.8517227172851562
curr_diff: 0 tensor(0.0026, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0026, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629357
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.017955780029296875
overhead3:: 0.04654884338378906
overhead4:: 0.21699047088623047
overhead5:: 0
memory usage:: 3791261696
time_provenance:: 1.1663157939910889
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0028, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0028, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629322
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.0202634334564209
overhead3:: 0.04939556121826172
overhead4:: 0.24687623977661133
overhead5:: 0
memory usage:: 3806408704
time_provenance:: 1.1904549598693848
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0028, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0028, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629374
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.02392268180847168
overhead3:: 0.05413246154785156
overhead4:: 0.281024694442749
overhead5:: 0
memory usage:: 3769700352
time_provenance:: 1.2796030044555664
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0028, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0028, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629322
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.02509021759033203
overhead3:: 0.0652918815612793
overhead4:: 0.2967708110809326
overhead5:: 0
memory usage:: 3763679232
time_provenance:: 1.2134857177734375
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0028, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0028, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629374
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.028196334838867188
overhead3:: 0.06052970886230469
overhead4:: 0.36341071128845215
overhead5:: 0
memory usage:: 3771060224
time_provenance:: 1.3033721446990967
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0028, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0028, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629322
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.031668901443481445
overhead3:: 0.08248257637023926
overhead4:: 0.39725685119628906
overhead5:: 0
memory usage:: 3791314944
time_provenance:: 1.365673542022705
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0027, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0027, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629322
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.033455848693847656
overhead3:: 0.08957505226135254
overhead4:: 0.42903709411621094
overhead5:: 0
memory usage:: 3765641216
time_provenance:: 1.4197673797607422
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0027, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0027, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629322
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.04662132263183594
overhead3:: 0.07661056518554688
overhead4:: 0.4639706611633301
overhead5:: 0
memory usage:: 3799334912
time_provenance:: 1.4448189735412598
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0027, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0027, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629322
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.04144406318664551
overhead3:: 0.0939633846282959
overhead4:: 0.48635077476501465
overhead5:: 0
memory usage:: 3779784704
time_provenance:: 1.5155603885650635
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0027, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0027, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629322
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.04273843765258789
overhead3:: 0.08785867691040039
overhead4:: 0.5210609436035156
overhead5:: 0
memory usage:: 3789905920
time_provenance:: 1.5170793533325195
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0027, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0027, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629322
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.0586087703704834
overhead3:: 0.10046935081481934
overhead4:: 0.8007404804229736
overhead5:: 0
memory usage:: 3789066240
time_provenance:: 1.847362995147705
curr_diff: 0 tensor(4.2994e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.2994e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0027, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0027, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629357
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.06037545204162598
overhead3:: 0.10855484008789062
overhead4:: 0.8404541015625
overhead5:: 0
memory usage:: 3770167296
time_provenance:: 1.8850233554840088
curr_diff: 0 tensor(4.4388e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.4388e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0027, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0027, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629357
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.07216429710388184
overhead3:: 0.11141061782836914
overhead4:: 0.7927188873291016
overhead5:: 0
memory usage:: 3762524160
time_provenance:: 1.82285737991333
curr_diff: 0 tensor(4.6763e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.6763e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0027, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0027, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629357
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.06715106964111328
overhead3:: 0.1138770580291748
overhead4:: 0.8589046001434326
overhead5:: 0
memory usage:: 3789541376
time_provenance:: 1.9030296802520752
curr_diff: 0 tensor(4.7054e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.7054e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0027, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0027, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629357
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.06910181045532227
overhead3:: 0.1272447109222412
overhead4:: 0.9186186790466309
overhead5:: 0
memory usage:: 3762933760
time_provenance:: 1.9902546405792236
curr_diff: 0 tensor(4.8232e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.8232e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0027, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0027, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629357
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.150618314743042
overhead3:: 0.21344614028930664
overhead4:: 1.7487235069274902
overhead5:: 0
memory usage:: 3798867968
time_provenance:: 2.95068097114563
curr_diff: 0 tensor(8.3021e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.3021e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0026, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0026, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629357
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.1565084457397461
overhead3:: 0.23642611503601074
overhead4:: 2.087376594543457
overhead5:: 0
memory usage:: 3775447040
time_provenance:: 3.3896772861480713
curr_diff: 0 tensor(8.5016e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.5016e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0026, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0026, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629357
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.1880803108215332
overhead3:: 0.2965106964111328
overhead4:: 2.254885196685791
overhead5:: 0
memory usage:: 3790004224
time_provenance:: 3.7252399921417236
curr_diff: 0 tensor(8.9594e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.9594e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0026, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0026, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629357
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.19103264808654785
overhead3:: 0.29615044593811035
overhead4:: 2.6376872062683105
overhead5:: 0
memory usage:: 3769307136
time_provenance:: 4.191049575805664
curr_diff: 0 tensor(9.1451e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.1451e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0026, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0026, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629357
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.1483139991760254
overhead3:: 0.24150347709655762
overhead4:: 1.9683325290679932
overhead5:: 0
memory usage:: 3809464320
time_provenance:: 3.199119806289673
curr_diff: 0 tensor(9.3206e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.3206e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0026, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0026, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629357
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 0 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.2916526794433594
overhead3:: 0.39304471015930176
overhead4:: 3.1945154666900635
overhead5:: 0
memory usage:: 3772755968
time_provenance:: 4.29639196395874
curr_diff: 0 tensor(1.0022e-13, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0022e-13, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1., dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0026, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0026, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629357
repetition 1
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 1 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 2.008473
Train - Epoch 0, Batch: 10, Loss: 1.393623
Train - Epoch 0, Batch: 20, Loss: 1.237174
Train - Epoch 0, Batch: 30, Loss: 1.191914
Train - Epoch 1, Batch: 0, Loss: 1.173850
Train - Epoch 1, Batch: 10, Loss: 1.146160
Train - Epoch 1, Batch: 20, Loss: 1.146836
Train - Epoch 1, Batch: 30, Loss: 1.120950
Train - Epoch 2, Batch: 0, Loss: 1.116349
Train - Epoch 2, Batch: 10, Loss: 1.120367
Train - Epoch 2, Batch: 20, Loss: 1.107696
Train - Epoch 2, Batch: 30, Loss: 1.096745
Train - Epoch 3, Batch: 0, Loss: 1.108801
Train - Epoch 3, Batch: 10, Loss: 1.085706
Train - Epoch 3, Batch: 20, Loss: 1.082335
Train - Epoch 3, Batch: 30, Loss: 1.073643
Train - Epoch 4, Batch: 0, Loss: 1.080052
Train - Epoch 4, Batch: 10, Loss: 1.065494
Train - Epoch 4, Batch: 20, Loss: 1.058559
Train - Epoch 4, Batch: 30, Loss: 1.052690
Train - Epoch 5, Batch: 0, Loss: 1.045505
Train - Epoch 5, Batch: 10, Loss: 1.046420
Train - Epoch 5, Batch: 20, Loss: 1.042113
Train - Epoch 5, Batch: 30, Loss: 1.038886
Train - Epoch 6, Batch: 0, Loss: 1.048084
Train - Epoch 6, Batch: 10, Loss: 1.030484
Train - Epoch 6, Batch: 20, Loss: 1.029552
Train - Epoch 6, Batch: 30, Loss: 1.023616
Train - Epoch 7, Batch: 0, Loss: 1.018764
Train - Epoch 7, Batch: 10, Loss: 1.013619
Train - Epoch 7, Batch: 20, Loss: 1.017429
Train - Epoch 7, Batch: 30, Loss: 1.004255
Train - Epoch 8, Batch: 0, Loss: 1.011844
Train - Epoch 8, Batch: 10, Loss: 0.998599
Train - Epoch 8, Batch: 20, Loss: 1.011703
Train - Epoch 8, Batch: 30, Loss: 0.992054
Train - Epoch 9, Batch: 0, Loss: 0.993411
Train - Epoch 9, Batch: 10, Loss: 0.993515
Train - Epoch 9, Batch: 20, Loss: 0.983892
Train - Epoch 9, Batch: 30, Loss: 0.991428
Train - Epoch 10, Batch: 0, Loss: 0.989915
Train - Epoch 10, Batch: 10, Loss: 0.981533
Train - Epoch 10, Batch: 20, Loss: 0.979696
Train - Epoch 10, Batch: 30, Loss: 0.971766
Train - Epoch 11, Batch: 0, Loss: 0.980887
Train - Epoch 11, Batch: 10, Loss: 0.980021
Train - Epoch 11, Batch: 20, Loss: 0.976300
Train - Epoch 11, Batch: 30, Loss: 0.965382
Train - Epoch 12, Batch: 0, Loss: 0.979777
Train - Epoch 12, Batch: 10, Loss: 0.965809
Train - Epoch 12, Batch: 20, Loss: 0.964585
Train - Epoch 12, Batch: 30, Loss: 0.953870
Train - Epoch 13, Batch: 0, Loss: 0.961875
Train - Epoch 13, Batch: 10, Loss: 0.964481
Train - Epoch 13, Batch: 20, Loss: 0.968492
Train - Epoch 13, Batch: 30, Loss: 0.960013
Train - Epoch 14, Batch: 0, Loss: 0.952928
Train - Epoch 14, Batch: 10, Loss: 0.957082
Train - Epoch 14, Batch: 20, Loss: 0.960320
Train - Epoch 14, Batch: 30, Loss: 0.958189
Train - Epoch 15, Batch: 0, Loss: 0.953143
Train - Epoch 15, Batch: 10, Loss: 0.955233
Train - Epoch 15, Batch: 20, Loss: 0.948414
Train - Epoch 15, Batch: 30, Loss: 0.943435
Train - Epoch 16, Batch: 0, Loss: 0.950593
Train - Epoch 16, Batch: 10, Loss: 0.948365
Train - Epoch 16, Batch: 20, Loss: 0.945743
Train - Epoch 16, Batch: 30, Loss: 0.940352
Train - Epoch 17, Batch: 0, Loss: 0.941524
Train - Epoch 17, Batch: 10, Loss: 0.940307
Train - Epoch 17, Batch: 20, Loss: 0.940580
Train - Epoch 17, Batch: 30, Loss: 0.938113
Train - Epoch 18, Batch: 0, Loss: 0.937754
Train - Epoch 18, Batch: 10, Loss: 0.942039
Train - Epoch 18, Batch: 20, Loss: 0.921660
Train - Epoch 18, Batch: 30, Loss: 0.932488
Train - Epoch 19, Batch: 0, Loss: 0.936586
Train - Epoch 19, Batch: 10, Loss: 0.929920
Train - Epoch 19, Batch: 20, Loss: 0.920193
Train - Epoch 19, Batch: 30, Loss: 0.921611
Train - Epoch 20, Batch: 0, Loss: 0.930709
Train - Epoch 20, Batch: 10, Loss: 0.924796
Train - Epoch 20, Batch: 20, Loss: 0.932363
Train - Epoch 20, Batch: 30, Loss: 0.918228
Train - Epoch 21, Batch: 0, Loss: 0.926106
Train - Epoch 21, Batch: 10, Loss: 0.922950
Train - Epoch 21, Batch: 20, Loss: 0.922630
Train - Epoch 21, Batch: 30, Loss: 0.924338
Train - Epoch 22, Batch: 0, Loss: 0.913504
Train - Epoch 22, Batch: 10, Loss: 0.922354
Train - Epoch 22, Batch: 20, Loss: 0.915927
Train - Epoch 22, Batch: 30, Loss: 0.922520
Train - Epoch 23, Batch: 0, Loss: 0.926617
Train - Epoch 23, Batch: 10, Loss: 0.920223
Train - Epoch 23, Batch: 20, Loss: 0.921670
Train - Epoch 23, Batch: 30, Loss: 0.901785
Train - Epoch 24, Batch: 0, Loss: 0.912287
Train - Epoch 24, Batch: 10, Loss: 0.907376
Train - Epoch 24, Batch: 20, Loss: 0.912756
Train - Epoch 24, Batch: 30, Loss: 0.914273
Train - Epoch 25, Batch: 0, Loss: 0.912833
Train - Epoch 25, Batch: 10, Loss: 0.919986
Train - Epoch 25, Batch: 20, Loss: 0.913374
Train - Epoch 25, Batch: 30, Loss: 0.907468
Train - Epoch 26, Batch: 0, Loss: 0.914805
Train - Epoch 26, Batch: 10, Loss: 0.902046
Train - Epoch 26, Batch: 20, Loss: 0.909672
Train - Epoch 26, Batch: 30, Loss: 0.905687
Train - Epoch 27, Batch: 0, Loss: 0.908105
Train - Epoch 27, Batch: 10, Loss: 0.909178
Train - Epoch 27, Batch: 20, Loss: 0.903186
Train - Epoch 27, Batch: 30, Loss: 0.899105
Train - Epoch 28, Batch: 0, Loss: 0.909224
Train - Epoch 28, Batch: 10, Loss: 0.894546
Train - Epoch 28, Batch: 20, Loss: 0.904880
Train - Epoch 28, Batch: 30, Loss: 0.895615
Train - Epoch 29, Batch: 0, Loss: 0.902749
Train - Epoch 29, Batch: 10, Loss: 0.918891
Train - Epoch 29, Batch: 20, Loss: 0.892284
Train - Epoch 29, Batch: 30, Loss: 0.892927
Train - Epoch 30, Batch: 0, Loss: 0.902935
Train - Epoch 30, Batch: 10, Loss: 0.892314
Train - Epoch 30, Batch: 20, Loss: 0.890562
Train - Epoch 30, Batch: 30, Loss: 0.895105
Train - Epoch 31, Batch: 0, Loss: 0.889127
Train - Epoch 31, Batch: 10, Loss: 0.895831
Train - Epoch 31, Batch: 20, Loss: 0.905239
Train - Epoch 31, Batch: 30, Loss: 0.892525
Test Avg. Loss: 0.000070, Accuracy: 0.629856
training_time:: 3.3985798358917236
training time full:: 3.398643732070923
provenance prepare time:: 5.9604644775390625e-06
Test Avg. Loss: 0.000070, Accuracy: 0.629856
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
batch_size:: 16384
epoch:: 0
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 3.6655561923980713
overhead:: 0
overhead2:: 1.8033323287963867
overhead3:: 0
time_baseline:: 3.666543483734131
curr_diff: 0 tensor(0.0026, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0026, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629873
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.020134449005126953
overhead3:: 0.05802583694458008
overhead4:: 0.2311110496520996
overhead5:: 0
memory usage:: 3764133888
time_provenance:: 1.3000009059906006
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0028, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0028, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629873
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.021001100540161133
overhead3:: 0.05713677406311035
overhead4:: 0.2550318241119385
overhead5:: 0
memory usage:: 3784495104
time_provenance:: 1.2192542552947998
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629890
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.025037288665771484
overhead3:: 0.0640571117401123
overhead4:: 0.29128456115722656
overhead5:: 0
memory usage:: 3769868288
time_provenance:: 1.3386197090148926
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0028, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0028, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629873
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.03251147270202637
overhead3:: 0.05671858787536621
overhead4:: 0.3068268299102783
overhead5:: 0
memory usage:: 3763105792
time_provenance:: 1.2686874866485596
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629890
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.02956676483154297
overhead3:: 0.061814308166503906
overhead4:: 0.35156798362731934
overhead5:: 0
memory usage:: 3770191872
time_provenance:: 1.3993809223175049
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0028, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0028, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629873
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.03851723670959473
overhead3:: 0.07235264778137207
overhead4:: 0.4007740020751953
overhead5:: 0
memory usage:: 3772022784
time_provenance:: 1.3740732669830322
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0027, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0027, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629873
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.04286956787109375
overhead3:: 0.07843637466430664
overhead4:: 0.4189941883087158
overhead5:: 0
memory usage:: 3768627200
time_provenance:: 1.418482780456543
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0027, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0027, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629873
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.03773665428161621
overhead3:: 0.07596683502197266
overhead4:: 0.48336315155029297
overhead5:: 0
memory usage:: 3761881088
time_provenance:: 1.4827239513397217
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0027, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0027, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629873
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.03881573677062988
overhead3:: 0.08123445510864258
overhead4:: 0.4682130813598633
overhead5:: 0
memory usage:: 3801059328
time_provenance:: 1.4225904941558838
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0027, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0027, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629873
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.04185819625854492
overhead3:: 0.07955670356750488
overhead4:: 0.5370168685913086
overhead5:: 0
memory usage:: 3774472192
time_provenance:: 1.5157179832458496
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0028, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0028, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629873
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.059594154357910156
overhead3:: 0.10000896453857422
overhead4:: 0.7447259426116943
overhead5:: 0
memory usage:: 3764240384
time_provenance:: 1.7444324493408203
curr_diff: 0 tensor(5.1449e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.1449e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0027, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0027, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629873
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.06591081619262695
overhead3:: 0.1080169677734375
overhead4:: 0.7748987674713135
overhead5:: 0
memory usage:: 3790135296
time_provenance:: 1.8093762397766113
curr_diff: 0 tensor(5.2499e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.2499e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0027, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0027, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629873
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.06521725654602051
overhead3:: 0.11125016212463379
overhead4:: 0.8721606731414795
overhead5:: 0
memory usage:: 3776155648
time_provenance:: 1.93178391456604
curr_diff: 0 tensor(5.3814e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.3814e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0027, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0027, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629873
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.0686345100402832
overhead3:: 0.11832690238952637
overhead4:: 0.8964202404022217
overhead5:: 0
memory usage:: 3769946112
time_provenance:: 1.9896876811981201
curr_diff: 0 tensor(5.2190e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.2190e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0027, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0027, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629873
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.07293915748596191
overhead3:: 0.12183737754821777
overhead4:: 0.9145932197570801
overhead5:: 0
memory usage:: 3778023424
time_provenance:: 1.99796462059021
curr_diff: 0 tensor(5.6338e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.6338e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0027, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0027, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629873
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.15048742294311523
overhead3:: 0.23611044883728027
overhead4:: 1.9205970764160156
overhead5:: 0
memory usage:: 3763564544
time_provenance:: 3.175438642501831
curr_diff: 0 tensor(1.2345e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2345e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0026, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0026, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629873
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.15407443046569824
overhead3:: 0.2306075096130371
overhead4:: 1.9241597652435303
overhead5:: 0
memory usage:: 3785449472
time_provenance:: 3.1571502685546875
curr_diff: 0 tensor(1.2342e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2342e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0026, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0026, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629873
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.1929316520690918
overhead3:: 0.3051021099090576
overhead4:: 2.72147274017334
overhead5:: 0
memory usage:: 3777363968
time_provenance:: 4.330229759216309
curr_diff: 0 tensor(1.2422e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2422e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0026, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0026, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629873
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.15216994285583496
overhead3:: 0.23272228240966797
overhead4:: 1.8341162204742432
overhead5:: 0
memory usage:: 3779002368
time_provenance:: 3.067124128341675
curr_diff: 0 tensor(1.2552e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2552e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0026, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0026, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629873
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.14975810050964355
overhead3:: 0.24527621269226074
overhead4:: 1.9622135162353516
overhead5:: 0
memory usage:: 3777220608
time_provenance:: 3.2038912773132324
curr_diff: 0 tensor(1.3039e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3039e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0026, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0026, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629873
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 1 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.35484957695007324
overhead3:: 0.5640225410461426
overhead4:: 3.5660412311553955
overhead5:: 0
memory usage:: 3801808896
time_provenance:: 4.824933052062988
curr_diff: 0 tensor(1.0095e-13, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0095e-13, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0026, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0026, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629873
repetition 2
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 2 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 1.924058
Train - Epoch 0, Batch: 10, Loss: 1.389503
Train - Epoch 0, Batch: 20, Loss: 1.256957
Train - Epoch 0, Batch: 30, Loss: 1.199163
Train - Epoch 1, Batch: 0, Loss: 1.187565
Train - Epoch 1, Batch: 10, Loss: 1.163633
Train - Epoch 1, Batch: 20, Loss: 1.148659
Train - Epoch 1, Batch: 30, Loss: 1.135504
Train - Epoch 2, Batch: 0, Loss: 1.137360
Train - Epoch 2, Batch: 10, Loss: 1.126511
Train - Epoch 2, Batch: 20, Loss: 1.104941
Train - Epoch 2, Batch: 30, Loss: 1.119582
Train - Epoch 3, Batch: 0, Loss: 1.105172
Train - Epoch 3, Batch: 10, Loss: 1.105954
Train - Epoch 3, Batch: 20, Loss: 1.085263
Train - Epoch 3, Batch: 30, Loss: 1.089927
Train - Epoch 4, Batch: 0, Loss: 1.079902
Train - Epoch 4, Batch: 10, Loss: 1.065479
Train - Epoch 4, Batch: 20, Loss: 1.064791
Train - Epoch 4, Batch: 30, Loss: 1.053770
Train - Epoch 5, Batch: 0, Loss: 1.057723
Train - Epoch 5, Batch: 10, Loss: 1.058398
Train - Epoch 5, Batch: 20, Loss: 1.041675
Train - Epoch 5, Batch: 30, Loss: 1.043482
Train - Epoch 6, Batch: 0, Loss: 1.045232
Train - Epoch 6, Batch: 10, Loss: 1.027841
Train - Epoch 6, Batch: 20, Loss: 1.036416
Train - Epoch 6, Batch: 30, Loss: 1.020651
Train - Epoch 7, Batch: 0, Loss: 1.021143
Train - Epoch 7, Batch: 10, Loss: 1.028204
Train - Epoch 7, Batch: 20, Loss: 1.016941
Train - Epoch 7, Batch: 30, Loss: 1.017315
Train - Epoch 8, Batch: 0, Loss: 1.008015
Train - Epoch 8, Batch: 10, Loss: 1.007837
Train - Epoch 8, Batch: 20, Loss: 1.004722
Train - Epoch 8, Batch: 30, Loss: 1.004771
Train - Epoch 9, Batch: 0, Loss: 1.001066
Train - Epoch 9, Batch: 10, Loss: 1.001542
Train - Epoch 9, Batch: 20, Loss: 1.001172
Train - Epoch 9, Batch: 30, Loss: 0.986469
Train - Epoch 10, Batch: 0, Loss: 0.993001
Train - Epoch 10, Batch: 10, Loss: 0.988971
Train - Epoch 10, Batch: 20, Loss: 0.975323
Train - Epoch 10, Batch: 30, Loss: 0.987404
Train - Epoch 11, Batch: 0, Loss: 0.982781
Train - Epoch 11, Batch: 10, Loss: 0.973099
Train - Epoch 11, Batch: 20, Loss: 0.975996
Train - Epoch 11, Batch: 30, Loss: 0.972594
Train - Epoch 12, Batch: 0, Loss: 0.972028
Train - Epoch 12, Batch: 10, Loss: 0.977534
Train - Epoch 12, Batch: 20, Loss: 0.967164
Train - Epoch 12, Batch: 30, Loss: 0.974554
Train - Epoch 13, Batch: 0, Loss: 0.967949
Train - Epoch 13, Batch: 10, Loss: 0.957437
Train - Epoch 13, Batch: 20, Loss: 0.961198
Train - Epoch 13, Batch: 30, Loss: 0.974600
Train - Epoch 14, Batch: 0, Loss: 0.958542
Train - Epoch 14, Batch: 10, Loss: 0.965481
Train - Epoch 14, Batch: 20, Loss: 0.957356
Train - Epoch 14, Batch: 30, Loss: 0.962038
Train - Epoch 15, Batch: 0, Loss: 0.956143
Train - Epoch 15, Batch: 10, Loss: 0.949295
Train - Epoch 15, Batch: 20, Loss: 0.944134
Train - Epoch 15, Batch: 30, Loss: 0.945458
Train - Epoch 16, Batch: 0, Loss: 0.954879
Train - Epoch 16, Batch: 10, Loss: 0.943944
Train - Epoch 16, Batch: 20, Loss: 0.936251
Train - Epoch 16, Batch: 30, Loss: 0.948656
Train - Epoch 17, Batch: 0, Loss: 0.950848
Train - Epoch 17, Batch: 10, Loss: 0.945604
Train - Epoch 17, Batch: 20, Loss: 0.924987
Train - Epoch 17, Batch: 30, Loss: 0.938495
Train - Epoch 18, Batch: 0, Loss: 0.933533
Train - Epoch 18, Batch: 10, Loss: 0.933702
Train - Epoch 18, Batch: 20, Loss: 0.932745
Train - Epoch 18, Batch: 30, Loss: 0.935722
Train - Epoch 19, Batch: 0, Loss: 0.931394
Train - Epoch 19, Batch: 10, Loss: 0.923248
Train - Epoch 19, Batch: 20, Loss: 0.934350
Train - Epoch 19, Batch: 30, Loss: 0.922341
Train - Epoch 20, Batch: 0, Loss: 0.926944
Train - Epoch 20, Batch: 10, Loss: 0.922975
Train - Epoch 20, Batch: 20, Loss: 0.920510
Train - Epoch 20, Batch: 30, Loss: 0.923336
Train - Epoch 21, Batch: 0, Loss: 0.928052
Train - Epoch 21, Batch: 10, Loss: 0.920278
Train - Epoch 21, Batch: 20, Loss: 0.916963
Train - Epoch 21, Batch: 30, Loss: 0.917033
Train - Epoch 22, Batch: 0, Loss: 0.921631
Train - Epoch 22, Batch: 10, Loss: 0.924634
Train - Epoch 22, Batch: 20, Loss: 0.922362
Train - Epoch 22, Batch: 30, Loss: 0.915694
Train - Epoch 23, Batch: 0, Loss: 0.917936
Train - Epoch 23, Batch: 10, Loss: 0.915914
Train - Epoch 23, Batch: 20, Loss: 0.910940
Train - Epoch 23, Batch: 30, Loss: 0.919907
Train - Epoch 24, Batch: 0, Loss: 0.910154
Train - Epoch 24, Batch: 10, Loss: 0.920608
Train - Epoch 24, Batch: 20, Loss: 0.907231
Train - Epoch 24, Batch: 30, Loss: 0.910415
Train - Epoch 25, Batch: 0, Loss: 0.905320
Train - Epoch 25, Batch: 10, Loss: 0.917684
Train - Epoch 25, Batch: 20, Loss: 0.916588
Train - Epoch 25, Batch: 30, Loss: 0.919313
Train - Epoch 26, Batch: 0, Loss: 0.906410
Train - Epoch 26, Batch: 10, Loss: 0.906644
Train - Epoch 26, Batch: 20, Loss: 0.900700
Train - Epoch 26, Batch: 30, Loss: 0.909371
Train - Epoch 27, Batch: 0, Loss: 0.910457
Train - Epoch 27, Batch: 10, Loss: 0.901843
Train - Epoch 27, Batch: 20, Loss: 0.917661
Train - Epoch 27, Batch: 30, Loss: 0.897870
Train - Epoch 28, Batch: 0, Loss: 0.907035
Train - Epoch 28, Batch: 10, Loss: 0.903632
Train - Epoch 28, Batch: 20, Loss: 0.910029
Train - Epoch 28, Batch: 30, Loss: 0.904416
Train - Epoch 29, Batch: 0, Loss: 0.900852
Train - Epoch 29, Batch: 10, Loss: 0.893348
Train - Epoch 29, Batch: 20, Loss: 0.901253
Train - Epoch 29, Batch: 30, Loss: 0.899028
Train - Epoch 30, Batch: 0, Loss: 0.898263
Train - Epoch 30, Batch: 10, Loss: 0.900177
Train - Epoch 30, Batch: 20, Loss: 0.903333
Train - Epoch 30, Batch: 30, Loss: 0.890363
Train - Epoch 31, Batch: 0, Loss: 0.890651
Train - Epoch 31, Batch: 10, Loss: 0.891890
Train - Epoch 31, Batch: 20, Loss: 0.895145
Train - Epoch 31, Batch: 30, Loss: 0.882639
Test Avg. Loss: 0.000070, Accuracy: 0.630183
training_time:: 3.5245237350463867
training time full:: 3.5245895385742188
provenance prepare time:: 6.4373016357421875e-06
Test Avg. Loss: 0.000070, Accuracy: 0.630183
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
batch_size:: 16384
epoch:: 0
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 3.4967429637908936
overhead:: 0
overhead2:: 1.7471985816955566
overhead3:: 0
time_baseline:: 3.497612237930298
curr_diff: 0 tensor(0.0026, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0026, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630269
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.018393278121948242
overhead3:: 0.04691743850708008
overhead4:: 0.21442437171936035
overhead5:: 0
memory usage:: 3779170304
time_provenance:: 1.1716139316558838
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0028, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0028, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630252
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.021767139434814453
overhead3:: 0.06270408630371094
overhead4:: 0.25173473358154297
overhead5:: 0
memory usage:: 3764256768
time_provenance:: 1.2416625022888184
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0028, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0028, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630269
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.023816823959350586
overhead3:: 0.055227041244506836
overhead4:: 0.28667783737182617
overhead5:: 0
memory usage:: 3764363264
time_provenance:: 1.2872028350830078
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0028, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0028, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630252
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.0252225399017334
overhead3:: 0.058191776275634766
overhead4:: 0.2998347282409668
overhead5:: 0
memory usage:: 3803181056
time_provenance:: 1.2513127326965332
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630269
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.028388261795043945
overhead3:: 0.06836104393005371
overhead4:: 0.33201098442077637
overhead5:: 0
memory usage:: 3789996032
time_provenance:: 1.2840726375579834
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630252
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.03391122817993164
overhead3:: 0.08188152313232422
overhead4:: 0.40268826484680176
overhead5:: 0
memory usage:: 3768836096
time_provenance:: 1.409102439880371
curr_diff: 0 tensor(6.3153e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.3153e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0027, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0027, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630269
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.033440351486206055
overhead3:: 0.06696653366088867
overhead4:: 0.42537641525268555
overhead5:: 0
memory usage:: 3763367936
time_provenance:: 1.3486247062683105
curr_diff: 0 tensor(6.7274e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.7274e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0027, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0027, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630269
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.044408321380615234
overhead3:: 0.08318209648132324
overhead4:: 0.4452846050262451
overhead5:: 0
memory usage:: 3765288960
time_provenance:: 1.4352784156799316
curr_diff: 0 tensor(7.0390e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.0390e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0027, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0027, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630269
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.04068946838378906
overhead3:: 0.09159326553344727
overhead4:: 0.4918844699859619
overhead5:: 0
memory usage:: 3799220224
time_provenance:: 1.5252599716186523
curr_diff: 0 tensor(7.2296e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.2296e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0027, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0027, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630269
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.04596590995788574
overhead3:: 0.09157633781433105
overhead4:: 0.5401473045349121
overhead5:: 0
memory usage:: 3769516032
time_provenance:: 1.5883870124816895
curr_diff: 0 tensor(7.1277e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.1277e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0027, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0027, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630269
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.06334948539733887
overhead3:: 0.11334037780761719
overhead4:: 0.8111450672149658
overhead5:: 0
memory usage:: 3775774720
time_provenance:: 1.8700940608978271
curr_diff: 0 tensor(3.8599e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.8599e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0027, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0027, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630269
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.07093477249145508
overhead3:: 0.11687064170837402
overhead4:: 0.8187718391418457
overhead5:: 0
memory usage:: 3778834432
time_provenance:: 1.9572575092315674
curr_diff: 0 tensor(3.9542e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.9542e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0027, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0027, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630269
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.07696676254272461
overhead3:: 0.12514042854309082
overhead4:: 0.9752054214477539
overhead5:: 0
memory usage:: 3786133504
time_provenance:: 2.1729679107666016
curr_diff: 0 tensor(4.0435e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.0435e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0027, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0027, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630269
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.06920814514160156
overhead3:: 0.11633992195129395
overhead4:: 0.8439843654632568
overhead5:: 0
memory usage:: 3789459456
time_provenance:: 1.90317964553833
curr_diff: 0 tensor(4.1464e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.1464e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0027, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0027, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630269
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.0728144645690918
overhead3:: 0.12375259399414062
overhead4:: 0.8798916339874268
overhead5:: 0
memory usage:: 3771396096
time_provenance:: 1.9678504467010498
curr_diff: 0 tensor(4.1502e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.1502e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0027, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0027, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630269
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.14500117301940918
overhead3:: 0.2233119010925293
overhead4:: 1.954512596130371
overhead5:: 0
memory usage:: 3775578112
time_provenance:: 3.1890957355499268
curr_diff: 0 tensor(7.3375e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.3375e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0026, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0026, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630269
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.14604520797729492
overhead3:: 0.22847747802734375
overhead4:: 1.9571123123168945
overhead5:: 0
memory usage:: 3767091200
time_provenance:: 3.1856064796447754
curr_diff: 0 tensor(7.6776e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.6776e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0026, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0026, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630269
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.14518022537231445
overhead3:: 0.23410844802856445
overhead4:: 1.954751968383789
overhead5:: 0
memory usage:: 3791179776
time_provenance:: 3.195814847946167
curr_diff: 0 tensor(7.8777e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.8777e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0026, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0026, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630269
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.1846005916595459
overhead3:: 0.28803586959838867
overhead4:: 2.564173698425293
overhead5:: 0
memory usage:: 3812253696
time_provenance:: 4.082782745361328
curr_diff: 0 tensor(8.2422e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.2422e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0026, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0026, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630269
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.19505906105041504
overhead3:: 0.28911685943603516
overhead4:: 2.750958204269409
overhead5:: 0
memory usage:: 3770404864
time_provenance:: 4.329062461853027
curr_diff: 0 tensor(8.4380e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.4380e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0026, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0026, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630269
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 2 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.31977128982543945
overhead3:: 0.49026942253112793
overhead4:: 3.4467973709106445
overhead5:: 0
memory usage:: 3780091904
time_provenance:: 4.637514352798462
curr_diff: 0 tensor(9.9725e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.9725e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0026, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0026, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630269
repetition 3
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 3 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 2.007491
Train - Epoch 0, Batch: 10, Loss: 1.392076
Train - Epoch 0, Batch: 20, Loss: 1.238473
Train - Epoch 0, Batch: 30, Loss: 1.188129
Train - Epoch 1, Batch: 0, Loss: 1.169918
Train - Epoch 1, Batch: 10, Loss: 1.153425
Train - Epoch 1, Batch: 20, Loss: 1.125512
Train - Epoch 1, Batch: 30, Loss: 1.124992
Train - Epoch 2, Batch: 0, Loss: 1.126705
Train - Epoch 2, Batch: 10, Loss: 1.108772
Train - Epoch 2, Batch: 20, Loss: 1.100275
Train - Epoch 2, Batch: 30, Loss: 1.099045
Train - Epoch 3, Batch: 0, Loss: 1.099227
Train - Epoch 3, Batch: 10, Loss: 1.081969
Train - Epoch 3, Batch: 20, Loss: 1.071108
Train - Epoch 3, Batch: 30, Loss: 1.063609
Train - Epoch 4, Batch: 0, Loss: 1.063572
Train - Epoch 4, Batch: 10, Loss: 1.065289
Train - Epoch 4, Batch: 20, Loss: 1.050501
Train - Epoch 4, Batch: 30, Loss: 1.061035
Train - Epoch 5, Batch: 0, Loss: 1.048525
Train - Epoch 5, Batch: 10, Loss: 1.045584
Train - Epoch 5, Batch: 20, Loss: 1.046460
Train - Epoch 5, Batch: 30, Loss: 1.035367
Train - Epoch 6, Batch: 0, Loss: 1.024162
Train - Epoch 6, Batch: 10, Loss: 1.033504
Train - Epoch 6, Batch: 20, Loss: 1.022739
Train - Epoch 6, Batch: 30, Loss: 1.022369
Train - Epoch 7, Batch: 0, Loss: 1.019366
Train - Epoch 7, Batch: 10, Loss: 1.002801
Train - Epoch 7, Batch: 20, Loss: 0.997474
Train - Epoch 7, Batch: 30, Loss: 1.003162
Train - Epoch 8, Batch: 0, Loss: 1.002049
Train - Epoch 8, Batch: 10, Loss: 1.002031
Train - Epoch 8, Batch: 20, Loss: 1.006140
Train - Epoch 8, Batch: 30, Loss: 0.996338
Train - Epoch 9, Batch: 0, Loss: 0.998294
Train - Epoch 9, Batch: 10, Loss: 0.992385
Train - Epoch 9, Batch: 20, Loss: 0.997609
Train - Epoch 9, Batch: 30, Loss: 0.988492
Train - Epoch 10, Batch: 0, Loss: 0.989195
Train - Epoch 10, Batch: 10, Loss: 0.979991
Train - Epoch 10, Batch: 20, Loss: 0.979389
Train - Epoch 10, Batch: 30, Loss: 0.973378
Train - Epoch 11, Batch: 0, Loss: 0.965035
Train - Epoch 11, Batch: 10, Loss: 0.974122
Train - Epoch 11, Batch: 20, Loss: 0.969103
Train - Epoch 11, Batch: 30, Loss: 0.964697
Train - Epoch 12, Batch: 0, Loss: 0.966568
Train - Epoch 12, Batch: 10, Loss: 0.963798
Train - Epoch 12, Batch: 20, Loss: 0.955847
Train - Epoch 12, Batch: 30, Loss: 0.965879
Train - Epoch 13, Batch: 0, Loss: 0.967571
Train - Epoch 13, Batch: 10, Loss: 0.962674
Train - Epoch 13, Batch: 20, Loss: 0.960325
Train - Epoch 13, Batch: 30, Loss: 0.949453
Train - Epoch 14, Batch: 0, Loss: 0.962305
Train - Epoch 14, Batch: 10, Loss: 0.951450
Train - Epoch 14, Batch: 20, Loss: 0.956923
Train - Epoch 14, Batch: 30, Loss: 0.943214
Train - Epoch 15, Batch: 0, Loss: 0.955707
Train - Epoch 15, Batch: 10, Loss: 0.954210
Train - Epoch 15, Batch: 20, Loss: 0.940589
Train - Epoch 15, Batch: 30, Loss: 0.945461
Train - Epoch 16, Batch: 0, Loss: 0.938717
Train - Epoch 16, Batch: 10, Loss: 0.942967
Train - Epoch 16, Batch: 20, Loss: 0.951064
Train - Epoch 16, Batch: 30, Loss: 0.940010
Train - Epoch 17, Batch: 0, Loss: 0.943378
Train - Epoch 17, Batch: 10, Loss: 0.939857
Train - Epoch 17, Batch: 20, Loss: 0.929645
Train - Epoch 17, Batch: 30, Loss: 0.937470
Train - Epoch 18, Batch: 0, Loss: 0.926681
Train - Epoch 18, Batch: 10, Loss: 0.929767
Train - Epoch 18, Batch: 20, Loss: 0.929325
Train - Epoch 18, Batch: 30, Loss: 0.923402
Train - Epoch 19, Batch: 0, Loss: 0.924705
Train - Epoch 19, Batch: 10, Loss: 0.932451
Train - Epoch 19, Batch: 20, Loss: 0.926195
Train - Epoch 19, Batch: 30, Loss: 0.922659
Train - Epoch 20, Batch: 0, Loss: 0.913627
Train - Epoch 20, Batch: 10, Loss: 0.916566
Train - Epoch 20, Batch: 20, Loss: 0.921125
Train - Epoch 20, Batch: 30, Loss: 0.927788
Train - Epoch 21, Batch: 0, Loss: 0.919383
Train - Epoch 21, Batch: 10, Loss: 0.930524
Train - Epoch 21, Batch: 20, Loss: 0.927432
Train - Epoch 21, Batch: 30, Loss: 0.911951
Train - Epoch 22, Batch: 0, Loss: 0.920938
Train - Epoch 22, Batch: 10, Loss: 0.912381
Train - Epoch 22, Batch: 20, Loss: 0.925369
Train - Epoch 22, Batch: 30, Loss: 0.918909
Train - Epoch 23, Batch: 0, Loss: 0.913154
Train - Epoch 23, Batch: 10, Loss: 0.911201
Train - Epoch 23, Batch: 20, Loss: 0.921582
Train - Epoch 23, Batch: 30, Loss: 0.914930
Train - Epoch 24, Batch: 0, Loss: 0.916057
Train - Epoch 24, Batch: 10, Loss: 0.906602
Train - Epoch 24, Batch: 20, Loss: 0.904814
Train - Epoch 24, Batch: 30, Loss: 0.904639
Train - Epoch 25, Batch: 0, Loss: 0.912737
Train - Epoch 25, Batch: 10, Loss: 0.909282
Train - Epoch 25, Batch: 20, Loss: 0.903112
Train - Epoch 25, Batch: 30, Loss: 0.906263
Train - Epoch 26, Batch: 0, Loss: 0.910394
Train - Epoch 26, Batch: 10, Loss: 0.903573
Train - Epoch 26, Batch: 20, Loss: 0.898815
Train - Epoch 26, Batch: 30, Loss: 0.896156
Train - Epoch 27, Batch: 0, Loss: 0.903013
Train - Epoch 27, Batch: 10, Loss: 0.904248
Train - Epoch 27, Batch: 20, Loss: 0.908774
Train - Epoch 27, Batch: 30, Loss: 0.906625
Train - Epoch 28, Batch: 0, Loss: 0.900513
Train - Epoch 28, Batch: 10, Loss: 0.892360
Train - Epoch 28, Batch: 20, Loss: 0.905682
Train - Epoch 28, Batch: 30, Loss: 0.897435
Train - Epoch 29, Batch: 0, Loss: 0.894987
Train - Epoch 29, Batch: 10, Loss: 0.899499
Train - Epoch 29, Batch: 20, Loss: 0.895104
Train - Epoch 29, Batch: 30, Loss: 0.896692
Train - Epoch 30, Batch: 0, Loss: 0.913490
Train - Epoch 30, Batch: 10, Loss: 0.894284
Train - Epoch 30, Batch: 20, Loss: 0.896988
Train - Epoch 30, Batch: 30, Loss: 0.900914
Train - Epoch 31, Batch: 0, Loss: 0.895034
Train - Epoch 31, Batch: 10, Loss: 0.899484
Train - Epoch 31, Batch: 20, Loss: 0.893186
Train - Epoch 31, Batch: 30, Loss: 0.886734
Test Avg. Loss: 0.000070, Accuracy: 0.628858
training_time:: 3.3862380981445312
training time full:: 3.386303424835205
provenance prepare time:: 7.867813110351562e-06
Test Avg. Loss: 0.000070, Accuracy: 0.628858
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
batch_size:: 16384
epoch:: 0
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 3.5062978267669678
overhead:: 0
overhead2:: 1.7589216232299805
overhead3:: 0
time_baseline:: 3.507181167602539
curr_diff: 0 tensor(0.0026, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0026, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628858
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.018328189849853516
overhead3:: 0.04770803451538086
overhead4:: 0.23842477798461914
overhead5:: 0
memory usage:: 3799023616
time_provenance:: 1.169055461883545
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0027, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0027, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628892
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.01997518539428711
overhead3:: 0.06115889549255371
overhead4:: 0.23874902725219727
overhead5:: 0
memory usage:: 3800113152
time_provenance:: 1.1548497676849365
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0028, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0028, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628892
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.02395033836364746
overhead3:: 0.05228257179260254
overhead4:: 0.28428030014038086
overhead5:: 0
memory usage:: 3828518912
time_provenance:: 1.3038175106048584
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0027, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0027, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628875
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.02923440933227539
overhead3:: 0.059711456298828125
overhead4:: 0.3093891143798828
overhead5:: 0
memory usage:: 3789905920
time_provenance:: 1.2815790176391602
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0028, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0028, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628892
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.029185771942138672
overhead3:: 0.0590815544128418
overhead4:: 0.33232712745666504
overhead5:: 0
memory usage:: 3770232832
time_provenance:: 1.2958533763885498
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0028, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0028, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628875
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.03398537635803223
overhead3:: 0.07677388191223145
overhead4:: 0.4022085666656494
overhead5:: 0
memory usage:: 3767259136
time_provenance:: 1.3969523906707764
curr_diff: 0 tensor(9.3529e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.3529e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0027, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0027, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628858
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.04366803169250488
overhead3:: 0.07426095008850098
overhead4:: 0.44128847122192383
overhead5:: 0
memory usage:: 3791917056
time_provenance:: 1.434990406036377
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0027, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0027, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628858
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.037035226821899414
overhead3:: 0.0835268497467041
overhead4:: 0.44402623176574707
overhead5:: 0
memory usage:: 3767226368
time_provenance:: 1.4312074184417725
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0027, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0027, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628858
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.04001641273498535
overhead3:: 0.07787632942199707
overhead4:: 0.4990420341491699
overhead5:: 0
memory usage:: 3776344064
time_provenance:: 1.5167531967163086
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0027, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0027, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628858
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.04780936241149902
overhead3:: 0.07832908630371094
overhead4:: 0.5137341022491455
overhead5:: 0
memory usage:: 3782762496
time_provenance:: 1.4969303607940674
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0027, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0027, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628858
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.06306862831115723
overhead3:: 0.10864090919494629
overhead4:: 0.8189952373504639
overhead5:: 0
memory usage:: 3766525952
time_provenance:: 1.8968780040740967
curr_diff: 0 tensor(4.9934e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.9934e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0026, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0026, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628858
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.06128501892089844
overhead3:: 0.12056922912597656
overhead4:: 0.8412017822265625
overhead5:: 0
memory usage:: 3770216448
time_provenance:: 1.8923544883728027
curr_diff: 0 tensor(5.1441e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.1441e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0026, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0026, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628858
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.06881284713745117
overhead3:: 0.12288880348205566
overhead4:: 0.8141109943389893
overhead5:: 0
memory usage:: 3773374464
time_provenance:: 1.8540308475494385
curr_diff: 0 tensor(5.1803e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.1803e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0026, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0026, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628858
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.06668877601623535
overhead3:: 0.1156623363494873
overhead4:: 0.8904211521148682
overhead5:: 0
memory usage:: 3799035904
time_provenance:: 1.9361088275909424
curr_diff: 0 tensor(5.4666e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.4666e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0027, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0027, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628858
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.06750297546386719
overhead3:: 0.12305712699890137
overhead4:: 0.8852121829986572
overhead5:: 0
memory usage:: 3762450432
time_provenance:: 1.9289524555206299
curr_diff: 0 tensor(5.5491e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.5491e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0027, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0027, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628858
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.15467453002929688
overhead3:: 0.22870945930480957
overhead4:: 1.9375524520874023
overhead5:: 0
memory usage:: 3782459392
time_provenance:: 3.174983263015747
curr_diff: 0 tensor(9.3299e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.3299e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0026, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0026, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628858
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.17806458473205566
overhead3:: 0.2677798271179199
overhead4:: 2.43512225151062
overhead5:: 0
memory usage:: 3800449024
time_provenance:: 3.8975603580474854
curr_diff: 0 tensor(9.4076e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.4076e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0026, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0026, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628858
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.15863370895385742
overhead3:: 0.24127459526062012
overhead4:: 1.8926029205322266
overhead5:: 0
memory usage:: 3764097024
time_provenance:: 3.155722141265869
curr_diff: 0 tensor(9.3130e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.3130e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0026, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0026, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628858
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.1432478427886963
overhead3:: 0.22393107414245605
overhead4:: 1.9478919506072998
overhead5:: 0
memory usage:: 3783135232
time_provenance:: 3.1582982540130615
curr_diff: 0 tensor(9.8480e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.8480e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0026, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0026, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628858
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.1472160816192627
overhead3:: 0.23659753799438477
overhead4:: 1.9721190929412842
overhead5:: 0
memory usage:: 3790512128
time_provenance:: 3.193514823913574
curr_diff: 0 tensor(9.7380e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.7380e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0026, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0026, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628858
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 3 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.3073842525482178
overhead3:: 0.43861818313598633
overhead4:: 3.3354439735412598
overhead5:: 0
memory usage:: 3754188800
time_provenance:: 4.4765944480896
curr_diff: 0 tensor(1.0210e-13, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0210e-13, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0026, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0026, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628858
repetition 4
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 4 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 1.979110
Train - Epoch 0, Batch: 10, Loss: 1.394575
Train - Epoch 0, Batch: 20, Loss: 1.242052
Train - Epoch 0, Batch: 30, Loss: 1.187119
Train - Epoch 1, Batch: 0, Loss: 1.179003
Train - Epoch 1, Batch: 10, Loss: 1.165713
Train - Epoch 1, Batch: 20, Loss: 1.138959
Train - Epoch 1, Batch: 30, Loss: 1.131737
Train - Epoch 2, Batch: 0, Loss: 1.140125
Train - Epoch 2, Batch: 10, Loss: 1.121284
Train - Epoch 2, Batch: 20, Loss: 1.108213
Train - Epoch 2, Batch: 30, Loss: 1.103987
Train - Epoch 3, Batch: 0, Loss: 1.089145
Train - Epoch 3, Batch: 10, Loss: 1.082400
Train - Epoch 3, Batch: 20, Loss: 1.082029
Train - Epoch 3, Batch: 30, Loss: 1.079588
Train - Epoch 4, Batch: 0, Loss: 1.083171
Train - Epoch 4, Batch: 10, Loss: 1.064156
Train - Epoch 4, Batch: 20, Loss: 1.053213
Train - Epoch 4, Batch: 30, Loss: 1.049354
Train - Epoch 5, Batch: 0, Loss: 1.049199
Train - Epoch 5, Batch: 10, Loss: 1.040864
Train - Epoch 5, Batch: 20, Loss: 1.048047
Train - Epoch 5, Batch: 30, Loss: 1.033189
Train - Epoch 6, Batch: 0, Loss: 1.033442
Train - Epoch 6, Batch: 10, Loss: 1.033009
Train - Epoch 6, Batch: 20, Loss: 1.024562
Train - Epoch 6, Batch: 30, Loss: 1.023041
Train - Epoch 7, Batch: 0, Loss: 1.018506
Train - Epoch 7, Batch: 10, Loss: 1.013320
Train - Epoch 7, Batch: 20, Loss: 1.010479
Train - Epoch 7, Batch: 30, Loss: 0.999562
Train - Epoch 8, Batch: 0, Loss: 1.005602
Train - Epoch 8, Batch: 10, Loss: 1.003003
Train - Epoch 8, Batch: 20, Loss: 0.998402
Train - Epoch 8, Batch: 30, Loss: 0.995407
Train - Epoch 9, Batch: 0, Loss: 0.989797
Train - Epoch 9, Batch: 10, Loss: 0.988114
Train - Epoch 9, Batch: 20, Loss: 0.990097
Train - Epoch 9, Batch: 30, Loss: 0.985137
Train - Epoch 10, Batch: 0, Loss: 0.995910
Train - Epoch 10, Batch: 10, Loss: 0.984651
Train - Epoch 10, Batch: 20, Loss: 0.978965
Train - Epoch 10, Batch: 30, Loss: 0.981363
Train - Epoch 11, Batch: 0, Loss: 0.974940
Train - Epoch 11, Batch: 10, Loss: 0.966514
Train - Epoch 11, Batch: 20, Loss: 0.976952
Train - Epoch 11, Batch: 30, Loss: 0.972510
Train - Epoch 12, Batch: 0, Loss: 0.977476
Train - Epoch 12, Batch: 10, Loss: 0.970773
Train - Epoch 12, Batch: 20, Loss: 0.967578
Train - Epoch 12, Batch: 30, Loss: 0.956273
Train - Epoch 13, Batch: 0, Loss: 0.961683
Train - Epoch 13, Batch: 10, Loss: 0.957167
Train - Epoch 13, Batch: 20, Loss: 0.957274
Train - Epoch 13, Batch: 30, Loss: 0.959293
Train - Epoch 14, Batch: 0, Loss: 0.959991
Train - Epoch 14, Batch: 10, Loss: 0.954109
Train - Epoch 14, Batch: 20, Loss: 0.953129
Train - Epoch 14, Batch: 30, Loss: 0.948773
Train - Epoch 15, Batch: 0, Loss: 0.953846
Train - Epoch 15, Batch: 10, Loss: 0.951659
Train - Epoch 15, Batch: 20, Loss: 0.946405
Train - Epoch 15, Batch: 30, Loss: 0.947654
Train - Epoch 16, Batch: 0, Loss: 0.937261
Train - Epoch 16, Batch: 10, Loss: 0.937963
Train - Epoch 16, Batch: 20, Loss: 0.949144
Train - Epoch 16, Batch: 30, Loss: 0.946241
Train - Epoch 17, Batch: 0, Loss: 0.939790
Train - Epoch 17, Batch: 10, Loss: 0.942522
Train - Epoch 17, Batch: 20, Loss: 0.945232
Train - Epoch 17, Batch: 30, Loss: 0.937048
Train - Epoch 18, Batch: 0, Loss: 0.937932
Train - Epoch 18, Batch: 10, Loss: 0.934857
Train - Epoch 18, Batch: 20, Loss: 0.930175
Train - Epoch 18, Batch: 30, Loss: 0.934676
Train - Epoch 19, Batch: 0, Loss: 0.929540
Train - Epoch 19, Batch: 10, Loss: 0.936670
Train - Epoch 19, Batch: 20, Loss: 0.921579
Train - Epoch 19, Batch: 30, Loss: 0.922393
Train - Epoch 20, Batch: 0, Loss: 0.929936
Train - Epoch 20, Batch: 10, Loss: 0.919054
Train - Epoch 20, Batch: 20, Loss: 0.923564
Train - Epoch 20, Batch: 30, Loss: 0.913617
Train - Epoch 21, Batch: 0, Loss: 0.926176
Train - Epoch 21, Batch: 10, Loss: 0.931480
Train - Epoch 21, Batch: 20, Loss: 0.922079
Train - Epoch 21, Batch: 30, Loss: 0.915199
Train - Epoch 22, Batch: 0, Loss: 0.917033
Train - Epoch 22, Batch: 10, Loss: 0.920111
Train - Epoch 22, Batch: 20, Loss: 0.911244
Train - Epoch 22, Batch: 30, Loss: 0.919887
Train - Epoch 23, Batch: 0, Loss: 0.912029
Train - Epoch 23, Batch: 10, Loss: 0.916769
Train - Epoch 23, Batch: 20, Loss: 0.915456
Train - Epoch 23, Batch: 30, Loss: 0.909876
Train - Epoch 24, Batch: 0, Loss: 0.924741
Train - Epoch 24, Batch: 10, Loss: 0.898492
Train - Epoch 24, Batch: 20, Loss: 0.911554
Train - Epoch 24, Batch: 30, Loss: 0.909459
Train - Epoch 25, Batch: 0, Loss: 0.903396
Train - Epoch 25, Batch: 10, Loss: 0.898066
Train - Epoch 25, Batch: 20, Loss: 0.908729
Train - Epoch 25, Batch: 30, Loss: 0.911411
Train - Epoch 26, Batch: 0, Loss: 0.895934
Train - Epoch 26, Batch: 10, Loss: 0.914678
Train - Epoch 26, Batch: 20, Loss: 0.903308
Train - Epoch 26, Batch: 30, Loss: 0.907540
Train - Epoch 27, Batch: 0, Loss: 0.894771
Train - Epoch 27, Batch: 10, Loss: 0.899879
Train - Epoch 27, Batch: 20, Loss: 0.902200
Train - Epoch 27, Batch: 30, Loss: 0.907414
Train - Epoch 28, Batch: 0, Loss: 0.899952
Train - Epoch 28, Batch: 10, Loss: 0.898890
Train - Epoch 28, Batch: 20, Loss: 0.899599
Train - Epoch 28, Batch: 30, Loss: 0.902214
Train - Epoch 29, Batch: 0, Loss: 0.901447
Train - Epoch 29, Batch: 10, Loss: 0.903826
Train - Epoch 29, Batch: 20, Loss: 0.904729
Train - Epoch 29, Batch: 30, Loss: 0.896878
Train - Epoch 30, Batch: 0, Loss: 0.895202
Train - Epoch 30, Batch: 10, Loss: 0.897320
Train - Epoch 30, Batch: 20, Loss: 0.905141
Train - Epoch 30, Batch: 30, Loss: 0.904576
Train - Epoch 31, Batch: 0, Loss: 0.894269
Train - Epoch 31, Batch: 10, Loss: 0.890505
Train - Epoch 31, Batch: 20, Loss: 0.893738
Train - Epoch 31, Batch: 30, Loss: 0.899814
Test Avg. Loss: 0.000070, Accuracy: 0.629064
training_time:: 3.4987192153930664
training time full:: 3.498783826828003
provenance prepare time:: 5.4836273193359375e-06
Test Avg. Loss: 0.000070, Accuracy: 0.629064
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
batch_size:: 16384
epoch:: 0
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 3.6884753704071045
overhead:: 0
overhead2:: 1.7497344017028809
overhead3:: 0
time_baseline:: 3.6894729137420654
curr_diff: 0 tensor(0.0026, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0026, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629202
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.018154621124267578
overhead3:: 0.05460786819458008
overhead4:: 0.21027302742004395
overhead5:: 0
memory usage:: 3791257600
time_provenance:: 1.1419117450714111
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629219
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.023824214935302734
overhead3:: 0.058115482330322266
overhead4:: 0.2861003875732422
overhead5:: 0
memory usage:: 3789828096
time_provenance:: 1.3121142387390137
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629202
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.02419567108154297
overhead3:: 0.055382490158081055
overhead4:: 0.293048620223999
overhead5:: 0
memory usage:: 3782684672
time_provenance:: 1.3277158737182617
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629219
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.026346206665039062
overhead3:: 0.056028127670288086
overhead4:: 0.31243371963500977
overhead5:: 0
memory usage:: 3770527744
time_provenance:: 1.2983505725860596
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629202
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.02797102928161621
overhead3:: 0.06176638603210449
overhead4:: 0.3428027629852295
overhead5:: 0
memory usage:: 3770908672
time_provenance:: 1.3142924308776855
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629219
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.03280997276306152
overhead3:: 0.0825798511505127
overhead4:: 0.38728976249694824
overhead5:: 0
memory usage:: 3770986496
time_provenance:: 1.3645503520965576
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0027, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0027, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629219
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.03441667556762695
overhead3:: 0.08449387550354004
overhead4:: 0.4076087474822998
overhead5:: 0
memory usage:: 3798921216
time_provenance:: 1.381610631942749
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0027, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0027, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629219
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.036241769790649414
overhead3:: 0.08360934257507324
overhead4:: 0.461254358291626
overhead5:: 0
memory usage:: 3766554624
time_provenance:: 1.4620463848114014
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0028, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0028, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629219
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.0386049747467041
overhead3:: 0.07906699180603027
overhead4:: 0.47513461112976074
overhead5:: 0
memory usage:: 3782549504
time_provenance:: 1.4504573345184326
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0028, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0028, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629219
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.04423403739929199
overhead3:: 0.0898582935333252
overhead4:: 0.5572926998138428
overhead5:: 0
memory usage:: 3769135104
time_provenance:: 1.596921443939209
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0027, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0027, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629219
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.06114029884338379
overhead3:: 0.10455036163330078
overhead4:: 0.744699239730835
overhead5:: 0
memory usage:: 3789303808
time_provenance:: 1.774397850036621
curr_diff: 0 tensor(7.3771e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.3771e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0027, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0027, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629202
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.06005358695983887
overhead3:: 0.1057889461517334
overhead4:: 0.8282978534698486
overhead5:: 0
memory usage:: 3783188480
time_provenance:: 1.864222764968872
curr_diff: 0 tensor(7.3935e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.3935e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0027, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0027, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629202
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.0671224594116211
overhead3:: 0.11310052871704102
overhead4:: 0.8263444900512695
overhead5:: 0
memory usage:: 3763060736
time_provenance:: 1.8748893737792969
curr_diff: 0 tensor(7.4686e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.4686e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0027, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0027, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629202
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.06887006759643555
overhead3:: 0.11550354957580566
overhead4:: 0.8785200119018555
overhead5:: 0
memory usage:: 3774517248
time_provenance:: 1.9359962940216064
curr_diff: 0 tensor(7.4379e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.4379e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0027, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0027, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629202
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.07405304908752441
overhead3:: 0.12684249877929688
overhead4:: 0.89042067527771
overhead5:: 0
memory usage:: 3764908032
time_provenance:: 1.9697675704956055
curr_diff: 0 tensor(7.4431e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.4431e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0027, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0027, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629202
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.15879297256469727
overhead3:: 0.25699758529663086
overhead4:: 2.1682605743408203
overhead5:: 0
memory usage:: 3799695360
time_provenance:: 3.5247232913970947
curr_diff: 0 tensor(1.0600e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0600e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0026, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0026, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629202
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.14454340934753418
overhead3:: 0.23979520797729492
overhead4:: 1.9567344188690186
overhead5:: 0
memory usage:: 3782959104
time_provenance:: 3.196213960647583
curr_diff: 0 tensor(1.0801e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0801e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0026, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0026, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629202
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.19536209106445312
overhead3:: 0.3049604892730713
overhead4:: 2.6827921867370605
overhead5:: 0
memory usage:: 3769901056
time_provenance:: 4.281532049179077
curr_diff: 0 tensor(1.1147e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1147e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0026, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0026, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629202
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.1453092098236084
overhead3:: 0.24548625946044922
overhead4:: 1.9616029262542725
overhead5:: 0
memory usage:: 3799572480
time_provenance:: 3.2030341625213623
curr_diff: 0 tensor(1.1233e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1233e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0026, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0026, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629202
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.15857553482055664
overhead3:: 0.24269437789916992
overhead4:: 1.924880027770996
overhead5:: 0
memory usage:: 3781042176
time_provenance:: 3.1594977378845215
curr_diff: 0 tensor(1.1156e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1156e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0026, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0026, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629202
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 4 0.002 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 1045
max_epoch:: 32
overhead:: 0
overhead2:: 0.3021538257598877
overhead3:: 0.4616549015045166
overhead4:: 3.372115135192871
overhead5:: 0
memory usage:: 3780071424
time_provenance:: 4.5331597328186035
curr_diff: 0 tensor(1.0170e-13, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0170e-13, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0026, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0026, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629202
deletion rate:: 0.01
python3 generate_rand_ids 0.01  covtype 0
tensor([ 81923, 442376, 393233,  ..., 376824, 344058, 196603])
batch size:: 16384
repetition 0
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 0 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 1.995899
Train - Epoch 0, Batch: 10, Loss: 1.411929
Train - Epoch 0, Batch: 20, Loss: 1.271544
Train - Epoch 0, Batch: 30, Loss: 1.219621
Train - Epoch 1, Batch: 0, Loss: 1.203038
Train - Epoch 1, Batch: 10, Loss: 1.184971
Train - Epoch 1, Batch: 20, Loss: 1.155099
Train - Epoch 1, Batch: 30, Loss: 1.142659
Train - Epoch 2, Batch: 0, Loss: 1.155524
Train - Epoch 2, Batch: 10, Loss: 1.122776
Train - Epoch 2, Batch: 20, Loss: 1.132780
Train - Epoch 2, Batch: 30, Loss: 1.108753
Train - Epoch 3, Batch: 0, Loss: 1.111226
Train - Epoch 3, Batch: 10, Loss: 1.101770
Train - Epoch 3, Batch: 20, Loss: 1.096873
Train - Epoch 3, Batch: 30, Loss: 1.078536
Train - Epoch 4, Batch: 0, Loss: 1.085670
Train - Epoch 4, Batch: 10, Loss: 1.080721
Train - Epoch 4, Batch: 20, Loss: 1.072639
Train - Epoch 4, Batch: 30, Loss: 1.058654
Train - Epoch 5, Batch: 0, Loss: 1.069798
Train - Epoch 5, Batch: 10, Loss: 1.052453
Train - Epoch 5, Batch: 20, Loss: 1.052032
Train - Epoch 5, Batch: 30, Loss: 1.039907
Train - Epoch 6, Batch: 0, Loss: 1.042421
Train - Epoch 6, Batch: 10, Loss: 1.030341
Train - Epoch 6, Batch: 20, Loss: 1.024875
Train - Epoch 6, Batch: 30, Loss: 1.021809
Train - Epoch 7, Batch: 0, Loss: 1.025972
Train - Epoch 7, Batch: 10, Loss: 1.021456
Train - Epoch 7, Batch: 20, Loss: 1.025164
Train - Epoch 7, Batch: 30, Loss: 1.009759
Train - Epoch 8, Batch: 0, Loss: 1.012930
Train - Epoch 8, Batch: 10, Loss: 1.004332
Train - Epoch 8, Batch: 20, Loss: 1.013742
Train - Epoch 8, Batch: 30, Loss: 0.997990
Train - Epoch 9, Batch: 0, Loss: 1.003151
Train - Epoch 9, Batch: 10, Loss: 1.008257
Train - Epoch 9, Batch: 20, Loss: 0.998973
Train - Epoch 9, Batch: 30, Loss: 0.988276
Train - Epoch 10, Batch: 0, Loss: 0.981143
Train - Epoch 10, Batch: 10, Loss: 0.981870
Train - Epoch 10, Batch: 20, Loss: 0.998135
Train - Epoch 10, Batch: 30, Loss: 0.997227
Train - Epoch 11, Batch: 0, Loss: 0.976327
Train - Epoch 11, Batch: 10, Loss: 0.983518
Train - Epoch 11, Batch: 20, Loss: 0.975142
Train - Epoch 11, Batch: 30, Loss: 0.975772
Train - Epoch 12, Batch: 0, Loss: 0.964257
Train - Epoch 12, Batch: 10, Loss: 0.963362
Train - Epoch 12, Batch: 20, Loss: 0.966787
Train - Epoch 12, Batch: 30, Loss: 0.965255
Train - Epoch 13, Batch: 0, Loss: 0.959728
Train - Epoch 13, Batch: 10, Loss: 0.972583
Train - Epoch 13, Batch: 20, Loss: 0.960764
Train - Epoch 13, Batch: 30, Loss: 0.962727
Train - Epoch 14, Batch: 0, Loss: 0.959773
Train - Epoch 14, Batch: 10, Loss: 0.965237
Train - Epoch 14, Batch: 20, Loss: 0.949608
Train - Epoch 14, Batch: 30, Loss: 0.955376
Train - Epoch 15, Batch: 0, Loss: 0.953360
Train - Epoch 15, Batch: 10, Loss: 0.949108
Train - Epoch 15, Batch: 20, Loss: 0.944287
Train - Epoch 15, Batch: 30, Loss: 0.955390
Train - Epoch 16, Batch: 0, Loss: 0.941967
Train - Epoch 16, Batch: 10, Loss: 0.948176
Train - Epoch 16, Batch: 20, Loss: 0.946678
Train - Epoch 16, Batch: 30, Loss: 0.939096
Train - Epoch 17, Batch: 0, Loss: 0.947943
Train - Epoch 17, Batch: 10, Loss: 0.932368
Train - Epoch 17, Batch: 20, Loss: 0.942882
Train - Epoch 17, Batch: 30, Loss: 0.936038
Train - Epoch 18, Batch: 0, Loss: 0.930484
Train - Epoch 18, Batch: 10, Loss: 0.936875
Train - Epoch 18, Batch: 20, Loss: 0.936031
Train - Epoch 18, Batch: 30, Loss: 0.932085
Train - Epoch 19, Batch: 0, Loss: 0.936845
Train - Epoch 19, Batch: 10, Loss: 0.930933
Train - Epoch 19, Batch: 20, Loss: 0.925644
Train - Epoch 19, Batch: 30, Loss: 0.930623
Train - Epoch 20, Batch: 0, Loss: 0.926451
Train - Epoch 20, Batch: 10, Loss: 0.933034
Train - Epoch 20, Batch: 20, Loss: 0.920993
Train - Epoch 20, Batch: 30, Loss: 0.923972
Train - Epoch 21, Batch: 0, Loss: 0.921226
Train - Epoch 21, Batch: 10, Loss: 0.927011
Train - Epoch 21, Batch: 20, Loss: 0.921943
Train - Epoch 21, Batch: 30, Loss: 0.925987
Train - Epoch 22, Batch: 0, Loss: 0.922738
Train - Epoch 22, Batch: 10, Loss: 0.917419
Train - Epoch 22, Batch: 20, Loss: 0.926265
Train - Epoch 22, Batch: 30, Loss: 0.921757
Train - Epoch 23, Batch: 0, Loss: 0.913579
Train - Epoch 23, Batch: 10, Loss: 0.924587
Train - Epoch 23, Batch: 20, Loss: 0.921414
Train - Epoch 23, Batch: 30, Loss: 0.909412
Train - Epoch 24, Batch: 0, Loss: 0.916248
Train - Epoch 24, Batch: 10, Loss: 0.910960
Train - Epoch 24, Batch: 20, Loss: 0.916739
Train - Epoch 24, Batch: 30, Loss: 0.918692
Train - Epoch 25, Batch: 0, Loss: 0.913139
Train - Epoch 25, Batch: 10, Loss: 0.910990
Train - Epoch 25, Batch: 20, Loss: 0.900126
Train - Epoch 25, Batch: 30, Loss: 0.913994
Train - Epoch 26, Batch: 0, Loss: 0.911569
Train - Epoch 26, Batch: 10, Loss: 0.905280
Train - Epoch 26, Batch: 20, Loss: 0.915229
Train - Epoch 26, Batch: 30, Loss: 0.906460
Train - Epoch 27, Batch: 0, Loss: 0.897899
Train - Epoch 27, Batch: 10, Loss: 0.913441
Train - Epoch 27, Batch: 20, Loss: 0.901492
Train - Epoch 27, Batch: 30, Loss: 0.901703
Train - Epoch 28, Batch: 0, Loss: 0.895950
Train - Epoch 28, Batch: 10, Loss: 0.898267
Train - Epoch 28, Batch: 20, Loss: 0.897483
Train - Epoch 28, Batch: 30, Loss: 0.901751
Train - Epoch 29, Batch: 0, Loss: 0.900574
Train - Epoch 29, Batch: 10, Loss: 0.902538
Train - Epoch 29, Batch: 20, Loss: 0.898907
Train - Epoch 29, Batch: 30, Loss: 0.893382
Train - Epoch 30, Batch: 0, Loss: 0.887481
Train - Epoch 30, Batch: 10, Loss: 0.893210
Train - Epoch 30, Batch: 20, Loss: 0.889435
Train - Epoch 30, Batch: 30, Loss: 0.901810
Train - Epoch 31, Batch: 0, Loss: 0.885190
Train - Epoch 31, Batch: 10, Loss: 0.899977
Train - Epoch 31, Batch: 20, Loss: 0.886592
Train - Epoch 31, Batch: 30, Loss: 0.891228
Test Avg. Loss: 0.000070, Accuracy: 0.630596
training_time:: 3.354645013809204
training time full:: 3.3547070026397705
provenance prepare time:: 8.344650268554688e-06
Test Avg. Loss: 0.000070, Accuracy: 0.630596
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
batch_size:: 16384
epoch:: 0
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 3.78878116607666
overhead:: 0
overhead2:: 1.7985386848449707
overhead3:: 0
time_baseline:: 3.7897756099700928
curr_diff: 0 tensor(0.0057, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0057, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630699
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.01827383041381836
overhead3:: 0.0630497932434082
overhead4:: 0.21242785453796387
overhead5:: 0
memory usage:: 3763494912
time_provenance:: 1.203789472579956
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0060, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0060, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630682
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.021416902542114258
overhead3:: 0.07223725318908691
overhead4:: 0.26798200607299805
overhead5:: 0
memory usage:: 3784122368
time_provenance:: 1.3329904079437256
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0059, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0059, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630699
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.023583412170410156
overhead3:: 0.062227487564086914
overhead4:: 0.2662062644958496
overhead5:: 0
memory usage:: 3772022784
time_provenance:: 1.2585115432739258
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0061, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0061, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630682
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.025682687759399414
overhead3:: 0.07687568664550781
overhead4:: 0.3101053237915039
overhead5:: 0
memory usage:: 3762937856
time_provenance:: 1.328089714050293
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0060, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0060, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630699
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.03149819374084473
overhead3:: 0.08564329147338867
overhead4:: 0.3621804714202881
overhead5:: 0
memory usage:: 3763658752
time_provenance:: 1.3884878158569336
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0062, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0062, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630682
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.033716678619384766
overhead3:: 0.09366273880004883
overhead4:: 0.41888952255249023
overhead5:: 0
memory usage:: 3791138816
time_provenance:: 1.4927215576171875
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0058, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0058, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630699
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.03521418571472168
overhead3:: 0.08919382095336914
overhead4:: 0.4267716407775879
overhead5:: 0
memory usage:: 3764109312
time_provenance:: 1.482625961303711
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0058, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0058, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630699
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.0372927188873291
overhead3:: 0.08851480484008789
overhead4:: 0.5117311477661133
overhead5:: 0
memory usage:: 3763535872
time_provenance:: 1.550037145614624
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0059, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0059, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630699
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.03872251510620117
overhead3:: 0.10805225372314453
overhead4:: 0.4939606189727783
overhead5:: 0
memory usage:: 3775401984
time_provenance:: 1.5442817211151123
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0059, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0059, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630717
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.0534510612487793
overhead3:: 0.09302330017089844
overhead4:: 0.5219762325286865
overhead5:: 0
memory usage:: 3770716160
time_provenance:: 1.5747547149658203
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0059, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0059, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630717
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.059131622314453125
overhead3:: 0.11544418334960938
overhead4:: 0.8319942951202393
overhead5:: 0
memory usage:: 3766263808
time_provenance:: 1.9326817989349365
curr_diff: 0 tensor(8.9693e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.9693e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0057, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0057, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630699
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.06354832649230957
overhead3:: 0.12133002281188965
overhead4:: 0.8663640022277832
overhead5:: 0
memory usage:: 3775074304
time_provenance:: 2.0057103633880615
curr_diff: 0 tensor(9.0110e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.0110e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0057, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0057, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630699
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.06313538551330566
overhead3:: 0.13034296035766602
overhead4:: 0.872394323348999
overhead5:: 0
memory usage:: 3765039104
time_provenance:: 1.983393907546997
curr_diff: 0 tensor(9.6366e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.6366e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0057, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0057, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630699
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.06614851951599121
overhead3:: 0.12642788887023926
overhead4:: 0.885631799697876
overhead5:: 0
memory usage:: 3763048448
time_provenance:: 1.9964072704315186
curr_diff: 0 tensor(9.7721e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.7721e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0057, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0057, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630699
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.07307052612304688
overhead3:: 0.12875938415527344
overhead4:: 0.8755013942718506
overhead5:: 0
memory usage:: 3783061504
time_provenance:: 2.008418560028076
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0057, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0057, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630699
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.13992524147033691
overhead3:: 0.23041653633117676
overhead4:: 1.9393482208251953
overhead5:: 0
memory usage:: 3764887552
time_provenance:: 3.2070209980010986
curr_diff: 0 tensor(2.7664e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.7664e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0057, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0057, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630699
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.16301774978637695
overhead3:: 0.25881266593933105
overhead4:: 2.027261257171631
overhead5:: 0
memory usage:: 3765182464
time_provenance:: 3.3681867122650146
curr_diff: 0 tensor(2.7978e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.7978e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0057, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0057, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630699
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.14422369003295898
overhead3:: 0.2419118881225586
overhead4:: 1.930624008178711
overhead5:: 0
memory usage:: 3780857856
time_provenance:: 3.2092714309692383
curr_diff: 0 tensor(2.8316e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.8316e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0057, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0057, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630699
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.14682745933532715
overhead3:: 0.2392418384552002
overhead4:: 1.969407558441162
overhead5:: 0
memory usage:: 3790069760
time_provenance:: 3.244321346282959
curr_diff: 0 tensor(2.8495e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.8495e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0057, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0057, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630699
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.1532754898071289
overhead3:: 0.27245306968688965
overhead4:: 1.977367639541626
overhead5:: 0
memory usage:: 3798839296
time_provenance:: 3.294412136077881
curr_diff: 0 tensor(2.8993e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.8993e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0057, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0057, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630699
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.29956841468811035
overhead3:: 0.43096470832824707
overhead4:: 3.2901010513305664
overhead5:: 0
memory usage:: 3757395968
time_provenance:: 4.458050489425659
curr_diff: 0 tensor(1.0070e-13, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0070e-13, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0057, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0057, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.630699
repetition 1
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 1 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 1.870292
Train - Epoch 0, Batch: 10, Loss: 1.368130
Train - Epoch 0, Batch: 20, Loss: 1.230797
Train - Epoch 0, Batch: 30, Loss: 1.190099
Train - Epoch 1, Batch: 0, Loss: 1.172508
Train - Epoch 1, Batch: 10, Loss: 1.145638
Train - Epoch 1, Batch: 20, Loss: 1.146538
Train - Epoch 1, Batch: 30, Loss: 1.121197
Train - Epoch 2, Batch: 0, Loss: 1.116824
Train - Epoch 2, Batch: 10, Loss: 1.118290
Train - Epoch 2, Batch: 20, Loss: 1.106702
Train - Epoch 2, Batch: 30, Loss: 1.095143
Train - Epoch 3, Batch: 0, Loss: 1.107457
Train - Epoch 3, Batch: 10, Loss: 1.084774
Train - Epoch 3, Batch: 20, Loss: 1.081049
Train - Epoch 3, Batch: 30, Loss: 1.071724
Train - Epoch 4, Batch: 0, Loss: 1.078864
Train - Epoch 4, Batch: 10, Loss: 1.063692
Train - Epoch 4, Batch: 20, Loss: 1.058351
Train - Epoch 4, Batch: 30, Loss: 1.052108
Train - Epoch 5, Batch: 0, Loss: 1.043852
Train - Epoch 5, Batch: 10, Loss: 1.044367
Train - Epoch 5, Batch: 20, Loss: 1.040497
Train - Epoch 5, Batch: 30, Loss: 1.037191
Train - Epoch 6, Batch: 0, Loss: 1.045832
Train - Epoch 6, Batch: 10, Loss: 1.028473
Train - Epoch 6, Batch: 20, Loss: 1.027934
Train - Epoch 6, Batch: 30, Loss: 1.021453
Train - Epoch 7, Batch: 0, Loss: 1.016762
Train - Epoch 7, Batch: 10, Loss: 1.011934
Train - Epoch 7, Batch: 20, Loss: 1.014788
Train - Epoch 7, Batch: 30, Loss: 1.001510
Train - Epoch 8, Batch: 0, Loss: 1.010729
Train - Epoch 8, Batch: 10, Loss: 0.996053
Train - Epoch 8, Batch: 20, Loss: 1.009365
Train - Epoch 8, Batch: 30, Loss: 0.989375
Train - Epoch 9, Batch: 0, Loss: 0.991751
Train - Epoch 9, Batch: 10, Loss: 0.991260
Train - Epoch 9, Batch: 20, Loss: 0.981728
Train - Epoch 9, Batch: 30, Loss: 0.989274
Train - Epoch 10, Batch: 0, Loss: 0.988177
Train - Epoch 10, Batch: 10, Loss: 0.980081
Train - Epoch 10, Batch: 20, Loss: 0.977619
Train - Epoch 10, Batch: 30, Loss: 0.969460
Train - Epoch 11, Batch: 0, Loss: 0.978577
Train - Epoch 11, Batch: 10, Loss: 0.977177
Train - Epoch 11, Batch: 20, Loss: 0.973896
Train - Epoch 11, Batch: 30, Loss: 0.963788
Train - Epoch 12, Batch: 0, Loss: 0.977145
Train - Epoch 12, Batch: 10, Loss: 0.963286
Train - Epoch 12, Batch: 20, Loss: 0.962313
Train - Epoch 12, Batch: 30, Loss: 0.951470
Train - Epoch 13, Batch: 0, Loss: 0.959299
Train - Epoch 13, Batch: 10, Loss: 0.962074
Train - Epoch 13, Batch: 20, Loss: 0.965750
Train - Epoch 13, Batch: 30, Loss: 0.958082
Train - Epoch 14, Batch: 0, Loss: 0.951182
Train - Epoch 14, Batch: 10, Loss: 0.954527
Train - Epoch 14, Batch: 20, Loss: 0.957486
Train - Epoch 14, Batch: 30, Loss: 0.956329
Train - Epoch 15, Batch: 0, Loss: 0.950691
Train - Epoch 15, Batch: 10, Loss: 0.952734
Train - Epoch 15, Batch: 20, Loss: 0.946242
Train - Epoch 15, Batch: 30, Loss: 0.941141
Train - Epoch 16, Batch: 0, Loss: 0.948190
Train - Epoch 16, Batch: 10, Loss: 0.946131
Train - Epoch 16, Batch: 20, Loss: 0.943941
Train - Epoch 16, Batch: 30, Loss: 0.938598
Train - Epoch 17, Batch: 0, Loss: 0.938619
Train - Epoch 17, Batch: 10, Loss: 0.937660
Train - Epoch 17, Batch: 20, Loss: 0.938742
Train - Epoch 17, Batch: 30, Loss: 0.936197
Train - Epoch 18, Batch: 0, Loss: 0.935878
Train - Epoch 18, Batch: 10, Loss: 0.939770
Train - Epoch 18, Batch: 20, Loss: 0.919558
Train - Epoch 18, Batch: 30, Loss: 0.930626
Train - Epoch 19, Batch: 0, Loss: 0.934984
Train - Epoch 19, Batch: 10, Loss: 0.928312
Train - Epoch 19, Batch: 20, Loss: 0.918414
Train - Epoch 19, Batch: 30, Loss: 0.919750
Train - Epoch 20, Batch: 0, Loss: 0.928745
Train - Epoch 20, Batch: 10, Loss: 0.922872
Train - Epoch 20, Batch: 20, Loss: 0.930020
Train - Epoch 20, Batch: 30, Loss: 0.916054
Train - Epoch 21, Batch: 0, Loss: 0.923816
Train - Epoch 21, Batch: 10, Loss: 0.920326
Train - Epoch 21, Batch: 20, Loss: 0.920921
Train - Epoch 21, Batch: 30, Loss: 0.921931
Train - Epoch 22, Batch: 0, Loss: 0.911576
Train - Epoch 22, Batch: 10, Loss: 0.920643
Train - Epoch 22, Batch: 20, Loss: 0.913737
Train - Epoch 22, Batch: 30, Loss: 0.920693
Train - Epoch 23, Batch: 0, Loss: 0.924554
Train - Epoch 23, Batch: 10, Loss: 0.918511
Train - Epoch 23, Batch: 20, Loss: 0.919955
Train - Epoch 23, Batch: 30, Loss: 0.900114
Train - Epoch 24, Batch: 0, Loss: 0.910713
Train - Epoch 24, Batch: 10, Loss: 0.905893
Train - Epoch 24, Batch: 20, Loss: 0.910851
Train - Epoch 24, Batch: 30, Loss: 0.911742
Train - Epoch 25, Batch: 0, Loss: 0.911148
Train - Epoch 25, Batch: 10, Loss: 0.917652
Train - Epoch 25, Batch: 20, Loss: 0.912061
Train - Epoch 25, Batch: 30, Loss: 0.906090
Train - Epoch 26, Batch: 0, Loss: 0.912794
Train - Epoch 26, Batch: 10, Loss: 0.900388
Train - Epoch 26, Batch: 20, Loss: 0.907921
Train - Epoch 26, Batch: 30, Loss: 0.904452
Train - Epoch 27, Batch: 0, Loss: 0.905741
Train - Epoch 27, Batch: 10, Loss: 0.907214
Train - Epoch 27, Batch: 20, Loss: 0.901777
Train - Epoch 27, Batch: 30, Loss: 0.897523
Train - Epoch 28, Batch: 0, Loss: 0.907542
Train - Epoch 28, Batch: 10, Loss: 0.893297
Train - Epoch 28, Batch: 20, Loss: 0.903545
Train - Epoch 28, Batch: 30, Loss: 0.894197
Train - Epoch 29, Batch: 0, Loss: 0.901228
Train - Epoch 29, Batch: 10, Loss: 0.917358
Train - Epoch 29, Batch: 20, Loss: 0.890667
Train - Epoch 29, Batch: 30, Loss: 0.891232
Train - Epoch 30, Batch: 0, Loss: 0.901243
Train - Epoch 30, Batch: 10, Loss: 0.890771
Train - Epoch 30, Batch: 20, Loss: 0.889393
Train - Epoch 30, Batch: 30, Loss: 0.893235
Train - Epoch 31, Batch: 0, Loss: 0.887628
Train - Epoch 31, Batch: 10, Loss: 0.894222
Train - Epoch 31, Batch: 20, Loss: 0.903795
Train - Epoch 31, Batch: 30, Loss: 0.891447
Test Avg. Loss: 0.000070, Accuracy: 0.628393
training_time:: 3.5400917530059814
training time full:: 3.540154457092285
provenance prepare time:: 8.58306884765625e-06
Test Avg. Loss: 0.000070, Accuracy: 0.628393
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
batch_size:: 16384
epoch:: 0
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 3.6413826942443848
overhead:: 0
overhead2:: 1.7727677822113037
overhead3:: 0
time_baseline:: 3.6423630714416504
curr_diff: 0 tensor(0.0056, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0056, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628479
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.019145488739013672
overhead3:: 0.058322906494140625
overhead4:: 0.21161556243896484
overhead5:: 0
memory usage:: 3766845440
time_provenance:: 1.2334322929382324
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0060, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0060, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628479
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.020682573318481445
overhead3:: 0.05867266654968262
overhead4:: 0.23969817161560059
overhead5:: 0
memory usage:: 3764404224
time_provenance:: 1.2396385669708252
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0060, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0060, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628565
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.02410435676574707
overhead3:: 0.07257270812988281
overhead4:: 0.2857694625854492
overhead5:: 0
memory usage:: 3782692864
time_provenance:: 1.3497159481048584
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0060, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0060, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628479
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.02929091453552246
overhead3:: 0.08273649215698242
overhead4:: 0.3257296085357666
overhead5:: 0
memory usage:: 3770896384
time_provenance:: 1.4541347026824951
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0061, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0061, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628565
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.02823162078857422
overhead3:: 0.07256674766540527
overhead4:: 0.32861852645874023
overhead5:: 0
memory usage:: 3769860096
time_provenance:: 1.3263366222381592
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0060, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0060, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628479
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.038205623626708984
overhead3:: 0.08471465110778809
overhead4:: 0.45323920249938965
overhead5:: 0
memory usage:: 3791060992
time_provenance:: 1.6205108165740967
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0058, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0058, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628410
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.03543853759765625
overhead3:: 0.08197641372680664
overhead4:: 0.4174613952636719
overhead5:: 0
memory usage:: 3834695680
time_provenance:: 1.4595706462860107
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0058, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0058, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628410
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.0485074520111084
overhead3:: 0.11396932601928711
overhead4:: 0.5231764316558838
overhead5:: 0
memory usage:: 3762257920
time_provenance:: 1.7305576801300049
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0058, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0058, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628410
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.039022207260131836
overhead3:: 0.10757303237915039
overhead4:: 0.4922142028808594
overhead5:: 0
memory usage:: 3799728128
time_provenance:: 1.535902976989746
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0058, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0058, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628410
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.041638851165771484
overhead3:: 0.10809540748596191
overhead4:: 0.5216019153594971
overhead5:: 0
memory usage:: 3769729024
time_provenance:: 1.564194679260254
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0059, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0059, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628410
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.06409096717834473
overhead3:: 0.1219937801361084
overhead4:: 0.8390748500823975
overhead5:: 0
memory usage:: 3765608448
time_provenance:: 2.013603448867798
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0057, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0057, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628445
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.059884071350097656
overhead3:: 0.11572909355163574
overhead4:: 0.7505550384521484
overhead5:: 0
memory usage:: 3762659328
time_provenance:: 1.7941219806671143
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0057, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0057, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628445
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.0658726692199707
overhead3:: 0.1235809326171875
overhead4:: 0.8494679927825928
overhead5:: 0
memory usage:: 3776524288
time_provenance:: 1.960533618927002
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0057, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0057, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628445
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.06406807899475098
overhead3:: 0.12046194076538086
overhead4:: 0.8072733879089355
overhead5:: 0
memory usage:: 3765309440
time_provenance:: 1.8658673763275146
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0058, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0058, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628445
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.07004952430725098
overhead3:: 0.13160419464111328
overhead4:: 0.946509838104248
overhead5:: 0
memory usage:: 3762630656
time_provenance:: 2.080092191696167
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0058, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0058, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628445
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.14064812660217285
overhead3:: 0.23315882682800293
overhead4:: 1.9586765766143799
overhead5:: 0
memory usage:: 3781582848
time_provenance:: 3.228602886199951
curr_diff: 0 tensor(1.6180e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6180e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0056, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0056, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628479
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.159987211227417
overhead3:: 0.27013182640075684
overhead4:: 2.1710410118103027
overhead5:: 0
memory usage:: 3782320128
time_provenance:: 3.5883851051330566
curr_diff: 0 tensor(1.6113e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6113e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0056, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0056, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628479
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.14585185050964355
overhead3:: 0.23504352569580078
overhead4:: 1.7975139617919922
overhead5:: 0
memory usage:: 3786838016
time_provenance:: 3.0639419555664062
curr_diff: 0 tensor(1.6354e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.6354e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0057, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0057, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628479
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.1494460105895996
overhead3:: 0.25760340690612793
overhead4:: 1.9466536045074463
overhead5:: 0
memory usage:: 3765784576
time_provenance:: 3.2350316047668457
curr_diff: 0 tensor(1.7133e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.7133e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0057, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0057, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628479
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.2055068016052246
overhead3:: 0.32179737091064453
overhead4:: 2.7150888442993164
overhead5:: 0
memory usage:: 3762823168
time_provenance:: 4.398524522781372
curr_diff: 0 tensor(1.8502e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.8502e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0057, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0057, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628479
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.34433484077453613
overhead3:: 0.5652310848236084
overhead4:: 3.6052026748657227
overhead5:: 0
memory usage:: 3757170688
time_provenance:: 4.9067254066467285
curr_diff: 0 tensor(1.0161e-13, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0161e-13, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0056, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0056, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628479
repetition 2
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 2 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 1.996209
Train - Epoch 0, Batch: 10, Loss: 1.389622
Train - Epoch 0, Batch: 20, Loss: 1.249904
Train - Epoch 0, Batch: 30, Loss: 1.188902
Train - Epoch 1, Batch: 0, Loss: 1.177312
Train - Epoch 1, Batch: 10, Loss: 1.155995
Train - Epoch 1, Batch: 20, Loss: 1.139966
Train - Epoch 1, Batch: 30, Loss: 1.127421
Train - Epoch 2, Batch: 0, Loss: 1.126795
Train - Epoch 2, Batch: 10, Loss: 1.117670
Train - Epoch 2, Batch: 20, Loss: 1.096547
Train - Epoch 2, Batch: 30, Loss: 1.113276
Train - Epoch 3, Batch: 0, Loss: 1.097714
Train - Epoch 3, Batch: 10, Loss: 1.097753
Train - Epoch 3, Batch: 20, Loss: 1.078448
Train - Epoch 3, Batch: 30, Loss: 1.083883
Train - Epoch 4, Batch: 0, Loss: 1.072710
Train - Epoch 4, Batch: 10, Loss: 1.058973
Train - Epoch 4, Batch: 20, Loss: 1.058351
Train - Epoch 4, Batch: 30, Loss: 1.048848
Train - Epoch 5, Batch: 0, Loss: 1.051846
Train - Epoch 5, Batch: 10, Loss: 1.052528
Train - Epoch 5, Batch: 20, Loss: 1.037038
Train - Epoch 5, Batch: 30, Loss: 1.038807
Train - Epoch 6, Batch: 0, Loss: 1.040833
Train - Epoch 6, Batch: 10, Loss: 1.024207
Train - Epoch 6, Batch: 20, Loss: 1.031162
Train - Epoch 6, Batch: 30, Loss: 1.016649
Train - Epoch 7, Batch: 0, Loss: 1.015465
Train - Epoch 7, Batch: 10, Loss: 1.024516
Train - Epoch 7, Batch: 20, Loss: 1.012341
Train - Epoch 7, Batch: 30, Loss: 1.012639
Train - Epoch 8, Batch: 0, Loss: 1.004327
Train - Epoch 8, Batch: 10, Loss: 1.004077
Train - Epoch 8, Batch: 20, Loss: 1.001538
Train - Epoch 8, Batch: 30, Loss: 1.001332
Train - Epoch 9, Batch: 0, Loss: 0.997850
Train - Epoch 9, Batch: 10, Loss: 0.997853
Train - Epoch 9, Batch: 20, Loss: 0.997912
Train - Epoch 9, Batch: 30, Loss: 0.983109
Train - Epoch 10, Batch: 0, Loss: 0.989375
Train - Epoch 10, Batch: 10, Loss: 0.986689
Train - Epoch 10, Batch: 20, Loss: 0.971926
Train - Epoch 10, Batch: 30, Loss: 0.985170
Train - Epoch 11, Batch: 0, Loss: 0.980227
Train - Epoch 11, Batch: 10, Loss: 0.970791
Train - Epoch 11, Batch: 20, Loss: 0.973202
Train - Epoch 11, Batch: 30, Loss: 0.969901
Train - Epoch 12, Batch: 0, Loss: 0.969290
Train - Epoch 12, Batch: 10, Loss: 0.974734
Train - Epoch 12, Batch: 20, Loss: 0.965007
Train - Epoch 12, Batch: 30, Loss: 0.972572
Train - Epoch 13, Batch: 0, Loss: 0.966348
Train - Epoch 13, Batch: 10, Loss: 0.954977
Train - Epoch 13, Batch: 20, Loss: 0.958752
Train - Epoch 13, Batch: 30, Loss: 0.972888
Train - Epoch 14, Batch: 0, Loss: 0.956354
Train - Epoch 14, Batch: 10, Loss: 0.963431
Train - Epoch 14, Batch: 20, Loss: 0.955276
Train - Epoch 14, Batch: 30, Loss: 0.960518
Train - Epoch 15, Batch: 0, Loss: 0.954912
Train - Epoch 15, Batch: 10, Loss: 0.947158
Train - Epoch 15, Batch: 20, Loss: 0.942138
Train - Epoch 15, Batch: 30, Loss: 0.943508
Train - Epoch 16, Batch: 0, Loss: 0.953592
Train - Epoch 16, Batch: 10, Loss: 0.942403
Train - Epoch 16, Batch: 20, Loss: 0.933958
Train - Epoch 16, Batch: 30, Loss: 0.947142
Train - Epoch 17, Batch: 0, Loss: 0.948823
Train - Epoch 17, Batch: 10, Loss: 0.943922
Train - Epoch 17, Batch: 20, Loss: 0.923375
Train - Epoch 17, Batch: 30, Loss: 0.937249
Train - Epoch 18, Batch: 0, Loss: 0.932197
Train - Epoch 18, Batch: 10, Loss: 0.931811
Train - Epoch 18, Batch: 20, Loss: 0.932037
Train - Epoch 18, Batch: 30, Loss: 0.934275
Train - Epoch 19, Batch: 0, Loss: 0.929770
Train - Epoch 19, Batch: 10, Loss: 0.922007
Train - Epoch 19, Batch: 20, Loss: 0.933063
Train - Epoch 19, Batch: 30, Loss: 0.921563
Train - Epoch 20, Batch: 0, Loss: 0.925712
Train - Epoch 20, Batch: 10, Loss: 0.922118
Train - Epoch 20, Batch: 20, Loss: 0.919339
Train - Epoch 20, Batch: 30, Loss: 0.922120
Train - Epoch 21, Batch: 0, Loss: 0.926869
Train - Epoch 21, Batch: 10, Loss: 0.919126
Train - Epoch 21, Batch: 20, Loss: 0.915691
Train - Epoch 21, Batch: 30, Loss: 0.915389
Train - Epoch 22, Batch: 0, Loss: 0.920409
Train - Epoch 22, Batch: 10, Loss: 0.923513
Train - Epoch 22, Batch: 20, Loss: 0.921426
Train - Epoch 22, Batch: 30, Loss: 0.915080
Train - Epoch 23, Batch: 0, Loss: 0.917451
Train - Epoch 23, Batch: 10, Loss: 0.914739
Train - Epoch 23, Batch: 20, Loss: 0.910138
Train - Epoch 23, Batch: 30, Loss: 0.918940
Train - Epoch 24, Batch: 0, Loss: 0.909295
Train - Epoch 24, Batch: 10, Loss: 0.919695
Train - Epoch 24, Batch: 20, Loss: 0.906500
Train - Epoch 24, Batch: 30, Loss: 0.909294
Train - Epoch 25, Batch: 0, Loss: 0.904462
Train - Epoch 25, Batch: 10, Loss: 0.916814
Train - Epoch 25, Batch: 20, Loss: 0.915554
Train - Epoch 25, Batch: 30, Loss: 0.918491
Train - Epoch 26, Batch: 0, Loss: 0.905830
Train - Epoch 26, Batch: 10, Loss: 0.905779
Train - Epoch 26, Batch: 20, Loss: 0.899793
Train - Epoch 26, Batch: 30, Loss: 0.908726
Train - Epoch 27, Batch: 0, Loss: 0.909881
Train - Epoch 27, Batch: 10, Loss: 0.901202
Train - Epoch 27, Batch: 20, Loss: 0.917252
Train - Epoch 27, Batch: 30, Loss: 0.897478
Train - Epoch 28, Batch: 0, Loss: 0.906299
Train - Epoch 28, Batch: 10, Loss: 0.902873
Train - Epoch 28, Batch: 20, Loss: 0.909655
Train - Epoch 28, Batch: 30, Loss: 0.903972
Train - Epoch 29, Batch: 0, Loss: 0.900167
Train - Epoch 29, Batch: 10, Loss: 0.892811
Train - Epoch 29, Batch: 20, Loss: 0.900550
Train - Epoch 29, Batch: 30, Loss: 0.898918
Train - Epoch 30, Batch: 0, Loss: 0.897899
Train - Epoch 30, Batch: 10, Loss: 0.899676
Train - Epoch 30, Batch: 20, Loss: 0.902561
Train - Epoch 30, Batch: 30, Loss: 0.889917
Train - Epoch 31, Batch: 0, Loss: 0.889814
Train - Epoch 31, Batch: 10, Loss: 0.891445
Train - Epoch 31, Batch: 20, Loss: 0.894508
Train - Epoch 31, Batch: 30, Loss: 0.882248
Test Avg. Loss: 0.000070, Accuracy: 0.629581
training_time:: 3.0364179611206055
training time full:: 3.0364794731140137
provenance prepare time:: 5.9604644775390625e-06
Test Avg. Loss: 0.000070, Accuracy: 0.629581
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
batch_size:: 16384
epoch:: 0
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 3.6086983680725098
overhead:: 0
overhead2:: 1.778609037399292
overhead3:: 0
time_baseline:: 3.6098129749298096
curr_diff: 0 tensor(0.0057, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0057, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629787
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.02101612091064453
overhead3:: 0.07681035995483398
overhead4:: 0.21692776679992676
overhead5:: 0
memory usage:: 3763019776
time_provenance:: 1.2629725933074951
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0061, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0061, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629770
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.02148914337158203
overhead3:: 0.07080626487731934
overhead4:: 0.25669312477111816
overhead5:: 0
memory usage:: 3770560512
time_provenance:: 1.3124172687530518
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0059, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0059, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629787
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.02330946922302246
overhead3:: 0.06478738784790039
overhead4:: 0.2793118953704834
overhead5:: 0
memory usage:: 3775401984
time_provenance:: 1.3110079765319824
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0062, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0062, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629770
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.026613473892211914
overhead3:: 0.07293105125427246
overhead4:: 0.3254096508026123
overhead5:: 0
memory usage:: 3775528960
time_provenance:: 1.3442862033843994
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0060, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0060, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629787
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.029653549194335938
overhead3:: 0.07300281524658203
overhead4:: 0.3439815044403076
overhead5:: 0
memory usage:: 3770806272
time_provenance:: 1.4274163246154785
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0061, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0061, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629770
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.03258395195007324
overhead3:: 0.09479594230651855
overhead4:: 0.3905503749847412
overhead5:: 0
memory usage:: 3762778112
time_provenance:: 1.444568395614624
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0057, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0057, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629787
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.03624987602233887
overhead3:: 0.0814065933227539
overhead4:: 0.42572832107543945
overhead5:: 0
memory usage:: 3799121920
time_provenance:: 1.457082986831665
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0057, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0057, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629787
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.03957509994506836
overhead3:: 0.09521698951721191
overhead4:: 0.4642040729522705
overhead5:: 0
memory usage:: 3782004736
time_provenance:: 1.5709073543548584
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0057, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0057, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629787
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.040891408920288086
overhead3:: 0.10849976539611816
overhead4:: 0.5049407482147217
overhead5:: 0
memory usage:: 3818098688
time_provenance:: 1.5797803401947021
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0058, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0058, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629787
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.04517102241516113
overhead3:: 0.0956122875213623
overhead4:: 0.5442523956298828
overhead5:: 0
memory usage:: 3852730368
time_provenance:: 1.6443753242492676
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0058, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0058, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629787
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.062122344970703125
overhead3:: 0.11916422843933105
overhead4:: 0.8390202522277832
overhead5:: 0
memory usage:: 3770789888
time_provenance:: 1.97615647315979
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0057, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0057, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629787
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.06255054473876953
overhead3:: 0.12089848518371582
overhead4:: 0.8423798084259033
overhead5:: 0
memory usage:: 3783417856
time_provenance:: 1.974961280822754
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0057, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0057, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629787
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.06598734855651855
overhead3:: 0.12707233428955078
overhead4:: 0.8611667156219482
overhead5:: 0
memory usage:: 3782778880
time_provenance:: 1.9448118209838867
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0057, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0057, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629787
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.06539011001586914
overhead3:: 0.12337112426757812
overhead4:: 0.8886582851409912
overhead5:: 0
memory usage:: 3790536704
time_provenance:: 1.9846014976501465
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0057, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0057, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629787
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.07299232482910156
overhead3:: 0.1301136016845703
overhead4:: 0.9121196269989014
overhead5:: 0
memory usage:: 3763130368
time_provenance:: 2.0612049102783203
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0057, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0057, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629787
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.14331746101379395
overhead3:: 0.2370591163635254
overhead4:: 1.912557601928711
overhead5:: 0
memory usage:: 3775455232
time_provenance:: 3.1918671131134033
curr_diff: 0 tensor(1.9089e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.9089e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0057, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0057, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629787
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.20280146598815918
overhead3:: 0.3073697090148926
overhead4:: 2.586766004562378
overhead5:: 0
memory usage:: 3761086464
time_provenance:: 4.255398988723755
curr_diff: 0 tensor(1.9365e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.9365e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0057, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0057, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629787
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.14532470703125
overhead3:: 0.23571062088012695
overhead4:: 1.9445464611053467
overhead5:: 0
memory usage:: 3780919296
time_provenance:: 3.211764335632324
curr_diff: 0 tensor(1.9624e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.9624e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0057, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0057, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629787
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.14544320106506348
overhead3:: 0.24521899223327637
overhead4:: 1.8881072998046875
overhead5:: 0
memory usage:: 3766992896
time_provenance:: 3.1596922874450684
curr_diff: 0 tensor(1.9789e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.9789e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0057, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0057, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629787
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.14973020553588867
overhead3:: 0.2633364200592041
overhead4:: 1.9216654300689697
overhead5:: 0
memory usage:: 3799306240
time_provenance:: 3.206385850906372
curr_diff: 0 tensor(2.0222e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.0222e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0057, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0057, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629787
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.3010573387145996
overhead3:: 0.4502079486846924
overhead4:: 3.381838083267212
overhead5:: 0
memory usage:: 3797716992
time_provenance:: 4.5729522705078125
curr_diff: 0 tensor(9.9478e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.9478e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0057, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0057, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629787
repetition 3
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 3 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 1.974255
Train - Epoch 0, Batch: 10, Loss: 1.394939
Train - Epoch 0, Batch: 20, Loss: 1.248160
Train - Epoch 0, Batch: 30, Loss: 1.199317
Train - Epoch 1, Batch: 0, Loss: 1.180598
Train - Epoch 1, Batch: 10, Loss: 1.164129
Train - Epoch 1, Batch: 20, Loss: 1.135745
Train - Epoch 1, Batch: 30, Loss: 1.133644
Train - Epoch 2, Batch: 0, Loss: 1.136371
Train - Epoch 2, Batch: 10, Loss: 1.117722
Train - Epoch 2, Batch: 20, Loss: 1.108033
Train - Epoch 2, Batch: 30, Loss: 1.107491
Train - Epoch 3, Batch: 0, Loss: 1.106330
Train - Epoch 3, Batch: 10, Loss: 1.088684
Train - Epoch 3, Batch: 20, Loss: 1.079113
Train - Epoch 3, Batch: 30, Loss: 1.070767
Train - Epoch 4, Batch: 0, Loss: 1.069479
Train - Epoch 4, Batch: 10, Loss: 1.072434
Train - Epoch 4, Batch: 20, Loss: 1.055698
Train - Epoch 4, Batch: 30, Loss: 1.067523
Train - Epoch 5, Batch: 0, Loss: 1.054962
Train - Epoch 5, Batch: 10, Loss: 1.050990
Train - Epoch 5, Batch: 20, Loss: 1.052737
Train - Epoch 5, Batch: 30, Loss: 1.041438
Train - Epoch 6, Batch: 0, Loss: 1.030306
Train - Epoch 6, Batch: 10, Loss: 1.038284
Train - Epoch 6, Batch: 20, Loss: 1.028596
Train - Epoch 6, Batch: 30, Loss: 1.028235
Train - Epoch 7, Batch: 0, Loss: 1.023619
Train - Epoch 7, Batch: 10, Loss: 1.007443
Train - Epoch 7, Batch: 20, Loss: 1.002538
Train - Epoch 7, Batch: 30, Loss: 1.008106
Train - Epoch 8, Batch: 0, Loss: 1.006517
Train - Epoch 8, Batch: 10, Loss: 1.008133
Train - Epoch 8, Batch: 20, Loss: 1.011448
Train - Epoch 8, Batch: 30, Loss: 1.000656
Train - Epoch 9, Batch: 0, Loss: 1.002572
Train - Epoch 9, Batch: 10, Loss: 0.996102
Train - Epoch 9, Batch: 20, Loss: 1.002223
Train - Epoch 9, Batch: 30, Loss: 0.992811
Train - Epoch 10, Batch: 0, Loss: 0.993672
Train - Epoch 10, Batch: 10, Loss: 0.984274
Train - Epoch 10, Batch: 20, Loss: 0.983934
Train - Epoch 10, Batch: 30, Loss: 0.978250
Train - Epoch 11, Batch: 0, Loss: 0.969511
Train - Epoch 11, Batch: 10, Loss: 0.977780
Train - Epoch 11, Batch: 20, Loss: 0.973965
Train - Epoch 11, Batch: 30, Loss: 0.968935
Train - Epoch 12, Batch: 0, Loss: 0.969963
Train - Epoch 12, Batch: 10, Loss: 0.968191
Train - Epoch 12, Batch: 20, Loss: 0.959772
Train - Epoch 12, Batch: 30, Loss: 0.969803
Train - Epoch 13, Batch: 0, Loss: 0.970994
Train - Epoch 13, Batch: 10, Loss: 0.966295
Train - Epoch 13, Batch: 20, Loss: 0.963957
Train - Epoch 13, Batch: 30, Loss: 0.953336
Train - Epoch 14, Batch: 0, Loss: 0.965758
Train - Epoch 14, Batch: 10, Loss: 0.954990
Train - Epoch 14, Batch: 20, Loss: 0.960669
Train - Epoch 14, Batch: 30, Loss: 0.946408
Train - Epoch 15, Batch: 0, Loss: 0.959624
Train - Epoch 15, Batch: 10, Loss: 0.957505
Train - Epoch 15, Batch: 20, Loss: 0.944263
Train - Epoch 15, Batch: 30, Loss: 0.947833
Train - Epoch 16, Batch: 0, Loss: 0.941905
Train - Epoch 16, Batch: 10, Loss: 0.945955
Train - Epoch 16, Batch: 20, Loss: 0.955058
Train - Epoch 16, Batch: 30, Loss: 0.944017
Train - Epoch 17, Batch: 0, Loss: 0.946356
Train - Epoch 17, Batch: 10, Loss: 0.943455
Train - Epoch 17, Batch: 20, Loss: 0.932673
Train - Epoch 17, Batch: 30, Loss: 0.940969
Train - Epoch 18, Batch: 0, Loss: 0.929879
Train - Epoch 18, Batch: 10, Loss: 0.933239
Train - Epoch 18, Batch: 20, Loss: 0.932196
Train - Epoch 18, Batch: 30, Loss: 0.926725
Train - Epoch 19, Batch: 0, Loss: 0.927979
Train - Epoch 19, Batch: 10, Loss: 0.935595
Train - Epoch 19, Batch: 20, Loss: 0.928884
Train - Epoch 19, Batch: 30, Loss: 0.925226
Train - Epoch 20, Batch: 0, Loss: 0.917027
Train - Epoch 20, Batch: 10, Loss: 0.919332
Train - Epoch 20, Batch: 20, Loss: 0.924414
Train - Epoch 20, Batch: 30, Loss: 0.930631
Train - Epoch 21, Batch: 0, Loss: 0.922440
Train - Epoch 21, Batch: 10, Loss: 0.933268
Train - Epoch 21, Batch: 20, Loss: 0.930352
Train - Epoch 21, Batch: 30, Loss: 0.914764
Train - Epoch 22, Batch: 0, Loss: 0.923810
Train - Epoch 22, Batch: 10, Loss: 0.915120
Train - Epoch 22, Batch: 20, Loss: 0.928227
Train - Epoch 22, Batch: 30, Loss: 0.921959
Train - Epoch 23, Batch: 0, Loss: 0.915973
Train - Epoch 23, Batch: 10, Loss: 0.913536
Train - Epoch 23, Batch: 20, Loss: 0.924774
Train - Epoch 23, Batch: 30, Loss: 0.917699
Train - Epoch 24, Batch: 0, Loss: 0.917848
Train - Epoch 24, Batch: 10, Loss: 0.909007
Train - Epoch 24, Batch: 20, Loss: 0.907304
Train - Epoch 24, Batch: 30, Loss: 0.906547
Train - Epoch 25, Batch: 0, Loss: 0.915334
Train - Epoch 25, Batch: 10, Loss: 0.911479
Train - Epoch 25, Batch: 20, Loss: 0.905449
Train - Epoch 25, Batch: 30, Loss: 0.908623
Train - Epoch 26, Batch: 0, Loss: 0.913231
Train - Epoch 26, Batch: 10, Loss: 0.906293
Train - Epoch 26, Batch: 20, Loss: 0.901136
Train - Epoch 26, Batch: 30, Loss: 0.898751
Train - Epoch 27, Batch: 0, Loss: 0.905330
Train - Epoch 27, Batch: 10, Loss: 0.906326
Train - Epoch 27, Batch: 20, Loss: 0.911192
Train - Epoch 27, Batch: 30, Loss: 0.908988
Train - Epoch 28, Batch: 0, Loss: 0.902907
Train - Epoch 28, Batch: 10, Loss: 0.894357
Train - Epoch 28, Batch: 20, Loss: 0.907910
Train - Epoch 28, Batch: 30, Loss: 0.899330
Train - Epoch 29, Batch: 0, Loss: 0.896879
Train - Epoch 29, Batch: 10, Loss: 0.901833
Train - Epoch 29, Batch: 20, Loss: 0.897396
Train - Epoch 29, Batch: 30, Loss: 0.898419
Train - Epoch 30, Batch: 0, Loss: 0.915593
Train - Epoch 30, Batch: 10, Loss: 0.896779
Train - Epoch 30, Batch: 20, Loss: 0.899363
Train - Epoch 30, Batch: 30, Loss: 0.903351
Train - Epoch 31, Batch: 0, Loss: 0.896703
Train - Epoch 31, Batch: 10, Loss: 0.901274
Train - Epoch 31, Batch: 20, Loss: 0.894997
Train - Epoch 31, Batch: 30, Loss: 0.888369
Test Avg. Loss: 0.000070, Accuracy: 0.628823
training_time:: 3.5630388259887695
training time full:: 3.56310772895813
provenance prepare time:: 6.67572021484375e-06
Test Avg. Loss: 0.000070, Accuracy: 0.628823
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
batch_size:: 16384
epoch:: 0
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 3.657374382019043
overhead:: 0
overhead2:: 1.7760429382324219
overhead3:: 0
time_baseline:: 3.658240556716919
curr_diff: 0 tensor(0.0056, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0056, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628875
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.01843118667602539
overhead3:: 0.05760622024536133
overhead4:: 0.20964717864990234
overhead5:: 0
memory usage:: 3781251072
time_provenance:: 1.212592601776123
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0059, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0059, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628978
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.021569252014160156
overhead3:: 0.06129622459411621
overhead4:: 0.24698162078857422
overhead5:: 0
memory usage:: 3804925952
time_provenance:: 1.288487195968628
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0060, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0060, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628926
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.0234220027923584
overhead3:: 0.07251501083374023
overhead4:: 0.2666192054748535
overhead5:: 0
memory usage:: 3789688832
time_provenance:: 1.2732694149017334
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0060, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0060, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628978
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.02652430534362793
overhead3:: 0.07162833213806152
overhead4:: 0.3212707042694092
overhead5:: 0
memory usage:: 3762167808
time_provenance:: 1.365461826324463
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0060, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0060, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628926
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.029150009155273438
overhead3:: 0.07200288772583008
overhead4:: 0.3448905944824219
overhead5:: 0
memory usage:: 3763453952
time_provenance:: 1.415168046951294
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0061, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0061, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628995
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.03506350517272949
overhead3:: 0.07845592498779297
overhead4:: 0.4150404930114746
overhead5:: 0
memory usage:: 3808804864
time_provenance:: 1.513425588607788
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0058, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0058, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628909
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.037274837493896484
overhead3:: 0.08321356773376465
overhead4:: 0.4386179447174072
overhead5:: 0
memory usage:: 3764584448
time_provenance:: 1.5557401180267334
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0058, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0058, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628909
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.040654659271240234
overhead3:: 0.08584761619567871
overhead4:: 0.46923375129699707
overhead5:: 0
memory usage:: 3853742080
time_provenance:: 1.5892465114593506
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0059, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0059, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628909
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.05084800720214844
overhead3:: 0.08882474899291992
overhead4:: 0.48256349563598633
overhead5:: 0
memory usage:: 3765628928
time_provenance:: 1.529505729675293
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0059, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0059, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628909
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.043857574462890625
overhead3:: 0.10043716430664062
overhead4:: 0.5381190776824951
overhead5:: 0
memory usage:: 3798335488
time_provenance:: 1.629883050918579
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0059, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0059, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628909
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.06370878219604492
overhead3:: 0.11999392509460449
overhead4:: 0.802166223526001
overhead5:: 0
memory usage:: 3781632000
time_provenance:: 1.9469020366668701
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0057, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0057, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628909
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.06050562858581543
overhead3:: 0.11907386779785156
overhead4:: 0.8375518321990967
overhead5:: 0
memory usage:: 3781218304
time_provenance:: 1.9189910888671875
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0057, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0057, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628909
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.0709693431854248
overhead3:: 0.13141751289367676
overhead4:: 0.7943291664123535
overhead5:: 0
memory usage:: 3802296320
time_provenance:: 1.918424367904663
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0057, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0057, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628909
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.07301044464111328
overhead3:: 0.12774276733398438
overhead4:: 0.8824822902679443
overhead5:: 0
memory usage:: 3763576832
time_provenance:: 2.021531820297241
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0057, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0057, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628909
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.0713968276977539
overhead3:: 0.13077378273010254
overhead4:: 0.9353494644165039
overhead5:: 0
memory usage:: 3786047488
time_provenance:: 2.1005733013153076
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0057, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0057, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628909
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.14294791221618652
overhead3:: 0.23965930938720703
overhead4:: 1.9352447986602783
overhead5:: 0
memory usage:: 3775606784
time_provenance:: 3.224123477935791
curr_diff: 0 tensor(2.0427e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.0427e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0056, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0056, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628892
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.1431107521057129
overhead3:: 0.2333815097808838
overhead4:: 1.956273078918457
overhead5:: 0
memory usage:: 3769188352
time_provenance:: 3.227024793624878
curr_diff: 0 tensor(2.0283e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.0283e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0056, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0056, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628892
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.14971160888671875
overhead3:: 0.24851083755493164
overhead4:: 1.971801519393921
overhead5:: 0
memory usage:: 3769999360
time_provenance:: 3.2751996517181396
curr_diff: 0 tensor(2.0299e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.0299e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0056, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0056, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628892
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.14970111846923828
overhead3:: 0.26132726669311523
overhead4:: 1.9273006916046143
overhead5:: 0
memory usage:: 3781726208
time_provenance:: 3.220632314682007
curr_diff: 0 tensor(2.1043e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1043e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0056, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0056, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628892
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.15102124214172363
overhead3:: 0.24431586265563965
overhead4:: 1.9668347835540771
overhead5:: 0
memory usage:: 3769909248
time_provenance:: 3.2461440563201904
curr_diff: 0 tensor(2.0918e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.0918e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0056, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0056, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628892
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.295215368270874
overhead3:: 0.4589056968688965
overhead4:: 3.4283978939056396
overhead5:: 0
memory usage:: 3773796352
time_provenance:: 4.623900890350342
curr_diff: 0 tensor(9.8140e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.8140e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0056, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0056, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.628875
repetition 4
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression covtype 4 0.005 1 1
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 2.071663
Train - Epoch 0, Batch: 10, Loss: 1.432201
Train - Epoch 0, Batch: 20, Loss: 1.267009
Train - Epoch 0, Batch: 30, Loss: 1.208638
Train - Epoch 1, Batch: 0, Loss: 1.200411
Train - Epoch 1, Batch: 10, Loss: 1.184393
Train - Epoch 1, Batch: 20, Loss: 1.157612
Train - Epoch 1, Batch: 30, Loss: 1.148406
Train - Epoch 2, Batch: 0, Loss: 1.158182
Train - Epoch 2, Batch: 10, Loss: 1.137843
Train - Epoch 2, Batch: 20, Loss: 1.124876
Train - Epoch 2, Batch: 30, Loss: 1.119733
Train - Epoch 3, Batch: 0, Loss: 1.105022
Train - Epoch 3, Batch: 10, Loss: 1.096310
Train - Epoch 3, Batch: 20, Loss: 1.096360
Train - Epoch 3, Batch: 30, Loss: 1.093449
Train - Epoch 4, Batch: 0, Loss: 1.096978
Train - Epoch 4, Batch: 10, Loss: 1.077356
Train - Epoch 4, Batch: 20, Loss: 1.065915
Train - Epoch 4, Batch: 30, Loss: 1.061078
Train - Epoch 5, Batch: 0, Loss: 1.060632
Train - Epoch 5, Batch: 10, Loss: 1.052316
Train - Epoch 5, Batch: 20, Loss: 1.058980
Train - Epoch 5, Batch: 30, Loss: 1.043175
Train - Epoch 6, Batch: 0, Loss: 1.043119
Train - Epoch 6, Batch: 10, Loss: 1.042315
Train - Epoch 6, Batch: 20, Loss: 1.034352
Train - Epoch 6, Batch: 30, Loss: 1.032096
Train - Epoch 7, Batch: 0, Loss: 1.027413
Train - Epoch 7, Batch: 10, Loss: 1.021623
Train - Epoch 7, Batch: 20, Loss: 1.018719
Train - Epoch 7, Batch: 30, Loss: 1.007786
Train - Epoch 8, Batch: 0, Loss: 1.013776
Train - Epoch 8, Batch: 10, Loss: 1.010093
Train - Epoch 8, Batch: 20, Loss: 1.006112
Train - Epoch 8, Batch: 30, Loss: 1.002022
Train - Epoch 9, Batch: 0, Loss: 0.997271
Train - Epoch 9, Batch: 10, Loss: 0.994249
Train - Epoch 9, Batch: 20, Loss: 0.996238
Train - Epoch 9, Batch: 30, Loss: 0.990897
Train - Epoch 10, Batch: 0, Loss: 1.002380
Train - Epoch 10, Batch: 10, Loss: 0.990178
Train - Epoch 10, Batch: 20, Loss: 0.985039
Train - Epoch 10, Batch: 30, Loss: 0.987118
Train - Epoch 11, Batch: 0, Loss: 0.980245
Train - Epoch 11, Batch: 10, Loss: 0.972060
Train - Epoch 11, Batch: 20, Loss: 0.981795
Train - Epoch 11, Batch: 30, Loss: 0.976848
Train - Epoch 12, Batch: 0, Loss: 0.981590
Train - Epoch 12, Batch: 10, Loss: 0.975902
Train - Epoch 12, Batch: 20, Loss: 0.971838
Train - Epoch 12, Batch: 30, Loss: 0.960211
Train - Epoch 13, Batch: 0, Loss: 0.965577
Train - Epoch 13, Batch: 10, Loss: 0.960743
Train - Epoch 13, Batch: 20, Loss: 0.961757
Train - Epoch 13, Batch: 30, Loss: 0.963292
Train - Epoch 14, Batch: 0, Loss: 0.963447
Train - Epoch 14, Batch: 10, Loss: 0.958036
Train - Epoch 14, Batch: 20, Loss: 0.956816
Train - Epoch 14, Batch: 30, Loss: 0.952260
Train - Epoch 15, Batch: 0, Loss: 0.957168
Train - Epoch 15, Batch: 10, Loss: 0.954905
Train - Epoch 15, Batch: 20, Loss: 0.949233
Train - Epoch 15, Batch: 30, Loss: 0.950394
Train - Epoch 16, Batch: 0, Loss: 0.940319
Train - Epoch 16, Batch: 10, Loss: 0.940842
Train - Epoch 16, Batch: 20, Loss: 0.952209
Train - Epoch 16, Batch: 30, Loss: 0.948748
Train - Epoch 17, Batch: 0, Loss: 0.942808
Train - Epoch 17, Batch: 10, Loss: 0.945207
Train - Epoch 17, Batch: 20, Loss: 0.948232
Train - Epoch 17, Batch: 30, Loss: 0.939427
Train - Epoch 18, Batch: 0, Loss: 0.940658
Train - Epoch 18, Batch: 10, Loss: 0.937098
Train - Epoch 18, Batch: 20, Loss: 0.932864
Train - Epoch 18, Batch: 30, Loss: 0.936835
Train - Epoch 19, Batch: 0, Loss: 0.931879
Train - Epoch 19, Batch: 10, Loss: 0.938943
Train - Epoch 19, Batch: 20, Loss: 0.923643
Train - Epoch 19, Batch: 30, Loss: 0.924353
Train - Epoch 20, Batch: 0, Loss: 0.931761
Train - Epoch 20, Batch: 10, Loss: 0.921615
Train - Epoch 20, Batch: 20, Loss: 0.925581
Train - Epoch 20, Batch: 30, Loss: 0.915874
Train - Epoch 21, Batch: 0, Loss: 0.927887
Train - Epoch 21, Batch: 10, Loss: 0.933349
Train - Epoch 21, Batch: 20, Loss: 0.923909
Train - Epoch 21, Batch: 30, Loss: 0.916831
Train - Epoch 22, Batch: 0, Loss: 0.918779
Train - Epoch 22, Batch: 10, Loss: 0.921948
Train - Epoch 22, Batch: 20, Loss: 0.913036
Train - Epoch 22, Batch: 30, Loss: 0.921452
Train - Epoch 23, Batch: 0, Loss: 0.913770
Train - Epoch 23, Batch: 10, Loss: 0.918552
Train - Epoch 23, Batch: 20, Loss: 0.917423
Train - Epoch 23, Batch: 30, Loss: 0.911588
Train - Epoch 24, Batch: 0, Loss: 0.926289
Train - Epoch 24, Batch: 10, Loss: 0.900054
Train - Epoch 24, Batch: 20, Loss: 0.913434
Train - Epoch 24, Batch: 30, Loss: 0.910655
Train - Epoch 25, Batch: 0, Loss: 0.904605
Train - Epoch 25, Batch: 10, Loss: 0.899904
Train - Epoch 25, Batch: 20, Loss: 0.910143
Train - Epoch 25, Batch: 30, Loss: 0.913119
Train - Epoch 26, Batch: 0, Loss: 0.897830
Train - Epoch 26, Batch: 10, Loss: 0.916033
Train - Epoch 26, Batch: 20, Loss: 0.904546
Train - Epoch 26, Batch: 30, Loss: 0.908814
Train - Epoch 27, Batch: 0, Loss: 0.896311
Train - Epoch 27, Batch: 10, Loss: 0.901183
Train - Epoch 27, Batch: 20, Loss: 0.903363
Train - Epoch 27, Batch: 30, Loss: 0.908612
Train - Epoch 28, Batch: 0, Loss: 0.901439
Train - Epoch 28, Batch: 10, Loss: 0.900176
Train - Epoch 28, Batch: 20, Loss: 0.901109
Train - Epoch 28, Batch: 30, Loss: 0.903098
Train - Epoch 29, Batch: 0, Loss: 0.902529
Train - Epoch 29, Batch: 10, Loss: 0.904642
Train - Epoch 29, Batch: 20, Loss: 0.905902
Train - Epoch 29, Batch: 30, Loss: 0.898094
Train - Epoch 30, Batch: 0, Loss: 0.896255
Train - Epoch 30, Batch: 10, Loss: 0.898673
Train - Epoch 30, Batch: 20, Loss: 0.905944
Train - Epoch 30, Batch: 30, Loss: 0.905557
Train - Epoch 31, Batch: 0, Loss: 0.895589
Train - Epoch 31, Batch: 10, Loss: 0.891459
Train - Epoch 31, Batch: 20, Loss: 0.894654
Train - Epoch 31, Batch: 30, Loss: 0.900720
Test Avg. Loss: 0.000070, Accuracy: 0.629133
training_time:: 3.4046528339385986
training time full:: 3.4047160148620605
provenance prepare time:: 6.67572021484375e-06
Test Avg. Loss: 0.000070, Accuracy: 0.629133
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
batch_size:: 16384
epoch:: 0
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
training time is 3.7170774936676025
overhead:: 0
overhead2:: 1.7570164203643799
overhead3:: 0
time_baseline:: 3.7181739807128906
curr_diff: 0 tensor(0.0056, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0056, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629288
period:: 20
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 20 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.018528223037719727
overhead3:: 0.0653069019317627
overhead4:: 0.21450018882751465
overhead5:: 0
memory usage:: 3785695232
time_provenance:: 1.2247447967529297
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0058, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0058, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629322
period:: 20
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 20 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.029445409774780273
overhead3:: 0.060854196548461914
overhead4:: 0.24637651443481445
overhead5:: 0
memory usage:: 3782270976
time_provenance:: 1.2842309474945068
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0062, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0062, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629322
period:: 20
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 20 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.02368021011352539
overhead3:: 0.0696718692779541
overhead4:: 0.2816131114959717
overhead5:: 0
memory usage:: 3774103552
time_provenance:: 1.328308343887329
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0059, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0059, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629322
period:: 20
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 20 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.02668905258178711
overhead3:: 0.07886815071105957
overhead4:: 0.3068807125091553
overhead5:: 0
memory usage:: 3765055488
time_provenance:: 1.342301845550537
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0062, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0062, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629322
period:: 20
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 20 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.030244827270507812
overhead3:: 0.07469749450683594
overhead4:: 0.3509948253631592
overhead5:: 0
memory usage:: 3768225792
time_provenance:: 1.3807296752929688
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0059, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0059, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629322
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.03262186050415039
overhead3:: 0.0805816650390625
overhead4:: 0.3902120590209961
overhead5:: 0
memory usage:: 3774054400
time_provenance:: 1.4244353771209717
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0059, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0059, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629288
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.03686714172363281
overhead3:: 0.10178780555725098
overhead4:: 0.45169925689697266
overhead5:: 0
memory usage:: 3768958976
time_provenance:: 1.5478489398956299
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0059, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0059, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629288
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.03829169273376465
overhead3:: 0.08641767501831055
overhead4:: 0.46064138412475586
overhead5:: 0
memory usage:: 3808841728
time_provenance:: 1.5347509384155273
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0059, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0059, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629288
period:: 10
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 10 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.04177212715148926
overhead3:: 0.09736895561218262
overhead4:: 0.4821896553039551
overhead5:: 0
memory usage:: 3791187968
time_provenance:: 1.5745816230773926
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0059, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0059, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629288
period:: 10
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 10 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.04175448417663574
overhead3:: 0.09512805938720703
overhead4:: 0.5123858451843262
overhead5:: 0
memory usage:: 3782336512
time_provenance:: 1.5765211582183838
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0059, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0059, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629288
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.05884957313537598
overhead3:: 0.12699389457702637
overhead4:: 0.809807538986206
overhead5:: 0
memory usage:: 3785031680
time_provenance:: 1.9179599285125732
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0057, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0057, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629288
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.06545472145080566
overhead3:: 0.13342881202697754
overhead4:: 0.7593474388122559
overhead5:: 0
memory usage:: 3799068672
time_provenance:: 1.8605732917785645
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0057, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0057, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629288
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.06896448135375977
overhead3:: 0.12653517723083496
overhead4:: 0.8802006244659424
overhead5:: 0
memory usage:: 3775176704
time_provenance:: 1.9970412254333496
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0057, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0057, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629288
period:: 5
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 5 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.0659322738647461
overhead3:: 0.12442636489868164
overhead4:: 0.8695478439331055
overhead5:: 0
memory usage:: 3765698560
time_provenance:: 1.969815731048584
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0057, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0057, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629288
period:: 5
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 5 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.07058191299438477
overhead3:: 0.13211321830749512
overhead4:: 0.9368851184844971
overhead5:: 0
memory usage:: 3776544768
time_provenance:: 2.0629708766937256
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0057, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0057, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629288
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.14131665229797363
overhead3:: 0.23343348503112793
overhead4:: 1.9305555820465088
overhead5:: 0
memory usage:: 3790004224
time_provenance:: 3.2048635482788086
curr_diff: 0 tensor(3.1465e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.1465e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0057, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0057, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629288
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.14643454551696777
overhead3:: 0.2405085563659668
overhead4:: 1.8861606121063232
overhead5:: 0
memory usage:: 3763957760
time_provenance:: 3.1665103435516357
curr_diff: 0 tensor(3.1451e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.1451e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0057, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0057, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629288
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.1667492389678955
overhead3:: 0.26603221893310547
overhead4:: 2.141827344894409
overhead5:: 0
memory usage:: 3782533120
time_provenance:: 3.549894332885742
curr_diff: 0 tensor(3.1565e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.1565e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0057, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0057, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629288
period:: 2
init_iters:: 40
incremental updates::
python3 incremental_updates_provenance3_lr.py 40 2 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.14832544326782227
overhead3:: 0.25769686698913574
overhead4:: 1.9127812385559082
overhead5:: 0
memory usage:: 3762212864
time_provenance:: 3.1942379474639893
curr_diff: 0 tensor(3.2001e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.2001e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0057, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0057, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629288
period:: 2
init_iters:: 50
incremental updates::
python3 incremental_updates_provenance3_lr.py 50 2 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.15099263191223145
overhead3:: 0.2582433223724365
overhead4:: 2.0479133129119873
overhead5:: 0
memory usage:: 3789500416
time_provenance:: 3.360429286956787
curr_diff: 0 tensor(3.1866e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.1866e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0057, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0057, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629288
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 32
delta_size:: 5229
max_epoch:: 32
overhead:: 0
overhead2:: 0.3581080436706543
overhead3:: 0.5898270606994629
overhead4:: 3.6249852180480957
overhead5:: 0
memory usage:: 3772940288
time_provenance:: 4.949779033660889
curr_diff: 0 tensor(9.9647e-14, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.9647e-14, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0056, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0056, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000070, Accuracy: 0.629288
