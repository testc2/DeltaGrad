varied deletion rate::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type MSELoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
cuda:2
Train - Epoch 0, Batch: 0, Loss: 0.091544
Train - Epoch 0, Batch: 10, Loss: 0.090281
Test Avg. Loss: 0.000027, Accuracy: 0.326300
Train - Epoch 1, Batch: 0, Loss: 0.089719
Train - Epoch 1, Batch: 10, Loss: 0.089165
Test Avg. Loss: 0.000026, Accuracy: 0.574700
Train - Epoch 2, Batch: 0, Loss: 0.088606
Train - Epoch 2, Batch: 10, Loss: 0.088131
Test Avg. Loss: 0.000026, Accuracy: 0.640000
Train - Epoch 3, Batch: 0, Loss: 0.087413
Train - Epoch 3, Batch: 10, Loss: 0.086967
Test Avg. Loss: 0.000025, Accuracy: 0.657200
Train - Epoch 4, Batch: 0, Loss: 0.086142
Train - Epoch 4, Batch: 10, Loss: 0.085224
Test Avg. Loss: 0.000025, Accuracy: 0.662700
Train - Epoch 5, Batch: 0, Loss: 0.085057
Train - Epoch 5, Batch: 10, Loss: 0.084319
Test Avg. Loss: 0.000025, Accuracy: 0.676800
Train - Epoch 6, Batch: 0, Loss: 0.083921
Train - Epoch 6, Batch: 10, Loss: 0.083097
Test Avg. Loss: 0.000024, Accuracy: 0.690200
Train - Epoch 7, Batch: 0, Loss: 0.083321
Train - Epoch 7, Batch: 10, Loss: 0.082739
Test Avg. Loss: 0.000024, Accuracy: 0.701300
training_time:: 63.35552716255188
varied number of samples::
deletion rate:: 0.00002
repetition 1
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 50.02582931518555
time_baseline:: 50.04455780982971
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.1535e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.3543e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.701300
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(6.0348e-33, dtype=torch.float64)
secont condition:: tensor(6.0348e-33, dtype=torch.float64)
curr_secont condition:: tensor(1.4958e-32, dtype=torch.float64)
secont condition:: tensor(1.4958e-32, dtype=torch.float64)
curr_secont condition:: tensor(2.6747e-32, dtype=torch.float64)
secont condition:: tensor(2.6747e-32, dtype=torch.float64)
curr_secont condition:: tensor(4.1223e-32, dtype=torch.float64)
secont condition:: tensor(4.1223e-32, dtype=torch.float64)
curr_secont condition:: tensor(5.8547e-32, dtype=torch.float64)
secont condition:: tensor(5.8547e-32, dtype=torch.float64)
curr_secont condition:: tensor(7.8546e-32, dtype=torch.float64)
secont condition:: tensor(7.8546e-32, dtype=torch.float64)
curr_secont condition:: tensor(1.0110e-31, dtype=torch.float64)
secont condition:: tensor(1.0110e-31, dtype=torch.float64)
curr_secont condition:: tensor(1.2650e-31, dtype=torch.float64)
secont condition:: tensor(1.2650e-31, dtype=torch.float64)
curr_secont condition:: tensor(1.5452e-31, dtype=torch.float64)
secont condition:: tensor(1.5452e-31, dtype=torch.float64)
curr_secont condition:: tensor(1.8514e-31, dtype=torch.float64)
secont condition:: tensor(1.8514e-31, dtype=torch.float64)
curr_secont condition:: tensor(2.1853e-31, dtype=torch.float64)
secont condition:: tensor(2.1853e-31, dtype=torch.float64)
curr_secont condition:: tensor(2.5429e-31, dtype=torch.float64)
secont condition:: tensor(2.5429e-31, dtype=torch.float64)
curr_secont condition:: tensor(1.2808e-10, dtype=torch.float64)
secont condition:: tensor(1.2808e-10, dtype=torch.float64)
curr_secont condition:: tensor(9.2307e-11, dtype=torch.float64)
secont condition:: tensor(9.2307e-11, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(6.9004e-11, dtype=torch.float64)
secont condition:: tensor(6.9004e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.3164e-11, dtype=torch.float64)
secont condition:: tensor(5.3164e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.5662e-11, dtype=torch.float64)
secont condition:: tensor(4.5662e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.1134e-11, dtype=torch.float64)
secont condition:: tensor(4.1134e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.5750e-11, dtype=torch.float64)
secont condition:: tensor(4.5750e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.1359e-11, dtype=torch.float64)
secont condition:: tensor(3.1359e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.8268e-11, dtype=torch.float64)
secont condition:: tensor(1.8268e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.6194e-11, dtype=torch.float64)
secont condition:: tensor(2.6194e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.2698e-11, dtype=torch.float64)
secont condition:: tensor(1.2698e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.6589e-11, dtype=torch.float64)
secont condition:: tensor(1.6589e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.6070e-11, dtype=torch.float64)
secont condition:: tensor(1.6070e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.8058e-11, dtype=torch.float64)
secont condition:: tensor(1.8058e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.2065e-11, dtype=torch.float64)
secont condition:: tensor(2.2065e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.2334e-11, dtype=torch.float64)
secont condition:: tensor(1.2334e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.3469e-11, dtype=torch.float64)
secont condition:: tensor(1.3469e-11, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(2.0682e-11, dtype=torch.float64)
secont condition:: tensor(2.0682e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.5321e-11, dtype=torch.float64)
secont condition:: tensor(1.5321e-11, dtype=torch.float64)
curr_secont condition:: tensor(-4.9516e-12, dtype=torch.float64)
curr_secont condition:: tensor(2.8383e-11, dtype=torch.float64)
secont condition:: tensor(2.8383e-11, dtype=torch.float64)
curr_secont condition:: tensor(-5.3201e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.2615e-11, dtype=torch.float64)
secont condition:: tensor(2.2615e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.8755e-11, dtype=torch.float64)
secont condition:: tensor(1.8755e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.7927e-11, dtype=torch.float64)
secont condition:: tensor(1.7927e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.5493e-12, dtype=torch.float64)
secont condition:: tensor(5.5493e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.8644e-11, dtype=torch.float64)
secont condition:: tensor(1.8644e-11, dtype=torch.float64)
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(1.8745e-11, dtype=torch.float64)
secont condition:: tensor(1.8745e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.4917e-11, dtype=torch.float64)
explicit_evaluation epoch:: 58
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(2.8832e-11, dtype=torch.float64)
secont condition:: tensor(2.8832e-11, dtype=torch.float64)
curr_secont condition:: tensor(-4.6793e-10, dtype=torch.float64)
explicit_evaluation epoch:: 72
curr_secont condition:: tensor(5.8777e-10, dtype=torch.float64)
secont condition:: tensor(5.8777e-10, dtype=torch.float64)
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(4.4132e-10, dtype=torch.float64)
secont condition:: tensor(4.4132e-10, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(5.3385e-10, dtype=torch.float64)
secont condition:: tensor(5.3385e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.3007e-10, dtype=torch.float64)
secont condition:: tensor(4.3007e-10, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(2.3651e-10, dtype=torch.float64)
secont condition:: tensor(2.3651e-10, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 26.741000652313232
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(6.2427e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(3.1580e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(4.6928e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.1024e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.8154e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.698300
repetition 2
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 49.910377979278564
time_baseline:: 49.92845582962036
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.0182e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(7.0709e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.701300
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(6.0348e-33, dtype=torch.float64)
secont condition:: tensor(6.0348e-33, dtype=torch.float64)
curr_secont condition:: tensor(1.4958e-32, dtype=torch.float64)
secont condition:: tensor(1.4958e-32, dtype=torch.float64)
curr_secont condition:: tensor(2.6747e-32, dtype=torch.float64)
secont condition:: tensor(2.6747e-32, dtype=torch.float64)
curr_secont condition:: tensor(4.1223e-32, dtype=torch.float64)
secont condition:: tensor(4.1223e-32, dtype=torch.float64)
curr_secont condition:: tensor(5.8547e-32, dtype=torch.float64)
secont condition:: tensor(5.8547e-32, dtype=torch.float64)
curr_secont condition:: tensor(7.8546e-32, dtype=torch.float64)
secont condition:: tensor(7.8546e-32, dtype=torch.float64)
curr_secont condition:: tensor(1.0110e-31, dtype=torch.float64)
secont condition:: tensor(1.0110e-31, dtype=torch.float64)
curr_secont condition:: tensor(1.2650e-31, dtype=torch.float64)
secont condition:: tensor(1.2650e-31, dtype=torch.float64)
curr_secont condition:: tensor(1.5456e-31, dtype=torch.float64)
secont condition:: tensor(1.5456e-31, dtype=torch.float64)
curr_secont condition:: tensor(1.4377e-10, dtype=torch.float64)
secont condition:: tensor(1.4377e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.1460e-10, dtype=torch.float64)
secont condition:: tensor(1.1460e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.4730e-10, dtype=torch.float64)
secont condition:: tensor(1.4730e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.0050e-10, dtype=torch.float64)
secont condition:: tensor(1.0050e-10, dtype=torch.float64)
curr_secont condition:: tensor(8.4087e-11, dtype=torch.float64)
secont condition:: tensor(8.4087e-11, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(7.0550e-11, dtype=torch.float64)
secont condition:: tensor(7.0550e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.1421e-11, dtype=torch.float64)
secont condition:: tensor(6.1421e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.7893e-11, dtype=torch.float64)
secont condition:: tensor(5.7893e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.3143e-11, dtype=torch.float64)
secont condition:: tensor(5.3143e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.1989e-11, dtype=torch.float64)
secont condition:: tensor(4.1989e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.7160e-11, dtype=torch.float64)
secont condition:: tensor(3.7160e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.8639e-11, dtype=torch.float64)
secont condition:: tensor(3.8639e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.3513e-11, dtype=torch.float64)
secont condition:: tensor(5.3513e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.4555e-11, dtype=torch.float64)
secont condition:: tensor(4.4555e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.6829e-11, dtype=torch.float64)
secont condition:: tensor(3.6829e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.7838e-11, dtype=torch.float64)
secont condition:: tensor(3.7838e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.6466e-11, dtype=torch.float64)
secont condition:: tensor(3.6466e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.0722e-11, dtype=torch.float64)
secont condition:: tensor(3.0722e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.9805e-11, dtype=torch.float64)
secont condition:: tensor(2.9805e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.3247e-11, dtype=torch.float64)
secont condition:: tensor(3.3247e-11, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(3.0292e-11, dtype=torch.float64)
secont condition:: tensor(3.0292e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.2186e-11, dtype=torch.float64)
secont condition:: tensor(3.2186e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.0522e-11, dtype=torch.float64)
secont condition:: tensor(3.0522e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.0404e-11, dtype=torch.float64)
secont condition:: tensor(2.0404e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.7897e-12, dtype=torch.float64)
secont condition:: tensor(6.7897e-12, dtype=torch.float64)
curr_secont condition:: tensor(2.7968e-11, dtype=torch.float64)
secont condition:: tensor(2.7968e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.3603e-11, dtype=torch.float64)
secont condition:: tensor(2.3603e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.9820e-11, dtype=torch.float64)
secont condition:: tensor(2.9820e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.8232e-11, dtype=torch.float64)
secont condition:: tensor(2.8232e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.6762e-11, dtype=torch.float64)
secont condition:: tensor(2.6762e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.5409e-10, dtype=torch.float64)
explicit_evaluation epoch:: 40
curr_secont condition:: tensor(2.3156e-10, dtype=torch.float64)
explicit_evaluation epoch:: 41
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(1.3207e-10, dtype=torch.float64)
explicit_evaluation epoch:: 48
curr_secont condition:: tensor(1.2693e-08, dtype=torch.float64)
explicit_evaluation epoch:: 53
curr_secont condition:: tensor(5.5773e-10, dtype=torch.float64)
explicit_evaluation epoch:: 54
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(3.6637e-10, dtype=torch.float64)
secont condition:: tensor(3.6637e-10, dtype=torch.float64)
curr_secont condition:: tensor(7.9531e-10, dtype=torch.float64)
secont condition:: tensor(7.9531e-10, dtype=torch.float64)
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(1.0316e-09, dtype=torch.float64)
secont condition:: tensor(1.0316e-09, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(7.9595e-10, dtype=torch.float64)
secont condition:: tensor(7.9595e-10, dtype=torch.float64)
curr_secont condition:: tensor(9.7286e-10, dtype=torch.float64)
secont condition:: tensor(9.7286e-10, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(8.5837e-10, dtype=torch.float64)
secont condition:: tensor(8.5837e-10, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 27.064573287963867
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(7.9666e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(5.7481e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.0334e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.0209e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.7275e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.698100
repetition 3
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 50.10612154006958
time_baseline:: 50.13244843482971
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.0242e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(3.8387e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.701300
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(6.0348e-33, dtype=torch.float64)
secont condition:: tensor(6.0348e-33, dtype=torch.float64)
curr_secont condition:: tensor(1.4958e-32, dtype=torch.float64)
secont condition:: tensor(1.4958e-32, dtype=torch.float64)
curr_secont condition:: tensor(2.6747e-32, dtype=torch.float64)
secont condition:: tensor(2.6747e-32, dtype=torch.float64)
curr_secont condition:: tensor(4.1223e-32, dtype=torch.float64)
secont condition:: tensor(4.1223e-32, dtype=torch.float64)
curr_secont condition:: tensor(5.8547e-32, dtype=torch.float64)
secont condition:: tensor(5.8547e-32, dtype=torch.float64)
curr_secont condition:: tensor(7.8546e-32, dtype=torch.float64)
secont condition:: tensor(7.8546e-32, dtype=torch.float64)
curr_secont condition:: tensor(1.0110e-31, dtype=torch.float64)
secont condition:: tensor(1.0110e-31, dtype=torch.float64)
curr_secont condition:: tensor(1.2650e-31, dtype=torch.float64)
secont condition:: tensor(1.2650e-31, dtype=torch.float64)
curr_secont condition:: tensor(1.5452e-31, dtype=torch.float64)
secont condition:: tensor(1.5452e-31, dtype=torch.float64)
curr_secont condition:: tensor(1.8514e-31, dtype=torch.float64)
secont condition:: tensor(1.8514e-31, dtype=torch.float64)
curr_secont condition:: tensor(2.1853e-31, dtype=torch.float64)
secont condition:: tensor(2.1853e-31, dtype=torch.float64)
curr_secont condition:: tensor(2.5430e-31, dtype=torch.float64)
secont condition:: tensor(2.5430e-31, dtype=torch.float64)
curr_secont condition:: tensor(2.9254e-31, dtype=torch.float64)
secont condition:: tensor(2.9254e-31, dtype=torch.float64)
curr_secont condition:: tensor(7.7722e-12, dtype=torch.float64)
secont condition:: tensor(7.7722e-12, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(6.6443e-12, dtype=torch.float64)
secont condition:: tensor(6.6443e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.4592e-12, dtype=torch.float64)
secont condition:: tensor(1.4592e-12, dtype=torch.float64)
curr_secont condition:: tensor(4.6842e-12, dtype=torch.float64)
secont condition:: tensor(4.6842e-12, dtype=torch.float64)
curr_secont condition:: tensor(3.6162e-12, dtype=torch.float64)
secont condition:: tensor(3.6162e-12, dtype=torch.float64)
curr_secont condition:: tensor(3.3430e-12, dtype=torch.float64)
secont condition:: tensor(3.3430e-12, dtype=torch.float64)
curr_secont condition:: tensor(2.7305e-12, dtype=torch.float64)
secont condition:: tensor(2.7305e-12, dtype=torch.float64)
curr_secont condition:: tensor(2.2629e-12, dtype=torch.float64)
secont condition:: tensor(2.2629e-12, dtype=torch.float64)
curr_secont condition:: tensor(3.0689e-12, dtype=torch.float64)
secont condition:: tensor(3.0689e-12, dtype=torch.float64)
curr_secont condition:: tensor(2.0191e-12, dtype=torch.float64)
secont condition:: tensor(2.0191e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.8851e-12, dtype=torch.float64)
secont condition:: tensor(1.8851e-12, dtype=torch.float64)
curr_secont condition:: tensor(2.0018e-12, dtype=torch.float64)
secont condition:: tensor(2.0018e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.6254e-12, dtype=torch.float64)
secont condition:: tensor(1.6254e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.6233e-12, dtype=torch.float64)
secont condition:: tensor(1.6233e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.5191e-12, dtype=torch.float64)
secont condition:: tensor(1.5191e-12, dtype=torch.float64)
curr_secont condition:: tensor(2.9529e-11, dtype=torch.float64)
secont condition:: tensor(2.9529e-11, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(1.9710e-11, dtype=torch.float64)
secont condition:: tensor(1.9710e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.0857e-11, dtype=torch.float64)
secont condition:: tensor(2.0857e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.4675e-11, dtype=torch.float64)
secont condition:: tensor(1.4675e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.2861e-11, dtype=torch.float64)
secont condition:: tensor(1.2861e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.2625e-10, dtype=torch.float64)
secont condition:: tensor(6.2625e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.6318e-10, dtype=torch.float64)
secont condition:: tensor(4.6318e-10, dtype=torch.float64)
curr_secont condition:: tensor(5.3307e-10, dtype=torch.float64)
secont condition:: tensor(5.3307e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.6376e-10, dtype=torch.float64)
secont condition:: tensor(3.6376e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.0055e-10, dtype=torch.float64)
secont condition:: tensor(3.0055e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.5723e-10, dtype=torch.float64)
secont condition:: tensor(2.5723e-10, dtype=torch.float64)
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(5.3693e-10, dtype=torch.float64)
secont condition:: tensor(5.3693e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.1074e-10, dtype=torch.float64)
secont condition:: tensor(4.1074e-10, dtype=torch.float64)
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(3.4500e-10, dtype=torch.float64)
secont condition:: tensor(3.4500e-10, dtype=torch.float64)
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(3.8796e-10, dtype=torch.float64)
secont condition:: tensor(3.8796e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.0517e-09, dtype=torch.float64)
secont condition:: tensor(1.0517e-09, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(5.1297e-10, dtype=torch.float64)
secont condition:: tensor(5.1297e-10, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(1.6719e-09, dtype=torch.float64)
secont condition:: tensor(1.6719e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.8166e-09, dtype=torch.float64)
secont condition:: tensor(1.8166e-09, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.765533208847046
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(8.0291e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(6.5414e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.2676e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.0039e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.5895e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.701500
repetition 4
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 50.7885365486145
time_baseline:: 50.806493282318115
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.9964e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.9035e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.701300
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(5.3979e-10, dtype=torch.float64)
secont condition:: tensor(5.3979e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.1696e-10, dtype=torch.float64)
secont condition:: tensor(3.1696e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.8000e-10, dtype=torch.float64)
secont condition:: tensor(1.8000e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.3679e-10, dtype=torch.float64)
secont condition:: tensor(1.3679e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.1241e-10, dtype=torch.float64)
secont condition:: tensor(1.1241e-10, dtype=torch.float64)
curr_secont condition:: tensor(7.5935e-11, dtype=torch.float64)
secont condition:: tensor(7.5935e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.9139e-11, dtype=torch.float64)
secont condition:: tensor(4.9139e-11, dtype=torch.float64)
curr_secont condition:: tensor(7.5323e-11, dtype=torch.float64)
secont condition:: tensor(7.5323e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.5397e-11, dtype=torch.float64)
secont condition:: tensor(6.5397e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.7080e-11, dtype=torch.float64)
secont condition:: tensor(2.7080e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.3269e-11, dtype=torch.float64)
secont condition:: tensor(5.3269e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.2478e-11, dtype=torch.float64)
secont condition:: tensor(4.2478e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.1496e-12, dtype=torch.float64)
secont condition:: tensor(2.1496e-12, dtype=torch.float64)
curr_secont condition:: tensor(5.7037e-11, dtype=torch.float64)
secont condition:: tensor(5.7037e-11, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(1.7923e-11, dtype=torch.float64)
secont condition:: tensor(1.7923e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.2176e-11, dtype=torch.float64)
secont condition:: tensor(4.2176e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.7653e-11, dtype=torch.float64)
secont condition:: tensor(2.7653e-11, dtype=torch.float64)
curr_secont condition:: tensor(-1.4837e-12, dtype=torch.float64)
curr_secont condition:: tensor(2.0636e-11, dtype=torch.float64)
secont condition:: tensor(2.0636e-11, dtype=torch.float64)
curr_secont condition:: tensor(8.7058e-12, dtype=torch.float64)
secont condition:: tensor(8.7058e-12, dtype=torch.float64)
curr_secont condition:: tensor(3.2128e-11, dtype=torch.float64)
secont condition:: tensor(3.2128e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.0545e-12, dtype=torch.float64)
secont condition:: tensor(6.0545e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.2083e-11, dtype=torch.float64)
secont condition:: tensor(1.2083e-11, dtype=torch.float64)
curr_secont condition:: tensor(8.9690e-12, dtype=torch.float64)
secont condition:: tensor(8.9690e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.8557e-11, dtype=torch.float64)
secont condition:: tensor(1.8557e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.9418e-11, dtype=torch.float64)
secont condition:: tensor(3.9418e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.7406e-11, dtype=torch.float64)
secont condition:: tensor(4.7406e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.7808e-11, dtype=torch.float64)
secont condition:: tensor(3.7808e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.0806e-12, dtype=torch.float64)
secont condition:: tensor(5.0806e-12, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(1.2639e-09, dtype=torch.float64)
secont condition:: tensor(1.2639e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.7123e-09, dtype=torch.float64)
secont condition:: tensor(2.7123e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.1892e-09, dtype=torch.float64)
secont condition:: tensor(2.1892e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.3372e-09, dtype=torch.float64)
secont condition:: tensor(1.3372e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.2509e-09, dtype=torch.float64)
secont condition:: tensor(1.2509e-09, dtype=torch.float64)
curr_secont condition:: tensor(8.3395e-10, dtype=torch.float64)
secont condition:: tensor(8.3395e-10, dtype=torch.float64)
curr_secont condition:: tensor(8.6690e-10, dtype=torch.float64)
secont condition:: tensor(8.6690e-10, dtype=torch.float64)
curr_secont condition:: tensor(6.0588e-10, dtype=torch.float64)
secont condition:: tensor(6.0588e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.0154e-10, dtype=torch.float64)
secont condition:: tensor(4.0154e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.1543e-10, dtype=torch.float64)
secont condition:: tensor(4.1543e-10, dtype=torch.float64)
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(4.4564e-10, dtype=torch.float64)
secont condition:: tensor(4.4564e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.5083e-09, dtype=torch.float64)
secont condition:: tensor(3.5083e-09, dtype=torch.float64)
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(7.6218e-09, dtype=torch.float64)
secont condition:: tensor(7.6218e-09, dtype=torch.float64)
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(1.9932e-09, dtype=torch.float64)
secont condition:: tensor(1.9932e-09, dtype=torch.float64)
curr_secont condition:: tensor(5.2959e-09, dtype=torch.float64)
secont condition:: tensor(5.2959e-09, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(6.4524e-09, dtype=torch.float64)
secont condition:: tensor(6.4524e-09, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(2.3659e-09, dtype=torch.float64)
secont condition:: tensor(2.3659e-09, dtype=torch.float64)
curr_secont condition:: tensor(4.6352e-09, dtype=torch.float64)
secont condition:: tensor(4.6352e-09, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.5798602104187
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.2036e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(7.7284e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(9.1404e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.8918e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(7.9757e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.701600
repetition 5
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 50.18240809440613
time_baseline:: 50.20051193237305
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(9.2242e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(4.4595e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.701300
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(6.0348e-33, dtype=torch.float64)
secont condition:: tensor(6.0348e-33, dtype=torch.float64)
curr_secont condition:: tensor(1.4958e-32, dtype=torch.float64)
secont condition:: tensor(1.4958e-32, dtype=torch.float64)
curr_secont condition:: tensor(2.6747e-32, dtype=torch.float64)
secont condition:: tensor(2.6747e-32, dtype=torch.float64)
curr_secont condition:: tensor(4.1223e-32, dtype=torch.float64)
secont condition:: tensor(4.1223e-32, dtype=torch.float64)
curr_secont condition:: tensor(5.8547e-32, dtype=torch.float64)
secont condition:: tensor(5.8547e-32, dtype=torch.float64)
curr_secont condition:: tensor(7.8546e-32, dtype=torch.float64)
secont condition:: tensor(7.8546e-32, dtype=torch.float64)
curr_secont condition:: tensor(1.0110e-31, dtype=torch.float64)
secont condition:: tensor(1.0110e-31, dtype=torch.float64)
curr_secont condition:: tensor(1.2650e-31, dtype=torch.float64)
secont condition:: tensor(1.2650e-31, dtype=torch.float64)
curr_secont condition:: tensor(1.5452e-31, dtype=torch.float64)
secont condition:: tensor(1.5452e-31, dtype=torch.float64)
curr_secont condition:: tensor(1.8514e-31, dtype=torch.float64)
secont condition:: tensor(1.8514e-31, dtype=torch.float64)
curr_secont condition:: tensor(2.1853e-31, dtype=torch.float64)
secont condition:: tensor(2.1853e-31, dtype=torch.float64)
curr_secont condition:: tensor(2.5430e-31, dtype=torch.float64)
secont condition:: tensor(2.5430e-31, dtype=torch.float64)
curr_secont condition:: tensor(2.9255e-31, dtype=torch.float64)
secont condition:: tensor(2.9255e-31, dtype=torch.float64)
curr_secont condition:: tensor(3.9674e-11, dtype=torch.float64)
secont condition:: tensor(3.9674e-11, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(3.5678e-11, dtype=torch.float64)
secont condition:: tensor(3.5678e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.2471e-10, dtype=torch.float64)
secont condition:: tensor(3.2471e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.4520e-10, dtype=torch.float64)
secont condition:: tensor(2.4520e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.8175e-10, dtype=torch.float64)
secont condition:: tensor(1.8175e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.4873e-10, dtype=torch.float64)
secont condition:: tensor(1.4873e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.2779e-10, dtype=torch.float64)
secont condition:: tensor(1.2779e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.1595e-10, dtype=torch.float64)
secont condition:: tensor(1.1595e-10, dtype=torch.float64)
curr_secont condition:: tensor(9.8143e-11, dtype=torch.float64)
secont condition:: tensor(9.8143e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.2744e-11, dtype=torch.float64)
secont condition:: tensor(4.2744e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.5420e-11, dtype=torch.float64)
secont condition:: tensor(5.5420e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.6975e-11, dtype=torch.float64)
secont condition:: tensor(5.6975e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.1386e-11, dtype=torch.float64)
secont condition:: tensor(5.1386e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.0708e-11, dtype=torch.float64)
secont condition:: tensor(5.0708e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.4001e-11, dtype=torch.float64)
secont condition:: tensor(4.4001e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.1953e-11, dtype=torch.float64)
secont condition:: tensor(4.1953e-11, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(3.4737e-11, dtype=torch.float64)
secont condition:: tensor(3.4737e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.3242e-11, dtype=torch.float64)
secont condition:: tensor(4.3242e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.5367e-11, dtype=torch.float64)
secont condition:: tensor(3.5367e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.9820e-11, dtype=torch.float64)
secont condition:: tensor(3.9820e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.3681e-10, dtype=torch.float64)
secont condition:: tensor(5.3681e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.0469e-10, dtype=torch.float64)
secont condition:: tensor(4.0469e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.2225e-10, dtype=torch.float64)
secont condition:: tensor(3.2225e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.0260e-10, dtype=torch.float64)
secont condition:: tensor(3.0260e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.0974e-10, dtype=torch.float64)
secont condition:: tensor(2.0974e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.1194e-10, dtype=torch.float64)
secont condition:: tensor(2.1194e-10, dtype=torch.float64)
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(1.3707e-10, dtype=torch.float64)
secont condition:: tensor(1.3707e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.9970e-10, dtype=torch.float64)
explicit_evaluation epoch:: 51
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(3.6604e-10, dtype=torch.float64)
explicit_evaluation epoch:: 60
curr_secont condition:: tensor(3.2632e-10, dtype=torch.float64)
secont condition:: tensor(3.2632e-10, dtype=torch.float64)
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(8.3034e-10, dtype=torch.float64)
secont condition:: tensor(8.3034e-10, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(5.7083e-10, dtype=torch.float64)
secont condition:: tensor(5.7083e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.5514e-10, dtype=torch.float64)
secont condition:: tensor(1.5514e-10, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(1.5209e-10, dtype=torch.float64)
explicit_evaluation epoch:: 105
curr_secont condition:: tensor(1.2027e-10, dtype=torch.float64)
secont condition:: tensor(1.2027e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.1741e-10, dtype=torch.float64)
explicit_evaluation epoch:: 116
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 27.356430292129517
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(9.7767e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(2.7495e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(3.2990e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.0295e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(7.6839e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.701200
deletion rate:: 0.00005
repetition 1
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 3
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 50.996671199798584
time_baseline:: 51.0147705078125
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.6485e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(6.3056e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.701300
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 3
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(6.0348e-33, dtype=torch.float64)
secont condition:: tensor(6.0348e-33, dtype=torch.float64)
curr_secont condition:: tensor(1.4998e-32, dtype=torch.float64)
secont condition:: tensor(1.4998e-32, dtype=torch.float64)
curr_secont condition:: tensor(8.0345e-11, dtype=torch.float64)
secont condition:: tensor(8.0345e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.9863e-11, dtype=torch.float64)
secont condition:: tensor(4.9863e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.1224e-11, dtype=torch.float64)
secont condition:: tensor(4.1224e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.3828e-11, dtype=torch.float64)
secont condition:: tensor(3.3828e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.3462e-10, dtype=torch.float64)
secont condition:: tensor(6.3462e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.9604e-10, dtype=torch.float64)
secont condition:: tensor(4.9604e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.8239e-10, dtype=torch.float64)
secont condition:: tensor(3.8239e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.7956e-10, dtype=torch.float64)
secont condition:: tensor(2.7956e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.5992e-10, dtype=torch.float64)
secont condition:: tensor(2.5992e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.4920e-10, dtype=torch.float64)
secont condition:: tensor(2.4920e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.8993e-10, dtype=torch.float64)
secont condition:: tensor(2.8993e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.1108e-10, dtype=torch.float64)
secont condition:: tensor(2.1108e-10, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(1.7889e-10, dtype=torch.float64)
secont condition:: tensor(1.7889e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.7011e-10, dtype=torch.float64)
secont condition:: tensor(1.7011e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.3727e-10, dtype=torch.float64)
secont condition:: tensor(1.3727e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.2652e-10, dtype=torch.float64)
secont condition:: tensor(1.2652e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.1540e-10, dtype=torch.float64)
secont condition:: tensor(1.1540e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.3054e-09, dtype=torch.float64)
secont condition:: tensor(1.3054e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.0088e-09, dtype=torch.float64)
secont condition:: tensor(1.0088e-09, dtype=torch.float64)
curr_secont condition:: tensor(7.5258e-10, dtype=torch.float64)
secont condition:: tensor(7.5258e-10, dtype=torch.float64)
curr_secont condition:: tensor(6.9068e-10, dtype=torch.float64)
secont condition:: tensor(6.9068e-10, dtype=torch.float64)
curr_secont condition:: tensor(5.8093e-10, dtype=torch.float64)
secont condition:: tensor(5.8093e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.2427e-10, dtype=torch.float64)
secont condition:: tensor(4.2427e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.6448e-10, dtype=torch.float64)
secont condition:: tensor(4.6448e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.4674e-10, dtype=torch.float64)
secont condition:: tensor(4.4674e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.1957e-10, dtype=torch.float64)
secont condition:: tensor(3.1957e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.8307e-10, dtype=torch.float64)
secont condition:: tensor(2.8307e-10, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(4.5376e-10, dtype=torch.float64)
secont condition:: tensor(4.5376e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.9441e-10, dtype=torch.float64)
secont condition:: tensor(3.9441e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.8212e-10, dtype=torch.float64)
secont condition:: tensor(3.8212e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.3842e-10, dtype=torch.float64)
secont condition:: tensor(3.3842e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.8966e-10, dtype=torch.float64)
secont condition:: tensor(2.8966e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.9579e-10, dtype=torch.float64)
secont condition:: tensor(2.9579e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.3007e-10, dtype=torch.float64)
secont condition:: tensor(2.3007e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.0387e-09, dtype=torch.float64)
secont condition:: tensor(1.0387e-09, dtype=torch.float64)
curr_secont condition:: tensor(7.6331e-10, dtype=torch.float64)
secont condition:: tensor(7.6331e-10, dtype=torch.float64)
curr_secont condition:: tensor(7.1002e-10, dtype=torch.float64)
secont condition:: tensor(7.1002e-10, dtype=torch.float64)
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(1.5545e-09, dtype=torch.float64)
secont condition:: tensor(1.5545e-09, dtype=torch.float64)
curr_secont condition:: tensor(5.1603e-09, dtype=torch.float64)
secont condition:: tensor(5.1603e-09, dtype=torch.float64)
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(2.9904e-09, dtype=torch.float64)
secont condition:: tensor(2.9904e-09, dtype=torch.float64)
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(4.8591e-09, dtype=torch.float64)
secont condition:: tensor(4.8591e-09, dtype=torch.float64)
curr_secont condition:: tensor(3.5956e-09, dtype=torch.float64)
secont condition:: tensor(3.5956e-09, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(3.1854e-09, dtype=torch.float64)
secont condition:: tensor(3.1854e-09, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(3.9333e-09, dtype=torch.float64)
secont condition:: tensor(3.9333e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.6272e-09, dtype=torch.float64)
secont condition:: tensor(2.6272e-09, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 27.420258283615112
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.2570e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(8.8376e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.3571e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.5443e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.6033e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.701500
repetition 2
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 3
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 51.6285285949707
time_baseline:: 51.6465106010437
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(2.4604e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(6.4961e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.701300
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 3
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(4.7746e-10, dtype=torch.float64)
secont condition:: tensor(4.7746e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.8268e-10, dtype=torch.float64)
secont condition:: tensor(2.8268e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.5505e-10, dtype=torch.float64)
secont condition:: tensor(2.5505e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.1945e-10, dtype=torch.float64)
secont condition:: tensor(2.1945e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.7545e-10, dtype=torch.float64)
secont condition:: tensor(1.7545e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.5466e-10, dtype=torch.float64)
secont condition:: tensor(1.5466e-10, dtype=torch.float64)
curr_secont condition:: tensor(9.1951e-11, dtype=torch.float64)
secont condition:: tensor(9.1951e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.1086e-10, dtype=torch.float64)
secont condition:: tensor(1.1086e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.7114e-10, dtype=torch.float64)
secont condition:: tensor(2.7114e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.3452e-10, dtype=torch.float64)
secont condition:: tensor(2.3452e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.5475e-10, dtype=torch.float64)
secont condition:: tensor(1.5475e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.6729e-10, dtype=torch.float64)
secont condition:: tensor(1.6729e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.0981e-10, dtype=torch.float64)
secont condition:: tensor(1.0981e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.0488e-10, dtype=torch.float64)
secont condition:: tensor(1.0488e-10, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(8.3782e-11, dtype=torch.float64)
secont condition:: tensor(8.3782e-11, dtype=torch.float64)
curr_secont condition:: tensor(8.2695e-11, dtype=torch.float64)
secont condition:: tensor(8.2695e-11, dtype=torch.float64)
curr_secont condition:: tensor(7.1918e-11, dtype=torch.float64)
secont condition:: tensor(7.1918e-11, dtype=torch.float64)
curr_secont condition:: tensor(7.2307e-11, dtype=torch.float64)
secont condition:: tensor(7.2307e-11, dtype=torch.float64)
curr_secont condition:: tensor(7.2774e-11, dtype=torch.float64)
secont condition:: tensor(7.2774e-11, dtype=torch.float64)
curr_secont condition:: tensor(7.0654e-11, dtype=torch.float64)
secont condition:: tensor(7.0654e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.0451e-11, dtype=torch.float64)
secont condition:: tensor(6.0451e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.8863e-11, dtype=torch.float64)
secont condition:: tensor(6.8863e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.4644e-11, dtype=torch.float64)
secont condition:: tensor(5.4644e-11, dtype=torch.float64)
curr_secont condition:: tensor(9.0201e-10, dtype=torch.float64)
secont condition:: tensor(9.0201e-10, dtype=torch.float64)
curr_secont condition:: tensor(7.1718e-10, dtype=torch.float64)
secont condition:: tensor(7.1718e-10, dtype=torch.float64)
curr_secont condition:: tensor(5.2181e-10, dtype=torch.float64)
secont condition:: tensor(5.2181e-10, dtype=torch.float64)
curr_secont condition:: tensor(5.0485e-10, dtype=torch.float64)
secont condition:: tensor(5.0485e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.2949e-10, dtype=torch.float64)
secont condition:: tensor(4.2949e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.3416e-10, dtype=torch.float64)
secont condition:: tensor(3.3416e-10, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(2.6482e-10, dtype=torch.float64)
secont condition:: tensor(2.6482e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.8436e-10, dtype=torch.float64)
secont condition:: tensor(2.8436e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.2481e-10, dtype=torch.float64)
secont condition:: tensor(2.2481e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.6028e-10, dtype=torch.float64)
secont condition:: tensor(2.6028e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.3172e-10, dtype=torch.float64)
secont condition:: tensor(2.3172e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.6227e-10, dtype=torch.float64)
secont condition:: tensor(1.6227e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.6298e-10, dtype=torch.float64)
secont condition:: tensor(1.6298e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.5063e-10, dtype=torch.float64)
secont condition:: tensor(1.5063e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.9839e-10, dtype=torch.float64)
secont condition:: tensor(4.9839e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.6555e-10, dtype=torch.float64)
secont condition:: tensor(4.6555e-10, dtype=torch.float64)
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(1.4767e-09, dtype=torch.float64)
secont condition:: tensor(1.4767e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.4175e-09, dtype=torch.float64)
secont condition:: tensor(2.4175e-09, dtype=torch.float64)
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(7.0328e-09, dtype=torch.float64)
secont condition:: tensor(7.0328e-09, dtype=torch.float64)
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(4.4944e-09, dtype=torch.float64)
secont condition:: tensor(4.4944e-09, dtype=torch.float64)
curr_secont condition:: tensor(6.3176e-09, dtype=torch.float64)
secont condition:: tensor(6.3176e-09, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(8.3472e-09, dtype=torch.float64)
secont condition:: tensor(8.3472e-09, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(7.0666e-09, dtype=torch.float64)
secont condition:: tensor(7.0666e-09, dtype=torch.float64)
curr_secont condition:: tensor(6.2443e-09, dtype=torch.float64)
secont condition:: tensor(6.2443e-09, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 26.531578540802002
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.4765e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.5316e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(2.2128e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.6769e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.701700
repetition 3
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 3
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 51.166961669921875
time_baseline:: 51.185550928115845
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.9034e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(9.4100e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.701300
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 3
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(6.0348e-33, dtype=torch.float64)
secont condition:: tensor(6.0348e-33, dtype=torch.float64)
curr_secont condition:: tensor(1.4993e-32, dtype=torch.float64)
secont condition:: tensor(1.4993e-32, dtype=torch.float64)
curr_secont condition:: tensor(5.6776e-11, dtype=torch.float64)
secont condition:: tensor(5.6776e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.3263e-11, dtype=torch.float64)
secont condition:: tensor(5.3263e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.9602e-11, dtype=torch.float64)
secont condition:: tensor(3.9602e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.3930e-11, dtype=torch.float64)
secont condition:: tensor(6.3930e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.3163e-11, dtype=torch.float64)
secont condition:: tensor(5.3163e-11, dtype=torch.float64)
curr_secont condition:: tensor(8.0551e-10, dtype=torch.float64)
secont condition:: tensor(8.0551e-10, dtype=torch.float64)
curr_secont condition:: tensor(6.7077e-10, dtype=torch.float64)
secont condition:: tensor(6.7077e-10, dtype=torch.float64)
curr_secont condition:: tensor(5.1696e-10, dtype=torch.float64)
secont condition:: tensor(5.1696e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.8014e-10, dtype=torch.float64)
secont condition:: tensor(4.8014e-10, dtype=torch.float64)
curr_secont condition:: tensor(5.0677e-10, dtype=torch.float64)
secont condition:: tensor(5.0677e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.6019e-10, dtype=torch.float64)
secont condition:: tensor(4.6019e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.3587e-10, dtype=torch.float64)
secont condition:: tensor(3.3587e-10, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(2.7421e-10, dtype=torch.float64)
secont condition:: tensor(2.7421e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.8085e-10, dtype=torch.float64)
secont condition:: tensor(1.8085e-10, dtype=torch.float64)
curr_secont condition:: tensor(6.5447e-10, dtype=torch.float64)
secont condition:: tensor(6.5447e-10, dtype=torch.float64)
curr_secont condition:: tensor(9.5866e-10, dtype=torch.float64)
secont condition:: tensor(9.5866e-10, dtype=torch.float64)
curr_secont condition:: tensor(8.2692e-10, dtype=torch.float64)
secont condition:: tensor(8.2692e-10, dtype=torch.float64)
curr_secont condition:: tensor(7.0046e-10, dtype=torch.float64)
secont condition:: tensor(7.0046e-10, dtype=torch.float64)
curr_secont condition:: tensor(5.4234e-10, dtype=torch.float64)
secont condition:: tensor(5.4234e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.3374e-10, dtype=torch.float64)
secont condition:: tensor(4.3374e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.8912e-10, dtype=torch.float64)
secont condition:: tensor(3.8912e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.6148e-10, dtype=torch.float64)
secont condition:: tensor(3.6148e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.1203e-10, dtype=torch.float64)
secont condition:: tensor(4.1203e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.5979e-10, dtype=torch.float64)
secont condition:: tensor(3.5979e-10, dtype=torch.float64)
curr_secont condition:: tensor(8.4638e-10, dtype=torch.float64)
secont condition:: tensor(8.4638e-10, dtype=torch.float64)
curr_secont condition:: tensor(5.6160e-10, dtype=torch.float64)
secont condition:: tensor(5.6160e-10, dtype=torch.float64)
curr_secont condition:: tensor(6.0152e-10, dtype=torch.float64)
secont condition:: tensor(6.0152e-10, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(5.0533e-10, dtype=torch.float64)
secont condition:: tensor(5.0533e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.8046e-10, dtype=torch.float64)
secont condition:: tensor(4.8046e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.2095e-10, dtype=torch.float64)
secont condition:: tensor(3.2095e-10, dtype=torch.float64)
curr_secont condition:: tensor(6.5534e-10, dtype=torch.float64)
secont condition:: tensor(6.5534e-10, dtype=torch.float64)
curr_secont condition:: tensor(6.2377e-10, dtype=torch.float64)
secont condition:: tensor(6.2377e-10, dtype=torch.float64)
curr_secont condition:: tensor(5.0299e-10, dtype=torch.float64)
secont condition:: tensor(5.0299e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.4228e-09, dtype=torch.float64)
secont condition:: tensor(1.4228e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.2195e-09, dtype=torch.float64)
secont condition:: tensor(1.2195e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.0627e-09, dtype=torch.float64)
secont condition:: tensor(1.0627e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.0321e-09, dtype=torch.float64)
secont condition:: tensor(1.0321e-09, dtype=torch.float64)
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(1.0167e-09, dtype=torch.float64)
secont condition:: tensor(1.0167e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.3718e-09, dtype=torch.float64)
secont condition:: tensor(2.3718e-09, dtype=torch.float64)
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(4.1716e-09, dtype=torch.float64)
secont condition:: tensor(4.1716e-09, dtype=torch.float64)
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(4.0288e-09, dtype=torch.float64)
secont condition:: tensor(4.0288e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.9299e-09, dtype=torch.float64)
secont condition:: tensor(2.9299e-09, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(5.2826e-09, dtype=torch.float64)
secont condition:: tensor(5.2826e-09, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(4.3281e-09, dtype=torch.float64)
secont condition:: tensor(4.3281e-09, dtype=torch.float64)
curr_secont condition:: tensor(4.2507e-09, dtype=torch.float64)
secont condition:: tensor(4.2507e-09, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 27.226377964019775
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.2679e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.6676e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.6409e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.6609e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.701500
repetition 4
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 3
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 51.526957988739014
time_baseline:: 51.54372024536133
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.5322e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.6979e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.701300
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 3
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(6.0467e-33, dtype=torch.float64)
secont condition:: tensor(6.0467e-33, dtype=torch.float64)
curr_secont condition:: tensor(3.1275e-10, dtype=torch.float64)
secont condition:: tensor(3.1275e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.5710e-10, dtype=torch.float64)
secont condition:: tensor(2.5710e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.7988e-10, dtype=torch.float64)
secont condition:: tensor(1.7988e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.2873e-10, dtype=torch.float64)
secont condition:: tensor(1.2873e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.0192e-10, dtype=torch.float64)
secont condition:: tensor(1.0192e-10, dtype=torch.float64)
curr_secont condition:: tensor(8.6417e-11, dtype=torch.float64)
secont condition:: tensor(8.6417e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.1574e-11, dtype=torch.float64)
secont condition:: tensor(6.1574e-11, dtype=torch.float64)
curr_secont condition:: tensor(7.5213e-11, dtype=torch.float64)
secont condition:: tensor(7.5213e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.2511e-11, dtype=torch.float64)
secont condition:: tensor(6.2511e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.1997e-11, dtype=torch.float64)
secont condition:: tensor(4.1997e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.6114e-11, dtype=torch.float64)
secont condition:: tensor(2.6114e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.2070e-11, dtype=torch.float64)
secont condition:: tensor(5.2070e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.5278e-11, dtype=torch.float64)
secont condition:: tensor(4.5278e-11, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(5.0673e-11, dtype=torch.float64)
secont condition:: tensor(5.0673e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.8885e-11, dtype=torch.float64)
secont condition:: tensor(3.8885e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.9771e-11, dtype=torch.float64)
secont condition:: tensor(3.9771e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.4985e-11, dtype=torch.float64)
secont condition:: tensor(3.4985e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.7306e-11, dtype=torch.float64)
secont condition:: tensor(5.7306e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.9138e-11, dtype=torch.float64)
secont condition:: tensor(3.9138e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.9208e-11, dtype=torch.float64)
secont condition:: tensor(3.9208e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.3279e-10, dtype=torch.float64)
secont condition:: tensor(3.3279e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.7906e-10, dtype=torch.float64)
secont condition:: tensor(2.7906e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.5010e-10, dtype=torch.float64)
secont condition:: tensor(2.5010e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.1252e-10, dtype=torch.float64)
secont condition:: tensor(2.1252e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.6531e-10, dtype=torch.float64)
secont condition:: tensor(1.6531e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.6397e-10, dtype=torch.float64)
secont condition:: tensor(1.6397e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.7391e-10, dtype=torch.float64)
secont condition:: tensor(1.7391e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.4752e-10, dtype=torch.float64)
secont condition:: tensor(1.4752e-10, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(1.3602e-10, dtype=torch.float64)
secont condition:: tensor(1.3602e-10, dtype=torch.float64)
curr_secont condition:: tensor(8.2808e-11, dtype=torch.float64)
secont condition:: tensor(8.2808e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.4983e-10, dtype=torch.float64)
secont condition:: tensor(2.4983e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.8163e-10, dtype=torch.float64)
secont condition:: tensor(1.8163e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.1255e-10, dtype=torch.float64)
secont condition:: tensor(2.1255e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.2994e-10, dtype=torch.float64)
secont condition:: tensor(1.2994e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.5302e-10, dtype=torch.float64)
secont condition:: tensor(1.5302e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.3050e-10, dtype=torch.float64)
secont condition:: tensor(1.3050e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.8145e-10, dtype=torch.float64)
secont condition:: tensor(1.8145e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.5830e-10, dtype=torch.float64)
secont condition:: tensor(1.5830e-10, dtype=torch.float64)
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(2.0288e-10, dtype=torch.float64)
secont condition:: tensor(2.0288e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.3346e-10, dtype=torch.float64)
secont condition:: tensor(4.3346e-10, dtype=torch.float64)
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(2.8710e-09, dtype=torch.float64)
explicit_evaluation epoch:: 60
curr_secont condition:: tensor(6.1563e-10, dtype=torch.float64)
explicit_evaluation epoch:: 61
curr_secont condition:: tensor(4.8236e-10, dtype=torch.float64)
secont condition:: tensor(4.8236e-10, dtype=torch.float64)
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(5.0810e-10, dtype=torch.float64)
secont condition:: tensor(5.0810e-10, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(9.6496e-10, dtype=torch.float64)
secont condition:: tensor(9.6496e-10, dtype=torch.float64)
curr_secont condition:: tensor(8.6652e-10, dtype=torch.float64)
secont condition:: tensor(8.6652e-10, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(1.2919e-09, dtype=torch.float64)
secont condition:: tensor(1.2919e-09, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 27.83424997329712
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(8.9890e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(5.6208e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(9.5266e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.4204e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(2.3412e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.701700
repetition 5
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 3
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 51.430153608322144
time_baseline:: 51.4487726688385
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.5229e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(9.1289e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.701300
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 3
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(1.2425e-10, dtype=torch.float64)
secont condition:: tensor(1.2425e-10, dtype=torch.float64)
curr_secont condition:: tensor(7.0205e-11, dtype=torch.float64)
secont condition:: tensor(7.0205e-11, dtype=torch.float64)
curr_secont condition:: tensor(9.2469e-11, dtype=torch.float64)
secont condition:: tensor(9.2469e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.8138e-11, dtype=torch.float64)
secont condition:: tensor(6.8138e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.4468e-11, dtype=torch.float64)
secont condition:: tensor(6.4468e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.7159e-11, dtype=torch.float64)
secont condition:: tensor(5.7159e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.2353e-11, dtype=torch.float64)
secont condition:: tensor(5.2353e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.1024e-11, dtype=torch.float64)
secont condition:: tensor(4.1024e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.2689e-11, dtype=torch.float64)
secont condition:: tensor(3.2689e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.8461e-11, dtype=torch.float64)
secont condition:: tensor(4.8461e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.6458e-11, dtype=torch.float64)
secont condition:: tensor(3.6458e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.7104e-11, dtype=torch.float64)
secont condition:: tensor(3.7104e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.8211e-11, dtype=torch.float64)
secont condition:: tensor(2.8211e-11, dtype=torch.float64)
curr_secont condition:: tensor(7.9060e-11, dtype=torch.float64)
secont condition:: tensor(7.9060e-11, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(7.3155e-11, dtype=torch.float64)
secont condition:: tensor(7.3155e-11, dtype=torch.float64)
curr_secont condition:: tensor(7.2826e-11, dtype=torch.float64)
secont condition:: tensor(7.2826e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.0191e-10, dtype=torch.float64)
secont condition:: tensor(1.0191e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.0257e-10, dtype=torch.float64)
secont condition:: tensor(1.0257e-10, dtype=torch.float64)
curr_secont condition:: tensor(6.8798e-11, dtype=torch.float64)
secont condition:: tensor(6.8798e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.8279e-11, dtype=torch.float64)
secont condition:: tensor(6.8279e-11, dtype=torch.float64)
curr_secont condition:: tensor(7.1761e-11, dtype=torch.float64)
secont condition:: tensor(7.1761e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.8869e-11, dtype=torch.float64)
secont condition:: tensor(5.8869e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.6569e-11, dtype=torch.float64)
secont condition:: tensor(5.6569e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.7806e-11, dtype=torch.float64)
secont condition:: tensor(4.7806e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.5572e-11, dtype=torch.float64)
secont condition:: tensor(4.5572e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.7001e-11, dtype=torch.float64)
secont condition:: tensor(4.7001e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.8664e-11, dtype=torch.float64)
secont condition:: tensor(6.8664e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.2400e-11, dtype=torch.float64)
secont condition:: tensor(6.2400e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.1548e-11, dtype=torch.float64)
secont condition:: tensor(6.1548e-11, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(4.8565e-11, dtype=torch.float64)
secont condition:: tensor(4.8565e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.4668e-11, dtype=torch.float64)
secont condition:: tensor(6.4668e-11, dtype=torch.float64)
curr_secont condition:: tensor(7.1015e-11, dtype=torch.float64)
secont condition:: tensor(7.1015e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.0791e-10, dtype=torch.float64)
secont condition:: tensor(2.0791e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.9686e-10, dtype=torch.float64)
secont condition:: tensor(2.9686e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.5256e-10, dtype=torch.float64)
secont condition:: tensor(2.5256e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.0409e-10, dtype=torch.float64)
secont condition:: tensor(2.0409e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.7957e-10, dtype=torch.float64)
secont condition:: tensor(1.7957e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.6222e-10, dtype=torch.float64)
secont condition:: tensor(1.6222e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.2234e-10, dtype=torch.float64)
secont condition:: tensor(1.2234e-10, dtype=torch.float64)
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(-1.3010e-09, dtype=torch.float64)
explicit_evaluation epoch:: 48
curr_secont condition:: tensor(3.8719e-10, dtype=torch.float64)
secont condition:: tensor(3.8719e-10, dtype=torch.float64)
curr_secont condition:: tensor(6.3734e-10, dtype=torch.float64)
secont condition:: tensor(6.3734e-10, dtype=torch.float64)
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(2.1571e-09, dtype=torch.float64)
secont condition:: tensor(2.1571e-09, dtype=torch.float64)
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(1.7634e-09, dtype=torch.float64)
secont condition:: tensor(1.7634e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.9950e-09, dtype=torch.float64)
secont condition:: tensor(1.9950e-09, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(2.6349e-09, dtype=torch.float64)
secont condition:: tensor(2.6349e-09, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(2.0254e-09, dtype=torch.float64)
secont condition:: tensor(2.0254e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.4526e-09, dtype=torch.float64)
secont condition:: tensor(2.4526e-09, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 27.059128761291504
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(9.7558e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(7.2791e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.2271e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.3952e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(2.0306e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.701400
deletion rate:: 0.0001
repetition 1
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 6
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 53.16004753112793
time_baseline:: 53.17845058441162
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(2.9310e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.0984e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.701300
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 6
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(6.0462e-33, dtype=torch.float64)
secont condition:: tensor(6.0462e-33, dtype=torch.float64)
curr_secont condition:: tensor(2.5552e-11, dtype=torch.float64)
secont condition:: tensor(2.5552e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.0226e-11, dtype=torch.float64)
secont condition:: tensor(2.0226e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.6153e-11, dtype=torch.float64)
secont condition:: tensor(1.6153e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.0777e-09, dtype=torch.float64)
secont condition:: tensor(1.0777e-09, dtype=torch.float64)
curr_secont condition:: tensor(7.8252e-10, dtype=torch.float64)
secont condition:: tensor(7.8252e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.7725e-10, dtype=torch.float64)
secont condition:: tensor(3.7725e-10, dtype=torch.float64)
curr_secont condition:: tensor(6.7334e-10, dtype=torch.float64)
secont condition:: tensor(6.7334e-10, dtype=torch.float64)
curr_secont condition:: tensor(5.8728e-10, dtype=torch.float64)
secont condition:: tensor(5.8728e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.1079e-10, dtype=torch.float64)
secont condition:: tensor(4.1079e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.6351e-10, dtype=torch.float64)
secont condition:: tensor(3.6351e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.3785e-10, dtype=torch.float64)
secont condition:: tensor(3.3785e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.7814e-10, dtype=torch.float64)
secont condition:: tensor(2.7814e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.7337e-10, dtype=torch.float64)
secont condition:: tensor(2.7337e-10, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(2.8131e-10, dtype=torch.float64)
secont condition:: tensor(2.8131e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.5797e-10, dtype=torch.float64)
secont condition:: tensor(2.5797e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.3089e-10, dtype=torch.float64)
secont condition:: tensor(2.3089e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.7722e-10, dtype=torch.float64)
secont condition:: tensor(2.7722e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.9364e-10, dtype=torch.float64)
secont condition:: tensor(1.9364e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.9309e-10, dtype=torch.float64)
secont condition:: tensor(1.9309e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.8503e-10, dtype=torch.float64)
secont condition:: tensor(1.8503e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.1281e-10, dtype=torch.float64)
secont condition:: tensor(3.1281e-10, dtype=torch.float64)
curr_secont condition:: tensor(7.9089e-10, dtype=torch.float64)
secont condition:: tensor(7.9089e-10, dtype=torch.float64)
curr_secont condition:: tensor(5.7548e-10, dtype=torch.float64)
secont condition:: tensor(5.7548e-10, dtype=torch.float64)
curr_secont condition:: tensor(7.9675e-10, dtype=torch.float64)
secont condition:: tensor(7.9675e-10, dtype=torch.float64)
curr_secont condition:: tensor(5.1604e-10, dtype=torch.float64)
secont condition:: tensor(5.1604e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.2769e-10, dtype=torch.float64)
secont condition:: tensor(4.2769e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.7492e-10, dtype=torch.float64)
secont condition:: tensor(3.7492e-10, dtype=torch.float64)
curr_secont condition:: tensor(5.5733e-10, dtype=torch.float64)
secont condition:: tensor(5.5733e-10, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(1.1823e-09, dtype=torch.float64)
secont condition:: tensor(1.1823e-09, dtype=torch.float64)
curr_secont condition:: tensor(8.2302e-10, dtype=torch.float64)
secont condition:: tensor(8.2302e-10, dtype=torch.float64)
curr_secont condition:: tensor(9.2450e-10, dtype=torch.float64)
secont condition:: tensor(9.2450e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.7922e-09, dtype=torch.float64)
secont condition:: tensor(3.7922e-09, dtype=torch.float64)
curr_secont condition:: tensor(3.2603e-09, dtype=torch.float64)
secont condition:: tensor(3.2603e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.5323e-09, dtype=torch.float64)
secont condition:: tensor(2.5323e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.9421e-09, dtype=torch.float64)
secont condition:: tensor(1.9421e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.5197e-09, dtype=torch.float64)
secont condition:: tensor(1.5197e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.5175e-09, dtype=torch.float64)
secont condition:: tensor(2.5175e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.1629e-09, dtype=torch.float64)
secont condition:: tensor(2.1629e-09, dtype=torch.float64)
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(3.6103e-09, dtype=torch.float64)
secont condition:: tensor(3.6103e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.0159e-08, dtype=torch.float64)
secont condition:: tensor(1.0159e-08, dtype=torch.float64)
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(9.1181e-09, dtype=torch.float64)
secont condition:: tensor(9.1181e-09, dtype=torch.float64)
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(9.2720e-09, dtype=torch.float64)
secont condition:: tensor(9.2720e-09, dtype=torch.float64)
curr_secont condition:: tensor(6.5910e-09, dtype=torch.float64)
secont condition:: tensor(6.5910e-09, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(1.3088e-08, dtype=torch.float64)
secont condition:: tensor(1.3088e-08, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(7.5866e-09, dtype=torch.float64)
secont condition:: tensor(7.5866e-09, dtype=torch.float64)
curr_secont condition:: tensor(8.4769e-09, dtype=torch.float64)
secont condition:: tensor(8.4769e-09, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 28.42342972755432
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.9674e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(2.1826e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(2.9194e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(2.2438e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.701500
repetition 2
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 6
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 53.458587408065796
time_baseline:: 53.48123550415039
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(2.5302e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(2.1515e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.701300
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 6
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(6.0348e-33, dtype=torch.float64)
secont condition:: tensor(6.0348e-33, dtype=torch.float64)
curr_secont condition:: tensor(1.4958e-32, dtype=torch.float64)
secont condition:: tensor(1.4958e-32, dtype=torch.float64)
curr_secont condition:: tensor(2.6782e-32, dtype=torch.float64)
secont condition:: tensor(2.6782e-32, dtype=torch.float64)
curr_secont condition:: tensor(1.8236e-10, dtype=torch.float64)
secont condition:: tensor(1.8236e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.3335e-10, dtype=torch.float64)
secont condition:: tensor(1.3335e-10, dtype=torch.float64)
curr_secont condition:: tensor(9.5573e-11, dtype=torch.float64)
secont condition:: tensor(9.5573e-11, dtype=torch.float64)
curr_secont condition:: tensor(7.1432e-11, dtype=torch.float64)
secont condition:: tensor(7.1432e-11, dtype=torch.float64)
curr_secont condition:: tensor(7.7416e-11, dtype=torch.float64)
secont condition:: tensor(7.7416e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.7114e-11, dtype=torch.float64)
secont condition:: tensor(4.7114e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.9388e-10, dtype=torch.float64)
secont condition:: tensor(2.9388e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.5808e-10, dtype=torch.float64)
secont condition:: tensor(2.5808e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.8752e-10, dtype=torch.float64)
secont condition:: tensor(1.8752e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.9378e-10, dtype=torch.float64)
secont condition:: tensor(1.9378e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.0190e-09, dtype=torch.float64)
secont condition:: tensor(1.0190e-09, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(8.0867e-10, dtype=torch.float64)
secont condition:: tensor(8.0867e-10, dtype=torch.float64)
curr_secont condition:: tensor(7.7010e-10, dtype=torch.float64)
secont condition:: tensor(7.7010e-10, dtype=torch.float64)
curr_secont condition:: tensor(6.7901e-10, dtype=torch.float64)
secont condition:: tensor(6.7901e-10, dtype=torch.float64)
curr_secont condition:: tensor(5.2309e-10, dtype=torch.float64)
secont condition:: tensor(5.2309e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.0408e-10, dtype=torch.float64)
secont condition:: tensor(4.0408e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.6966e-10, dtype=torch.float64)
secont condition:: tensor(3.6966e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.7012e-10, dtype=torch.float64)
secont condition:: tensor(3.7012e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.3245e-10, dtype=torch.float64)
secont condition:: tensor(3.3245e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.7267e-10, dtype=torch.float64)
secont condition:: tensor(2.7267e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.1800e-10, dtype=torch.float64)
secont condition:: tensor(2.1800e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.3224e-10, dtype=torch.float64)
secont condition:: tensor(3.3224e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.6454e-10, dtype=torch.float64)
secont condition:: tensor(2.6454e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.9983e-10, dtype=torch.float64)
secont condition:: tensor(2.9983e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.9981e-10, dtype=torch.float64)
secont condition:: tensor(2.9981e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.1933e-10, dtype=torch.float64)
secont condition:: tensor(3.1933e-10, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(3.0163e-10, dtype=torch.float64)
secont condition:: tensor(3.0163e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.7990e-10, dtype=torch.float64)
secont condition:: tensor(2.7990e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.5517e-10, dtype=torch.float64)
secont condition:: tensor(3.5517e-10, dtype=torch.float64)
curr_secont condition:: tensor(7.5810e-10, dtype=torch.float64)
secont condition:: tensor(7.5810e-10, dtype=torch.float64)
curr_secont condition:: tensor(6.0021e-10, dtype=torch.float64)
secont condition:: tensor(6.0021e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.8545e-10, dtype=torch.float64)
secont condition:: tensor(4.8545e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.8159e-10, dtype=torch.float64)
secont condition:: tensor(3.8159e-10, dtype=torch.float64)
curr_secont condition:: tensor(7.2576e-10, dtype=torch.float64)
secont condition:: tensor(7.2576e-10, dtype=torch.float64)
curr_secont condition:: tensor(5.6518e-10, dtype=torch.float64)
secont condition:: tensor(5.6518e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.2269e-09, dtype=torch.float64)
secont condition:: tensor(1.2269e-09, dtype=torch.float64)
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(3.0162e-09, dtype=torch.float64)
secont condition:: tensor(3.0162e-09, dtype=torch.float64)
curr_secont condition:: tensor(4.6021e-09, dtype=torch.float64)
secont condition:: tensor(4.6021e-09, dtype=torch.float64)
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(4.4252e-09, dtype=torch.float64)
secont condition:: tensor(4.4252e-09, dtype=torch.float64)
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(9.3131e-09, dtype=torch.float64)
secont condition:: tensor(9.3131e-09, dtype=torch.float64)
curr_secont condition:: tensor(7.8611e-09, dtype=torch.float64)
secont condition:: tensor(7.8611e-09, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(9.5782e-09, dtype=torch.float64)
secont condition:: tensor(9.5782e-09, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(6.2041e-09, dtype=torch.float64)
secont condition:: tensor(6.2041e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.1122e-08, dtype=torch.float64)
secont condition:: tensor(1.1122e-08, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 28.35813283920288
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.5822e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(2.1354e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(2.5855e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(3.2947e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.702000
repetition 3
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 6
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 53.08197569847107
time_baseline:: 53.10557317733765
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(2.5815e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(2.3324e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.701400
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 6
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(6.0480e-33, dtype=torch.float64)
secont condition:: tensor(6.0480e-33, dtype=torch.float64)
curr_secont condition:: tensor(6.4478e-11, dtype=torch.float64)
secont condition:: tensor(6.4478e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.1740e-11, dtype=torch.float64)
secont condition:: tensor(6.1740e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.9074e-10, dtype=torch.float64)
secont condition:: tensor(1.9074e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.5948e-10, dtype=torch.float64)
secont condition:: tensor(1.5948e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.7408e-10, dtype=torch.float64)
secont condition:: tensor(1.7408e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.5962e-10, dtype=torch.float64)
secont condition:: tensor(1.5962e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.1618e-10, dtype=torch.float64)
secont condition:: tensor(1.1618e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.0334e-10, dtype=torch.float64)
secont condition:: tensor(1.0334e-10, dtype=torch.float64)
curr_secont condition:: tensor(9.7003e-11, dtype=torch.float64)
secont condition:: tensor(9.7003e-11, dtype=torch.float64)
curr_secont condition:: tensor(7.7782e-11, dtype=torch.float64)
secont condition:: tensor(7.7782e-11, dtype=torch.float64)
curr_secont condition:: tensor(8.6854e-11, dtype=torch.float64)
secont condition:: tensor(8.6854e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.9780e-10, dtype=torch.float64)
secont condition:: tensor(3.9780e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.8459e-10, dtype=torch.float64)
secont condition:: tensor(2.8459e-10, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(2.7420e-10, dtype=torch.float64)
secont condition:: tensor(2.7420e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.0538e-10, dtype=torch.float64)
secont condition:: tensor(2.0538e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.4406e-10, dtype=torch.float64)
secont condition:: tensor(1.4406e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.2985e-10, dtype=torch.float64)
secont condition:: tensor(1.2985e-10, dtype=torch.float64)
curr_secont condition:: tensor(8.1832e-10, dtype=torch.float64)
secont condition:: tensor(8.1832e-10, dtype=torch.float64)
curr_secont condition:: tensor(7.7865e-10, dtype=torch.float64)
secont condition:: tensor(7.7865e-10, dtype=torch.float64)
curr_secont condition:: tensor(6.5601e-10, dtype=torch.float64)
secont condition:: tensor(6.5601e-10, dtype=torch.float64)
curr_secont condition:: tensor(5.8326e-10, dtype=torch.float64)
secont condition:: tensor(5.8326e-10, dtype=torch.float64)
curr_secont condition:: tensor(5.3836e-10, dtype=torch.float64)
secont condition:: tensor(5.3836e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.6324e-10, dtype=torch.float64)
secont condition:: tensor(3.6324e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.2288e-10, dtype=torch.float64)
secont condition:: tensor(4.2288e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.2146e-10, dtype=torch.float64)
secont condition:: tensor(3.2146e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.5883e-10, dtype=torch.float64)
secont condition:: tensor(3.5883e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.8126e-10, dtype=torch.float64)
secont condition:: tensor(2.8126e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.3581e-10, dtype=torch.float64)
secont condition:: tensor(2.3581e-10, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(2.8430e-10, dtype=torch.float64)
secont condition:: tensor(2.8430e-10, dtype=torch.float64)
curr_secont condition:: tensor(5.0370e-10, dtype=torch.float64)
secont condition:: tensor(5.0370e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.6535e-10, dtype=torch.float64)
secont condition:: tensor(4.6535e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.4793e-10, dtype=torch.float64)
secont condition:: tensor(4.4793e-10, dtype=torch.float64)
curr_secont condition:: tensor(6.6029e-10, dtype=torch.float64)
secont condition:: tensor(6.6029e-10, dtype=torch.float64)
curr_secont condition:: tensor(7.2545e-10, dtype=torch.float64)
secont condition:: tensor(7.2545e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.3291e-09, dtype=torch.float64)
secont condition:: tensor(1.3291e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.1323e-09, dtype=torch.float64)
secont condition:: tensor(1.1323e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.0122e-09, dtype=torch.float64)
secont condition:: tensor(1.0122e-09, dtype=torch.float64)
curr_secont condition:: tensor(8.0239e-10, dtype=torch.float64)
secont condition:: tensor(8.0239e-10, dtype=torch.float64)
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(2.3996e-09, dtype=torch.float64)
secont condition:: tensor(2.3996e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.1750e-09, dtype=torch.float64)
secont condition:: tensor(2.1750e-09, dtype=torch.float64)
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(4.6224e-09, dtype=torch.float64)
secont condition:: tensor(4.6224e-09, dtype=torch.float64)
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(3.7485e-09, dtype=torch.float64)
secont condition:: tensor(3.7485e-09, dtype=torch.float64)
curr_secont condition:: tensor(4.8740e-09, dtype=torch.float64)
secont condition:: tensor(4.8740e-09, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(5.3457e-09, dtype=torch.float64)
secont condition:: tensor(5.3457e-09, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(6.1064e-09, dtype=torch.float64)
secont condition:: tensor(6.1064e-09, dtype=torch.float64)
curr_secont condition:: tensor(5.5638e-09, dtype=torch.float64)
secont condition:: tensor(5.5638e-09, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 28.306597471237183
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.5132e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(9.5799e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.4215e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(2.5104e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(3.5519e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.701700
repetition 4
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 6
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 53.045206785202026
time_baseline:: 53.06313967704773
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(2.1929e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.2017e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.701400
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 6
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(1.2048e-10, dtype=torch.float64)
secont condition:: tensor(1.2048e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.1213e-10, dtype=torch.float64)
secont condition:: tensor(1.1213e-10, dtype=torch.float64)
curr_secont condition:: tensor(9.2850e-11, dtype=torch.float64)
secont condition:: tensor(9.2850e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.1911e-10, dtype=torch.float64)
secont condition:: tensor(4.1911e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.7319e-10, dtype=torch.float64)
secont condition:: tensor(3.7319e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.9539e-10, dtype=torch.float64)
secont condition:: tensor(2.9539e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.6441e-10, dtype=torch.float64)
secont condition:: tensor(2.6441e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.4445e-10, dtype=torch.float64)
secont condition:: tensor(2.4445e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.1620e-10, dtype=torch.float64)
secont condition:: tensor(2.1620e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.0259e-10, dtype=torch.float64)
secont condition:: tensor(2.0259e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.5493e-10, dtype=torch.float64)
secont condition:: tensor(1.5493e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.5412e-10, dtype=torch.float64)
secont condition:: tensor(1.5412e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.1969e-10, dtype=torch.float64)
secont condition:: tensor(1.1969e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.0709e-10, dtype=torch.float64)
secont condition:: tensor(1.0709e-10, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(1.0653e-10, dtype=torch.float64)
secont condition:: tensor(1.0653e-10, dtype=torch.float64)
curr_secont condition:: tensor(9.3922e-11, dtype=torch.float64)
secont condition:: tensor(9.3922e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.2846e-10, dtype=torch.float64)
secont condition:: tensor(1.2846e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.0893e-10, dtype=torch.float64)
secont condition:: tensor(1.0893e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.0345e-10, dtype=torch.float64)
secont condition:: tensor(2.0345e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.0488e-09, dtype=torch.float64)
secont condition:: tensor(1.0488e-09, dtype=torch.float64)
curr_secont condition:: tensor(9.4468e-10, dtype=torch.float64)
secont condition:: tensor(9.4468e-10, dtype=torch.float64)
curr_secont condition:: tensor(7.1795e-10, dtype=torch.float64)
secont condition:: tensor(7.1795e-10, dtype=torch.float64)
curr_secont condition:: tensor(6.0111e-10, dtype=torch.float64)
secont condition:: tensor(6.0111e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.5172e-10, dtype=torch.float64)
secont condition:: tensor(4.5172e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.7359e-10, dtype=torch.float64)
secont condition:: tensor(4.7359e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.0582e-10, dtype=torch.float64)
secont condition:: tensor(3.0582e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.9856e-10, dtype=torch.float64)
secont condition:: tensor(2.9856e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.9087e-10, dtype=torch.float64)
secont condition:: tensor(2.9087e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.8451e-10, dtype=torch.float64)
secont condition:: tensor(1.8451e-10, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(2.5103e-10, dtype=torch.float64)
secont condition:: tensor(2.5103e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.4964e-10, dtype=torch.float64)
secont condition:: tensor(1.4964e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.2955e-10, dtype=torch.float64)
secont condition:: tensor(4.2955e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.2421e-10, dtype=torch.float64)
secont condition:: tensor(4.2421e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.3796e-10, dtype=torch.float64)
secont condition:: tensor(3.3796e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.7932e-10, dtype=torch.float64)
secont condition:: tensor(2.7932e-10, dtype=torch.float64)
curr_secont condition:: tensor(5.0984e-10, dtype=torch.float64)
secont condition:: tensor(5.0984e-10, dtype=torch.float64)
curr_secont condition:: tensor(5.4016e-10, dtype=torch.float64)
secont condition:: tensor(5.4016e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.0628e-10, dtype=torch.float64)
secont condition:: tensor(4.0628e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.7847e-10, dtype=torch.float64)
secont condition:: tensor(2.7847e-10, dtype=torch.float64)
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(2.6764e-09, dtype=torch.float64)
explicit_evaluation epoch:: 46
curr_secont condition:: tensor(6.7904e-10, dtype=torch.float64)
explicit_evaluation epoch:: 47
curr_secont condition:: tensor(1.1964e-09, dtype=torch.float64)
secont condition:: tensor(1.1964e-09, dtype=torch.float64)
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(2.8818e-09, dtype=torch.float64)
secont condition:: tensor(2.8818e-09, dtype=torch.float64)
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(2.9690e-09, dtype=torch.float64)
secont condition:: tensor(2.9690e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.6844e-09, dtype=torch.float64)
secont condition:: tensor(2.6844e-09, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(3.2863e-09, dtype=torch.float64)
secont condition:: tensor(3.2863e-09, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(2.8338e-09, dtype=torch.float64)
secont condition:: tensor(2.8338e-09, dtype=torch.float64)
curr_secont condition:: tensor(3.6007e-09, dtype=torch.float64)
secont condition:: tensor(3.6007e-09, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 29.43925642967224
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.3517e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(7.4088e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.1690e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(2.0974e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(2.2293e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.701700
repetition 5
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 6
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 52.88221287727356
time_baseline:: 52.90648102760315
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(2.4118e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.9780e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.701400
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 6
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(6.0348e-33, dtype=torch.float64)
secont condition:: tensor(6.0348e-33, dtype=torch.float64)
curr_secont condition:: tensor(1.4958e-32, dtype=torch.float64)
secont condition:: tensor(1.4958e-32, dtype=torch.float64)
curr_secont condition:: tensor(2.6747e-32, dtype=torch.float64)
secont condition:: tensor(2.6747e-32, dtype=torch.float64)
curr_secont condition:: tensor(4.1266e-32, dtype=torch.float64)
secont condition:: tensor(4.1266e-32, dtype=torch.float64)
curr_secont condition:: tensor(8.4757e-11, dtype=torch.float64)
secont condition:: tensor(8.4757e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.1530e-09, dtype=torch.float64)
secont condition:: tensor(1.1530e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.0694e-09, dtype=torch.float64)
secont condition:: tensor(1.0694e-09, dtype=torch.float64)
curr_secont condition:: tensor(8.8150e-10, dtype=torch.float64)
secont condition:: tensor(8.8150e-10, dtype=torch.float64)
curr_secont condition:: tensor(6.8984e-10, dtype=torch.float64)
secont condition:: tensor(6.8984e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.0003e-09, dtype=torch.float64)
secont condition:: tensor(1.0003e-09, dtype=torch.float64)
curr_secont condition:: tensor(8.7401e-10, dtype=torch.float64)
secont condition:: tensor(8.7401e-10, dtype=torch.float64)
curr_secont condition:: tensor(7.0139e-10, dtype=torch.float64)
secont condition:: tensor(7.0139e-10, dtype=torch.float64)
curr_secont condition:: tensor(6.0616e-10, dtype=torch.float64)
secont condition:: tensor(6.0616e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.6943e-10, dtype=torch.float64)
secont condition:: tensor(4.6943e-10, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(4.4967e-10, dtype=torch.float64)
secont condition:: tensor(4.4967e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.0795e-10, dtype=torch.float64)
secont condition:: tensor(4.0795e-10, dtype=torch.float64)
curr_secont condition:: tensor(6.7736e-10, dtype=torch.float64)
secont condition:: tensor(6.7736e-10, dtype=torch.float64)
curr_secont condition:: tensor(7.1320e-10, dtype=torch.float64)
secont condition:: tensor(7.1320e-10, dtype=torch.float64)
curr_secont condition:: tensor(5.7247e-10, dtype=torch.float64)
secont condition:: tensor(5.7247e-10, dtype=torch.float64)
curr_secont condition:: tensor(5.8402e-10, dtype=torch.float64)
secont condition:: tensor(5.8402e-10, dtype=torch.float64)
curr_secont condition:: tensor(6.2093e-10, dtype=torch.float64)
secont condition:: tensor(6.2093e-10, dtype=torch.float64)
curr_secont condition:: tensor(5.2579e-10, dtype=torch.float64)
secont condition:: tensor(5.2579e-10, dtype=torch.float64)
curr_secont condition:: tensor(5.0074e-10, dtype=torch.float64)
secont condition:: tensor(5.0074e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.4117e-09, dtype=torch.float64)
secont condition:: tensor(1.4117e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.0740e-09, dtype=torch.float64)
secont condition:: tensor(1.0740e-09, dtype=torch.float64)
curr_secont condition:: tensor(8.0356e-10, dtype=torch.float64)
secont condition:: tensor(8.0356e-10, dtype=torch.float64)
curr_secont condition:: tensor(7.8330e-10, dtype=torch.float64)
secont condition:: tensor(7.8330e-10, dtype=torch.float64)
curr_secont condition:: tensor(6.3543e-10, dtype=torch.float64)
secont condition:: tensor(6.3543e-10, dtype=torch.float64)
curr_secont condition:: tensor(6.1334e-10, dtype=torch.float64)
secont condition:: tensor(6.1334e-10, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(4.6386e-10, dtype=torch.float64)
secont condition:: tensor(4.6386e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.4150e-09, dtype=torch.float64)
secont condition:: tensor(1.4150e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.0323e-09, dtype=torch.float64)
secont condition:: tensor(1.0323e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.8156e-09, dtype=torch.float64)
secont condition:: tensor(1.8156e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.7411e-09, dtype=torch.float64)
secont condition:: tensor(1.7411e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.0741e-09, dtype=torch.float64)
secont condition:: tensor(1.0741e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.0024e-09, dtype=torch.float64)
secont condition:: tensor(1.0024e-09, dtype=torch.float64)
curr_secont condition:: tensor(9.8734e-10, dtype=torch.float64)
secont condition:: tensor(9.8734e-10, dtype=torch.float64)
curr_secont condition:: tensor(8.3155e-10, dtype=torch.float64)
secont condition:: tensor(8.3155e-10, dtype=torch.float64)
curr_secont condition:: tensor(7.0074e-10, dtype=torch.float64)
secont condition:: tensor(7.0074e-10, dtype=torch.float64)
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(7.9687e-10, dtype=torch.float64)
secont condition:: tensor(7.9687e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.3654e-09, dtype=torch.float64)
secont condition:: tensor(2.3654e-09, dtype=torch.float64)
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(2.2781e-09, dtype=torch.float64)
secont condition:: tensor(2.2781e-09, dtype=torch.float64)
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(4.9706e-09, dtype=torch.float64)
secont condition:: tensor(4.9706e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.7883e-09, dtype=torch.float64)
secont condition:: tensor(2.7883e-09, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(6.2843e-09, dtype=torch.float64)
secont condition:: tensor(6.2843e-09, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(5.4670e-09, dtype=torch.float64)
secont condition:: tensor(5.4670e-09, dtype=torch.float64)
curr_secont condition:: tensor(5.3563e-09, dtype=torch.float64)
secont condition:: tensor(5.3563e-09, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 28.45489263534546
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.4313e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.5714e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(2.2629e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(3.2326e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.701800
deletion rate:: 0.0002
repetition 1
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 12
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 55.879724740982056
time_baseline:: 55.897624492645264
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(3.1583e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(4.0679e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.701400
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 12
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(4.5345e-10, dtype=torch.float64)
secont condition:: tensor(4.5345e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.8332e-10, dtype=torch.float64)
secont condition:: tensor(4.8332e-10, dtype=torch.float64)
curr_secont condition:: tensor(7.0872e-10, dtype=torch.float64)
secont condition:: tensor(7.0872e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.9565e-10, dtype=torch.float64)
secont condition:: tensor(4.9565e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.7568e-10, dtype=torch.float64)
secont condition:: tensor(4.7568e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.4346e-10, dtype=torch.float64)
secont condition:: tensor(4.4346e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.6153e-10, dtype=torch.float64)
secont condition:: tensor(3.6153e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.8757e-10, dtype=torch.float64)
secont condition:: tensor(2.8757e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.4159e-09, dtype=torch.float64)
secont condition:: tensor(1.4159e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.1546e-09, dtype=torch.float64)
secont condition:: tensor(1.1546e-09, dtype=torch.float64)
curr_secont condition:: tensor(8.4480e-10, dtype=torch.float64)
secont condition:: tensor(8.4480e-10, dtype=torch.float64)
curr_secont condition:: tensor(5.3497e-10, dtype=torch.float64)
secont condition:: tensor(5.3497e-10, dtype=torch.float64)
curr_secont condition:: tensor(8.2901e-10, dtype=torch.float64)
secont condition:: tensor(8.2901e-10, dtype=torch.float64)
curr_secont condition:: tensor(6.9265e-10, dtype=torch.float64)
secont condition:: tensor(6.9265e-10, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(6.3863e-10, dtype=torch.float64)
secont condition:: tensor(6.3863e-10, dtype=torch.float64)
curr_secont condition:: tensor(6.1465e-10, dtype=torch.float64)
secont condition:: tensor(6.1465e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.9281e-10, dtype=torch.float64)
secont condition:: tensor(4.9281e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.7331e-10, dtype=torch.float64)
secont condition:: tensor(4.7331e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.0810e-10, dtype=torch.float64)
secont condition:: tensor(4.0810e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.9239e-10, dtype=torch.float64)
secont condition:: tensor(3.9239e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.2649e-10, dtype=torch.float64)
secont condition:: tensor(3.2649e-10, dtype=torch.float64)
curr_secont condition:: tensor(6.3369e-10, dtype=torch.float64)
secont condition:: tensor(6.3369e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.5715e-09, dtype=torch.float64)
secont condition:: tensor(2.5715e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.7000e-09, dtype=torch.float64)
secont condition:: tensor(1.7000e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.6736e-09, dtype=torch.float64)
secont condition:: tensor(1.6736e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.2221e-09, dtype=torch.float64)
secont condition:: tensor(1.2221e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.1292e-09, dtype=torch.float64)
secont condition:: tensor(1.1292e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.1158e-09, dtype=torch.float64)
secont condition:: tensor(1.1158e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.3401e-09, dtype=torch.float64)
secont condition:: tensor(1.3401e-09, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(1.0287e-09, dtype=torch.float64)
secont condition:: tensor(1.0287e-09, dtype=torch.float64)
curr_secont condition:: tensor(8.5898e-10, dtype=torch.float64)
secont condition:: tensor(8.5898e-10, dtype=torch.float64)
curr_secont condition:: tensor(6.8964e-10, dtype=torch.float64)
secont condition:: tensor(6.8964e-10, dtype=torch.float64)
curr_secont condition:: tensor(6.7811e-10, dtype=torch.float64)
secont condition:: tensor(6.7811e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.4583e-09, dtype=torch.float64)
secont condition:: tensor(1.4583e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.5867e-09, dtype=torch.float64)
secont condition:: tensor(1.5867e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.2594e-09, dtype=torch.float64)
secont condition:: tensor(1.2594e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.2447e-09, dtype=torch.float64)
secont condition:: tensor(1.2447e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.0837e-09, dtype=torch.float64)
secont condition:: tensor(1.0837e-09, dtype=torch.float64)
curr_secont condition:: tensor(8.4239e-10, dtype=torch.float64)
secont condition:: tensor(8.4239e-10, dtype=torch.float64)
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(3.1786e-09, dtype=torch.float64)
secont condition:: tensor(3.1786e-09, dtype=torch.float64)
curr_secont condition:: tensor(3.7429e-09, dtype=torch.float64)
secont condition:: tensor(3.7429e-09, dtype=torch.float64)
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(6.7966e-09, dtype=torch.float64)
secont condition:: tensor(6.7966e-09, dtype=torch.float64)
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(4.5218e-09, dtype=torch.float64)
secont condition:: tensor(4.5218e-09, dtype=torch.float64)
curr_secont condition:: tensor(4.8629e-09, dtype=torch.float64)
secont condition:: tensor(4.8629e-09, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(6.6270e-09, dtype=torch.float64)
secont condition:: tensor(6.6270e-09, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(6.0849e-09, dtype=torch.float64)
secont condition:: tensor(6.0849e-09, dtype=torch.float64)
curr_secont condition:: tensor(6.7197e-09, dtype=torch.float64)
secont condition:: tensor(6.7197e-09, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 31.80791449546814
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.6777e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.7114e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(3.2483e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(5.6212e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.701500
repetition 2
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 12
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 55.63602113723755
time_baseline:: 55.65411901473999
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(3.2020e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.7493e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.701300
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 12
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(6.0412e-33, dtype=torch.float64)
secont condition:: tensor(6.0412e-33, dtype=torch.float64)
curr_secont condition:: tensor(3.3206e-10, dtype=torch.float64)
secont condition:: tensor(3.3206e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.6046e-10, dtype=torch.float64)
secont condition:: tensor(1.6046e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.6712e-10, dtype=torch.float64)
secont condition:: tensor(1.6712e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.1427e-10, dtype=torch.float64)
secont condition:: tensor(1.1427e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.6492e-10, dtype=torch.float64)
secont condition:: tensor(2.6492e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.6698e-10, dtype=torch.float64)
secont condition:: tensor(3.6698e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.5246e-10, dtype=torch.float64)
secont condition:: tensor(3.5246e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.9671e-10, dtype=torch.float64)
secont condition:: tensor(2.9671e-10, dtype=torch.float64)
curr_secont condition:: tensor(5.2146e-10, dtype=torch.float64)
secont condition:: tensor(5.2146e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.6485e-10, dtype=torch.float64)
secont condition:: tensor(4.6485e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.9563e-10, dtype=torch.float64)
secont condition:: tensor(3.9563e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.3229e-10, dtype=torch.float64)
secont condition:: tensor(3.3229e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.8622e-10, dtype=torch.float64)
secont condition:: tensor(2.8622e-10, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(2.2091e-10, dtype=torch.float64)
secont condition:: tensor(2.2091e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.4288e-10, dtype=torch.float64)
secont condition:: tensor(4.4288e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.6086e-10, dtype=torch.float64)
secont condition:: tensor(4.6086e-10, dtype=torch.float64)
curr_secont condition:: tensor(9.2917e-10, dtype=torch.float64)
secont condition:: tensor(9.2917e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.1314e-09, dtype=torch.float64)
secont condition:: tensor(1.1314e-09, dtype=torch.float64)
curr_secont condition:: tensor(9.1094e-10, dtype=torch.float64)
secont condition:: tensor(9.1094e-10, dtype=torch.float64)
curr_secont condition:: tensor(7.8404e-10, dtype=torch.float64)
secont condition:: tensor(7.8404e-10, dtype=torch.float64)
curr_secont condition:: tensor(7.1548e-10, dtype=torch.float64)
secont condition:: tensor(7.1548e-10, dtype=torch.float64)
curr_secont condition:: tensor(6.0100e-10, dtype=torch.float64)
secont condition:: tensor(6.0100e-10, dtype=torch.float64)
curr_secont condition:: tensor(5.5319e-10, dtype=torch.float64)
secont condition:: tensor(5.5319e-10, dtype=torch.float64)
curr_secont condition:: tensor(5.3616e-10, dtype=torch.float64)
secont condition:: tensor(5.3616e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.8779e-10, dtype=torch.float64)
secont condition:: tensor(4.8779e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.4027e-10, dtype=torch.float64)
secont condition:: tensor(4.4027e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.0609e-09, dtype=torch.float64)
secont condition:: tensor(1.0609e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.3246e-09, dtype=torch.float64)
secont condition:: tensor(1.3246e-09, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(1.0699e-09, dtype=torch.float64)
secont condition:: tensor(1.0699e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.9436e-09, dtype=torch.float64)
secont condition:: tensor(1.9436e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.1505e-09, dtype=torch.float64)
secont condition:: tensor(1.1505e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.2950e-09, dtype=torch.float64)
secont condition:: tensor(1.2950e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.0346e-09, dtype=torch.float64)
secont condition:: tensor(1.0346e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.4768e-09, dtype=torch.float64)
secont condition:: tensor(1.4768e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.1535e-09, dtype=torch.float64)
secont condition:: tensor(1.1535e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.1747e-09, dtype=torch.float64)
secont condition:: tensor(1.1747e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.0784e-09, dtype=torch.float64)
secont condition:: tensor(1.0784e-09, dtype=torch.float64)
curr_secont condition:: tensor(9.2194e-10, dtype=torch.float64)
secont condition:: tensor(9.2194e-10, dtype=torch.float64)
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(2.7821e-09, dtype=torch.float64)
secont condition:: tensor(2.7821e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.4043e-09, dtype=torch.float64)
secont condition:: tensor(2.4043e-09, dtype=torch.float64)
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(3.7270e-09, dtype=torch.float64)
secont condition:: tensor(3.7270e-09, dtype=torch.float64)
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(1.0264e-08, dtype=torch.float64)
secont condition:: tensor(1.0264e-08, dtype=torch.float64)
curr_secont condition:: tensor(7.2061e-09, dtype=torch.float64)
secont condition:: tensor(7.2061e-09, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(8.0397e-09, dtype=torch.float64)
secont condition:: tensor(8.0397e-09, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(1.1360e-08, dtype=torch.float64)
secont condition:: tensor(1.1360e-08, dtype=torch.float64)
curr_secont condition:: tensor(1.0163e-08, dtype=torch.float64)
secont condition:: tensor(1.0163e-08, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 31.289907217025757
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.7348e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.9699e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(3.0407e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(3.3715e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.701400
repetition 3
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 12
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 55.35246729850769
time_baseline:: 55.37232160568237
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(4.1735e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(2.6291e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.701300
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 12
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(1.3600e-10, dtype=torch.float64)
secont condition:: tensor(1.3600e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.4392e-10, dtype=torch.float64)
secont condition:: tensor(1.4392e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.1697e-10, dtype=torch.float64)
secont condition:: tensor(1.1697e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.2600e-10, dtype=torch.float64)
secont condition:: tensor(1.2600e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.0452e-10, dtype=torch.float64)
secont condition:: tensor(1.0452e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.2673e-10, dtype=torch.float64)
secont condition:: tensor(1.2673e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.3563e-10, dtype=torch.float64)
secont condition:: tensor(1.3563e-10, dtype=torch.float64)
curr_secont condition:: tensor(8.9837e-10, dtype=torch.float64)
secont condition:: tensor(8.9837e-10, dtype=torch.float64)
curr_secont condition:: tensor(9.2563e-10, dtype=torch.float64)
secont condition:: tensor(9.2563e-10, dtype=torch.float64)
curr_secont condition:: tensor(8.0665e-10, dtype=torch.float64)
secont condition:: tensor(8.0665e-10, dtype=torch.float64)
curr_secont condition:: tensor(5.5678e-10, dtype=torch.float64)
secont condition:: tensor(5.5678e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.3176e-10, dtype=torch.float64)
secont condition:: tensor(4.3176e-10, dtype=torch.float64)
curr_secont condition:: tensor(5.2002e-10, dtype=torch.float64)
secont condition:: tensor(5.2002e-10, dtype=torch.float64)
curr_secont condition:: tensor(5.5433e-10, dtype=torch.float64)
secont condition:: tensor(5.5433e-10, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(5.5499e-10, dtype=torch.float64)
secont condition:: tensor(5.5499e-10, dtype=torch.float64)
curr_secont condition:: tensor(9.0930e-10, dtype=torch.float64)
secont condition:: tensor(9.0930e-10, dtype=torch.float64)
curr_secont condition:: tensor(7.7385e-10, dtype=torch.float64)
secont condition:: tensor(7.7385e-10, dtype=torch.float64)
curr_secont condition:: tensor(6.9031e-10, dtype=torch.float64)
secont condition:: tensor(6.9031e-10, dtype=torch.float64)
curr_secont condition:: tensor(5.8854e-10, dtype=torch.float64)
secont condition:: tensor(5.8854e-10, dtype=torch.float64)
curr_secont condition:: tensor(5.4407e-10, dtype=torch.float64)
secont condition:: tensor(5.4407e-10, dtype=torch.float64)
curr_secont condition:: tensor(6.7173e-10, dtype=torch.float64)
secont condition:: tensor(6.7173e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.1532e-09, dtype=torch.float64)
secont condition:: tensor(1.1532e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.3015e-09, dtype=torch.float64)
secont condition:: tensor(2.3015e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.0140e-09, dtype=torch.float64)
secont condition:: tensor(2.0140e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.7484e-09, dtype=torch.float64)
secont condition:: tensor(1.7484e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.2751e-09, dtype=torch.float64)
secont condition:: tensor(1.2751e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.1317e-09, dtype=torch.float64)
secont condition:: tensor(1.1317e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.0528e-09, dtype=torch.float64)
secont condition:: tensor(1.0528e-09, dtype=torch.float64)
curr_secont condition:: tensor(8.1784e-10, dtype=torch.float64)
secont condition:: tensor(8.1784e-10, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(7.9113e-10, dtype=torch.float64)
secont condition:: tensor(7.9113e-10, dtype=torch.float64)
curr_secont condition:: tensor(6.3025e-10, dtype=torch.float64)
secont condition:: tensor(6.3025e-10, dtype=torch.float64)
curr_secont condition:: tensor(5.4965e-10, dtype=torch.float64)
secont condition:: tensor(5.4965e-10, dtype=torch.float64)
curr_secont condition:: tensor(5.2228e-10, dtype=torch.float64)
secont condition:: tensor(5.2228e-10, dtype=torch.float64)
curr_secont condition:: tensor(6.5401e-10, dtype=torch.float64)
secont condition:: tensor(6.5401e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.7146e-09, dtype=torch.float64)
secont condition:: tensor(1.7146e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.8903e-09, dtype=torch.float64)
secont condition:: tensor(1.8903e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.7231e-09, dtype=torch.float64)
secont condition:: tensor(1.7231e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.1999e-09, dtype=torch.float64)
secont condition:: tensor(1.1999e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.1814e-09, dtype=torch.float64)
secont condition:: tensor(1.1814e-09, dtype=torch.float64)
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(4.7531e-09, dtype=torch.float64)
secont condition:: tensor(4.7531e-09, dtype=torch.float64)
curr_secont condition:: tensor(8.0587e-09, dtype=torch.float64)
secont condition:: tensor(8.0587e-09, dtype=torch.float64)
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(7.8542e-09, dtype=torch.float64)
secont condition:: tensor(7.8542e-09, dtype=torch.float64)
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(1.3861e-08, dtype=torch.float64)
secont condition:: tensor(1.3861e-08, dtype=torch.float64)
curr_secont condition:: tensor(1.0578e-08, dtype=torch.float64)
secont condition:: tensor(1.0578e-08, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(1.0842e-08, dtype=torch.float64)
secont condition:: tensor(1.0842e-08, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(1.6927e-08, dtype=torch.float64)
secont condition:: tensor(1.6927e-08, dtype=torch.float64)
curr_secont condition:: tensor(1.5090e-08, dtype=torch.float64)
secont condition:: tensor(1.5090e-08, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 31.499263763427734
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(2.1144e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(2.1441e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(4.1905e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(4.2730e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.701500
repetition 4
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 12
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 55.52125811576843
time_baseline:: 55.53915810585022
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(3.5394e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(3.0041e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.701300
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 12
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(6.0479e-33, dtype=torch.float64)
secont condition:: tensor(6.0479e-33, dtype=torch.float64)
curr_secont condition:: tensor(1.7707e-10, dtype=torch.float64)
secont condition:: tensor(1.7707e-10, dtype=torch.float64)
curr_secont condition:: tensor(7.4194e-10, dtype=torch.float64)
secont condition:: tensor(7.4194e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.9186e-10, dtype=torch.float64)
secont condition:: tensor(4.9186e-10, dtype=torch.float64)
curr_secont condition:: tensor(9.6046e-10, dtype=torch.float64)
secont condition:: tensor(9.6046e-10, dtype=torch.float64)
curr_secont condition:: tensor(5.9768e-10, dtype=torch.float64)
secont condition:: tensor(5.9768e-10, dtype=torch.float64)
curr_secont condition:: tensor(6.3987e-10, dtype=torch.float64)
secont condition:: tensor(6.3987e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.9292e-10, dtype=torch.float64)
secont condition:: tensor(4.9292e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.4976e-10, dtype=torch.float64)
secont condition:: tensor(3.4976e-10, dtype=torch.float64)
curr_secont condition:: tensor(5.9115e-10, dtype=torch.float64)
secont condition:: tensor(5.9115e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.4511e-10, dtype=torch.float64)
secont condition:: tensor(4.4511e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.4972e-09, dtype=torch.float64)
secont condition:: tensor(1.4972e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.6430e-09, dtype=torch.float64)
secont condition:: tensor(1.6430e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.4554e-09, dtype=torch.float64)
secont condition:: tensor(1.4554e-09, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(1.0768e-09, dtype=torch.float64)
secont condition:: tensor(1.0768e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.0392e-09, dtype=torch.float64)
secont condition:: tensor(1.0392e-09, dtype=torch.float64)
curr_secont condition:: tensor(6.9655e-10, dtype=torch.float64)
secont condition:: tensor(6.9655e-10, dtype=torch.float64)
curr_secont condition:: tensor(7.5304e-10, dtype=torch.float64)
secont condition:: tensor(7.5304e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.0778e-09, dtype=torch.float64)
secont condition:: tensor(1.0778e-09, dtype=torch.float64)
curr_secont condition:: tensor(7.9929e-10, dtype=torch.float64)
secont condition:: tensor(7.9929e-10, dtype=torch.float64)
curr_secont condition:: tensor(7.0576e-10, dtype=torch.float64)
secont condition:: tensor(7.0576e-10, dtype=torch.float64)
curr_secont condition:: tensor(5.5989e-10, dtype=torch.float64)
secont condition:: tensor(5.5989e-10, dtype=torch.float64)
curr_secont condition:: tensor(6.0486e-10, dtype=torch.float64)
secont condition:: tensor(6.0486e-10, dtype=torch.float64)
curr_secont condition:: tensor(6.2059e-10, dtype=torch.float64)
secont condition:: tensor(6.2059e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.9236e-10, dtype=torch.float64)
secont condition:: tensor(4.9236e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.5337e-09, dtype=torch.float64)
secont condition:: tensor(2.5337e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.1978e-09, dtype=torch.float64)
secont condition:: tensor(2.1978e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.6165e-09, dtype=torch.float64)
secont condition:: tensor(1.6165e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.4682e-09, dtype=torch.float64)
secont condition:: tensor(1.4682e-09, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(1.1782e-09, dtype=torch.float64)
secont condition:: tensor(1.1782e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.0726e-09, dtype=torch.float64)
secont condition:: tensor(1.0726e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.0718e-09, dtype=torch.float64)
secont condition:: tensor(1.0718e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.3276e-09, dtype=torch.float64)
secont condition:: tensor(1.3276e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.3111e-09, dtype=torch.float64)
secont condition:: tensor(1.3111e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.0330e-09, dtype=torch.float64)
secont condition:: tensor(1.0330e-09, dtype=torch.float64)
curr_secont condition:: tensor(8.7204e-10, dtype=torch.float64)
secont condition:: tensor(8.7204e-10, dtype=torch.float64)
curr_secont condition:: tensor(9.8143e-10, dtype=torch.float64)
secont condition:: tensor(9.8143e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.0202e-09, dtype=torch.float64)
secont condition:: tensor(1.0202e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.8270e-09, dtype=torch.float64)
secont condition:: tensor(2.8270e-09, dtype=torch.float64)
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(3.8178e-09, dtype=torch.float64)
secont condition:: tensor(3.8178e-09, dtype=torch.float64)
curr_secont condition:: tensor(8.7195e-09, dtype=torch.float64)
secont condition:: tensor(8.7195e-09, dtype=torch.float64)
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(6.9645e-09, dtype=torch.float64)
secont condition:: tensor(6.9645e-09, dtype=torch.float64)
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(7.9370e-09, dtype=torch.float64)
secont condition:: tensor(7.9370e-09, dtype=torch.float64)
curr_secont condition:: tensor(9.0439e-09, dtype=torch.float64)
secont condition:: tensor(9.0439e-09, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(1.3206e-08, dtype=torch.float64)
secont condition:: tensor(1.3206e-08, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(1.0254e-08, dtype=torch.float64)
secont condition:: tensor(1.0254e-08, dtype=torch.float64)
curr_secont condition:: tensor(7.7903e-09, dtype=torch.float64)
secont condition:: tensor(7.7903e-09, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 31.288031339645386
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.8853e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(2.0860e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(3.3453e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(4.0986e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.701800
repetition 5
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 12
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 55.908042430877686
time_baseline:: 55.92792272567749
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(3.3416e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(2.2020e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.701300
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 12
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(9.3421e-10, dtype=torch.float64)
secont condition:: tensor(9.3421e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.4994e-10, dtype=torch.float64)
secont condition:: tensor(3.4994e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.9318e-09, dtype=torch.float64)
secont condition:: tensor(2.9318e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.2561e-09, dtype=torch.float64)
secont condition:: tensor(2.2561e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.7672e-09, dtype=torch.float64)
secont condition:: tensor(1.7672e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.4113e-09, dtype=torch.float64)
secont condition:: tensor(1.4113e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.1518e-09, dtype=torch.float64)
secont condition:: tensor(1.1518e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.0234e-09, dtype=torch.float64)
secont condition:: tensor(1.0234e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.5783e-09, dtype=torch.float64)
secont condition:: tensor(1.5783e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.2415e-09, dtype=torch.float64)
secont condition:: tensor(1.2415e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.1685e-09, dtype=torch.float64)
secont condition:: tensor(1.1685e-09, dtype=torch.float64)
curr_secont condition:: tensor(9.9481e-10, dtype=torch.float64)
secont condition:: tensor(9.9481e-10, dtype=torch.float64)
curr_secont condition:: tensor(9.0853e-10, dtype=torch.float64)
secont condition:: tensor(9.0853e-10, dtype=torch.float64)
curr_secont condition:: tensor(8.6773e-10, dtype=torch.float64)
secont condition:: tensor(8.6773e-10, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(7.6396e-10, dtype=torch.float64)
secont condition:: tensor(7.6396e-10, dtype=torch.float64)
curr_secont condition:: tensor(5.8716e-10, dtype=torch.float64)
secont condition:: tensor(5.8716e-10, dtype=torch.float64)
curr_secont condition:: tensor(7.5373e-10, dtype=torch.float64)
secont condition:: tensor(7.5373e-10, dtype=torch.float64)
curr_secont condition:: tensor(6.9555e-10, dtype=torch.float64)
secont condition:: tensor(6.9555e-10, dtype=torch.float64)
curr_secont condition:: tensor(5.0453e-10, dtype=torch.float64)
secont condition:: tensor(5.0453e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.8898e-10, dtype=torch.float64)
secont condition:: tensor(4.8898e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.3211e-09, dtype=torch.float64)
secont condition:: tensor(1.3211e-09, dtype=torch.float64)
curr_secont condition:: tensor(9.6551e-10, dtype=torch.float64)
secont condition:: tensor(9.6551e-10, dtype=torch.float64)
curr_secont condition:: tensor(7.4509e-10, dtype=torch.float64)
secont condition:: tensor(7.4509e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.0078e-09, dtype=torch.float64)
secont condition:: tensor(1.0078e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.7427e-09, dtype=torch.float64)
secont condition:: tensor(1.7427e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.5547e-09, dtype=torch.float64)
secont condition:: tensor(1.5547e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.3103e-09, dtype=torch.float64)
secont condition:: tensor(1.3103e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.4051e-09, dtype=torch.float64)
secont condition:: tensor(1.4051e-09, dtype=torch.float64)
curr_secont condition:: tensor(9.8541e-10, dtype=torch.float64)
secont condition:: tensor(9.8541e-10, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(1.3776e-09, dtype=torch.float64)
secont condition:: tensor(1.3776e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.1626e-09, dtype=torch.float64)
secont condition:: tensor(1.1626e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.1693e-09, dtype=torch.float64)
secont condition:: tensor(1.1693e-09, dtype=torch.float64)
curr_secont condition:: tensor(9.4051e-10, dtype=torch.float64)
secont condition:: tensor(9.4051e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.1544e-09, dtype=torch.float64)
secont condition:: tensor(1.1544e-09, dtype=torch.float64)
curr_secont condition:: tensor(9.4767e-10, dtype=torch.float64)
secont condition:: tensor(9.4767e-10, dtype=torch.float64)
curr_secont condition:: tensor(7.6195e-10, dtype=torch.float64)
secont condition:: tensor(7.6195e-10, dtype=torch.float64)
curr_secont condition:: tensor(8.2265e-10, dtype=torch.float64)
secont condition:: tensor(8.2265e-10, dtype=torch.float64)
curr_secont condition:: tensor(9.6607e-10, dtype=torch.float64)
secont condition:: tensor(9.6607e-10, dtype=torch.float64)
curr_secont condition:: tensor(9.7025e-10, dtype=torch.float64)
secont condition:: tensor(9.7025e-10, dtype=torch.float64)
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(4.3370e-09, dtype=torch.float64)
secont condition:: tensor(4.3370e-09, dtype=torch.float64)
curr_secont condition:: tensor(7.2220e-09, dtype=torch.float64)
secont condition:: tensor(7.2220e-09, dtype=torch.float64)
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(5.7857e-09, dtype=torch.float64)
secont condition:: tensor(5.7857e-09, dtype=torch.float64)
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(7.0247e-09, dtype=torch.float64)
secont condition:: tensor(7.0247e-09, dtype=torch.float64)
curr_secont condition:: tensor(8.6431e-09, dtype=torch.float64)
secont condition:: tensor(8.6431e-09, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(1.5753e-08, dtype=torch.float64)
secont condition:: tensor(1.5753e-08, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(1.2441e-08, dtype=torch.float64)
secont condition:: tensor(1.2441e-08, dtype=torch.float64)
curr_secont condition:: tensor(1.3279e-08, dtype=torch.float64)
secont condition:: tensor(1.3279e-08, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 31.03083300590515
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.8881e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(2.6075e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(3.3043e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(4.0684e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.701700
deletion rate:: 0.0005
repetition 1
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 30
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 59.84575152397156
time_baseline:: 59.86634612083435
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(5.4243e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(4.4477e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0016, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.701100
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 30
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(6.3866e-10, dtype=torch.float64)
secont condition:: tensor(6.3866e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.6358e-09, dtype=torch.float64)
secont condition:: tensor(1.6358e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.2950e-09, dtype=torch.float64)
secont condition:: tensor(1.2950e-09, dtype=torch.float64)
curr_secont condition:: tensor(8.3880e-10, dtype=torch.float64)
secont condition:: tensor(8.3880e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.5286e-09, dtype=torch.float64)
secont condition:: tensor(1.5286e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.2576e-09, dtype=torch.float64)
secont condition:: tensor(1.2576e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.0437e-09, dtype=torch.float64)
secont condition:: tensor(1.0437e-09, dtype=torch.float64)
curr_secont condition:: tensor(8.4176e-10, dtype=torch.float64)
secont condition:: tensor(8.4176e-10, dtype=torch.float64)
curr_secont condition:: tensor(9.2159e-10, dtype=torch.float64)
secont condition:: tensor(9.2159e-10, dtype=torch.float64)
curr_secont condition:: tensor(9.0434e-10, dtype=torch.float64)
secont condition:: tensor(9.0434e-10, dtype=torch.float64)
curr_secont condition:: tensor(7.0857e-10, dtype=torch.float64)
secont condition:: tensor(7.0857e-10, dtype=torch.float64)
curr_secont condition:: tensor(7.6711e-10, dtype=torch.float64)
secont condition:: tensor(7.6711e-10, dtype=torch.float64)
curr_secont condition:: tensor(7.7028e-10, dtype=torch.float64)
secont condition:: tensor(7.7028e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.1040e-09, dtype=torch.float64)
secont condition:: tensor(1.1040e-09, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(2.2172e-09, dtype=torch.float64)
secont condition:: tensor(2.2172e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.1515e-09, dtype=torch.float64)
secont condition:: tensor(2.1515e-09, dtype=torch.float64)
curr_secont condition:: tensor(3.2338e-09, dtype=torch.float64)
secont condition:: tensor(3.2338e-09, dtype=torch.float64)
curr_secont condition:: tensor(3.2223e-09, dtype=torch.float64)
secont condition:: tensor(3.2223e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.0048e-08, dtype=torch.float64)
secont condition:: tensor(1.0048e-08, dtype=torch.float64)
curr_secont condition:: tensor(7.4941e-09, dtype=torch.float64)
secont condition:: tensor(7.4941e-09, dtype=torch.float64)
curr_secont condition:: tensor(6.2407e-09, dtype=torch.float64)
secont condition:: tensor(6.2407e-09, dtype=torch.float64)
curr_secont condition:: tensor(5.4871e-09, dtype=torch.float64)
secont condition:: tensor(5.4871e-09, dtype=torch.float64)
curr_secont condition:: tensor(4.1942e-09, dtype=torch.float64)
secont condition:: tensor(4.1942e-09, dtype=torch.float64)
curr_secont condition:: tensor(4.7964e-09, dtype=torch.float64)
secont condition:: tensor(4.7964e-09, dtype=torch.float64)
curr_secont condition:: tensor(4.3164e-09, dtype=torch.float64)
secont condition:: tensor(4.3164e-09, dtype=torch.float64)
curr_secont condition:: tensor(3.5949e-09, dtype=torch.float64)
secont condition:: tensor(3.5949e-09, dtype=torch.float64)
curr_secont condition:: tensor(4.0100e-09, dtype=torch.float64)
secont condition:: tensor(4.0100e-09, dtype=torch.float64)
curr_secont condition:: tensor(3.4058e-09, dtype=torch.float64)
secont condition:: tensor(3.4058e-09, dtype=torch.float64)
curr_secont condition:: tensor(3.7865e-09, dtype=torch.float64)
secont condition:: tensor(3.7865e-09, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(3.7955e-09, dtype=torch.float64)
secont condition:: tensor(3.7955e-09, dtype=torch.float64)
curr_secont condition:: tensor(3.5876e-09, dtype=torch.float64)
secont condition:: tensor(3.5876e-09, dtype=torch.float64)
curr_secont condition:: tensor(3.2696e-09, dtype=torch.float64)
secont condition:: tensor(3.2696e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.8767e-09, dtype=torch.float64)
secont condition:: tensor(2.8767e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.9787e-09, dtype=torch.float64)
secont condition:: tensor(2.9787e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.8840e-09, dtype=torch.float64)
secont condition:: tensor(2.8840e-09, dtype=torch.float64)
curr_secont condition:: tensor(4.5830e-09, dtype=torch.float64)
secont condition:: tensor(4.5830e-09, dtype=torch.float64)
curr_secont condition:: tensor(3.7870e-09, dtype=torch.float64)
secont condition:: tensor(3.7870e-09, dtype=torch.float64)
curr_secont condition:: tensor(4.3797e-09, dtype=torch.float64)
secont condition:: tensor(4.3797e-09, dtype=torch.float64)
curr_secont condition:: tensor(4.8352e-09, dtype=torch.float64)
secont condition:: tensor(4.8352e-09, dtype=torch.float64)
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(8.2671e-09, dtype=torch.float64)
secont condition:: tensor(8.2671e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.1137e-08, dtype=torch.float64)
secont condition:: tensor(1.1137e-08, dtype=torch.float64)
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(1.6740e-08, dtype=torch.float64)
secont condition:: tensor(1.6740e-08, dtype=torch.float64)
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(2.2142e-08, dtype=torch.float64)
secont condition:: tensor(2.2142e-08, dtype=torch.float64)
curr_secont condition:: tensor(1.5843e-08, dtype=torch.float64)
secont condition:: tensor(1.5843e-08, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(1.7883e-08, dtype=torch.float64)
secont condition:: tensor(1.7883e-08, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(2.6953e-08, dtype=torch.float64)
secont condition:: tensor(2.6953e-08, dtype=torch.float64)
curr_secont condition:: tensor(2.3906e-08, dtype=torch.float64)
secont condition:: tensor(2.3906e-08, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 36.419413328170776
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(2.6952e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(3.3569e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(5.3296e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(5.6579e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.701500
repetition 2
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 30
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 61.31645846366882
time_baseline:: 61.334593772888184
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(5.5067e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(3.8741e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.701300
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 30
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(1.1986e-10, dtype=torch.float64)
secont condition:: tensor(1.1986e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.4295e-10, dtype=torch.float64)
secont condition:: tensor(2.4295e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.3944e-10, dtype=torch.float64)
secont condition:: tensor(3.3944e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.9786e-10, dtype=torch.float64)
secont condition:: tensor(4.9786e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.2237e-09, dtype=torch.float64)
secont condition:: tensor(1.2237e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.0369e-09, dtype=torch.float64)
secont condition:: tensor(1.0369e-09, dtype=torch.float64)
curr_secont condition:: tensor(8.6398e-10, dtype=torch.float64)
secont condition:: tensor(8.6398e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.2741e-09, dtype=torch.float64)
secont condition:: tensor(1.2741e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.4165e-09, dtype=torch.float64)
secont condition:: tensor(1.4165e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.5338e-09, dtype=torch.float64)
secont condition:: tensor(1.5338e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.2780e-09, dtype=torch.float64)
secont condition:: tensor(1.2780e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.7013e-09, dtype=torch.float64)
secont condition:: tensor(2.7013e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.9009e-09, dtype=torch.float64)
secont condition:: tensor(2.9009e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.9842e-09, dtype=torch.float64)
secont condition:: tensor(1.9842e-09, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(6.0634e-09, dtype=torch.float64)
secont condition:: tensor(6.0634e-09, dtype=torch.float64)
curr_secont condition:: tensor(4.4380e-09, dtype=torch.float64)
secont condition:: tensor(4.4380e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.0580e-09, dtype=torch.float64)
secont condition:: tensor(2.0580e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.8800e-09, dtype=torch.float64)
secont condition:: tensor(2.8800e-09, dtype=torch.float64)
curr_secont condition:: tensor(3.0305e-09, dtype=torch.float64)
secont condition:: tensor(3.0305e-09, dtype=torch.float64)
curr_secont condition:: tensor(3.9046e-09, dtype=torch.float64)
secont condition:: tensor(3.9046e-09, dtype=torch.float64)
curr_secont condition:: tensor(3.3403e-09, dtype=torch.float64)
secont condition:: tensor(3.3403e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.6606e-09, dtype=torch.float64)
secont condition:: tensor(2.6606e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.2564e-09, dtype=torch.float64)
secont condition:: tensor(2.2564e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.8400e-09, dtype=torch.float64)
secont condition:: tensor(2.8400e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.2579e-09, dtype=torch.float64)
secont condition:: tensor(2.2579e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.3801e-09, dtype=torch.float64)
secont condition:: tensor(2.3801e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.8282e-09, dtype=torch.float64)
secont condition:: tensor(2.8282e-09, dtype=torch.float64)
curr_secont condition:: tensor(3.2306e-09, dtype=torch.float64)
secont condition:: tensor(3.2306e-09, dtype=torch.float64)
curr_secont condition:: tensor(5.3572e-09, dtype=torch.float64)
secont condition:: tensor(5.3572e-09, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(3.8700e-09, dtype=torch.float64)
secont condition:: tensor(3.8700e-09, dtype=torch.float64)
curr_secont condition:: tensor(4.7231e-09, dtype=torch.float64)
secont condition:: tensor(4.7231e-09, dtype=torch.float64)
curr_secont condition:: tensor(4.0158e-09, dtype=torch.float64)
secont condition:: tensor(4.0158e-09, dtype=torch.float64)
curr_secont condition:: tensor(5.0975e-09, dtype=torch.float64)
secont condition:: tensor(5.0975e-09, dtype=torch.float64)
curr_secont condition:: tensor(4.7526e-09, dtype=torch.float64)
secont condition:: tensor(4.7526e-09, dtype=torch.float64)
curr_secont condition:: tensor(3.7344e-09, dtype=torch.float64)
secont condition:: tensor(3.7344e-09, dtype=torch.float64)
curr_secont condition:: tensor(3.7752e-09, dtype=torch.float64)
secont condition:: tensor(3.7752e-09, dtype=torch.float64)
curr_secont condition:: tensor(3.3171e-09, dtype=torch.float64)
secont condition:: tensor(3.3171e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.9500e-09, dtype=torch.float64)
secont condition:: tensor(2.9500e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.1791e-09, dtype=torch.float64)
secont condition:: tensor(2.1791e-09, dtype=torch.float64)
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(1.8073e-08, dtype=torch.float64)
secont condition:: tensor(1.8073e-08, dtype=torch.float64)
curr_secont condition:: tensor(1.5895e-08, dtype=torch.float64)
secont condition:: tensor(1.5895e-08, dtype=torch.float64)
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(1.6502e-08, dtype=torch.float64)
secont condition:: tensor(1.6502e-08, dtype=torch.float64)
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(1.8820e-08, dtype=torch.float64)
secont condition:: tensor(1.8820e-08, dtype=torch.float64)
curr_secont condition:: tensor(2.9285e-08, dtype=torch.float64)
secont condition:: tensor(2.9285e-08, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(2.0832e-08, dtype=torch.float64)
secont condition:: tensor(2.0832e-08, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(1.7285e-08, dtype=torch.float64)
secont condition:: tensor(1.7285e-08, dtype=torch.float64)
curr_secont condition:: tensor(1.7349e-08, dtype=torch.float64)
secont condition:: tensor(1.7349e-08, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 36.89228081703186
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(2.4665e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.8575e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(5.2538e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(3.9431e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.701500
repetition 3
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 30
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 60.05231165885925
time_baseline:: 60.071727991104126
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(5.1858e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(4.9583e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0016, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.701200
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 30
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(1.5525e-10, dtype=torch.float64)
secont condition:: tensor(1.5525e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.3498e-10, dtype=torch.float64)
secont condition:: tensor(2.3498e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.6739e-09, dtype=torch.float64)
secont condition:: tensor(1.6739e-09, dtype=torch.float64)
curr_secont condition:: tensor(7.3289e-10, dtype=torch.float64)
secont condition:: tensor(7.3289e-10, dtype=torch.float64)
curr_secont condition:: tensor(6.9635e-10, dtype=torch.float64)
secont condition:: tensor(6.9635e-10, dtype=torch.float64)
curr_secont condition:: tensor(7.7845e-10, dtype=torch.float64)
secont condition:: tensor(7.7845e-10, dtype=torch.float64)
curr_secont condition:: tensor(8.6271e-10, dtype=torch.float64)
secont condition:: tensor(8.6271e-10, dtype=torch.float64)
curr_secont condition:: tensor(7.5750e-10, dtype=torch.float64)
secont condition:: tensor(7.5750e-10, dtype=torch.float64)
curr_secont condition:: tensor(9.0186e-10, dtype=torch.float64)
secont condition:: tensor(9.0186e-10, dtype=torch.float64)
curr_secont condition:: tensor(7.3296e-10, dtype=torch.float64)
secont condition:: tensor(7.3296e-10, dtype=torch.float64)
curr_secont condition:: tensor(6.3745e-10, dtype=torch.float64)
secont condition:: tensor(6.3745e-10, dtype=torch.float64)
curr_secont condition:: tensor(6.8068e-10, dtype=torch.float64)
secont condition:: tensor(6.8068e-10, dtype=torch.float64)
curr_secont condition:: tensor(9.8190e-10, dtype=torch.float64)
secont condition:: tensor(9.8190e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.1258e-09, dtype=torch.float64)
secont condition:: tensor(1.1258e-09, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(1.8471e-09, dtype=torch.float64)
secont condition:: tensor(1.8471e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.9977e-09, dtype=torch.float64)
secont condition:: tensor(1.9977e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.6588e-09, dtype=torch.float64)
secont condition:: tensor(1.6588e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.4368e-09, dtype=torch.float64)
secont condition:: tensor(1.4368e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.3332e-09, dtype=torch.float64)
secont condition:: tensor(1.3332e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.2069e-09, dtype=torch.float64)
secont condition:: tensor(1.2069e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.3714e-09, dtype=torch.float64)
secont condition:: tensor(1.3714e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.9647e-09, dtype=torch.float64)
secont condition:: tensor(1.9647e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.7131e-09, dtype=torch.float64)
secont condition:: tensor(1.7131e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.0174e-09, dtype=torch.float64)
secont condition:: tensor(2.0174e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.8485e-09, dtype=torch.float64)
secont condition:: tensor(1.8485e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.7235e-09, dtype=torch.float64)
secont condition:: tensor(1.7235e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.9533e-09, dtype=torch.float64)
secont condition:: tensor(1.9533e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.9217e-09, dtype=torch.float64)
secont condition:: tensor(1.9217e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.7744e-09, dtype=torch.float64)
secont condition:: tensor(1.7744e-09, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(2.6227e-09, dtype=torch.float64)
secont condition:: tensor(2.6227e-09, dtype=torch.float64)
curr_secont condition:: tensor(4.0496e-09, dtype=torch.float64)
secont condition:: tensor(4.0496e-09, dtype=torch.float64)
curr_secont condition:: tensor(3.4351e-09, dtype=torch.float64)
secont condition:: tensor(3.4351e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.7795e-09, dtype=torch.float64)
secont condition:: tensor(2.7795e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.5974e-09, dtype=torch.float64)
secont condition:: tensor(2.5974e-09, dtype=torch.float64)
curr_secont condition:: tensor(4.5308e-09, dtype=torch.float64)
secont condition:: tensor(4.5308e-09, dtype=torch.float64)
curr_secont condition:: tensor(7.2100e-09, dtype=torch.float64)
secont condition:: tensor(7.2100e-09, dtype=torch.float64)
curr_secont condition:: tensor(6.8389e-09, dtype=torch.float64)
secont condition:: tensor(6.8389e-09, dtype=torch.float64)
curr_secont condition:: tensor(5.2704e-09, dtype=torch.float64)
secont condition:: tensor(5.2704e-09, dtype=torch.float64)
curr_secont condition:: tensor(4.5788e-09, dtype=torch.float64)
secont condition:: tensor(4.5788e-09, dtype=torch.float64)
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(1.3877e-08, dtype=torch.float64)
secont condition:: tensor(1.3877e-08, dtype=torch.float64)
curr_secont condition:: tensor(1.6160e-08, dtype=torch.float64)
secont condition:: tensor(1.6160e-08, dtype=torch.float64)
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(2.9525e-08, dtype=torch.float64)
secont condition:: tensor(2.9525e-08, dtype=torch.float64)
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(2.8385e-08, dtype=torch.float64)
secont condition:: tensor(2.8385e-08, dtype=torch.float64)
curr_secont condition:: tensor(2.2557e-08, dtype=torch.float64)
secont condition:: tensor(2.2557e-08, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(2.3485e-08, dtype=torch.float64)
secont condition:: tensor(2.3485e-08, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(2.6230e-08, dtype=torch.float64)
secont condition:: tensor(2.6230e-08, dtype=torch.float64)
curr_secont condition:: tensor(2.0161e-08, dtype=torch.float64)
secont condition:: tensor(2.0161e-08, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 36.887107133865356
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(2.5383e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(2.4667e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(5.2418e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(6.5695e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0016, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.701000
repetition 4
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 30
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 60.361865282058716
time_baseline:: 60.38287544250488
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(5.1911e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(3.6265e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0016, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.701300
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 30
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(5.0553e-10, dtype=torch.float64)
secont condition:: tensor(5.0553e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.5155e-10, dtype=torch.float64)
secont condition:: tensor(3.5155e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.1647e-10, dtype=torch.float64)
secont condition:: tensor(3.1647e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.6813e-10, dtype=torch.float64)
secont condition:: tensor(2.6813e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.5538e-09, dtype=torch.float64)
secont condition:: tensor(1.5538e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.5661e-09, dtype=torch.float64)
secont condition:: tensor(1.5661e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.4667e-09, dtype=torch.float64)
secont condition:: tensor(2.4667e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.5635e-09, dtype=torch.float64)
secont condition:: tensor(2.5635e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.1957e-09, dtype=torch.float64)
secont condition:: tensor(2.1957e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.1420e-09, dtype=torch.float64)
secont condition:: tensor(2.1420e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.3595e-09, dtype=torch.float64)
secont condition:: tensor(2.3595e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.9901e-09, dtype=torch.float64)
secont condition:: tensor(2.9901e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.8985e-09, dtype=torch.float64)
secont condition:: tensor(2.8985e-09, dtype=torch.float64)
curr_secont condition:: tensor(3.2090e-09, dtype=torch.float64)
secont condition:: tensor(3.2090e-09, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(3.5092e-09, dtype=torch.float64)
secont condition:: tensor(3.5092e-09, dtype=torch.float64)
curr_secont condition:: tensor(3.0919e-09, dtype=torch.float64)
secont condition:: tensor(3.0919e-09, dtype=torch.float64)
curr_secont condition:: tensor(3.0635e-09, dtype=torch.float64)
secont condition:: tensor(3.0635e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.4609e-09, dtype=torch.float64)
secont condition:: tensor(2.4609e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.6583e-09, dtype=torch.float64)
secont condition:: tensor(2.6583e-09, dtype=torch.float64)
curr_secont condition:: tensor(3.0082e-09, dtype=torch.float64)
secont condition:: tensor(3.0082e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.7196e-09, dtype=torch.float64)
secont condition:: tensor(2.7196e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.3453e-09, dtype=torch.float64)
secont condition:: tensor(2.3453e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.0136e-09, dtype=torch.float64)
secont condition:: tensor(2.0136e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.0898e-09, dtype=torch.float64)
secont condition:: tensor(2.0898e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.9569e-09, dtype=torch.float64)
secont condition:: tensor(1.9569e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.2974e-09, dtype=torch.float64)
secont condition:: tensor(2.2974e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.8320e-09, dtype=torch.float64)
secont condition:: tensor(2.8320e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.1733e-09, dtype=torch.float64)
secont condition:: tensor(2.1733e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.0198e-09, dtype=torch.float64)
secont condition:: tensor(2.0198e-09, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(3.0053e-09, dtype=torch.float64)
secont condition:: tensor(3.0053e-09, dtype=torch.float64)
curr_secont condition:: tensor(3.3591e-09, dtype=torch.float64)
secont condition:: tensor(3.3591e-09, dtype=torch.float64)
curr_secont condition:: tensor(3.0980e-09, dtype=torch.float64)
secont condition:: tensor(3.0980e-09, dtype=torch.float64)
curr_secont condition:: tensor(3.1145e-09, dtype=torch.float64)
secont condition:: tensor(3.1145e-09, dtype=torch.float64)
curr_secont condition:: tensor(3.6483e-09, dtype=torch.float64)
secont condition:: tensor(3.6483e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.7230e-09, dtype=torch.float64)
secont condition:: tensor(2.7230e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.5887e-09, dtype=torch.float64)
secont condition:: tensor(2.5887e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.5053e-09, dtype=torch.float64)
secont condition:: tensor(2.5053e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.1499e-09, dtype=torch.float64)
secont condition:: tensor(2.1499e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.0954e-09, dtype=torch.float64)
secont condition:: tensor(2.0954e-09, dtype=torch.float64)
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(8.8871e-09, dtype=torch.float64)
secont condition:: tensor(8.8871e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.8350e-08, dtype=torch.float64)
secont condition:: tensor(1.8350e-08, dtype=torch.float64)
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(1.6743e-08, dtype=torch.float64)
secont condition:: tensor(1.6743e-08, dtype=torch.float64)
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(1.8031e-08, dtype=torch.float64)
secont condition:: tensor(1.8031e-08, dtype=torch.float64)
curr_secont condition:: tensor(1.7503e-08, dtype=torch.float64)
secont condition:: tensor(1.7503e-08, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(1.5490e-08, dtype=torch.float64)
secont condition:: tensor(1.5490e-08, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(1.7159e-08, dtype=torch.float64)
secont condition:: tensor(1.7159e-08, dtype=torch.float64)
curr_secont condition:: tensor(2.3690e-08, dtype=torch.float64)
secont condition:: tensor(2.3690e-08, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 36.61061096191406
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(2.5941e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(3.4841e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(5.3983e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(6.3122e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.701400
repetition 5
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 30
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 59.83076763153076
time_baseline:: 59.84895372390747
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(5.2958e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(4.5560e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.701200
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 30
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(8.2704e-10, dtype=torch.float64)
secont condition:: tensor(8.2704e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.1843e-09, dtype=torch.float64)
secont condition:: tensor(1.1843e-09, dtype=torch.float64)
curr_secont condition:: tensor(9.7921e-10, dtype=torch.float64)
secont condition:: tensor(9.7921e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.8288e-09, dtype=torch.float64)
secont condition:: tensor(2.8288e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.2379e-09, dtype=torch.float64)
secont condition:: tensor(2.2379e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.0684e-09, dtype=torch.float64)
secont condition:: tensor(2.0684e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.6889e-09, dtype=torch.float64)
secont condition:: tensor(1.6889e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.4248e-09, dtype=torch.float64)
secont condition:: tensor(1.4248e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.9501e-09, dtype=torch.float64)
secont condition:: tensor(1.9501e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.2997e-09, dtype=torch.float64)
secont condition:: tensor(1.2997e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.0013e-09, dtype=torch.float64)
secont condition:: tensor(1.0013e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.1359e-09, dtype=torch.float64)
secont condition:: tensor(1.1359e-09, dtype=torch.float64)
curr_secont condition:: tensor(9.4335e-10, dtype=torch.float64)
secont condition:: tensor(9.4335e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.2763e-09, dtype=torch.float64)
secont condition:: tensor(1.2763e-09, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(1.2106e-09, dtype=torch.float64)
secont condition:: tensor(1.2106e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.1031e-09, dtype=torch.float64)
secont condition:: tensor(1.1031e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.6594e-09, dtype=torch.float64)
secont condition:: tensor(1.6594e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.5542e-09, dtype=torch.float64)
secont condition:: tensor(1.5542e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.2789e-09, dtype=torch.float64)
secont condition:: tensor(1.2789e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.2241e-09, dtype=torch.float64)
secont condition:: tensor(2.2241e-09, dtype=torch.float64)
curr_secont condition:: tensor(3.7022e-09, dtype=torch.float64)
secont condition:: tensor(3.7022e-09, dtype=torch.float64)
curr_secont condition:: tensor(3.1371e-09, dtype=torch.float64)
secont condition:: tensor(3.1371e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.6915e-09, dtype=torch.float64)
secont condition:: tensor(2.6915e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.8771e-09, dtype=torch.float64)
secont condition:: tensor(1.8771e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.1477e-09, dtype=torch.float64)
secont condition:: tensor(2.1477e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.0818e-09, dtype=torch.float64)
secont condition:: tensor(2.0818e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.0055e-09, dtype=torch.float64)
secont condition:: tensor(2.0055e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.2669e-09, dtype=torch.float64)
secont condition:: tensor(2.2669e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.5375e-09, dtype=torch.float64)
secont condition:: tensor(2.5375e-09, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(3.1704e-09, dtype=torch.float64)
secont condition:: tensor(3.1704e-09, dtype=torch.float64)
curr_secont condition:: tensor(4.1857e-09, dtype=torch.float64)
secont condition:: tensor(4.1857e-09, dtype=torch.float64)
curr_secont condition:: tensor(3.4102e-09, dtype=torch.float64)
secont condition:: tensor(3.4102e-09, dtype=torch.float64)
curr_secont condition:: tensor(5.1400e-09, dtype=torch.float64)
secont condition:: tensor(5.1400e-09, dtype=torch.float64)
curr_secont condition:: tensor(4.2781e-09, dtype=torch.float64)
secont condition:: tensor(4.2781e-09, dtype=torch.float64)
curr_secont condition:: tensor(4.9135e-09, dtype=torch.float64)
secont condition:: tensor(4.9135e-09, dtype=torch.float64)
curr_secont condition:: tensor(7.6982e-09, dtype=torch.float64)
secont condition:: tensor(7.6982e-09, dtype=torch.float64)
curr_secont condition:: tensor(6.4253e-09, dtype=torch.float64)
secont condition:: tensor(6.4253e-09, dtype=torch.float64)
curr_secont condition:: tensor(5.1858e-09, dtype=torch.float64)
secont condition:: tensor(5.1858e-09, dtype=torch.float64)
curr_secont condition:: tensor(5.2982e-09, dtype=torch.float64)
secont condition:: tensor(5.2982e-09, dtype=torch.float64)
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(8.7715e-09, dtype=torch.float64)
secont condition:: tensor(8.7715e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.1541e-08, dtype=torch.float64)
secont condition:: tensor(1.1541e-08, dtype=torch.float64)
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(2.7179e-08, dtype=torch.float64)
secont condition:: tensor(2.7179e-08, dtype=torch.float64)
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(1.8060e-08, dtype=torch.float64)
secont condition:: tensor(1.8060e-08, dtype=torch.float64)
curr_secont condition:: tensor(2.3118e-08, dtype=torch.float64)
secont condition:: tensor(2.3118e-08, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(2.1291e-08, dtype=torch.float64)
secont condition:: tensor(2.1291e-08, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(2.1377e-08, dtype=torch.float64)
secont condition:: tensor(2.1377e-08, dtype=torch.float64)
curr_secont condition:: tensor(3.2919e-08, dtype=torch.float64)
secont condition:: tensor(3.2919e-08, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 37.06587600708008
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(2.6820e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(3.1195e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(5.4256e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(5.4341e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.701200
deletion rate:: 0.001
repetition 1
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 60
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 64.90343904495239
time_baseline:: 64.92248368263245
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(7.3905e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0016, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(6.3865e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0024, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.701100
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 60
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(3.2180e-10, dtype=torch.float64)
secont condition:: tensor(3.2180e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.1442e-09, dtype=torch.float64)
secont condition:: tensor(1.1442e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.2156e-09, dtype=torch.float64)
secont condition:: tensor(1.2156e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.9706e-09, dtype=torch.float64)
secont condition:: tensor(1.9706e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.1870e-09, dtype=torch.float64)
secont condition:: tensor(2.1870e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.9311e-09, dtype=torch.float64)
secont condition:: tensor(1.9311e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.5012e-09, dtype=torch.float64)
secont condition:: tensor(2.5012e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.8541e-09, dtype=torch.float64)
secont condition:: tensor(1.8541e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.7873e-09, dtype=torch.float64)
secont condition:: tensor(1.7873e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.6965e-09, dtype=torch.float64)
secont condition:: tensor(1.6965e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.4744e-09, dtype=torch.float64)
secont condition:: tensor(2.4744e-09, dtype=torch.float64)
curr_secont condition:: tensor(4.6307e-09, dtype=torch.float64)
secont condition:: tensor(4.6307e-09, dtype=torch.float64)
curr_secont condition:: tensor(3.9886e-09, dtype=torch.float64)
secont condition:: tensor(3.9886e-09, dtype=torch.float64)
curr_secont condition:: tensor(3.0774e-09, dtype=torch.float64)
secont condition:: tensor(3.0774e-09, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(3.8156e-09, dtype=torch.float64)
secont condition:: tensor(3.8156e-09, dtype=torch.float64)
curr_secont condition:: tensor(3.1970e-09, dtype=torch.float64)
secont condition:: tensor(3.1970e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.7783e-09, dtype=torch.float64)
secont condition:: tensor(2.7783e-09, dtype=torch.float64)
curr_secont condition:: tensor(4.0810e-09, dtype=torch.float64)
secont condition:: tensor(4.0810e-09, dtype=torch.float64)
curr_secont condition:: tensor(3.1791e-09, dtype=torch.float64)
secont condition:: tensor(3.1791e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.9748e-09, dtype=torch.float64)
secont condition:: tensor(2.9748e-09, dtype=torch.float64)
curr_secont condition:: tensor(7.4710e-09, dtype=torch.float64)
secont condition:: tensor(7.4710e-09, dtype=torch.float64)
curr_secont condition:: tensor(7.8015e-09, dtype=torch.float64)
secont condition:: tensor(7.8015e-09, dtype=torch.float64)
curr_secont condition:: tensor(7.7691e-09, dtype=torch.float64)
secont condition:: tensor(7.7691e-09, dtype=torch.float64)
curr_secont condition:: tensor(6.7668e-09, dtype=torch.float64)
secont condition:: tensor(6.7668e-09, dtype=torch.float64)
curr_secont condition:: tensor(6.2622e-09, dtype=torch.float64)
secont condition:: tensor(6.2622e-09, dtype=torch.float64)
curr_secont condition:: tensor(5.6955e-09, dtype=torch.float64)
secont condition:: tensor(5.6955e-09, dtype=torch.float64)
curr_secont condition:: tensor(5.5799e-09, dtype=torch.float64)
secont condition:: tensor(5.5799e-09, dtype=torch.float64)
curr_secont condition:: tensor(6.9429e-09, dtype=torch.float64)
secont condition:: tensor(6.9429e-09, dtype=torch.float64)
curr_secont condition:: tensor(6.9723e-09, dtype=torch.float64)
secont condition:: tensor(6.9723e-09, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(5.2503e-09, dtype=torch.float64)
secont condition:: tensor(5.2503e-09, dtype=torch.float64)
curr_secont condition:: tensor(6.5544e-09, dtype=torch.float64)
secont condition:: tensor(6.5544e-09, dtype=torch.float64)
curr_secont condition:: tensor(6.3564e-09, dtype=torch.float64)
secont condition:: tensor(6.3564e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.0065e-08, dtype=torch.float64)
secont condition:: tensor(1.0065e-08, dtype=torch.float64)
curr_secont condition:: tensor(8.5655e-09, dtype=torch.float64)
secont condition:: tensor(8.5655e-09, dtype=torch.float64)
curr_secont condition:: tensor(7.2217e-09, dtype=torch.float64)
secont condition:: tensor(7.2217e-09, dtype=torch.float64)
curr_secont condition:: tensor(5.9855e-09, dtype=torch.float64)
secont condition:: tensor(5.9855e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.0138e-08, dtype=torch.float64)
secont condition:: tensor(1.0138e-08, dtype=torch.float64)
curr_secont condition:: tensor(9.9797e-09, dtype=torch.float64)
secont condition:: tensor(9.9797e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.0787e-08, dtype=torch.float64)
secont condition:: tensor(1.0787e-08, dtype=torch.float64)
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(1.7960e-08, dtype=torch.float64)
secont condition:: tensor(1.7960e-08, dtype=torch.float64)
curr_secont condition:: tensor(2.7768e-08, dtype=torch.float64)
secont condition:: tensor(2.7768e-08, dtype=torch.float64)
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(2.0006e-08, dtype=torch.float64)
secont condition:: tensor(2.0006e-08, dtype=torch.float64)
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(3.2118e-08, dtype=torch.float64)
secont condition:: tensor(3.2118e-08, dtype=torch.float64)
curr_secont condition:: tensor(3.8991e-08, dtype=torch.float64)
secont condition:: tensor(3.8991e-08, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(4.9244e-08, dtype=torch.float64)
secont condition:: tensor(4.9244e-08, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(3.3943e-08, dtype=torch.float64)
secont condition:: tensor(3.3943e-08, dtype=torch.float64)
curr_secont condition:: tensor(3.7874e-08, dtype=torch.float64)
secont condition:: tensor(3.7874e-08, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 42.53625416755676
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(3.1367e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(2.4883e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(7.4481e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(7.9451e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0024, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.701600
repetition 2
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 60
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 64.07359600067139
time_baseline:: 64.09192180633545
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(7.1290e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0016, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(4.9114e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0023, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.701300
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 60
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(5.1163e-09, dtype=torch.float64)
secont condition:: tensor(5.1163e-09, dtype=torch.float64)
curr_secont condition:: tensor(3.2679e-09, dtype=torch.float64)
secont condition:: tensor(3.2679e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.6147e-09, dtype=torch.float64)
secont condition:: tensor(2.6147e-09, dtype=torch.float64)
curr_secont condition:: tensor(3.2246e-09, dtype=torch.float64)
secont condition:: tensor(3.2246e-09, dtype=torch.float64)
curr_secont condition:: tensor(3.0063e-09, dtype=torch.float64)
secont condition:: tensor(3.0063e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.0983e-09, dtype=torch.float64)
secont condition:: tensor(2.0983e-09, dtype=torch.float64)
curr_secont condition:: tensor(3.4777e-09, dtype=torch.float64)
secont condition:: tensor(3.4777e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.2363e-09, dtype=torch.float64)
secont condition:: tensor(2.2363e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.8479e-09, dtype=torch.float64)
secont condition:: tensor(1.8479e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.4472e-09, dtype=torch.float64)
secont condition:: tensor(2.4472e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.2050e-09, dtype=torch.float64)
secont condition:: tensor(2.2050e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.2216e-09, dtype=torch.float64)
secont condition:: tensor(2.2216e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.9382e-09, dtype=torch.float64)
secont condition:: tensor(1.9382e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.7567e-09, dtype=torch.float64)
secont condition:: tensor(1.7567e-09, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(3.4478e-09, dtype=torch.float64)
secont condition:: tensor(3.4478e-09, dtype=torch.float64)
curr_secont condition:: tensor(3.3828e-09, dtype=torch.float64)
secont condition:: tensor(3.3828e-09, dtype=torch.float64)
curr_secont condition:: tensor(3.8968e-09, dtype=torch.float64)
secont condition:: tensor(3.8968e-09, dtype=torch.float64)
curr_secont condition:: tensor(5.4368e-09, dtype=torch.float64)
secont condition:: tensor(5.4368e-09, dtype=torch.float64)
curr_secont condition:: tensor(8.3928e-09, dtype=torch.float64)
secont condition:: tensor(8.3928e-09, dtype=torch.float64)
curr_secont condition:: tensor(5.8981e-09, dtype=torch.float64)
secont condition:: tensor(5.8981e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.0087e-08, dtype=torch.float64)
secont condition:: tensor(1.0087e-08, dtype=torch.float64)
curr_secont condition:: tensor(1.0478e-08, dtype=torch.float64)
secont condition:: tensor(1.0478e-08, dtype=torch.float64)
curr_secont condition:: tensor(1.5142e-08, dtype=torch.float64)
secont condition:: tensor(1.5142e-08, dtype=torch.float64)
curr_secont condition:: tensor(9.7039e-09, dtype=torch.float64)
secont condition:: tensor(9.7039e-09, dtype=torch.float64)
curr_secont condition:: tensor(7.9696e-09, dtype=torch.float64)
secont condition:: tensor(7.9696e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.1672e-08, dtype=torch.float64)
secont condition:: tensor(1.1672e-08, dtype=torch.float64)
curr_secont condition:: tensor(1.1349e-08, dtype=torch.float64)
secont condition:: tensor(1.1349e-08, dtype=torch.float64)
curr_secont condition:: tensor(8.5657e-09, dtype=torch.float64)
secont condition:: tensor(8.5657e-09, dtype=torch.float64)
curr_secont condition:: tensor(8.5743e-09, dtype=torch.float64)
secont condition:: tensor(8.5743e-09, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(6.4711e-09, dtype=torch.float64)
secont condition:: tensor(6.4711e-09, dtype=torch.float64)
curr_secont condition:: tensor(7.0615e-09, dtype=torch.float64)
secont condition:: tensor(7.0615e-09, dtype=torch.float64)
curr_secont condition:: tensor(8.5017e-09, dtype=torch.float64)
secont condition:: tensor(8.5017e-09, dtype=torch.float64)
curr_secont condition:: tensor(9.0072e-09, dtype=torch.float64)
secont condition:: tensor(9.0072e-09, dtype=torch.float64)
curr_secont condition:: tensor(8.9006e-09, dtype=torch.float64)
secont condition:: tensor(8.9006e-09, dtype=torch.float64)
curr_secont condition:: tensor(8.4116e-09, dtype=torch.float64)
secont condition:: tensor(8.4116e-09, dtype=torch.float64)
curr_secont condition:: tensor(7.5215e-09, dtype=torch.float64)
secont condition:: tensor(7.5215e-09, dtype=torch.float64)
curr_secont condition:: tensor(8.8217e-09, dtype=torch.float64)
secont condition:: tensor(8.8217e-09, dtype=torch.float64)
curr_secont condition:: tensor(6.5338e-09, dtype=torch.float64)
secont condition:: tensor(6.5338e-09, dtype=torch.float64)
curr_secont condition:: tensor(5.3207e-09, dtype=torch.float64)
secont condition:: tensor(5.3207e-09, dtype=torch.float64)
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(1.7673e-08, dtype=torch.float64)
secont condition:: tensor(1.7673e-08, dtype=torch.float64)
curr_secont condition:: tensor(4.8189e-08, dtype=torch.float64)
secont condition:: tensor(4.8189e-08, dtype=torch.float64)
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(4.5047e-08, dtype=torch.float64)
secont condition:: tensor(4.5047e-08, dtype=torch.float64)
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(5.4487e-08, dtype=torch.float64)
secont condition:: tensor(5.4487e-08, dtype=torch.float64)
curr_secont condition:: tensor(3.0191e-08, dtype=torch.float64)
secont condition:: tensor(3.0191e-08, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(3.4546e-08, dtype=torch.float64)
secont condition:: tensor(3.4546e-08, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(4.0139e-08, dtype=torch.float64)
secont condition:: tensor(4.0139e-08, dtype=torch.float64)
curr_secont condition:: tensor(3.6646e-08, dtype=torch.float64)
secont condition:: tensor(3.6646e-08, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 41.56664681434631
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(3.3978e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(3.2390e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(7.2757e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0016, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(6.3948e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0024, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.701700
repetition 3
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 60
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 63.8026237487793
time_baseline:: 63.820992946624756
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(7.5881e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0016, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(7.1208e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0024, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.701400
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 60
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(2.5709e-10, dtype=torch.float64)
secont condition:: tensor(2.5709e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.1790e-09, dtype=torch.float64)
secont condition:: tensor(1.1790e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.5875e-09, dtype=torch.float64)
secont condition:: tensor(2.5875e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.7157e-09, dtype=torch.float64)
secont condition:: tensor(2.7157e-09, dtype=torch.float64)
curr_secont condition:: tensor(4.4572e-09, dtype=torch.float64)
secont condition:: tensor(4.4572e-09, dtype=torch.float64)
curr_secont condition:: tensor(4.7145e-09, dtype=torch.float64)
secont condition:: tensor(4.7145e-09, dtype=torch.float64)
curr_secont condition:: tensor(4.6831e-09, dtype=torch.float64)
secont condition:: tensor(4.6831e-09, dtype=torch.float64)
curr_secont condition:: tensor(4.5564e-09, dtype=torch.float64)
secont condition:: tensor(4.5564e-09, dtype=torch.float64)
curr_secont condition:: tensor(3.8989e-09, dtype=torch.float64)
secont condition:: tensor(3.8989e-09, dtype=torch.float64)
curr_secont condition:: tensor(5.2972e-09, dtype=torch.float64)
secont condition:: tensor(5.2972e-09, dtype=torch.float64)
curr_secont condition:: tensor(6.0772e-09, dtype=torch.float64)
secont condition:: tensor(6.0772e-09, dtype=torch.float64)
curr_secont condition:: tensor(5.5263e-09, dtype=torch.float64)
secont condition:: tensor(5.5263e-09, dtype=torch.float64)
curr_secont condition:: tensor(5.2233e-09, dtype=torch.float64)
secont condition:: tensor(5.2233e-09, dtype=torch.float64)
curr_secont condition:: tensor(5.1816e-09, dtype=torch.float64)
secont condition:: tensor(5.1816e-09, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(4.6654e-09, dtype=torch.float64)
secont condition:: tensor(4.6654e-09, dtype=torch.float64)
curr_secont condition:: tensor(6.4992e-09, dtype=torch.float64)
secont condition:: tensor(6.4992e-09, dtype=torch.float64)
curr_secont condition:: tensor(5.4311e-09, dtype=torch.float64)
secont condition:: tensor(5.4311e-09, dtype=torch.float64)
curr_secont condition:: tensor(7.8629e-09, dtype=torch.float64)
secont condition:: tensor(7.8629e-09, dtype=torch.float64)
curr_secont condition:: tensor(7.6105e-09, dtype=torch.float64)
secont condition:: tensor(7.6105e-09, dtype=torch.float64)
curr_secont condition:: tensor(8.0630e-09, dtype=torch.float64)
secont condition:: tensor(8.0630e-09, dtype=torch.float64)
curr_secont condition:: tensor(9.2857e-09, dtype=torch.float64)
secont condition:: tensor(9.2857e-09, dtype=torch.float64)
curr_secont condition:: tensor(6.6305e-09, dtype=torch.float64)
secont condition:: tensor(6.6305e-09, dtype=torch.float64)
curr_secont condition:: tensor(5.7006e-09, dtype=torch.float64)
secont condition:: tensor(5.7006e-09, dtype=torch.float64)
curr_secont condition:: tensor(6.4113e-09, dtype=torch.float64)
secont condition:: tensor(6.4113e-09, dtype=torch.float64)
curr_secont condition:: tensor(5.6600e-09, dtype=torch.float64)
secont condition:: tensor(5.6600e-09, dtype=torch.float64)
curr_secont condition:: tensor(4.3999e-09, dtype=torch.float64)
secont condition:: tensor(4.3999e-09, dtype=torch.float64)
curr_secont condition:: tensor(4.0520e-09, dtype=torch.float64)
secont condition:: tensor(4.0520e-09, dtype=torch.float64)
curr_secont condition:: tensor(5.9184e-09, dtype=torch.float64)
secont condition:: tensor(5.9184e-09, dtype=torch.float64)
curr_secont condition:: tensor(6.1484e-09, dtype=torch.float64)
secont condition:: tensor(6.1484e-09, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(4.8544e-09, dtype=torch.float64)
secont condition:: tensor(4.8544e-09, dtype=torch.float64)
curr_secont condition:: tensor(5.2101e-09, dtype=torch.float64)
secont condition:: tensor(5.2101e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.0301e-08, dtype=torch.float64)
secont condition:: tensor(1.0301e-08, dtype=torch.float64)
curr_secont condition:: tensor(7.7715e-09, dtype=torch.float64)
secont condition:: tensor(7.7715e-09, dtype=torch.float64)
curr_secont condition:: tensor(8.6578e-09, dtype=torch.float64)
secont condition:: tensor(8.6578e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.0997e-08, dtype=torch.float64)
secont condition:: tensor(1.0997e-08, dtype=torch.float64)
curr_secont condition:: tensor(9.7515e-09, dtype=torch.float64)
secont condition:: tensor(9.7515e-09, dtype=torch.float64)
curr_secont condition:: tensor(7.8544e-09, dtype=torch.float64)
secont condition:: tensor(7.8544e-09, dtype=torch.float64)
curr_secont condition:: tensor(5.6830e-09, dtype=torch.float64)
secont condition:: tensor(5.6830e-09, dtype=torch.float64)
curr_secont condition:: tensor(6.6393e-09, dtype=torch.float64)
secont condition:: tensor(6.6393e-09, dtype=torch.float64)
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(1.7348e-08, dtype=torch.float64)
secont condition:: tensor(1.7348e-08, dtype=torch.float64)
curr_secont condition:: tensor(3.6846e-08, dtype=torch.float64)
secont condition:: tensor(3.6846e-08, dtype=torch.float64)
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(5.7210e-08, dtype=torch.float64)
secont condition:: tensor(5.7210e-08, dtype=torch.float64)
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(3.4252e-08, dtype=torch.float64)
secont condition:: tensor(3.4252e-08, dtype=torch.float64)
curr_secont condition:: tensor(4.5321e-08, dtype=torch.float64)
secont condition:: tensor(4.5321e-08, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(4.0826e-08, dtype=torch.float64)
secont condition:: tensor(4.0826e-08, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(4.1159e-08, dtype=torch.float64)
secont condition:: tensor(4.1159e-08, dtype=torch.float64)
curr_secont condition:: tensor(4.9047e-08, dtype=torch.float64)
secont condition:: tensor(4.9047e-08, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 42.18288826942444
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(3.4461e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(3.1140e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(7.8387e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(8.7412e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0024, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.702100
repetition 4
baseline::
max_epoch:: 8
delta_size:: 60
max_epoch:: 8
batch_size:: 4096
Traceback (most recent call last):
  File "incremental_updates_base_line_skipnet.py", line 239, in <module>
    model, count, exp_para_list_all_epochs, exp_gradient_list_all_epochs = model_update_standard_lib_skipnet(max_epoch, dataset_train, model, random_ids_multi_super_iterations, selected_rows, batch_size, learning_rate_all_epochs, criterion, optimizer, para_list_all_epochs, gradient_list_all_epochs, is_GPU, device, record_params, all_ids_list_all_epochs)
  File "/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Benchmark_experiments/benchmark_exp.py", line 788, in model_update_standard_lib_skipnet
    curr_matched_ids = get_subset_data_per_epoch_skipnet(curr_rand_ids, selected_rows_set, all_ids_list_all_epochs[count])
  File "/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Benchmark_experiments/benchmark_exp.py", line 428, in get_subset_data_per_epoch_skipnet
    max_id_list.append(max(curr_removed_ids))
ValueError: max() arg is an empty sequence
incremental updates::
max_epoch:: 8
delta_size:: 60
max_epoch:: 8
Traceback (most recent call last):
  File "incremental_updates_provenance5_skipnet.py", line 479, in <module>
    model_para_list = model_update_provenance_test1_3_skipnet(period, 1, init_epochs, None, None, exp_gradient_list_all_epochs, exp_para_list_all_epochs, dataset_train, model, model_cp, gradient_list_all_epochs, para_list_all_epochs, max_epoch, delta_data_ids, 2, learning_rate_all_epochs, random_ids_multi_super_iterations, sorted_ids_multi_super_iterations, batch_size, dim, criterion, optimizer, lr_scheduler, regularization_coeff, is_GPU, device, all_ids_list_all_epochs)
  File "/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Benchmark_experiments/benchmark_exp.py", line 5511, in model_update_provenance_test1_3_skipnet
    all_curr_removed_ids_list = get_remaining_subset_data_per_epoch_skipnet(curr_rand_ids, curr_matched_ids, all_ids_list_all_epochs[i])
  File "/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Benchmark_experiments/benchmark_exp.py", line 510, in get_remaining_subset_data_per_epoch_skipnet
    max_id_list.append(max(curr_removed_ids))
ValueError: max() arg is an empty sequence
repetition 5
baseline::
max_epoch:: 8
delta_size:: 60
max_epoch:: 8
batch_size:: 4096
Traceback (most recent call last):
  File "incremental_updates_base_line_skipnet.py", line 239, in <module>
    model, count, exp_para_list_all_epochs, exp_gradient_list_all_epochs = model_update_standard_lib_skipnet(max_epoch, dataset_train, model, random_ids_multi_super_iterations, selected_rows, batch_size, learning_rate_all_epochs, criterion, optimizer, para_list_all_epochs, gradient_list_all_epochs, is_GPU, device, record_params, all_ids_list_all_epochs)
  File "/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Benchmark_experiments/benchmark_exp.py", line 788, in model_update_standard_lib_skipnet
    curr_matched_ids = get_subset_data_per_epoch_skipnet(curr_rand_ids, selected_rows_set, all_ids_list_all_epochs[count])
  File "/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Benchmark_experiments/benchmark_exp.py", line 428, in get_subset_data_per_epoch_skipnet
    max_id_list.append(max(curr_removed_ids))
ValueError: max() arg is an empty sequence
incremental updates::
max_epoch:: 8
delta_size:: 60
max_epoch:: 8
Traceback (most recent call last):
  File "incremental_updates_provenance5_skipnet.py", line 479, in <module>
    model_para_list = model_update_provenance_test1_3_skipnet(period, 1, init_epochs, None, None, exp_gradient_list_all_epochs, exp_para_list_all_epochs, dataset_train, model, model_cp, gradient_list_all_epochs, para_list_all_epochs, max_epoch, delta_data_ids, 2, learning_rate_all_epochs, random_ids_multi_super_iterations, sorted_ids_multi_super_iterations, batch_size, dim, criterion, optimizer, lr_scheduler, regularization_coeff, is_GPU, device, all_ids_list_all_epochs)
  File "/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Benchmark_experiments/benchmark_exp.py", line 5511, in model_update_provenance_test1_3_skipnet
    all_curr_removed_ids_list = get_remaining_subset_data_per_epoch_skipnet(curr_rand_ids, curr_matched_ids, all_ids_list_all_epochs[i])
  File "/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Benchmark_experiments/benchmark_exp.py", line 510, in get_remaining_subset_data_per_epoch_skipnet
    max_id_list.append(max(curr_removed_ids))
ValueError: max() arg is an empty sequence
deletion rate:: 0.002
repetition 1
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 120
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 69.41640448570251
time_baseline:: 69.43967723846436
curr_diff: 0 tensor(0.0023, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0022, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0032, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.701300
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 120
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(1.6090e-09, dtype=torch.float64)
secont condition:: tensor(1.6090e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.2585e-09, dtype=torch.float64)
secont condition:: tensor(2.2585e-09, dtype=torch.float64)
curr_secont condition:: tensor(3.5742e-09, dtype=torch.float64)
secont condition:: tensor(3.5742e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.7779e-09, dtype=torch.float64)
secont condition:: tensor(2.7779e-09, dtype=torch.float64)
curr_secont condition:: tensor(7.8342e-09, dtype=torch.float64)
secont condition:: tensor(7.8342e-09, dtype=torch.float64)
curr_secont condition:: tensor(7.9468e-09, dtype=torch.float64)
secont condition:: tensor(7.9468e-09, dtype=torch.float64)
curr_secont condition:: tensor(7.2321e-09, dtype=torch.float64)
secont condition:: tensor(7.2321e-09, dtype=torch.float64)
curr_secont condition:: tensor(9.2923e-09, dtype=torch.float64)
secont condition:: tensor(9.2923e-09, dtype=torch.float64)
curr_secont condition:: tensor(8.4200e-09, dtype=torch.float64)
secont condition:: tensor(8.4200e-09, dtype=torch.float64)
curr_secont condition:: tensor(8.6416e-09, dtype=torch.float64)
secont condition:: tensor(8.6416e-09, dtype=torch.float64)
curr_secont condition:: tensor(6.2035e-09, dtype=torch.float64)
secont condition:: tensor(6.2035e-09, dtype=torch.float64)
curr_secont condition:: tensor(8.4189e-09, dtype=torch.float64)
secont condition:: tensor(8.4189e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.2674e-08, dtype=torch.float64)
secont condition:: tensor(1.2674e-08, dtype=torch.float64)
curr_secont condition:: tensor(1.1088e-08, dtype=torch.float64)
secont condition:: tensor(1.1088e-08, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(1.5901e-08, dtype=torch.float64)
secont condition:: tensor(1.5901e-08, dtype=torch.float64)
curr_secont condition:: tensor(1.4644e-08, dtype=torch.float64)
secont condition:: tensor(1.4644e-08, dtype=torch.float64)
curr_secont condition:: tensor(1.0930e-08, dtype=torch.float64)
secont condition:: tensor(1.0930e-08, dtype=torch.float64)
curr_secont condition:: tensor(9.7006e-09, dtype=torch.float64)
secont condition:: tensor(9.7006e-09, dtype=torch.float64)
curr_secont condition:: tensor(8.3624e-09, dtype=torch.float64)
secont condition:: tensor(8.3624e-09, dtype=torch.float64)
curr_secont condition:: tensor(8.2694e-09, dtype=torch.float64)
secont condition:: tensor(8.2694e-09, dtype=torch.float64)
curr_secont condition:: tensor(7.5782e-09, dtype=torch.float64)
secont condition:: tensor(7.5782e-09, dtype=torch.float64)
curr_secont condition:: tensor(8.4689e-09, dtype=torch.float64)
secont condition:: tensor(8.4689e-09, dtype=torch.float64)
curr_secont condition:: tensor(9.3351e-09, dtype=torch.float64)
secont condition:: tensor(9.3351e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.2669e-08, dtype=torch.float64)
secont condition:: tensor(1.2669e-08, dtype=torch.float64)
curr_secont condition:: tensor(1.0896e-08, dtype=torch.float64)
secont condition:: tensor(1.0896e-08, dtype=torch.float64)
curr_secont condition:: tensor(1.1930e-08, dtype=torch.float64)
secont condition:: tensor(1.1930e-08, dtype=torch.float64)
curr_secont condition:: tensor(1.2880e-08, dtype=torch.float64)
secont condition:: tensor(1.2880e-08, dtype=torch.float64)
curr_secont condition:: tensor(1.2238e-08, dtype=torch.float64)
secont condition:: tensor(1.2238e-08, dtype=torch.float64)
curr_secont condition:: tensor(9.4112e-09, dtype=torch.float64)
secont condition:: tensor(9.4112e-09, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(2.1385e-08, dtype=torch.float64)
secont condition:: tensor(2.1385e-08, dtype=torch.float64)
curr_secont condition:: tensor(1.9810e-08, dtype=torch.float64)
secont condition:: tensor(1.9810e-08, dtype=torch.float64)
curr_secont condition:: tensor(1.5379e-08, dtype=torch.float64)
secont condition:: tensor(1.5379e-08, dtype=torch.float64)
curr_secont condition:: tensor(1.2556e-08, dtype=torch.float64)
secont condition:: tensor(1.2556e-08, dtype=torch.float64)
curr_secont condition:: tensor(1.1845e-08, dtype=torch.float64)
secont condition:: tensor(1.1845e-08, dtype=torch.float64)
curr_secont condition:: tensor(1.4409e-08, dtype=torch.float64)
secont condition:: tensor(1.4409e-08, dtype=torch.float64)
curr_secont condition:: tensor(1.3101e-08, dtype=torch.float64)
secont condition:: tensor(1.3101e-08, dtype=torch.float64)
curr_secont condition:: tensor(1.1650e-08, dtype=torch.float64)
secont condition:: tensor(1.1650e-08, dtype=torch.float64)
curr_secont condition:: tensor(1.1915e-08, dtype=torch.float64)
secont condition:: tensor(1.1915e-08, dtype=torch.float64)
curr_secont condition:: tensor(1.1661e-08, dtype=torch.float64)
secont condition:: tensor(1.1661e-08, dtype=torch.float64)
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(3.2784e-08, dtype=torch.float64)
secont condition:: tensor(3.2784e-08, dtype=torch.float64)
curr_secont condition:: tensor(5.0327e-08, dtype=torch.float64)
secont condition:: tensor(5.0327e-08, dtype=torch.float64)
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(4.0373e-08, dtype=torch.float64)
secont condition:: tensor(4.0373e-08, dtype=torch.float64)
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(7.9426e-08, dtype=torch.float64)
secont condition:: tensor(7.9426e-08, dtype=torch.float64)
curr_secont condition:: tensor(7.4161e-08, dtype=torch.float64)
secont condition:: tensor(7.4161e-08, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(9.5723e-08, dtype=torch.float64)
secont condition:: tensor(9.5723e-08, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(6.4596e-08, dtype=torch.float64)
secont condition:: tensor(6.4596e-08, dtype=torch.float64)
curr_secont condition:: tensor(6.6439e-08, dtype=torch.float64)
secont condition:: tensor(6.6439e-08, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 50.896952390670776
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(4.0670e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(4.1512e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0024, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0023, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0033, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.700900
repetition 2
baseline::
max_epoch:: 8
delta_size:: 120
max_epoch:: 8
batch_size:: 4096
Traceback (most recent call last):
  File "incremental_updates_base_line_skipnet.py", line 239, in <module>
    model, count, exp_para_list_all_epochs, exp_gradient_list_all_epochs = model_update_standard_lib_skipnet(max_epoch, dataset_train, model, random_ids_multi_super_iterations, selected_rows, batch_size, learning_rate_all_epochs, criterion, optimizer, para_list_all_epochs, gradient_list_all_epochs, is_GPU, device, record_params, all_ids_list_all_epochs)
  File "/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Benchmark_experiments/benchmark_exp.py", line 788, in model_update_standard_lib_skipnet
    curr_matched_ids = get_subset_data_per_epoch_skipnet(curr_rand_ids, selected_rows_set, all_ids_list_all_epochs[count])
  File "/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Benchmark_experiments/benchmark_exp.py", line 428, in get_subset_data_per_epoch_skipnet
    max_id_list.append(max(curr_removed_ids))
ValueError: max() arg is an empty sequence
incremental updates::
max_epoch:: 8
delta_size:: 120
max_epoch:: 8
Traceback (most recent call last):
  File "incremental_updates_provenance5_skipnet.py", line 479, in <module>
    model_para_list = model_update_provenance_test1_3_skipnet(period, 1, init_epochs, None, None, exp_gradient_list_all_epochs, exp_para_list_all_epochs, dataset_train, model, model_cp, gradient_list_all_epochs, para_list_all_epochs, max_epoch, delta_data_ids, 2, learning_rate_all_epochs, random_ids_multi_super_iterations, sorted_ids_multi_super_iterations, batch_size, dim, criterion, optimizer, lr_scheduler, regularization_coeff, is_GPU, device, all_ids_list_all_epochs)
  File "/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Benchmark_experiments/benchmark_exp.py", line 5511, in model_update_provenance_test1_3_skipnet
    all_curr_removed_ids_list = get_remaining_subset_data_per_epoch_skipnet(curr_rand_ids, curr_matched_ids, all_ids_list_all_epochs[i])
  File "/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Benchmark_experiments/benchmark_exp.py", line 510, in get_remaining_subset_data_per_epoch_skipnet
    max_id_list.append(max(curr_removed_ids))
ValueError: max() arg is an empty sequence
repetition 3
baseline::
max_epoch:: 8
delta_size:: 120
max_epoch:: 8
batch_size:: 4096
Traceback (most recent call last):
  File "incremental_updates_base_line_skipnet.py", line 239, in <module>
    model, count, exp_para_list_all_epochs, exp_gradient_list_all_epochs = model_update_standard_lib_skipnet(max_epoch, dataset_train, model, random_ids_multi_super_iterations, selected_rows, batch_size, learning_rate_all_epochs, criterion, optimizer, para_list_all_epochs, gradient_list_all_epochs, is_GPU, device, record_params, all_ids_list_all_epochs)
  File "/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Benchmark_experiments/benchmark_exp.py", line 788, in model_update_standard_lib_skipnet
    curr_matched_ids = get_subset_data_per_epoch_skipnet(curr_rand_ids, selected_rows_set, all_ids_list_all_epochs[count])
  File "/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Benchmark_experiments/benchmark_exp.py", line 428, in get_subset_data_per_epoch_skipnet
    max_id_list.append(max(curr_removed_ids))
ValueError: max() arg is an empty sequence
incremental updates::
max_epoch:: 8
delta_size:: 120
max_epoch:: 8
Traceback (most recent call last):
  File "incremental_updates_provenance5_skipnet.py", line 479, in <module>
    model_para_list = model_update_provenance_test1_3_skipnet(period, 1, init_epochs, None, None, exp_gradient_list_all_epochs, exp_para_list_all_epochs, dataset_train, model, model_cp, gradient_list_all_epochs, para_list_all_epochs, max_epoch, delta_data_ids, 2, learning_rate_all_epochs, random_ids_multi_super_iterations, sorted_ids_multi_super_iterations, batch_size, dim, criterion, optimizer, lr_scheduler, regularization_coeff, is_GPU, device, all_ids_list_all_epochs)
  File "/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Benchmark_experiments/benchmark_exp.py", line 5511, in model_update_provenance_test1_3_skipnet
    all_curr_removed_ids_list = get_remaining_subset_data_per_epoch_skipnet(curr_rand_ids, curr_matched_ids, all_ids_list_all_epochs[i])
  File "/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Benchmark_experiments/benchmark_exp.py", line 510, in get_remaining_subset_data_per_epoch_skipnet
    max_id_list.append(max(curr_removed_ids))
ValueError: max() arg is an empty sequence
repetition 4
baseline::
max_epoch:: 8
delta_size:: 120
max_epoch:: 8
batch_size:: 4096
Traceback (most recent call last):
  File "incremental_updates_base_line_skipnet.py", line 239, in <module>
    model, count, exp_para_list_all_epochs, exp_gradient_list_all_epochs = model_update_standard_lib_skipnet(max_epoch, dataset_train, model, random_ids_multi_super_iterations, selected_rows, batch_size, learning_rate_all_epochs, criterion, optimizer, para_list_all_epochs, gradient_list_all_epochs, is_GPU, device, record_params, all_ids_list_all_epochs)
  File "/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Benchmark_experiments/benchmark_exp.py", line 788, in model_update_standard_lib_skipnet
    curr_matched_ids = get_subset_data_per_epoch_skipnet(curr_rand_ids, selected_rows_set, all_ids_list_all_epochs[count])
  File "/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Benchmark_experiments/benchmark_exp.py", line 428, in get_subset_data_per_epoch_skipnet
    max_id_list.append(max(curr_removed_ids))
ValueError: max() arg is an empty sequence
incremental updates::
max_epoch:: 8
delta_size:: 120
max_epoch:: 8
Traceback (most recent call last):
  File "incremental_updates_provenance5_skipnet.py", line 479, in <module>
    model_para_list = model_update_provenance_test1_3_skipnet(period, 1, init_epochs, None, None, exp_gradient_list_all_epochs, exp_para_list_all_epochs, dataset_train, model, model_cp, gradient_list_all_epochs, para_list_all_epochs, max_epoch, delta_data_ids, 2, learning_rate_all_epochs, random_ids_multi_super_iterations, sorted_ids_multi_super_iterations, batch_size, dim, criterion, optimizer, lr_scheduler, regularization_coeff, is_GPU, device, all_ids_list_all_epochs)
  File "/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Benchmark_experiments/benchmark_exp.py", line 5511, in model_update_provenance_test1_3_skipnet
    all_curr_removed_ids_list = get_remaining_subset_data_per_epoch_skipnet(curr_rand_ids, curr_matched_ids, all_ids_list_all_epochs[i])
  File "/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Benchmark_experiments/benchmark_exp.py", line 510, in get_remaining_subset_data_per_epoch_skipnet
    max_id_list.append(max(curr_removed_ids))
ValueError: max() arg is an empty sequence
repetition 5
baseline::
max_epoch:: 8
delta_size:: 120
max_epoch:: 8
batch_size:: 4096
Traceback (most recent call last):
  File "incremental_updates_base_line_skipnet.py", line 239, in <module>
    model, count, exp_para_list_all_epochs, exp_gradient_list_all_epochs = model_update_standard_lib_skipnet(max_epoch, dataset_train, model, random_ids_multi_super_iterations, selected_rows, batch_size, learning_rate_all_epochs, criterion, optimizer, para_list_all_epochs, gradient_list_all_epochs, is_GPU, device, record_params, all_ids_list_all_epochs)
  File "/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Benchmark_experiments/benchmark_exp.py", line 788, in model_update_standard_lib_skipnet
    curr_matched_ids = get_subset_data_per_epoch_skipnet(curr_rand_ids, selected_rows_set, all_ids_list_all_epochs[count])
  File "/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Benchmark_experiments/benchmark_exp.py", line 428, in get_subset_data_per_epoch_skipnet
    max_id_list.append(max(curr_removed_ids))
ValueError: max() arg is an empty sequence
incremental updates::
max_epoch:: 8
delta_size:: 120
max_epoch:: 8
Traceback (most recent call last):
  File "incremental_updates_provenance5_skipnet.py", line 479, in <module>
    model_para_list = model_update_provenance_test1_3_skipnet(period, 1, init_epochs, None, None, exp_gradient_list_all_epochs, exp_para_list_all_epochs, dataset_train, model, model_cp, gradient_list_all_epochs, para_list_all_epochs, max_epoch, delta_data_ids, 2, learning_rate_all_epochs, random_ids_multi_super_iterations, sorted_ids_multi_super_iterations, batch_size, dim, criterion, optimizer, lr_scheduler, regularization_coeff, is_GPU, device, all_ids_list_all_epochs)
  File "/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Benchmark_experiments/benchmark_exp.py", line 5511, in model_update_provenance_test1_3_skipnet
    all_curr_removed_ids_list = get_remaining_subset_data_per_epoch_skipnet(curr_rand_ids, curr_matched_ids, all_ids_list_all_epochs[i])
  File "/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Benchmark_experiments/benchmark_exp.py", line 510, in get_remaining_subset_data_per_epoch_skipnet
    max_id_list.append(max(curr_removed_ids))
ValueError: max() arg is an empty sequence
varied epochs::
epochs:: 4
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type MSELoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
cuda:2
Train - Epoch 0, Batch: 0, Loss: 0.091544
Train - Epoch 0, Batch: 10, Loss: 0.090281
Test Avg. Loss: 0.000027, Accuracy: 0.326300
Train - Epoch 1, Batch: 0, Loss: 0.089719
Train - Epoch 1, Batch: 10, Loss: 0.089165
Test Avg. Loss: 0.000026, Accuracy: 0.574700
Train - Epoch 2, Batch: 0, Loss: 0.088606
Train - Epoch 2, Batch: 10, Loss: 0.088131
Test Avg. Loss: 0.000026, Accuracy: 0.640000
Train - Epoch 3, Batch: 0, Loss: 0.087413
Train - Epoch 3, Batch: 10, Loss: 0.086967
Test Avg. Loss: 0.000025, Accuracy: 0.657200
training_time:: 31.654379844665527
repetition 1
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 4
delta_size:: 1
max_epoch:: 4
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
training time is 24.987493991851807
time_baseline:: 24.997074604034424
curr_diff: 0 tensor(9.5657e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(6.3182e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(7.4099e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(8.4390e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000025, Accuracy: 0.657200
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 4
delta_size:: 1
max_epoch:: 4
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(6.0348e-33, dtype=torch.float64)
secont condition:: tensor(6.0348e-33, dtype=torch.float64)
curr_secont condition:: tensor(1.4958e-32, dtype=torch.float64)
secont condition:: tensor(1.4958e-32, dtype=torch.float64)
curr_secont condition:: tensor(2.6747e-32, dtype=torch.float64)
secont condition:: tensor(2.6747e-32, dtype=torch.float64)
curr_secont condition:: tensor(4.1223e-32, dtype=torch.float64)
secont condition:: tensor(4.1223e-32, dtype=torch.float64)
curr_secont condition:: tensor(5.8547e-32, dtype=torch.float64)
secont condition:: tensor(5.8547e-32, dtype=torch.float64)
curr_secont condition:: tensor(7.8546e-32, dtype=torch.float64)
secont condition:: tensor(7.8546e-32, dtype=torch.float64)
curr_secont condition:: tensor(1.0110e-31, dtype=torch.float64)
secont condition:: tensor(1.0110e-31, dtype=torch.float64)
curr_secont condition:: tensor(1.2650e-31, dtype=torch.float64)
secont condition:: tensor(1.2650e-31, dtype=torch.float64)
curr_secont condition:: tensor(1.5452e-31, dtype=torch.float64)
secont condition:: tensor(1.5452e-31, dtype=torch.float64)
curr_secont condition:: tensor(1.8514e-31, dtype=torch.float64)
secont condition:: tensor(1.8514e-31, dtype=torch.float64)
curr_secont condition:: tensor(2.1853e-31, dtype=torch.float64)
secont condition:: tensor(2.1853e-31, dtype=torch.float64)
curr_secont condition:: tensor(2.5430e-31, dtype=torch.float64)
secont condition:: tensor(2.5430e-31, dtype=torch.float64)
curr_secont condition:: tensor(1.1321e-10, dtype=torch.float64)
secont condition:: tensor(1.1321e-10, dtype=torch.float64)
curr_secont condition:: tensor(8.2859e-11, dtype=torch.float64)
secont condition:: tensor(8.2859e-11, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(6.5622e-11, dtype=torch.float64)
secont condition:: tensor(6.5622e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.9763e-11, dtype=torch.float64)
secont condition:: tensor(4.9763e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.5158e-11, dtype=torch.float64)
secont condition:: tensor(3.5158e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.2929e-11, dtype=torch.float64)
secont condition:: tensor(3.2929e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.3216e-11, dtype=torch.float64)
secont condition:: tensor(2.3216e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.2030e-11, dtype=torch.float64)
secont condition:: tensor(2.2030e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.1242e-11, dtype=torch.float64)
secont condition:: tensor(2.1242e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.2369e-11, dtype=torch.float64)
secont condition:: tensor(3.2369e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.1309e-11, dtype=torch.float64)
secont condition:: tensor(1.1309e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.5826e-11, dtype=torch.float64)
secont condition:: tensor(1.5826e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.4638e-11, dtype=torch.float64)
secont condition:: tensor(1.4638e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.5902e-12, dtype=torch.float64)
secont condition:: tensor(6.5902e-12, dtype=torch.float64)
curr_secont condition:: tensor(9.1573e-12, dtype=torch.float64)
secont condition:: tensor(9.1573e-12, dtype=torch.float64)
curr_secont condition:: tensor(6.7365e-12, dtype=torch.float64)
secont condition:: tensor(6.7365e-12, dtype=torch.float64)
curr_secont condition:: tensor(3.7440e-12, dtype=torch.float64)
secont condition:: tensor(3.7440e-12, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(8.5569e-12, dtype=torch.float64)
secont condition:: tensor(8.5569e-12, dtype=torch.float64)
curr_secont condition:: tensor(-4.8630e-12, dtype=torch.float64)
curr_secont condition:: tensor(-9.5290e-13, dtype=torch.float64)
curr_secont condition:: tensor(8.5327e-12, dtype=torch.float64)
secont condition:: tensor(8.5327e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.0919e-11, dtype=torch.float64)
secont condition:: tensor(1.0919e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.4087e-12, dtype=torch.float64)
secont condition:: tensor(2.4087e-12, dtype=torch.float64)
curr_secont condition:: tensor(7.3398e-12, dtype=torch.float64)
secont condition:: tensor(7.3398e-12, dtype=torch.float64)
curr_secont condition:: tensor(3.1294e-11, dtype=torch.float64)
secont condition:: tensor(3.1294e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.2845e-12, dtype=torch.float64)
secont condition:: tensor(4.2845e-12, dtype=torch.float64)
curr_secont condition:: tensor(6.7861e-12, dtype=torch.float64)
secont condition:: tensor(6.7861e-12, dtype=torch.float64)
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(1.2413e-10, dtype=torch.float64)
secont condition:: tensor(1.2413e-10, dtype=torch.float64)
curr_secont condition:: tensor(9.5637e-11, dtype=torch.float64)
secont condition:: tensor(9.5637e-11, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 20.974924087524414
curr_diff: 0 tensor(4.7658e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(2.1691e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(1.4821e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.9814e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.9996e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(8.4376e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(6.2399e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(7.7226e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.0240e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000026, Accuracy: 0.656600
repetition 2
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 4
delta_size:: 1
max_epoch:: 4
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
training time is 25.186065912246704
time_baseline:: 25.195396661758423
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(4.5187e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(7.9770e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(3.1687e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000025, Accuracy: 0.657200
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 4
delta_size:: 1
max_epoch:: 4
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(6.0348e-33, dtype=torch.float64)
secont condition:: tensor(6.0348e-33, dtype=torch.float64)
curr_secont condition:: tensor(1.4958e-32, dtype=torch.float64)
secont condition:: tensor(1.4958e-32, dtype=torch.float64)
curr_secont condition:: tensor(2.6747e-32, dtype=torch.float64)
secont condition:: tensor(2.6747e-32, dtype=torch.float64)
curr_secont condition:: tensor(4.1223e-32, dtype=torch.float64)
secont condition:: tensor(4.1223e-32, dtype=torch.float64)
curr_secont condition:: tensor(5.8547e-32, dtype=torch.float64)
secont condition:: tensor(5.8547e-32, dtype=torch.float64)
curr_secont condition:: tensor(7.8546e-32, dtype=torch.float64)
secont condition:: tensor(7.8546e-32, dtype=torch.float64)
curr_secont condition:: tensor(1.0110e-31, dtype=torch.float64)
secont condition:: tensor(1.0110e-31, dtype=torch.float64)
curr_secont condition:: tensor(1.2650e-31, dtype=torch.float64)
secont condition:: tensor(1.2650e-31, dtype=torch.float64)
curr_secont condition:: tensor(1.5452e-31, dtype=torch.float64)
secont condition:: tensor(1.5452e-31, dtype=torch.float64)
curr_secont condition:: tensor(1.8514e-31, dtype=torch.float64)
secont condition:: tensor(1.8514e-31, dtype=torch.float64)
curr_secont condition:: tensor(2.1853e-31, dtype=torch.float64)
secont condition:: tensor(2.1853e-31, dtype=torch.float64)
curr_secont condition:: tensor(2.5430e-31, dtype=torch.float64)
secont condition:: tensor(2.5430e-31, dtype=torch.float64)
curr_secont condition:: tensor(2.9246e-31, dtype=torch.float64)
secont condition:: tensor(2.9246e-31, dtype=torch.float64)
curr_secont condition:: tensor(3.3358e-31, dtype=torch.float64)
secont condition:: tensor(3.3358e-31, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(5.6939e-11, dtype=torch.float64)
secont condition:: tensor(5.6939e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.8360e-11, dtype=torch.float64)
secont condition:: tensor(4.8360e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.5125e-11, dtype=torch.float64)
secont condition:: tensor(4.5125e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.3294e-11, dtype=torch.float64)
secont condition:: tensor(3.3294e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.0369e-11, dtype=torch.float64)
secont condition:: tensor(3.0369e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.7369e-11, dtype=torch.float64)
secont condition:: tensor(2.7369e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.3201e-11, dtype=torch.float64)
secont condition:: tensor(2.3201e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.7604e-11, dtype=torch.float64)
secont condition:: tensor(4.7604e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.7514e-11, dtype=torch.float64)
secont condition:: tensor(3.7514e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.3191e-11, dtype=torch.float64)
secont condition:: tensor(3.3191e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.0442e-11, dtype=torch.float64)
secont condition:: tensor(3.0442e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.6481e-11, dtype=torch.float64)
secont condition:: tensor(2.6481e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.9783e-11, dtype=torch.float64)
secont condition:: tensor(1.9783e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.5442e-11, dtype=torch.float64)
secont condition:: tensor(2.5442e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.8683e-11, dtype=torch.float64)
secont condition:: tensor(1.8683e-11, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(1.8994e-11, dtype=torch.float64)
secont condition:: tensor(1.8994e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.7127e-11, dtype=torch.float64)
secont condition:: tensor(1.7127e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.7283e-11, dtype=torch.float64)
secont condition:: tensor(1.7283e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.1321e-11, dtype=torch.float64)
secont condition:: tensor(1.1321e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.1697e-11, dtype=torch.float64)
secont condition:: tensor(1.1697e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.4810e-11, dtype=torch.float64)
secont condition:: tensor(1.4810e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.8402e-11, dtype=torch.float64)
secont condition:: tensor(1.8402e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.3362e-12, dtype=torch.float64)
secont condition:: tensor(2.3362e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.3090e-11, dtype=torch.float64)
secont condition:: tensor(1.3090e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.6346e-11, dtype=torch.float64)
secont condition:: tensor(3.6346e-11, dtype=torch.float64)
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(9.6356e-12, dtype=torch.float64)
secont condition:: tensor(9.6356e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.4844e-11, dtype=torch.float64)
explicit_evaluation epoch:: 50
curr_secont condition:: tensor(5.3710e-10, dtype=torch.float64)
explicit_evaluation epoch:: 51
curr_secont condition:: tensor(1.5783e-10, dtype=torch.float64)
explicit_evaluation epoch:: 52
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 22.342910766601562
curr_diff: 0 tensor(6.9496e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(3.2131e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(2.1691e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(4.0377e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.2985e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(8.9961e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(4.3222e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(8.7210e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(7.0082e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000026, Accuracy: 0.646000
repetition 3
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 4
delta_size:: 1
max_epoch:: 4
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
training time is 25.435736656188965
time_baseline:: 25.44748067855835
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(7.8360e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(4.7701e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000025, Accuracy: 0.657200
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 4
delta_size:: 1
max_epoch:: 4
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(6.0348e-33, dtype=torch.float64)
secont condition:: tensor(6.0348e-33, dtype=torch.float64)
curr_secont condition:: tensor(1.4958e-32, dtype=torch.float64)
secont condition:: tensor(1.4958e-32, dtype=torch.float64)
curr_secont condition:: tensor(2.6747e-32, dtype=torch.float64)
secont condition:: tensor(2.6747e-32, dtype=torch.float64)
curr_secont condition:: tensor(4.1223e-32, dtype=torch.float64)
secont condition:: tensor(4.1223e-32, dtype=torch.float64)
curr_secont condition:: tensor(5.8547e-32, dtype=torch.float64)
secont condition:: tensor(5.8547e-32, dtype=torch.float64)
curr_secont condition:: tensor(7.8546e-32, dtype=torch.float64)
secont condition:: tensor(7.8546e-32, dtype=torch.float64)
curr_secont condition:: tensor(1.0112e-31, dtype=torch.float64)
secont condition:: tensor(1.0112e-31, dtype=torch.float64)
curr_secont condition:: tensor(7.4521e-10, dtype=torch.float64)
secont condition:: tensor(7.4521e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.5873e-10, dtype=torch.float64)
secont condition:: tensor(4.5873e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.7505e-10, dtype=torch.float64)
secont condition:: tensor(3.7505e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.7638e-10, dtype=torch.float64)
secont condition:: tensor(3.7638e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.4651e-10, dtype=torch.float64)
secont condition:: tensor(2.4651e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.3982e-10, dtype=torch.float64)
secont condition:: tensor(2.3982e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.1788e-10, dtype=torch.float64)
secont condition:: tensor(2.1788e-10, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(1.3246e-10, dtype=torch.float64)
secont condition:: tensor(1.3246e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.5812e-10, dtype=torch.float64)
secont condition:: tensor(1.5812e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.4179e-10, dtype=torch.float64)
secont condition:: tensor(1.4179e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.7808e-10, dtype=torch.float64)
secont condition:: tensor(3.7808e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.6986e-10, dtype=torch.float64)
secont condition:: tensor(2.6986e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.4649e-10, dtype=torch.float64)
secont condition:: tensor(2.4649e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.7053e-10, dtype=torch.float64)
secont condition:: tensor(1.7053e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.0039e-10, dtype=torch.float64)
secont condition:: tensor(2.0039e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.6450e-10, dtype=torch.float64)
secont condition:: tensor(1.6450e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.1765e-10, dtype=torch.float64)
secont condition:: tensor(1.1765e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.1813e-10, dtype=torch.float64)
secont condition:: tensor(1.1813e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.3168e-10, dtype=torch.float64)
secont condition:: tensor(1.3168e-10, dtype=torch.float64)
curr_secont condition:: tensor(9.8920e-11, dtype=torch.float64)
secont condition:: tensor(9.8920e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.0832e-10, dtype=torch.float64)
secont condition:: tensor(1.0832e-10, dtype=torch.float64)
curr_secont condition:: tensor(7.2548e-11, dtype=torch.float64)
secont condition:: tensor(7.2548e-11, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(1.0308e-10, dtype=torch.float64)
secont condition:: tensor(1.0308e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.1995e-10, dtype=torch.float64)
secont condition:: tensor(1.1995e-10, dtype=torch.float64)
curr_secont condition:: tensor(7.1578e-11, dtype=torch.float64)
secont condition:: tensor(7.1578e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.1975e-10, dtype=torch.float64)
secont condition:: tensor(1.1975e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.0392e-10, dtype=torch.float64)
secont condition:: tensor(1.0392e-10, dtype=torch.float64)
curr_secont condition:: tensor(8.7945e-11, dtype=torch.float64)
secont condition:: tensor(8.7945e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.1086e-10, dtype=torch.float64)
secont condition:: tensor(1.1086e-10, dtype=torch.float64)
curr_secont condition:: tensor(9.2506e-11, dtype=torch.float64)
secont condition:: tensor(9.2506e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.3036e-10, dtype=torch.float64)
secont condition:: tensor(1.3036e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.0276e-10, dtype=torch.float64)
secont condition:: tensor(1.0276e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.0281e-10, dtype=torch.float64)
explicit_evaluation epoch:: 43
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(1.0991e-10, dtype=torch.float64)
explicit_evaluation epoch:: 48
curr_secont condition:: tensor(1.6721e-10, dtype=torch.float64)
secont condition:: tensor(1.6721e-10, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 21.545548677444458
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(5.6338e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(9.7797e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.0713e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(8.5446e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(4.9992e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000026, Accuracy: 0.656600
repetition 4
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 4
delta_size:: 1
max_epoch:: 4
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
training time is 25.392592906951904
time_baseline:: 25.40357494354248
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(5.5063e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(9.9658e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(2.3590e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000025, Accuracy: 0.657200
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 4
delta_size:: 1
max_epoch:: 4
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(1.9357e-10, dtype=torch.float64)
secont condition:: tensor(1.9357e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.0683e-10, dtype=torch.float64)
secont condition:: tensor(2.0683e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.6489e-10, dtype=torch.float64)
secont condition:: tensor(1.6489e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.3503e-10, dtype=torch.float64)
secont condition:: tensor(1.3503e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.1926e-10, dtype=torch.float64)
secont condition:: tensor(1.1926e-10, dtype=torch.float64)
curr_secont condition:: tensor(9.2269e-11, dtype=torch.float64)
secont condition:: tensor(9.2269e-11, dtype=torch.float64)
curr_secont condition:: tensor(9.3498e-11, dtype=torch.float64)
secont condition:: tensor(9.3498e-11, dtype=torch.float64)
curr_secont condition:: tensor(7.0727e-11, dtype=torch.float64)
secont condition:: tensor(7.0727e-11, dtype=torch.float64)
curr_secont condition:: tensor(7.9118e-11, dtype=torch.float64)
secont condition:: tensor(7.9118e-11, dtype=torch.float64)
curr_secont condition:: tensor(7.0900e-11, dtype=torch.float64)
secont condition:: tensor(7.0900e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.0914e-11, dtype=torch.float64)
secont condition:: tensor(5.0914e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.1977e-11, dtype=torch.float64)
secont condition:: tensor(5.1977e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.4925e-11, dtype=torch.float64)
secont condition:: tensor(4.4925e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.7050e-11, dtype=torch.float64)
secont condition:: tensor(4.7050e-11, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(3.6735e-11, dtype=torch.float64)
secont condition:: tensor(3.6735e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.8881e-10, dtype=torch.float64)
secont condition:: tensor(1.8881e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.0426e-10, dtype=torch.float64)
secont condition:: tensor(2.0426e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.7836e-10, dtype=torch.float64)
secont condition:: tensor(1.7836e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.2935e-10, dtype=torch.float64)
secont condition:: tensor(1.2935e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.1001e-10, dtype=torch.float64)
secont condition:: tensor(1.1001e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.0240e-10, dtype=torch.float64)
secont condition:: tensor(1.0240e-10, dtype=torch.float64)
curr_secont condition:: tensor(9.6461e-11, dtype=torch.float64)
secont condition:: tensor(9.6461e-11, dtype=torch.float64)
curr_secont condition:: tensor(9.2249e-11, dtype=torch.float64)
secont condition:: tensor(9.2249e-11, dtype=torch.float64)
curr_secont condition:: tensor(8.5555e-11, dtype=torch.float64)
secont condition:: tensor(8.5555e-11, dtype=torch.float64)
curr_secont condition:: tensor(8.4746e-11, dtype=torch.float64)
secont condition:: tensor(8.4746e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.8625e-11, dtype=torch.float64)
secont condition:: tensor(3.8625e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.9758e-11, dtype=torch.float64)
secont condition:: tensor(6.9758e-11, dtype=torch.float64)
curr_secont condition:: tensor(7.9397e-11, dtype=torch.float64)
secont condition:: tensor(7.9397e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.2893e-11, dtype=torch.float64)
secont condition:: tensor(2.2893e-11, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(6.6063e-11, dtype=torch.float64)
secont condition:: tensor(6.6063e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.4061e-11, dtype=torch.float64)
secont condition:: tensor(4.4061e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.4138e-11, dtype=torch.float64)
secont condition:: tensor(6.4138e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.8710e-11, dtype=torch.float64)
secont condition:: tensor(4.8710e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.1887e-11, dtype=torch.float64)
secont condition:: tensor(6.1887e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.9006e-11, dtype=torch.float64)
secont condition:: tensor(4.9006e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.3439e-11, dtype=torch.float64)
secont condition:: tensor(5.3439e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.7880e-11, dtype=torch.float64)
secont condition:: tensor(6.7880e-11, dtype=torch.float64)
curr_secont condition:: tensor(9.5523e-11, dtype=torch.float64)
secont condition:: tensor(9.5523e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.0733e-10, dtype=torch.float64)
secont condition:: tensor(1.0733e-10, dtype=torch.float64)
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(5.9462e-11, dtype=torch.float64)
secont condition:: tensor(5.9462e-11, dtype=torch.float64)
curr_secont condition:: tensor(9.2027e-11, dtype=torch.float64)
explicit_evaluation epoch:: 51
curr_secont condition:: tensor(1.3738e-10, dtype=torch.float64)
explicit_evaluation epoch:: 53
curr_secont condition:: tensor(1.0582e-10, dtype=torch.float64)
explicit_evaluation epoch:: 56
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 22.91848659515381
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(5.6836e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(9.9000e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.4908e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(6.8681e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(3.4129e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000026, Accuracy: 0.653500
repetition 5
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 4
delta_size:: 1
max_epoch:: 4
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
training time is 25.371583223342896
time_baseline:: 25.381149530410767
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(6.2791e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(3.9052e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000025, Accuracy: 0.657200
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 4
delta_size:: 1
max_epoch:: 4
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(6.0348e-33, dtype=torch.float64)
secont condition:: tensor(6.0348e-33, dtype=torch.float64)
curr_secont condition:: tensor(1.4958e-32, dtype=torch.float64)
secont condition:: tensor(1.4958e-32, dtype=torch.float64)
curr_secont condition:: tensor(2.6747e-32, dtype=torch.float64)
secont condition:: tensor(2.6747e-32, dtype=torch.float64)
curr_secont condition:: tensor(4.1223e-32, dtype=torch.float64)
secont condition:: tensor(4.1223e-32, dtype=torch.float64)
curr_secont condition:: tensor(5.8547e-32, dtype=torch.float64)
secont condition:: tensor(5.8547e-32, dtype=torch.float64)
curr_secont condition:: tensor(7.8546e-32, dtype=torch.float64)
secont condition:: tensor(7.8546e-32, dtype=torch.float64)
curr_secont condition:: tensor(1.0110e-31, dtype=torch.float64)
secont condition:: tensor(1.0110e-31, dtype=torch.float64)
curr_secont condition:: tensor(1.2650e-31, dtype=torch.float64)
secont condition:: tensor(1.2650e-31, dtype=torch.float64)
curr_secont condition:: tensor(1.5452e-31, dtype=torch.float64)
secont condition:: tensor(1.5452e-31, dtype=torch.float64)
curr_secont condition:: tensor(1.8514e-31, dtype=torch.float64)
secont condition:: tensor(1.8514e-31, dtype=torch.float64)
curr_secont condition:: tensor(2.1853e-31, dtype=torch.float64)
secont condition:: tensor(2.1853e-31, dtype=torch.float64)
curr_secont condition:: tensor(2.5430e-31, dtype=torch.float64)
secont condition:: tensor(2.5430e-31, dtype=torch.float64)
curr_secont condition:: tensor(2.9254e-31, dtype=torch.float64)
secont condition:: tensor(2.9254e-31, dtype=torch.float64)
curr_secont condition:: tensor(1.4619e-10, dtype=torch.float64)
secont condition:: tensor(1.4619e-10, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(1.2268e-10, dtype=torch.float64)
secont condition:: tensor(1.2268e-10, dtype=torch.float64)
curr_secont condition:: tensor(9.2392e-11, dtype=torch.float64)
secont condition:: tensor(9.2392e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.6973e-11, dtype=torch.float64)
secont condition:: tensor(5.6973e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.7036e-11, dtype=torch.float64)
secont condition:: tensor(5.7036e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.6427e-11, dtype=torch.float64)
secont condition:: tensor(4.6427e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.6334e-11, dtype=torch.float64)
secont condition:: tensor(3.6334e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.3891e-11, dtype=torch.float64)
secont condition:: tensor(3.3891e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.4861e-11, dtype=torch.float64)
secont condition:: tensor(3.4861e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.7663e-11, dtype=torch.float64)
secont condition:: tensor(2.7663e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.8915e-11, dtype=torch.float64)
secont condition:: tensor(1.8915e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.9400e-11, dtype=torch.float64)
secont condition:: tensor(1.9400e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.8123e-10, dtype=torch.float64)
secont condition:: tensor(1.8123e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.4546e-10, dtype=torch.float64)
secont condition:: tensor(1.4546e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.0604e-10, dtype=torch.float64)
secont condition:: tensor(1.0604e-10, dtype=torch.float64)
curr_secont condition:: tensor(8.7194e-11, dtype=torch.float64)
secont condition:: tensor(8.7194e-11, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(6.7462e-11, dtype=torch.float64)
secont condition:: tensor(6.7462e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.8499e-11, dtype=torch.float64)
secont condition:: tensor(4.8499e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.4336e-11, dtype=torch.float64)
secont condition:: tensor(6.4336e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.0803e-11, dtype=torch.float64)
secont condition:: tensor(5.0803e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.5100e-11, dtype=torch.float64)
secont condition:: tensor(4.5100e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.7590e-11, dtype=torch.float64)
secont condition:: tensor(3.7590e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.8861e-11, dtype=torch.float64)
secont condition:: tensor(3.8861e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.4208e-11, dtype=torch.float64)
secont condition:: tensor(3.4208e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.9292e-11, dtype=torch.float64)
secont condition:: tensor(3.9292e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.0538e-11, dtype=torch.float64)
secont condition:: tensor(3.0538e-11, dtype=torch.float64)
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(3.7663e-11, dtype=torch.float64)
secont condition:: tensor(3.7663e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.9573e-10, dtype=torch.float64)
secont condition:: tensor(6.9573e-10, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 21.732794284820557
curr_diff: 0 tensor(5.9009e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(3.1001e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(3.5125e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(6.1343e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.9016e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(6.6169e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(9.9402e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000026, Accuracy: 0.656600
epochs:: 8
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type MSELoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
cuda:2
Train - Epoch 0, Batch: 0, Loss: 0.091544
Train - Epoch 0, Batch: 10, Loss: 0.090281
Test Avg. Loss: 0.000027, Accuracy: 0.326300
Train - Epoch 1, Batch: 0, Loss: 0.089719
Train - Epoch 1, Batch: 10, Loss: 0.089165
Test Avg. Loss: 0.000026, Accuracy: 0.574700
Train - Epoch 2, Batch: 0, Loss: 0.088606
Train - Epoch 2, Batch: 10, Loss: 0.088131
Test Avg. Loss: 0.000026, Accuracy: 0.640000
Train - Epoch 3, Batch: 0, Loss: 0.087413
Train - Epoch 3, Batch: 10, Loss: 0.086967
Test Avg. Loss: 0.000025, Accuracy: 0.657200
Train - Epoch 4, Batch: 0, Loss: 0.086142
Train - Epoch 4, Batch: 10, Loss: 0.085224
Test Avg. Loss: 0.000025, Accuracy: 0.662700
Train - Epoch 5, Batch: 0, Loss: 0.085057
Train - Epoch 5, Batch: 10, Loss: 0.084319
Test Avg. Loss: 0.000025, Accuracy: 0.676800
Train - Epoch 6, Batch: 0, Loss: 0.083921
Train - Epoch 6, Batch: 10, Loss: 0.083097
Test Avg. Loss: 0.000024, Accuracy: 0.690200
Train - Epoch 7, Batch: 0, Loss: 0.083321
Train - Epoch 7, Batch: 10, Loss: 0.082739
Test Avg. Loss: 0.000024, Accuracy: 0.701300
training_time:: 62.49220275878906
repetition 1
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 50.476104974746704
time_baseline:: 50.4941942691803
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.2187e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.8985e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.701300
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(6.0348e-33, dtype=torch.float64)
secont condition:: tensor(6.0348e-33, dtype=torch.float64)
curr_secont condition:: tensor(1.4958e-32, dtype=torch.float64)
secont condition:: tensor(1.4958e-32, dtype=torch.float64)
curr_secont condition:: tensor(2.6747e-32, dtype=torch.float64)
secont condition:: tensor(2.6747e-32, dtype=torch.float64)
curr_secont condition:: tensor(4.1223e-32, dtype=torch.float64)
secont condition:: tensor(4.1223e-32, dtype=torch.float64)
curr_secont condition:: tensor(5.8547e-32, dtype=torch.float64)
secont condition:: tensor(5.8547e-32, dtype=torch.float64)
curr_secont condition:: tensor(7.8605e-32, dtype=torch.float64)
secont condition:: tensor(7.8605e-32, dtype=torch.float64)
curr_secont condition:: tensor(1.6170e-11, dtype=torch.float64)
secont condition:: tensor(1.6170e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.0788e-11, dtype=torch.float64)
secont condition:: tensor(1.0788e-11, dtype=torch.float64)
curr_secont condition:: tensor(9.7949e-12, dtype=torch.float64)
secont condition:: tensor(9.7949e-12, dtype=torch.float64)
curr_secont condition:: tensor(4.8264e-12, dtype=torch.float64)
secont condition:: tensor(4.8264e-12, dtype=torch.float64)
curr_secont condition:: tensor(6.9147e-12, dtype=torch.float64)
secont condition:: tensor(6.9147e-12, dtype=torch.float64)
curr_secont condition:: tensor(6.2615e-12, dtype=torch.float64)
secont condition:: tensor(6.2615e-12, dtype=torch.float64)
curr_secont condition:: tensor(5.2108e-12, dtype=torch.float64)
secont condition:: tensor(5.2108e-12, dtype=torch.float64)
curr_secont condition:: tensor(4.4951e-12, dtype=torch.float64)
secont condition:: tensor(4.4951e-12, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(3.7884e-12, dtype=torch.float64)
secont condition:: tensor(3.7884e-12, dtype=torch.float64)
curr_secont condition:: tensor(3.8105e-12, dtype=torch.float64)
secont condition:: tensor(3.8105e-12, dtype=torch.float64)
curr_secont condition:: tensor(3.6488e-12, dtype=torch.float64)
secont condition:: tensor(3.6488e-12, dtype=torch.float64)
curr_secont condition:: tensor(3.3160e-12, dtype=torch.float64)
secont condition:: tensor(3.3160e-12, dtype=torch.float64)
curr_secont condition:: tensor(2.8442e-12, dtype=torch.float64)
secont condition:: tensor(2.8442e-12, dtype=torch.float64)
curr_secont condition:: tensor(2.1988e-10, dtype=torch.float64)
secont condition:: tensor(2.1988e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.9776e-10, dtype=torch.float64)
secont condition:: tensor(1.9776e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.6285e-10, dtype=torch.float64)
secont condition:: tensor(1.6285e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.1819e-10, dtype=torch.float64)
secont condition:: tensor(1.1819e-10, dtype=torch.float64)
curr_secont condition:: tensor(9.2808e-11, dtype=torch.float64)
secont condition:: tensor(9.2808e-11, dtype=torch.float64)
curr_secont condition:: tensor(7.2743e-11, dtype=torch.float64)
secont condition:: tensor(7.2743e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.1811e-11, dtype=torch.float64)
secont condition:: tensor(6.1811e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.7663e-11, dtype=torch.float64)
secont condition:: tensor(5.7663e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.6094e-11, dtype=torch.float64)
secont condition:: tensor(4.6094e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.9512e-11, dtype=torch.float64)
secont condition:: tensor(4.9512e-11, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(4.2657e-11, dtype=torch.float64)
secont condition:: tensor(4.2657e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.9342e-11, dtype=torch.float64)
secont condition:: tensor(3.9342e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.1874e-11, dtype=torch.float64)
secont condition:: tensor(3.1874e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.0485e-11, dtype=torch.float64)
secont condition:: tensor(4.0485e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.3619e-11, dtype=torch.float64)
secont condition:: tensor(3.3619e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.9912e-11, dtype=torch.float64)
secont condition:: tensor(2.9912e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.3270e-11, dtype=torch.float64)
secont condition:: tensor(2.3270e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.6951e-11, dtype=torch.float64)
secont condition:: tensor(2.6951e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.5372e-11, dtype=torch.float64)
secont condition:: tensor(3.5372e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.4975e-11, dtype=torch.float64)
secont condition:: tensor(4.4975e-11, dtype=torch.float64)
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(4.1787e-11, dtype=torch.float64)
secont condition:: tensor(4.1787e-11, dtype=torch.float64)
curr_secont condition:: tensor(-1.1757e-11, dtype=torch.float64)
explicit_evaluation epoch:: 54
curr_secont condition:: tensor(1.5135e-11, dtype=torch.float64)
secont condition:: tensor(1.5135e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.8021e-11, dtype=torch.float64)
explicit_evaluation epoch:: 56
curr_secont condition:: tensor(7.9942e-09, dtype=torch.float64)
explicit_evaluation epoch:: 58
curr_secont condition:: tensor(1.3373e-09, dtype=torch.float64)
explicit_evaluation epoch:: 59
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(9.8792e-10, dtype=torch.float64)
secont condition:: tensor(9.8792e-10, dtype=torch.float64)
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(9.7960e-10, dtype=torch.float64)
secont condition:: tensor(9.7960e-10, dtype=torch.float64)
curr_secont condition:: tensor(7.4460e-09, dtype=torch.float64)
explicit_evaluation epoch:: 86
curr_secont condition:: tensor(1.9983e-09, dtype=torch.float64)
explicit_evaluation epoch:: 87
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(1.5244e-09, dtype=torch.float64)
secont condition:: tensor(1.5244e-09, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(1.2850e-09, dtype=torch.float64)
secont condition:: tensor(1.2850e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.1709e-09, dtype=torch.float64)
secont condition:: tensor(2.1709e-09, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 28.305806398391724
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(8.6723e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(6.3100e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.0400e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.1793e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.0549e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.702200
repetition 2
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 49.58000946044922
time_baseline:: 49.60003685951233
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.0609e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.1402e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.701300
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(6.0348e-33, dtype=torch.float64)
secont condition:: tensor(6.0348e-33, dtype=torch.float64)
curr_secont condition:: tensor(1.4958e-32, dtype=torch.float64)
secont condition:: tensor(1.4958e-32, dtype=torch.float64)
curr_secont condition:: tensor(2.6747e-32, dtype=torch.float64)
secont condition:: tensor(2.6747e-32, dtype=torch.float64)
curr_secont condition:: tensor(4.1259e-32, dtype=torch.float64)
secont condition:: tensor(4.1259e-32, dtype=torch.float64)
curr_secont condition:: tensor(1.9372e-10, dtype=torch.float64)
secont condition:: tensor(1.9372e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.2343e-10, dtype=torch.float64)
secont condition:: tensor(1.2343e-10, dtype=torch.float64)
curr_secont condition:: tensor(8.8490e-11, dtype=torch.float64)
secont condition:: tensor(8.8490e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.1082e-11, dtype=torch.float64)
secont condition:: tensor(6.1082e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.3098e-11, dtype=torch.float64)
secont condition:: tensor(4.3098e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.1224e-11, dtype=torch.float64)
secont condition:: tensor(4.1224e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.7707e-11, dtype=torch.float64)
secont condition:: tensor(2.7707e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.7987e-11, dtype=torch.float64)
secont condition:: tensor(2.7987e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.3701e-11, dtype=torch.float64)
secont condition:: tensor(2.3701e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.9209e-11, dtype=torch.float64)
secont condition:: tensor(1.9209e-11, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(1.9653e-11, dtype=torch.float64)
secont condition:: tensor(1.9653e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.2984e-11, dtype=torch.float64)
secont condition:: tensor(2.2984e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.1151e-11, dtype=torch.float64)
secont condition:: tensor(2.1151e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.8442e-11, dtype=torch.float64)
secont condition:: tensor(1.8442e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.7675e-11, dtype=torch.float64)
secont condition:: tensor(1.7675e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.7618e-11, dtype=torch.float64)
secont condition:: tensor(1.7618e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.4545e-11, dtype=torch.float64)
secont condition:: tensor(1.4545e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.7944e-11, dtype=torch.float64)
secont condition:: tensor(1.7944e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.9097e-11, dtype=torch.float64)
secont condition:: tensor(1.9097e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.3822e-11, dtype=torch.float64)
secont condition:: tensor(1.3822e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.6728e-11, dtype=torch.float64)
secont condition:: tensor(1.6728e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.4531e-11, dtype=torch.float64)
secont condition:: tensor(1.4531e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.4224e-11, dtype=torch.float64)
secont condition:: tensor(1.4224e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.4531e-11, dtype=torch.float64)
secont condition:: tensor(1.4531e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.2850e-11, dtype=torch.float64)
secont condition:: tensor(1.2850e-11, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(1.5980e-11, dtype=torch.float64)
secont condition:: tensor(1.5980e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.1908e-11, dtype=torch.float64)
secont condition:: tensor(1.1908e-11, dtype=torch.float64)
curr_secont condition:: tensor(-8.2419e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.5674e-11, dtype=torch.float64)
secont condition:: tensor(1.5674e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.9386e-11, dtype=torch.float64)
secont condition:: tensor(2.9386e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.1891e-11, dtype=torch.float64)
secont condition:: tensor(2.1891e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.3254e-11, dtype=torch.float64)
secont condition:: tensor(3.3254e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.0636e-11, dtype=torch.float64)
secont condition:: tensor(2.0636e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.8514e-11, dtype=torch.float64)
secont condition:: tensor(1.8514e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.8432e-11, dtype=torch.float64)
secont condition:: tensor(1.8432e-11, dtype=torch.float64)
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(2.0143e-11, dtype=torch.float64)
secont condition:: tensor(2.0143e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.7865e-11, dtype=torch.float64)
explicit_evaluation epoch:: 52
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(3.9035e-11, dtype=torch.float64)
secont condition:: tensor(3.9035e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.0236e-11, dtype=torch.float64)
explicit_evaluation epoch:: 68
curr_secont condition:: tensor(1.6861e-09, dtype=torch.float64)
explicit_evaluation epoch:: 70
curr_secont condition:: tensor(2.9637e-10, dtype=torch.float64)
explicit_evaluation epoch:: 71
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(2.8193e-10, dtype=torch.float64)
secont condition:: tensor(2.8193e-10, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(2.4419e-10, dtype=torch.float64)
secont condition:: tensor(2.4419e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.9287e-10, dtype=torch.float64)
secont condition:: tensor(1.9287e-10, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(1.4521e-10, dtype=torch.float64)
secont condition:: tensor(1.4521e-10, dtype=torch.float64)
curr_secont condition:: tensor(-3.3705e-09, dtype=torch.float64)
explicit_evaluation epoch:: 114
curr_secont condition:: tensor(8.1116e-10, dtype=torch.float64)
secont condition:: tensor(8.1116e-10, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 28.16662859916687
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(6.6427e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(2.9053e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(4.5555e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(9.8038e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.5758e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.699800
repetition 3
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 50.094366788864136
time_baseline:: 50.12992739677429
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.1102e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(5.2542e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.701300
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(6.0348e-33, dtype=torch.float64)
secont condition:: tensor(6.0348e-33, dtype=torch.float64)
curr_secont condition:: tensor(1.4958e-32, dtype=torch.float64)
secont condition:: tensor(1.4958e-32, dtype=torch.float64)
curr_secont condition:: tensor(2.6780e-32, dtype=torch.float64)
secont condition:: tensor(2.6780e-32, dtype=torch.float64)
curr_secont condition:: tensor(1.2423e-10, dtype=torch.float64)
secont condition:: tensor(1.2423e-10, dtype=torch.float64)
curr_secont condition:: tensor(8.9324e-11, dtype=torch.float64)
secont condition:: tensor(8.9324e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.6567e-11, dtype=torch.float64)
secont condition:: tensor(6.6567e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.7518e-11, dtype=torch.float64)
secont condition:: tensor(3.7518e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.0289e-11, dtype=torch.float64)
secont condition:: tensor(4.0289e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.2522e-11, dtype=torch.float64)
secont condition:: tensor(3.2522e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.4901e-11, dtype=torch.float64)
secont condition:: tensor(1.4901e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.2923e-11, dtype=torch.float64)
secont condition:: tensor(2.2923e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.8245e-11, dtype=torch.float64)
secont condition:: tensor(1.8245e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.6503e-11, dtype=torch.float64)
secont condition:: tensor(1.6503e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.3929e-11, dtype=torch.float64)
secont condition:: tensor(1.3929e-11, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(1.9105e-11, dtype=torch.float64)
secont condition:: tensor(1.9105e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.6180e-11, dtype=torch.float64)
secont condition:: tensor(1.6180e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.2297e-11, dtype=torch.float64)
secont condition:: tensor(1.2297e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.2223e-11, dtype=torch.float64)
secont condition:: tensor(1.2223e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.4653e-11, dtype=torch.float64)
secont condition:: tensor(1.4653e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.0816e-11, dtype=torch.float64)
secont condition:: tensor(1.0816e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.6791e-11, dtype=torch.float64)
secont condition:: tensor(3.6791e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.8705e-11, dtype=torch.float64)
secont condition:: tensor(2.8705e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.5043e-11, dtype=torch.float64)
secont condition:: tensor(2.5043e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.3033e-11, dtype=torch.float64)
secont condition:: tensor(2.3033e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.9143e-11, dtype=torch.float64)
secont condition:: tensor(1.9143e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.5527e-11, dtype=torch.float64)
secont condition:: tensor(2.5527e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.1571e-11, dtype=torch.float64)
secont condition:: tensor(2.1571e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.6027e-11, dtype=torch.float64)
secont condition:: tensor(1.6027e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.0264e-11, dtype=torch.float64)
secont condition:: tensor(2.0264e-11, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(2.1716e-11, dtype=torch.float64)
secont condition:: tensor(2.1716e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.1055e-11, dtype=torch.float64)
secont condition:: tensor(2.1055e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.3934e-10, dtype=torch.float64)
secont condition:: tensor(2.3934e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.8661e-10, dtype=torch.float64)
secont condition:: tensor(1.8661e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.6262e-10, dtype=torch.float64)
secont condition:: tensor(1.6262e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.2838e-10, dtype=torch.float64)
secont condition:: tensor(1.2838e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.1921e-10, dtype=torch.float64)
secont condition:: tensor(1.1921e-10, dtype=torch.float64)
curr_secont condition:: tensor(8.7051e-11, dtype=torch.float64)
secont condition:: tensor(8.7051e-11, dtype=torch.float64)
curr_secont condition:: tensor(8.0527e-11, dtype=torch.float64)
secont condition:: tensor(8.0527e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.9358e-11, dtype=torch.float64)
secont condition:: tensor(6.9358e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.7721e-11, dtype=torch.float64)
explicit_evaluation epoch:: 42
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(2.6872e-10, dtype=torch.float64)
explicit_evaluation epoch:: 49
curr_secont condition:: tensor(1.2088e-10, dtype=torch.float64)
explicit_evaluation epoch:: 50
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(1.3504e-10, dtype=torch.float64)
secont condition:: tensor(1.3504e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.0366e-10, dtype=torch.float64)
secont condition:: tensor(1.0366e-10, dtype=torch.float64)
curr_secont condition:: tensor(6.9754e-11, dtype=torch.float64)
explicit_evaluation epoch:: 73
curr_secont condition:: tensor(1.3327e-09, dtype=torch.float64)
explicit_evaluation epoch:: 74
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(4.3860e-10, dtype=torch.float64)
explicit_evaluation epoch:: 75
curr_secont condition:: tensor(-3.6671e-10, dtype=torch.float64)
explicit_evaluation epoch:: 83
curr_secont condition:: tensor(1.7535e-09, dtype=torch.float64)
secont condition:: tensor(1.7535e-09, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(1.0797e-09, dtype=torch.float64)
secont condition:: tensor(1.0797e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.0085e-09, dtype=torch.float64)
secont condition:: tensor(2.0085e-09, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(2.2261e-09, dtype=torch.float64)
secont condition:: tensor(2.2261e-09, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 28.251132011413574
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(9.2245e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(6.2663e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.0951e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.0155e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(6.5446e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.698200
repetition 4
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 50.395589113235474
time_baseline:: 50.41380834579468
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(8.4792e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(5.2564e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.701300
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(6.0348e-33, dtype=torch.float64)
secont condition:: tensor(6.0348e-33, dtype=torch.float64)
curr_secont condition:: tensor(1.4958e-32, dtype=torch.float64)
secont condition:: tensor(1.4958e-32, dtype=torch.float64)
curr_secont condition:: tensor(2.6747e-32, dtype=torch.float64)
secont condition:: tensor(2.6747e-32, dtype=torch.float64)
curr_secont condition:: tensor(4.1223e-32, dtype=torch.float64)
secont condition:: tensor(4.1223e-32, dtype=torch.float64)
curr_secont condition:: tensor(5.8547e-32, dtype=torch.float64)
secont condition:: tensor(5.8547e-32, dtype=torch.float64)
curr_secont condition:: tensor(7.8546e-32, dtype=torch.float64)
secont condition:: tensor(7.8546e-32, dtype=torch.float64)
curr_secont condition:: tensor(1.0113e-31, dtype=torch.float64)
secont condition:: tensor(1.0113e-31, dtype=torch.float64)
curr_secont condition:: tensor(1.2436e-10, dtype=torch.float64)
secont condition:: tensor(1.2436e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.0934e-10, dtype=torch.float64)
secont condition:: tensor(1.0934e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.1907e-10, dtype=torch.float64)
secont condition:: tensor(1.1907e-10, dtype=torch.float64)
curr_secont condition:: tensor(9.5622e-11, dtype=torch.float64)
secont condition:: tensor(9.5622e-11, dtype=torch.float64)
curr_secont condition:: tensor(8.8060e-11, dtype=torch.float64)
secont condition:: tensor(8.8060e-11, dtype=torch.float64)
curr_secont condition:: tensor(8.4020e-11, dtype=torch.float64)
secont condition:: tensor(8.4020e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.6637e-11, dtype=torch.float64)
secont condition:: tensor(6.6637e-11, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(5.0963e-11, dtype=torch.float64)
secont condition:: tensor(5.0963e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.3494e-11, dtype=torch.float64)
secont condition:: tensor(5.3494e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.7295e-11, dtype=torch.float64)
secont condition:: tensor(5.7295e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.3756e-11, dtype=torch.float64)
secont condition:: tensor(4.3756e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.2045e-11, dtype=torch.float64)
secont condition:: tensor(4.2045e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.3057e-11, dtype=torch.float64)
secont condition:: tensor(3.3057e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.7647e-11, dtype=torch.float64)
secont condition:: tensor(2.7647e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.6665e-11, dtype=torch.float64)
secont condition:: tensor(3.6665e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.3734e-11, dtype=torch.float64)
secont condition:: tensor(3.3734e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.4623e-11, dtype=torch.float64)
secont condition:: tensor(3.4623e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.8441e-11, dtype=torch.float64)
secont condition:: tensor(3.8441e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.3245e-11, dtype=torch.float64)
secont condition:: tensor(3.3245e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.4761e-11, dtype=torch.float64)
secont condition:: tensor(2.4761e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.9528e-11, dtype=torch.float64)
secont condition:: tensor(2.9528e-11, dtype=torch.float64)
curr_secont condition:: tensor(-1.0893e-11, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(2.7047e-11, dtype=torch.float64)
secont condition:: tensor(2.7047e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.1088e-11, dtype=torch.float64)
secont condition:: tensor(3.1088e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.2273e-11, dtype=torch.float64)
secont condition:: tensor(2.2273e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.7734e-11, dtype=torch.float64)
secont condition:: tensor(2.7734e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.7479e-11, dtype=torch.float64)
secont condition:: tensor(3.7479e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.2409e-11, dtype=torch.float64)
secont condition:: tensor(2.2409e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.3892e-10, dtype=torch.float64)
secont condition:: tensor(2.3892e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.0750e-10, dtype=torch.float64)
secont condition:: tensor(2.0750e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.7938e-10, dtype=torch.float64)
secont condition:: tensor(1.7938e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.4989e-10, dtype=torch.float64)
secont condition:: tensor(1.4989e-10, dtype=torch.float64)
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(1.1314e-10, dtype=torch.float64)
secont condition:: tensor(1.1314e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.0533e-11, dtype=torch.float64)
explicit_evaluation epoch:: 52
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(2.5824e-10, dtype=torch.float64)
secont condition:: tensor(2.5824e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.9823e-10, dtype=torch.float64)
secont condition:: tensor(2.9823e-10, dtype=torch.float64)
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(2.8504e-10, dtype=torch.float64)
secont condition:: tensor(2.8504e-10, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(3.8743e-10, dtype=torch.float64)
secont condition:: tensor(3.8743e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.6969e-10, dtype=torch.float64)
secont condition:: tensor(2.6969e-10, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(1.5915e-08, dtype=torch.float64)
explicit_evaluation epoch:: 107
curr_secont condition:: tensor(9.0123e-10, dtype=torch.float64)
explicit_evaluation epoch:: 108
curr_secont condition:: tensor(5.7570e-10, dtype=torch.float64)
secont condition:: tensor(5.7570e-10, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 27.250675678253174
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(8.4050e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(4.3290e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(7.1413e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(9.0117e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.2113e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.702600
repetition 5
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 50.319443702697754
time_baseline:: 50.33612012863159
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.1897e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(6.7177e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.701300
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(6.0348e-33, dtype=torch.float64)
secont condition:: tensor(6.0348e-33, dtype=torch.float64)
curr_secont condition:: tensor(1.4958e-32, dtype=torch.float64)
secont condition:: tensor(1.4958e-32, dtype=torch.float64)
curr_secont condition:: tensor(2.6747e-32, dtype=torch.float64)
secont condition:: tensor(2.6747e-32, dtype=torch.float64)
curr_secont condition:: tensor(4.1223e-32, dtype=torch.float64)
secont condition:: tensor(4.1223e-32, dtype=torch.float64)
curr_secont condition:: tensor(5.8547e-32, dtype=torch.float64)
secont condition:: tensor(5.8547e-32, dtype=torch.float64)
curr_secont condition:: tensor(7.8546e-32, dtype=torch.float64)
secont condition:: tensor(7.8546e-32, dtype=torch.float64)
curr_secont condition:: tensor(1.0110e-31, dtype=torch.float64)
secont condition:: tensor(1.0110e-31, dtype=torch.float64)
curr_secont condition:: tensor(1.2650e-31, dtype=torch.float64)
secont condition:: tensor(1.2650e-31, dtype=torch.float64)
curr_secont condition:: tensor(1.5452e-31, dtype=torch.float64)
secont condition:: tensor(1.5452e-31, dtype=torch.float64)
curr_secont condition:: tensor(1.8514e-31, dtype=torch.float64)
secont condition:: tensor(1.8514e-31, dtype=torch.float64)
curr_secont condition:: tensor(2.1853e-31, dtype=torch.float64)
secont condition:: tensor(2.1853e-31, dtype=torch.float64)
curr_secont condition:: tensor(2.5429e-31, dtype=torch.float64)
secont condition:: tensor(2.5429e-31, dtype=torch.float64)
curr_secont condition:: tensor(4.7139e-10, dtype=torch.float64)
secont condition:: tensor(4.7139e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.8886e-10, dtype=torch.float64)
secont condition:: tensor(3.8886e-10, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(2.4047e-10, dtype=torch.float64)
secont condition:: tensor(2.4047e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.2331e-10, dtype=torch.float64)
secont condition:: tensor(2.2331e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.6177e-10, dtype=torch.float64)
secont condition:: tensor(1.6177e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.5016e-10, dtype=torch.float64)
secont condition:: tensor(1.5016e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.1986e-10, dtype=torch.float64)
secont condition:: tensor(1.1986e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.4437e-10, dtype=torch.float64)
secont condition:: tensor(1.4437e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.2370e-10, dtype=torch.float64)
secont condition:: tensor(1.2370e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.1035e-10, dtype=torch.float64)
secont condition:: tensor(1.1035e-10, dtype=torch.float64)
curr_secont condition:: tensor(7.5668e-11, dtype=torch.float64)
secont condition:: tensor(7.5668e-11, dtype=torch.float64)
curr_secont condition:: tensor(7.5528e-11, dtype=torch.float64)
secont condition:: tensor(7.5528e-11, dtype=torch.float64)
curr_secont condition:: tensor(8.2066e-11, dtype=torch.float64)
secont condition:: tensor(8.2066e-11, dtype=torch.float64)
curr_secont condition:: tensor(7.9139e-11, dtype=torch.float64)
secont condition:: tensor(7.9139e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.3600e-11, dtype=torch.float64)
secont condition:: tensor(5.3600e-11, dtype=torch.float64)
curr_secont condition:: tensor(7.1595e-11, dtype=torch.float64)
secont condition:: tensor(7.1595e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.0969e-10, dtype=torch.float64)
secont condition:: tensor(1.0969e-10, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(4.6725e-11, dtype=torch.float64)
secont condition:: tensor(4.6725e-11, dtype=torch.float64)
curr_secont condition:: tensor(7.6231e-11, dtype=torch.float64)
secont condition:: tensor(7.6231e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.7192e-11, dtype=torch.float64)
secont condition:: tensor(3.7192e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.8328e-11, dtype=torch.float64)
secont condition:: tensor(3.8328e-11, dtype=torch.float64)
curr_secont condition:: tensor(8.7837e-11, dtype=torch.float64)
secont condition:: tensor(8.7837e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.0999e-11, dtype=torch.float64)
secont condition:: tensor(6.0999e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.5884e-11, dtype=torch.float64)
secont condition:: tensor(5.5884e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.7430e-11, dtype=torch.float64)
secont condition:: tensor(4.7430e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.4457e-11, dtype=torch.float64)
secont condition:: tensor(4.4457e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.6641e-11, dtype=torch.float64)
secont condition:: tensor(3.6641e-11, dtype=torch.float64)
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(5.2750e-11, dtype=torch.float64)
secont condition:: tensor(5.2750e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.2317e-10, dtype=torch.float64)
explicit_evaluation epoch:: 52
curr_secont condition:: tensor(1.0934e-10, dtype=torch.float64)
explicit_evaluation epoch:: 53
curr_secont condition:: tensor(1.0558e-10, dtype=torch.float64)
explicit_evaluation epoch:: 56
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(6.2341e-11, dtype=torch.float64)
secont condition:: tensor(6.2341e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.0809e-10, dtype=torch.float64)
explicit_evaluation epoch:: 67
curr_secont condition:: tensor(-6.8495e-10, dtype=torch.float64)
explicit_evaluation epoch:: 73
curr_secont condition:: tensor(9.7041e-10, dtype=torch.float64)
secont condition:: tensor(9.7041e-10, dtype=torch.float64)
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(9.3271e-10, dtype=torch.float64)
secont condition:: tensor(9.3271e-10, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(5.3570e-10, dtype=torch.float64)
secont condition:: tensor(5.3570e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.7109e-10, dtype=torch.float64)
secont condition:: tensor(4.7109e-10, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(9.0500e-10, dtype=torch.float64)
explicit_evaluation epoch:: 108
curr_secont condition:: tensor(1.7803e-09, dtype=torch.float64)
secont condition:: tensor(1.7803e-09, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 28.520774841308594
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(9.8798e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(5.5740e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(8.7392e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.2392e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.5176e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.703000
epochs:: 10
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type MSELoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
cuda:2
Train - Epoch 0, Batch: 0, Loss: 0.091544
Train - Epoch 0, Batch: 10, Loss: 0.090281
Test Avg. Loss: 0.000027, Accuracy: 0.326300
Train - Epoch 1, Batch: 0, Loss: 0.089719
Train - Epoch 1, Batch: 10, Loss: 0.089165
Test Avg. Loss: 0.000026, Accuracy: 0.574700
Train - Epoch 2, Batch: 0, Loss: 0.088606
Train - Epoch 2, Batch: 10, Loss: 0.088131
Test Avg. Loss: 0.000026, Accuracy: 0.640000
Train - Epoch 3, Batch: 0, Loss: 0.087413
Train - Epoch 3, Batch: 10, Loss: 0.086967
Test Avg. Loss: 0.000025, Accuracy: 0.657200
Train - Epoch 4, Batch: 0, Loss: 0.086142
Train - Epoch 4, Batch: 10, Loss: 0.085224
Test Avg. Loss: 0.000025, Accuracy: 0.662700
Train - Epoch 5, Batch: 0, Loss: 0.085057
Train - Epoch 5, Batch: 10, Loss: 0.084319
Test Avg. Loss: 0.000025, Accuracy: 0.676800
Train - Epoch 6, Batch: 0, Loss: 0.083921
Train - Epoch 6, Batch: 10, Loss: 0.083097
Test Avg. Loss: 0.000024, Accuracy: 0.690200
Train - Epoch 7, Batch: 0, Loss: 0.083321
Train - Epoch 7, Batch: 10, Loss: 0.082739
Test Avg. Loss: 0.000024, Accuracy: 0.701300
Train - Epoch 8, Batch: 0, Loss: 0.082079
Train - Epoch 8, Batch: 10, Loss: 0.081350
Test Avg. Loss: 0.000024, Accuracy: 0.705200
Train - Epoch 9, Batch: 0, Loss: 0.081480
Train - Epoch 9, Batch: 10, Loss: 0.080960
Test Avg. Loss: 0.000023, Accuracy: 0.722000
training_time:: 81.3985104560852
repetition 1
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 10
delta_size:: 1
max_epoch:: 10
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
training time is 62.32417321205139
time_baseline:: 62.347901582717896
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(9.6119e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(3.9061e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000023, Accuracy: 0.722000
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 10
delta_size:: 1
max_epoch:: 10
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(1.6433e-11, dtype=torch.float64)
secont condition:: tensor(1.6433e-11, dtype=torch.float64)
curr_secont condition:: tensor(8.8909e-12, dtype=torch.float64)
secont condition:: tensor(8.8909e-12, dtype=torch.float64)
curr_secont condition:: tensor(6.3700e-12, dtype=torch.float64)
secont condition:: tensor(6.3700e-12, dtype=torch.float64)
curr_secont condition:: tensor(5.2653e-12, dtype=torch.float64)
secont condition:: tensor(5.2653e-12, dtype=torch.float64)
curr_secont condition:: tensor(4.4577e-12, dtype=torch.float64)
secont condition:: tensor(4.4577e-12, dtype=torch.float64)
curr_secont condition:: tensor(3.6601e-12, dtype=torch.float64)
secont condition:: tensor(3.6601e-12, dtype=torch.float64)
curr_secont condition:: tensor(3.4168e-12, dtype=torch.float64)
secont condition:: tensor(3.4168e-12, dtype=torch.float64)
curr_secont condition:: tensor(3.3925e-12, dtype=torch.float64)
secont condition:: tensor(3.3925e-12, dtype=torch.float64)
curr_secont condition:: tensor(3.7979e-12, dtype=torch.float64)
secont condition:: tensor(3.7979e-12, dtype=torch.float64)
curr_secont condition:: tensor(3.1642e-12, dtype=torch.float64)
secont condition:: tensor(3.1642e-12, dtype=torch.float64)
curr_secont condition:: tensor(3.0036e-12, dtype=torch.float64)
secont condition:: tensor(3.0036e-12, dtype=torch.float64)
curr_secont condition:: tensor(4.4058e-12, dtype=torch.float64)
secont condition:: tensor(4.4058e-12, dtype=torch.float64)
curr_secont condition:: tensor(4.3161e-12, dtype=torch.float64)
secont condition:: tensor(4.3161e-12, dtype=torch.float64)
curr_secont condition:: tensor(3.0708e-12, dtype=torch.float64)
secont condition:: tensor(3.0708e-12, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(2.7271e-12, dtype=torch.float64)
secont condition:: tensor(2.7271e-12, dtype=torch.float64)
curr_secont condition:: tensor(3.4680e-12, dtype=torch.float64)
secont condition:: tensor(3.4680e-12, dtype=torch.float64)
curr_secont condition:: tensor(2.9362e-12, dtype=torch.float64)
secont condition:: tensor(2.9362e-12, dtype=torch.float64)
curr_secont condition:: tensor(3.0451e-12, dtype=torch.float64)
secont condition:: tensor(3.0451e-12, dtype=torch.float64)
curr_secont condition:: tensor(4.5655e-12, dtype=torch.float64)
secont condition:: tensor(4.5655e-12, dtype=torch.float64)
curr_secont condition:: tensor(3.2297e-12, dtype=torch.float64)
secont condition:: tensor(3.2297e-12, dtype=torch.float64)
curr_secont condition:: tensor(2.9741e-12, dtype=torch.float64)
secont condition:: tensor(2.9741e-12, dtype=torch.float64)
curr_secont condition:: tensor(3.5655e-12, dtype=torch.float64)
secont condition:: tensor(3.5655e-12, dtype=torch.float64)
curr_secont condition:: tensor(9.0582e-12, dtype=torch.float64)
secont condition:: tensor(9.0582e-12, dtype=torch.float64)
curr_secont condition:: tensor(5.1418e-12, dtype=torch.float64)
secont condition:: tensor(5.1418e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.5074e-11, dtype=torch.float64)
secont condition:: tensor(1.5074e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.1943e-11, dtype=torch.float64)
secont condition:: tensor(1.1943e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.4003e-12, dtype=torch.float64)
secont condition:: tensor(6.4003e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.0072e-11, dtype=torch.float64)
secont condition:: tensor(1.0072e-11, dtype=torch.float64)
curr_secont condition:: tensor(9.1633e-12, dtype=torch.float64)
secont condition:: tensor(9.1633e-12, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(1.1598e-11, dtype=torch.float64)
secont condition:: tensor(1.1598e-11, dtype=torch.float64)
curr_secont condition:: tensor(8.6669e-12, dtype=torch.float64)
secont condition:: tensor(8.6669e-12, dtype=torch.float64)
curr_secont condition:: tensor(8.4648e-12, dtype=torch.float64)
secont condition:: tensor(8.4648e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.1637e-12, dtype=torch.float64)
secont condition:: tensor(1.1637e-12, dtype=torch.float64)
curr_secont condition:: tensor(8.9929e-12, dtype=torch.float64)
secont condition:: tensor(8.9929e-12, dtype=torch.float64)
curr_secont condition:: tensor(5.7284e-12, dtype=torch.float64)
secont condition:: tensor(5.7284e-12, dtype=torch.float64)
curr_secont condition:: tensor(5.7005e-12, dtype=torch.float64)
secont condition:: tensor(5.7005e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.4345e-11, dtype=torch.float64)
secont condition:: tensor(1.4345e-11, dtype=torch.float64)
curr_secont condition:: tensor(7.2524e-12, dtype=torch.float64)
secont condition:: tensor(7.2524e-12, dtype=torch.float64)
curr_secont condition:: tensor(-1.1135e-12, dtype=torch.float64)
curr_secont condition:: tensor(6.4713e-12, dtype=torch.float64)
secont condition:: tensor(6.4713e-12, dtype=torch.float64)
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(2.3418e-10, dtype=torch.float64)
explicit_evaluation epoch:: 48
curr_secont condition:: tensor(5.8641e-11, dtype=torch.float64)
explicit_evaluation epoch:: 49
curr_secont condition:: tensor(5.6788e-11, dtype=torch.float64)
explicit_evaluation epoch:: 50
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(2.4095e-11, dtype=torch.float64)
secont condition:: tensor(2.4095e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.9278e-11, dtype=torch.float64)
secont condition:: tensor(1.9278e-11, dtype=torch.float64)
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(3.6937e-11, dtype=torch.float64)
secont condition:: tensor(3.6937e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.7953e-10, dtype=torch.float64)
explicit_evaluation epoch:: 85
curr_secont condition:: tensor(2.0094e-10, dtype=torch.float64)
explicit_evaluation epoch:: 86
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(1.7591e-10, dtype=torch.float64)
secont condition:: tensor(1.7591e-10, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(1.6741e-10, dtype=torch.float64)
secont condition:: tensor(1.6741e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.6994e-10, dtype=torch.float64)
secont condition:: tensor(1.6994e-10, dtype=torch.float64)
epoch  8
	calling Sampler:__iter__
curr_secont condition:: tensor(2.1484e-10, dtype=torch.float64)
secont condition:: tensor(2.1484e-10, dtype=torch.float64)
curr_secont condition:: tensor(7.3502e-09, dtype=torch.float64)
explicit_evaluation epoch:: 127
curr_secont condition:: tensor(8.5194e-10, dtype=torch.float64)
explicit_evaluation epoch:: 128
epoch  9
	calling Sampler:__iter__
curr_secont condition:: tensor(2.0059e-08, dtype=torch.float64)
explicit_evaluation epoch:: 137
curr_secont condition:: tensor(2.4000e-09, dtype=torch.float64)
explicit_evaluation epoch:: 138
curr_secont condition:: tensor(6.3721e-10, dtype=torch.float64)
secont condition:: tensor(6.3721e-10, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 32.3191351890564
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(7.2735e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(3.2462e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(5.3394e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(8.0912e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(8.7206e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000023, Accuracy: 0.721500
repetition 2
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 10
delta_size:: 1
max_epoch:: 10
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
training time is 62.63227558135986
time_baseline:: 62.65484547615051
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.9732e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.5612e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000023, Accuracy: 0.722000
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 10
delta_size:: 1
max_epoch:: 10
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(6.0348e-33, dtype=torch.float64)
secont condition:: tensor(6.0348e-33, dtype=torch.float64)
curr_secont condition:: tensor(1.4958e-32, dtype=torch.float64)
secont condition:: tensor(1.4958e-32, dtype=torch.float64)
curr_secont condition:: tensor(2.6747e-32, dtype=torch.float64)
secont condition:: tensor(2.6747e-32, dtype=torch.float64)
curr_secont condition:: tensor(4.1223e-32, dtype=torch.float64)
secont condition:: tensor(4.1223e-32, dtype=torch.float64)
curr_secont condition:: tensor(5.8577e-32, dtype=torch.float64)
secont condition:: tensor(5.8577e-32, dtype=torch.float64)
curr_secont condition:: tensor(1.4613e-09, dtype=torch.float64)
secont condition:: tensor(1.4613e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.1711e-09, dtype=torch.float64)
secont condition:: tensor(1.1711e-09, dtype=torch.float64)
curr_secont condition:: tensor(9.1072e-10, dtype=torch.float64)
secont condition:: tensor(9.1072e-10, dtype=torch.float64)
curr_secont condition:: tensor(8.3608e-10, dtype=torch.float64)
secont condition:: tensor(8.3608e-10, dtype=torch.float64)
curr_secont condition:: tensor(9.7574e-10, dtype=torch.float64)
secont condition:: tensor(9.7574e-10, dtype=torch.float64)
curr_secont condition:: tensor(6.6993e-10, dtype=torch.float64)
secont condition:: tensor(6.6993e-10, dtype=torch.float64)
curr_secont condition:: tensor(5.1698e-10, dtype=torch.float64)
secont condition:: tensor(5.1698e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.7553e-10, dtype=torch.float64)
secont condition:: tensor(4.7553e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.6453e-10, dtype=torch.float64)
secont condition:: tensor(3.6453e-10, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(3.4103e-10, dtype=torch.float64)
secont condition:: tensor(3.4103e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.3188e-09, dtype=torch.float64)
secont condition:: tensor(1.3188e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.1519e-09, dtype=torch.float64)
secont condition:: tensor(1.1519e-09, dtype=torch.float64)
curr_secont condition:: tensor(7.4937e-10, dtype=torch.float64)
secont condition:: tensor(7.4937e-10, dtype=torch.float64)
curr_secont condition:: tensor(7.7573e-10, dtype=torch.float64)
secont condition:: tensor(7.7573e-10, dtype=torch.float64)
curr_secont condition:: tensor(5.4541e-10, dtype=torch.float64)
secont condition:: tensor(5.4541e-10, dtype=torch.float64)
curr_secont condition:: tensor(6.2936e-10, dtype=torch.float64)
secont condition:: tensor(6.2936e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.0162e-10, dtype=torch.float64)
secont condition:: tensor(4.0162e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.9234e-10, dtype=torch.float64)
secont condition:: tensor(2.9234e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.4046e-10, dtype=torch.float64)
secont condition:: tensor(3.4046e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.3183e-10, dtype=torch.float64)
secont condition:: tensor(3.3183e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.0026e-10, dtype=torch.float64)
secont condition:: tensor(2.0026e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.3006e-10, dtype=torch.float64)
secont condition:: tensor(2.3006e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.2004e-10, dtype=torch.float64)
secont condition:: tensor(2.2004e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.6340e-10, dtype=torch.float64)
secont condition:: tensor(1.6340e-10, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(2.0187e-10, dtype=torch.float64)
secont condition:: tensor(2.0187e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.9749e-10, dtype=torch.float64)
secont condition:: tensor(2.9749e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.6976e-10, dtype=torch.float64)
secont condition:: tensor(1.6976e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.5754e-10, dtype=torch.float64)
secont condition:: tensor(1.5754e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.8468e-10, dtype=torch.float64)
secont condition:: tensor(2.8468e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.6313e-10, dtype=torch.float64)
secont condition:: tensor(1.6313e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.2396e-10, dtype=torch.float64)
secont condition:: tensor(4.2396e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.1160e-10, dtype=torch.float64)
secont condition:: tensor(4.1160e-10, dtype=torch.float64)
curr_secont condition:: tensor(6.3751e-11, dtype=torch.float64)
secont condition:: tensor(6.3751e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.7848e-10, dtype=torch.float64)
secont condition:: tensor(3.7848e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.4373e-10, dtype=torch.float64)
explicit_evaluation epoch:: 43
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(4.8088e-10, dtype=torch.float64)
secont condition:: tensor(4.8088e-10, dtype=torch.float64)
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(7.2479e-10, dtype=torch.float64)
secont condition:: tensor(7.2479e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.6534e-09, dtype=torch.float64)
secont condition:: tensor(3.6534e-09, dtype=torch.float64)
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(2.6408e-09, dtype=torch.float64)
secont condition:: tensor(2.6408e-09, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(3.0583e-09, dtype=torch.float64)
secont condition:: tensor(3.0583e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.2681e-09, dtype=torch.float64)
secont condition:: tensor(2.2681e-09, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(1.6590e-09, dtype=torch.float64)
secont condition:: tensor(1.6590e-09, dtype=torch.float64)
epoch  8
	calling Sampler:__iter__
curr_secont condition:: tensor(4.5429e-09, dtype=torch.float64)
secont condition:: tensor(4.5429e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.5862e-09, dtype=torch.float64)
secont condition:: tensor(2.5862e-09, dtype=torch.float64)
epoch  9
	calling Sampler:__iter__
curr_secont condition:: tensor(5.2848e-09, dtype=torch.float64)
secont condition:: tensor(5.2848e-09, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 28.588514804840088
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.5663e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(8.0439e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(7.1528e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.6771e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(8.8526e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000023, Accuracy: 0.715000
repetition 3
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 10
delta_size:: 1
max_epoch:: 10
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
training time is 61.979137659072876
time_baseline:: 62.00219464302063
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(9.0423e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(2.9645e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000023, Accuracy: 0.722000
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 10
delta_size:: 1
max_epoch:: 10
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(6.0348e-33, dtype=torch.float64)
secont condition:: tensor(6.0348e-33, dtype=torch.float64)
curr_secont condition:: tensor(1.4958e-32, dtype=torch.float64)
secont condition:: tensor(1.4958e-32, dtype=torch.float64)
curr_secont condition:: tensor(2.6747e-32, dtype=torch.float64)
secont condition:: tensor(2.6747e-32, dtype=torch.float64)
curr_secont condition:: tensor(4.1223e-32, dtype=torch.float64)
secont condition:: tensor(4.1223e-32, dtype=torch.float64)
curr_secont condition:: tensor(5.8547e-32, dtype=torch.float64)
secont condition:: tensor(5.8547e-32, dtype=torch.float64)
curr_secont condition:: tensor(7.8603e-32, dtype=torch.float64)
secont condition:: tensor(7.8603e-32, dtype=torch.float64)
curr_secont condition:: tensor(4.3987e-11, dtype=torch.float64)
secont condition:: tensor(4.3987e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.0659e-11, dtype=torch.float64)
secont condition:: tensor(3.0659e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.5016e-11, dtype=torch.float64)
secont condition:: tensor(2.5016e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.5029e-11, dtype=torch.float64)
secont condition:: tensor(2.5029e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.2063e-11, dtype=torch.float64)
secont condition:: tensor(2.2063e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.4500e-11, dtype=torch.float64)
secont condition:: tensor(2.4500e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.5685e-11, dtype=torch.float64)
secont condition:: tensor(1.5685e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.3122e-11, dtype=torch.float64)
secont condition:: tensor(1.3122e-11, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(1.3805e-11, dtype=torch.float64)
secont condition:: tensor(1.3805e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.2016e-11, dtype=torch.float64)
secont condition:: tensor(1.2016e-11, dtype=torch.float64)
curr_secont condition:: tensor(9.3083e-12, dtype=torch.float64)
secont condition:: tensor(9.3083e-12, dtype=torch.float64)
curr_secont condition:: tensor(9.3826e-12, dtype=torch.float64)
secont condition:: tensor(9.3826e-12, dtype=torch.float64)
curr_secont condition:: tensor(8.8966e-12, dtype=torch.float64)
secont condition:: tensor(8.8966e-12, dtype=torch.float64)
curr_secont condition:: tensor(9.2747e-12, dtype=torch.float64)
secont condition:: tensor(9.2747e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.0086e-11, dtype=torch.float64)
secont condition:: tensor(1.0086e-11, dtype=torch.float64)
curr_secont condition:: tensor(7.2040e-12, dtype=torch.float64)
secont condition:: tensor(7.2040e-12, dtype=torch.float64)
curr_secont condition:: tensor(-2.2034e-12, dtype=torch.float64)
curr_secont condition:: tensor(7.2320e-12, dtype=torch.float64)
secont condition:: tensor(7.2320e-12, dtype=torch.float64)
curr_secont condition:: tensor(9.0137e-12, dtype=torch.float64)
secont condition:: tensor(9.0137e-12, dtype=torch.float64)
curr_secont condition:: tensor(7.9320e-12, dtype=torch.float64)
secont condition:: tensor(7.9320e-12, dtype=torch.float64)
curr_secont condition:: tensor(5.9586e-12, dtype=torch.float64)
secont condition:: tensor(5.9586e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.9268e-11, dtype=torch.float64)
secont condition:: tensor(1.9268e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.3422e-11, dtype=torch.float64)
secont condition:: tensor(2.3422e-11, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(2.3565e-11, dtype=torch.float64)
secont condition:: tensor(2.3565e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.7533e-11, dtype=torch.float64)
secont condition:: tensor(1.7533e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.6828e-11, dtype=torch.float64)
secont condition:: tensor(1.6828e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.4523e-11, dtype=torch.float64)
secont condition:: tensor(1.4523e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.8790e-11, dtype=torch.float64)
secont condition:: tensor(1.8790e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.3290e-11, dtype=torch.float64)
secont condition:: tensor(1.3290e-11, dtype=torch.float64)
curr_secont condition:: tensor(9.8194e-12, dtype=torch.float64)
secont condition:: tensor(9.8194e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.3896e-11, dtype=torch.float64)
secont condition:: tensor(1.3896e-11, dtype=torch.float64)
curr_secont condition:: tensor(9.3199e-12, dtype=torch.float64)
secont condition:: tensor(9.3199e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.0853e-11, dtype=torch.float64)
secont condition:: tensor(1.0853e-11, dtype=torch.float64)
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(5.1947e-11, dtype=torch.float64)
secont condition:: tensor(5.1947e-11, dtype=torch.float64)
curr_secont condition:: tensor(8.7576e-11, dtype=torch.float64)
secont condition:: tensor(8.7576e-11, dtype=torch.float64)
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(1.5421e-10, dtype=torch.float64)
secont condition:: tensor(1.5421e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.9144e-10, dtype=torch.float64)
explicit_evaluation epoch:: 71
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(7.9074e-11, dtype=torch.float64)
secont condition:: tensor(7.9074e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.7464e-09, dtype=torch.float64)
explicit_evaluation epoch:: 88
curr_secont condition:: tensor(3.3247e-10, dtype=torch.float64)
explicit_evaluation epoch:: 89
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(4.0635e-09, dtype=torch.float64)
explicit_evaluation epoch:: 95
curr_secont condition:: tensor(8.9168e-10, dtype=torch.float64)
explicit_evaluation epoch:: 96
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(3.6893e-10, dtype=torch.float64)
secont condition:: tensor(3.6893e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.8095e-10, dtype=torch.float64)
secont condition:: tensor(3.8095e-10, dtype=torch.float64)
epoch  8
	calling Sampler:__iter__
curr_secont condition:: tensor(7.0740e-10, dtype=torch.float64)
explicit_evaluation epoch:: 120
curr_secont condition:: tensor(5.1030e-10, dtype=torch.float64)
secont condition:: tensor(5.1030e-10, dtype=torch.float64)
epoch  9
	calling Sampler:__iter__
curr_secont condition:: tensor(8.8336e-10, dtype=torch.float64)
secont condition:: tensor(8.8336e-10, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 29.752819538116455
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(8.1375e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(4.1278e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(7.0710e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(7.4177e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(8.9794e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000023, Accuracy: 0.712000
repetition 4
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 10
delta_size:: 1
max_epoch:: 10
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
training time is 62.522849559783936
time_baseline:: 62.54557776451111
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.4630e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(5.6154e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000023, Accuracy: 0.722000
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 10
delta_size:: 1
max_epoch:: 10
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(6.0348e-33, dtype=torch.float64)
secont condition:: tensor(6.0348e-33, dtype=torch.float64)
curr_secont condition:: tensor(1.4958e-32, dtype=torch.float64)
secont condition:: tensor(1.4958e-32, dtype=torch.float64)
curr_secont condition:: tensor(2.6747e-32, dtype=torch.float64)
secont condition:: tensor(2.6747e-32, dtype=torch.float64)
curr_secont condition:: tensor(4.1223e-32, dtype=torch.float64)
secont condition:: tensor(4.1223e-32, dtype=torch.float64)
curr_secont condition:: tensor(5.8547e-32, dtype=torch.float64)
secont condition:: tensor(5.8547e-32, dtype=torch.float64)
curr_secont condition:: tensor(7.8546e-32, dtype=torch.float64)
secont condition:: tensor(7.8546e-32, dtype=torch.float64)
curr_secont condition:: tensor(1.0110e-31, dtype=torch.float64)
secont condition:: tensor(1.0110e-31, dtype=torch.float64)
curr_secont condition:: tensor(1.2658e-31, dtype=torch.float64)
secont condition:: tensor(1.2658e-31, dtype=torch.float64)
curr_secont condition:: tensor(7.3056e-10, dtype=torch.float64)
secont condition:: tensor(7.3056e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.9544e-10, dtype=torch.float64)
secont condition:: tensor(3.9544e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.8683e-10, dtype=torch.float64)
secont condition:: tensor(3.8683e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.4894e-10, dtype=torch.float64)
secont condition:: tensor(2.4894e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.8586e-10, dtype=torch.float64)
secont condition:: tensor(1.8586e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.0635e-10, dtype=torch.float64)
secont condition:: tensor(1.0635e-10, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(1.4176e-10, dtype=torch.float64)
secont condition:: tensor(1.4176e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.3535e-10, dtype=torch.float64)
secont condition:: tensor(1.3535e-10, dtype=torch.float64)
curr_secont condition:: tensor(7.3900e-11, dtype=torch.float64)
secont condition:: tensor(7.3900e-11, dtype=torch.float64)
curr_secont condition:: tensor(8.3903e-11, dtype=torch.float64)
secont condition:: tensor(8.3903e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.3248e-11, dtype=torch.float64)
secont condition:: tensor(4.3248e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.8758e-11, dtype=torch.float64)
secont condition:: tensor(6.8758e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.2459e-11, dtype=torch.float64)
secont condition:: tensor(5.2459e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.3555e-11, dtype=torch.float64)
secont condition:: tensor(4.3555e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.6221e-11, dtype=torch.float64)
secont condition:: tensor(3.6221e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.6636e-11, dtype=torch.float64)
secont condition:: tensor(6.6636e-11, dtype=torch.float64)
curr_secont condition:: tensor(9.4282e-11, dtype=torch.float64)
secont condition:: tensor(9.4282e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.3301e-10, dtype=torch.float64)
secont condition:: tensor(1.3301e-10, dtype=torch.float64)
curr_secont condition:: tensor(7.5369e-11, dtype=torch.float64)
secont condition:: tensor(7.5369e-11, dtype=torch.float64)
curr_secont condition:: tensor(7.0432e-11, dtype=torch.float64)
secont condition:: tensor(7.0432e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.0125e-11, dtype=torch.float64)
secont condition:: tensor(3.0125e-11, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(6.4325e-11, dtype=torch.float64)
secont condition:: tensor(6.4325e-11, dtype=torch.float64)
curr_secont condition:: tensor(-4.5393e-11, dtype=torch.float64)
curr_secont condition:: tensor(7.3805e-11, dtype=torch.float64)
secont condition:: tensor(7.3805e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.8964e-11, dtype=torch.float64)
secont condition:: tensor(3.8964e-11, dtype=torch.float64)
curr_secont condition:: tensor(9.3715e-11, dtype=torch.float64)
secont condition:: tensor(9.3715e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.1231e-09, dtype=torch.float64)
secont condition:: tensor(2.1231e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.5796e-09, dtype=torch.float64)
secont condition:: tensor(1.5796e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.1670e-09, dtype=torch.float64)
secont condition:: tensor(1.1670e-09, dtype=torch.float64)
curr_secont condition:: tensor(8.6521e-10, dtype=torch.float64)
secont condition:: tensor(8.6521e-10, dtype=torch.float64)
curr_secont condition:: tensor(6.7106e-10, dtype=torch.float64)
secont condition:: tensor(6.7106e-10, dtype=torch.float64)
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(3.7516e-10, dtype=torch.float64)
secont condition:: tensor(3.7516e-10, dtype=torch.float64)
curr_secont condition:: tensor(7.7469e-10, dtype=torch.float64)
secont condition:: tensor(7.7469e-10, dtype=torch.float64)
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(9.7199e-10, dtype=torch.float64)
secont condition:: tensor(9.7199e-10, dtype=torch.float64)
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(6.1700e-10, dtype=torch.float64)
secont condition:: tensor(6.1700e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.4223e-09, dtype=torch.float64)
secont condition:: tensor(1.4223e-09, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(3.0856e-09, dtype=torch.float64)
secont condition:: tensor(3.0856e-09, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(2.2610e-09, dtype=torch.float64)
secont condition:: tensor(2.2610e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.7235e-09, dtype=torch.float64)
secont condition:: tensor(1.7235e-09, dtype=torch.float64)
epoch  8
	calling Sampler:__iter__
curr_secont condition:: tensor(1.4264e-09, dtype=torch.float64)
secont condition:: tensor(1.4264e-09, dtype=torch.float64)
epoch  9
	calling Sampler:__iter__
curr_secont condition:: tensor(1.2946e-09, dtype=torch.float64)
secont condition:: tensor(1.2946e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.1811e-09, dtype=torch.float64)
secont condition:: tensor(2.1811e-09, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 27.793615579605103
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.1004e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(6.4874e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(3.8950e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.1635e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(3.5707e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000023, Accuracy: 0.722100
repetition 5
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 10
delta_size:: 1
max_epoch:: 10
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
training time is 62.44379377365112
time_baseline:: 62.46735620498657
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.4490e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.4052e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000023, Accuracy: 0.722000
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 10
delta_size:: 1
max_epoch:: 10
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(6.0348e-33, dtype=torch.float64)
secont condition:: tensor(6.0348e-33, dtype=torch.float64)
curr_secont condition:: tensor(1.4958e-32, dtype=torch.float64)
secont condition:: tensor(1.4958e-32, dtype=torch.float64)
curr_secont condition:: tensor(2.6747e-32, dtype=torch.float64)
secont condition:: tensor(2.6747e-32, dtype=torch.float64)
curr_secont condition:: tensor(4.1223e-32, dtype=torch.float64)
secont condition:: tensor(4.1223e-32, dtype=torch.float64)
curr_secont condition:: tensor(5.8547e-32, dtype=torch.float64)
secont condition:: tensor(5.8547e-32, dtype=torch.float64)
curr_secont condition:: tensor(7.8546e-32, dtype=torch.float64)
secont condition:: tensor(7.8546e-32, dtype=torch.float64)
curr_secont condition:: tensor(1.0110e-31, dtype=torch.float64)
secont condition:: tensor(1.0110e-31, dtype=torch.float64)
curr_secont condition:: tensor(1.2650e-31, dtype=torch.float64)
secont condition:: tensor(1.2650e-31, dtype=torch.float64)
curr_secont condition:: tensor(1.5452e-31, dtype=torch.float64)
secont condition:: tensor(1.5452e-31, dtype=torch.float64)
curr_secont condition:: tensor(1.8514e-31, dtype=torch.float64)
secont condition:: tensor(1.8514e-31, dtype=torch.float64)
curr_secont condition:: tensor(2.1853e-31, dtype=torch.float64)
secont condition:: tensor(2.1853e-31, dtype=torch.float64)
curr_secont condition:: tensor(2.5430e-31, dtype=torch.float64)
secont condition:: tensor(2.5430e-31, dtype=torch.float64)
curr_secont condition:: tensor(2.9246e-31, dtype=torch.float64)
secont condition:: tensor(2.9246e-31, dtype=torch.float64)
curr_secont condition:: tensor(3.3359e-31, dtype=torch.float64)
secont condition:: tensor(3.3359e-31, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(2.7684e-10, dtype=torch.float64)
secont condition:: tensor(2.7684e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.8253e-10, dtype=torch.float64)
secont condition:: tensor(2.8253e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.1014e-10, dtype=torch.float64)
secont condition:: tensor(2.1014e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.9376e-10, dtype=torch.float64)
secont condition:: tensor(1.9376e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.4043e-10, dtype=torch.float64)
secont condition:: tensor(1.4043e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.4717e-10, dtype=torch.float64)
secont condition:: tensor(1.4717e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.0706e-10, dtype=torch.float64)
secont condition:: tensor(1.0706e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.1106e-10, dtype=torch.float64)
secont condition:: tensor(1.1106e-10, dtype=torch.float64)
curr_secont condition:: tensor(9.4846e-11, dtype=torch.float64)
secont condition:: tensor(9.4846e-11, dtype=torch.float64)
curr_secont condition:: tensor(8.0460e-11, dtype=torch.float64)
secont condition:: tensor(8.0460e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.2979e-11, dtype=torch.float64)
secont condition:: tensor(6.2979e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.7665e-10, dtype=torch.float64)
secont condition:: tensor(4.7665e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.3209e-10, dtype=torch.float64)
secont condition:: tensor(4.3209e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.0406e-10, dtype=torch.float64)
secont condition:: tensor(3.0406e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.5487e-10, dtype=torch.float64)
secont condition:: tensor(2.5487e-10, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(1.5670e-10, dtype=torch.float64)
secont condition:: tensor(1.5670e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.8816e-10, dtype=torch.float64)
secont condition:: tensor(1.8816e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.6374e-10, dtype=torch.float64)
secont condition:: tensor(1.6374e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.5787e-10, dtype=torch.float64)
secont condition:: tensor(1.5787e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.5673e-10, dtype=torch.float64)
secont condition:: tensor(1.5673e-10, dtype=torch.float64)
curr_secont condition:: tensor(8.2647e-11, dtype=torch.float64)
secont condition:: tensor(8.2647e-11, dtype=torch.float64)
curr_secont condition:: tensor(9.4845e-11, dtype=torch.float64)
secont condition:: tensor(9.4845e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.2140e-10, dtype=torch.float64)
secont condition:: tensor(1.2140e-10, dtype=torch.float64)
curr_secont condition:: tensor(7.0832e-11, dtype=torch.float64)
secont condition:: tensor(7.0832e-11, dtype=torch.float64)
curr_secont condition:: tensor(8.6906e-11, dtype=torch.float64)
secont condition:: tensor(8.6906e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.9336e-10, dtype=torch.float64)
explicit_evaluation epoch:: 41
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(3.8217e-10, dtype=torch.float64)
secont condition:: tensor(3.8217e-10, dtype=torch.float64)
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(3.7030e-10, dtype=torch.float64)
secont condition:: tensor(3.7030e-10, dtype=torch.float64)
curr_secont condition:: tensor(7.0120e-10, dtype=torch.float64)
secont condition:: tensor(7.0120e-10, dtype=torch.float64)
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(6.6853e-10, dtype=torch.float64)
secont condition:: tensor(6.6853e-10, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(1.0643e-09, dtype=torch.float64)
secont condition:: tensor(1.0643e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.0076e-09, dtype=torch.float64)
secont condition:: tensor(1.0076e-09, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(5.8495e-10, dtype=torch.float64)
secont condition:: tensor(5.8495e-10, dtype=torch.float64)
epoch  8
	calling Sampler:__iter__
curr_secont condition:: tensor(6.0245e-10, dtype=torch.float64)
secont condition:: tensor(6.0245e-10, dtype=torch.float64)
curr_secont condition:: tensor(5.2242e-10, dtype=torch.float64)
secont condition:: tensor(5.2242e-10, dtype=torch.float64)
epoch  9
	calling Sampler:__iter__
curr_secont condition:: tensor(1.0625e-09, dtype=torch.float64)
secont condition:: tensor(1.0625e-09, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 28.646058082580566
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.0323e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(6.8307e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.0242e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.3138e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(2.4203e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000023, Accuracy: 0.711500
epochs:: 12
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type MSELoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
cuda:2
Train - Epoch 0, Batch: 0, Loss: 0.091544
Train - Epoch 0, Batch: 10, Loss: 0.090281
Test Avg. Loss: 0.000027, Accuracy: 0.326300
Train - Epoch 1, Batch: 0, Loss: 0.089719
Train - Epoch 1, Batch: 10, Loss: 0.089165
Test Avg. Loss: 0.000026, Accuracy: 0.574700
Train - Epoch 2, Batch: 0, Loss: 0.088606
Train - Epoch 2, Batch: 10, Loss: 0.088131
Test Avg. Loss: 0.000026, Accuracy: 0.640000
Train - Epoch 3, Batch: 0, Loss: 0.087413
Train - Epoch 3, Batch: 10, Loss: 0.086967
Test Avg. Loss: 0.000025, Accuracy: 0.657200
Train - Epoch 4, Batch: 0, Loss: 0.086142
Train - Epoch 4, Batch: 10, Loss: 0.085224
Test Avg. Loss: 0.000025, Accuracy: 0.662700
Train - Epoch 5, Batch: 0, Loss: 0.085057
Train - Epoch 5, Batch: 10, Loss: 0.084319
Test Avg. Loss: 0.000025, Accuracy: 0.676800
Train - Epoch 6, Batch: 0, Loss: 0.083921
Train - Epoch 6, Batch: 10, Loss: 0.083097
Test Avg. Loss: 0.000024, Accuracy: 0.690200
Train - Epoch 7, Batch: 0, Loss: 0.083321
Train - Epoch 7, Batch: 10, Loss: 0.082739
Test Avg. Loss: 0.000024, Accuracy: 0.701300
Train - Epoch 8, Batch: 0, Loss: 0.082079
Train - Epoch 8, Batch: 10, Loss: 0.081350
Test Avg. Loss: 0.000024, Accuracy: 0.705200
Train - Epoch 9, Batch: 0, Loss: 0.081480
Train - Epoch 9, Batch: 10, Loss: 0.080960
Test Avg. Loss: 0.000023, Accuracy: 0.722000
Train - Epoch 10, Batch: 0, Loss: 0.080599
Train - Epoch 10, Batch: 10, Loss: 0.079949
Test Avg. Loss: 0.000023, Accuracy: 0.729700
Train - Epoch 11, Batch: 0, Loss: 0.079676
Train - Epoch 11, Batch: 10, Loss: 0.079518
Test Avg. Loss: 0.000023, Accuracy: 0.734300
training_time:: 93.28003430366516
repetition 1
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 12
delta_size:: 1
max_epoch:: 12
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
training time is 74.81824684143066
time_baseline:: 74.84471201896667
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.9422e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.8087e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000023, Accuracy: 0.734300
incremental updates::
max_epoch:: 12
delta_size:: 1
max_epoch:: 12
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(6.0482e-33, dtype=torch.float64)
secont condition:: tensor(6.0482e-33, dtype=torch.float64)
curr_secont condition:: tensor(4.6675e-10, dtype=torch.float64)
secont condition:: tensor(4.6675e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.4249e-10, dtype=torch.float64)
secont condition:: tensor(2.4249e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.3354e-10, dtype=torch.float64)
secont condition:: tensor(1.3354e-10, dtype=torch.float64)
curr_secont condition:: tensor(9.7295e-11, dtype=torch.float64)
secont condition:: tensor(9.7295e-11, dtype=torch.float64)
curr_secont condition:: tensor(8.5517e-11, dtype=torch.float64)
secont condition:: tensor(8.5517e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.4342e-11, dtype=torch.float64)
secont condition:: tensor(6.4342e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.5240e-11, dtype=torch.float64)
secont condition:: tensor(6.5240e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.5697e-11, dtype=torch.float64)
secont condition:: tensor(3.5697e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.4915e-11, dtype=torch.float64)
secont condition:: tensor(4.4915e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.2617e-11, dtype=torch.float64)
secont condition:: tensor(3.2617e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.8202e-11, dtype=torch.float64)
secont condition:: tensor(5.8202e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.0190e-11, dtype=torch.float64)
secont condition:: tensor(3.0190e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.0699e-11, dtype=torch.float64)
secont condition:: tensor(3.0699e-11, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(2.5791e-11, dtype=torch.float64)
secont condition:: tensor(2.5791e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.9009e-11, dtype=torch.float64)
secont condition:: tensor(2.9009e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.8751e-11, dtype=torch.float64)
secont condition:: tensor(2.8751e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.6259e-11, dtype=torch.float64)
secont condition:: tensor(2.6259e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.2829e-10, dtype=torch.float64)
secont condition:: tensor(2.2829e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.5226e-10, dtype=torch.float64)
secont condition:: tensor(2.5226e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.4114e-10, dtype=torch.float64)
secont condition:: tensor(1.4114e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.3502e-10, dtype=torch.float64)
secont condition:: tensor(1.3502e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.0678e-10, dtype=torch.float64)
secont condition:: tensor(1.0678e-10, dtype=torch.float64)
curr_secont condition:: tensor(8.2395e-11, dtype=torch.float64)
secont condition:: tensor(8.2395e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.2328e-10, dtype=torch.float64)
secont condition:: tensor(1.2328e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.3919e-11, dtype=torch.float64)
secont condition:: tensor(4.3919e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.5614e-11, dtype=torch.float64)
secont condition:: tensor(6.5614e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.8587e-11, dtype=torch.float64)
secont condition:: tensor(4.8587e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.3064e-11, dtype=torch.float64)
secont condition:: tensor(3.3064e-11, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(1.1096e-11, dtype=torch.float64)
secont condition:: tensor(1.1096e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.3567e-11, dtype=torch.float64)
secont condition:: tensor(1.3567e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.6904e-11, dtype=torch.float64)
secont condition:: tensor(4.6904e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.6701e-11, dtype=torch.float64)
secont condition:: tensor(6.6701e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.4696e-11, dtype=torch.float64)
secont condition:: tensor(5.4696e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.1484e-11, dtype=torch.float64)
secont condition:: tensor(5.1484e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.6384e-11, dtype=torch.float64)
secont condition:: tensor(3.6384e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.5288e-11, dtype=torch.float64)
secont condition:: tensor(4.5288e-11, dtype=torch.float64)
curr_secont condition:: tensor(-2.8905e-12, dtype=torch.float64)
curr_secont condition:: tensor(7.2595e-11, dtype=torch.float64)
secont condition:: tensor(7.2595e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.5811e-11, dtype=torch.float64)
explicit_evaluation epoch:: 40
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(1.6292e-11, dtype=torch.float64)
secont condition:: tensor(1.6292e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.5725e-11, dtype=torch.float64)
explicit_evaluation epoch:: 52
curr_secont condition:: tensor(3.4470e-11, dtype=torch.float64)
explicit_evaluation epoch:: 59
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(6.0349e-11, dtype=torch.float64)
explicit_evaluation epoch:: 60
curr_secont condition:: tensor(1.1329e-10, dtype=torch.float64)
secont condition:: tensor(1.1329e-10, dtype=torch.float64)
curr_secont condition:: tensor(-2.6539e-11, dtype=torch.float64)
explicit_evaluation epoch:: 73
curr_secont condition:: tensor(4.6523e-11, dtype=torch.float64)
secont condition:: tensor(4.6523e-11, dtype=torch.float64)
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(1.4628e-10, dtype=torch.float64)
explicit_evaluation epoch:: 75
curr_secont condition:: tensor(5.9695e-12, dtype=torch.float64)
explicit_evaluation epoch:: 79
curr_secont condition:: tensor(8.5814e-11, dtype=torch.float64)
explicit_evaluation epoch:: 80
curr_secont condition:: tensor(1.8680e-10, dtype=torch.float64)
explicit_evaluation epoch:: 82
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(2.7239e-10, dtype=torch.float64)
secont condition:: tensor(2.7239e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.5209e-10, dtype=torch.float64)
explicit_evaluation epoch:: 94
curr_secont condition:: tensor(5.6602e-10, dtype=torch.float64)
secont condition:: tensor(5.6602e-10, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(4.9235e-10, dtype=torch.float64)
secont condition:: tensor(4.9235e-10, dtype=torch.float64)
epoch  8
	calling Sampler:__iter__
curr_secont condition:: tensor(1.1601e-09, dtype=torch.float64)
secont condition:: tensor(1.1601e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.2635e-09, dtype=torch.float64)
secont condition:: tensor(2.2635e-09, dtype=torch.float64)
epoch  9
	calling Sampler:__iter__
curr_secont condition:: tensor(1.0587e-09, dtype=torch.float64)
secont condition:: tensor(1.0587e-09, dtype=torch.float64)
epoch  10
	calling Sampler:__iter__
curr_secont condition:: tensor(1.3529e-09, dtype=torch.float64)
secont condition:: tensor(1.3529e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.7846e-09, dtype=torch.float64)
secont condition:: tensor(1.7846e-09, dtype=torch.float64)
epoch  11
	calling Sampler:__iter__
curr_secont condition:: tensor(1.6277e-09, dtype=torch.float64)
secont condition:: tensor(1.6277e-09, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 33.89297389984131
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.4956e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(5.4949e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(8.3380e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)

curr_diff: 1 tensor(1.7546e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(2.6377e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000023, Accuracy: 0.734100
repetition 2
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 12
delta_size:: 1
max_epoch:: 12
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
training time is 75.6319682598114
time_baseline:: 75.65763473510742
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.6877e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(9.5258e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000023, Accuracy: 0.734300
incremental updates::
max_epoch:: 12
delta_size:: 1
max_epoch:: 12
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(6.0348e-33, dtype=torch.float64)
secont condition:: tensor(6.0348e-33, dtype=torch.float64)
curr_secont condition:: tensor(1.4958e-32, dtype=torch.float64)
secont condition:: tensor(1.4958e-32, dtype=torch.float64)
curr_secont condition:: tensor(2.6747e-32, dtype=torch.float64)
secont condition:: tensor(2.6747e-32, dtype=torch.float64)
curr_secont condition:: tensor(4.1223e-32, dtype=torch.float64)
secont condition:: tensor(4.1223e-32, dtype=torch.float64)
curr_secont condition:: tensor(5.8547e-32, dtype=torch.float64)
secont condition:: tensor(5.8547e-32, dtype=torch.float64)
curr_secont condition:: tensor(7.8546e-32, dtype=torch.float64)
secont condition:: tensor(7.8546e-32, dtype=torch.float64)
curr_secont condition:: tensor(1.0110e-31, dtype=torch.float64)
secont condition:: tensor(1.0110e-31, dtype=torch.float64)
curr_secont condition:: tensor(1.2650e-31, dtype=torch.float64)
secont condition:: tensor(1.2650e-31, dtype=torch.float64)
curr_secont condition:: tensor(1.5455e-31, dtype=torch.float64)
secont condition:: tensor(1.5455e-31, dtype=torch.float64)
curr_secont condition:: tensor(8.9415e-11, dtype=torch.float64)
secont condition:: tensor(8.9415e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.3778e-11, dtype=torch.float64)
secont condition:: tensor(6.3778e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.8997e-11, dtype=torch.float64)
secont condition:: tensor(4.8997e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.0379e-11, dtype=torch.float64)
secont condition:: tensor(4.0379e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.8400e-11, dtype=torch.float64)
secont condition:: tensor(2.8400e-11, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(3.4267e-11, dtype=torch.float64)
secont condition:: tensor(3.4267e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.9019e-11, dtype=torch.float64)
secont condition:: tensor(2.9019e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.5669e-11, dtype=torch.float64)
secont condition:: tensor(2.5669e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.2265e-11, dtype=torch.float64)
secont condition:: tensor(2.2265e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.1739e-11, dtype=torch.float64)
secont condition:: tensor(1.1739e-11, dtype=torch.float64)
curr_secont condition:: tensor(8.3485e-12, dtype=torch.float64)
secont condition:: tensor(8.3485e-12, dtype=torch.float64)
curr_secont condition:: tensor(2.3369e-11, dtype=torch.float64)
secont condition:: tensor(2.3369e-11, dtype=torch.float64)
curr_secont condition:: tensor(-2.4149e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.3233e-11, dtype=torch.float64)
secont condition:: tensor(1.3233e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.7779e-11, dtype=torch.float64)
secont condition:: tensor(5.7779e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.7160e-11, dtype=torch.float64)
secont condition:: tensor(4.7160e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.5058e-11, dtype=torch.float64)
secont condition:: tensor(5.5058e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.9033e-11, dtype=torch.float64)
secont condition:: tensor(3.9033e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.7423e-11, dtype=torch.float64)
secont condition:: tensor(3.7423e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.0607e-11, dtype=torch.float64)
secont condition:: tensor(3.0607e-11, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(2.7141e-11, dtype=torch.float64)
secont condition:: tensor(2.7141e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.1167e-11, dtype=torch.float64)
secont condition:: tensor(4.1167e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.6727e-11, dtype=torch.float64)
secont condition:: tensor(2.6727e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.8433e-11, dtype=torch.float64)
secont condition:: tensor(2.8433e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.4884e-11, dtype=torch.float64)
secont condition:: tensor(2.4884e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.2452e-11, dtype=torch.float64)
secont condition:: tensor(2.2452e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.0469e-11, dtype=torch.float64)
secont condition:: tensor(2.0469e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.5257e-11, dtype=torch.float64)
secont condition:: tensor(2.5257e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.0960e-11, dtype=torch.float64)
secont condition:: tensor(2.0960e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.5025e-11, dtype=torch.float64)
secont condition:: tensor(3.5025e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.0232e-11, dtype=torch.float64)
explicit_evaluation epoch:: 40
curr_secont condition:: tensor(2.1183e-10, dtype=torch.float64)
explicit_evaluation epoch:: 41
curr_secont condition:: tensor(2.1957e-10, dtype=torch.float64)
explicit_evaluation epoch:: 42
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(2.6673e-10, dtype=torch.float64)
secont condition:: tensor(2.6673e-10, dtype=torch.float64)
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(4.7834e-09, dtype=torch.float64)
explicit_evaluation epoch:: 60
curr_secont condition:: tensor(1.4879e-09, dtype=torch.float64)
explicit_evaluation epoch:: 61
curr_secont condition:: tensor(1.1606e-09, dtype=torch.float64)
secont condition:: tensor(1.1606e-09, dtype=torch.float64)
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(-6.3586e-09, dtype=torch.float64)
explicit_evaluation epoch:: 78
curr_secont condition:: tensor(3.0840e-09, dtype=torch.float64)
secont condition:: tensor(3.0840e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.9808e-09, dtype=torch.float64)
secont condition:: tensor(2.9808e-09, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(1.0174e-09, dtype=torch.float64)
secont condition:: tensor(1.0174e-09, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(1.0755e-09, dtype=torch.float64)
secont condition:: tensor(1.0755e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.0145e-09, dtype=torch.float64)
explicit_evaluation epoch:: 113
curr_secont condition:: tensor(1.9744e-09, dtype=torch.float64)
explicit_evaluation epoch:: 114
epoch  8
	calling Sampler:__iter__
curr_secont condition:: tensor(8.0155e-10, dtype=torch.float64)
secont condition:: tensor(8.0155e-10, dtype=torch.float64)
curr_secont condition:: tensor(7.2913e-10, dtype=torch.float64)
explicit_evaluation epoch:: 127
epoch  9
	calling Sampler:__iter__
curr_secont condition:: tensor(2.8515e-09, dtype=torch.float64)
secont condition:: tensor(2.8515e-09, dtype=torch.float64)
curr_secont condition:: tensor(5.3257e-09, dtype=torch.float64)
secont condition:: tensor(5.3257e-09, dtype=torch.float64)
epoch  10
	calling Sampler:__iter__
curr_secont condition:: tensor(5.1857e-09, dtype=torch.float64)
secont condition:: tensor(5.1857e-09, dtype=torch.float64)
epoch  11
	calling Sampler:__iter__
curr_secont condition:: tensor(3.0069e-09, dtype=torch.float64)
secont condition:: tensor(3.0069e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.6646e-09, dtype=torch.float64)
secont condition:: tensor(2.6646e-09, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 33.29533267021179
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.4212e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(6.4001e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(8.4013e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.4255e-05, dtype=torch.float64, grad_fn=<NormBackward0>)/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)

curr_diff: 2 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(4.0599e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000023, Accuracy: 0.732200
repetition 3
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 12
delta_size:: 1
max_epoch:: 12
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
training time is 75.57212805747986
time_baseline:: 75.59795188903809
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.4629e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(4.7129e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000023, Accuracy: 0.734200
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 12
delta_size:: 1
max_epoch:: 12
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(6.0348e-33, dtype=torch.float64)
secont condition:: tensor(6.0348e-33, dtype=torch.float64)
curr_secont condition:: tensor(1.4958e-32, dtype=torch.float64)
secont condition:: tensor(1.4958e-32, dtype=torch.float64)
curr_secont condition:: tensor(2.6747e-32, dtype=torch.float64)
secont condition:: tensor(2.6747e-32, dtype=torch.float64)
curr_secont condition:: tensor(4.1223e-32, dtype=torch.float64)
secont condition:: tensor(4.1223e-32, dtype=torch.float64)
curr_secont condition:: tensor(5.8547e-32, dtype=torch.float64)
secont condition:: tensor(5.8547e-32, dtype=torch.float64)
curr_secont condition:: tensor(7.8610e-32, dtype=torch.float64)
secont condition:: tensor(7.8610e-32, dtype=torch.float64)
curr_secont condition:: tensor(5.1080e-11, dtype=torch.float64)
secont condition:: tensor(5.1080e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.0464e-11, dtype=torch.float64)
secont condition:: tensor(4.0464e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.1292e-11, dtype=torch.float64)
secont condition:: tensor(3.1292e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.4357e-11, dtype=torch.float64)
secont condition:: tensor(2.4357e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.9777e-11, dtype=torch.float64)
secont condition:: tensor(1.9777e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.5739e-11, dtype=torch.float64)
secont condition:: tensor(1.5739e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.3520e-11, dtype=torch.float64)
secont condition:: tensor(1.3520e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.0058e-11, dtype=torch.float64)
secont condition:: tensor(1.0058e-11, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(1.0795e-11, dtype=torch.float64)
secont condition:: tensor(1.0795e-11, dtype=torch.float64)
curr_secont condition:: tensor(9.8943e-12, dtype=torch.float64)
secont condition:: tensor(9.8943e-12, dtype=torch.float64)
curr_secont condition:: tensor(9.1175e-12, dtype=torch.float64)
secont condition:: tensor(9.1175e-12, dtype=torch.float64)
curr_secont condition:: tensor(8.5422e-12, dtype=torch.float64)
secont condition:: tensor(8.5422e-12, dtype=torch.float64)
curr_secont condition:: tensor(7.6017e-12, dtype=torch.float64)
secont condition:: tensor(7.6017e-12, dtype=torch.float64)
curr_secont condition:: tensor(4.6752e-12, dtype=torch.float64)
secont condition:: tensor(4.6752e-12, dtype=torch.float64)
curr_secont condition:: tensor(8.3356e-12, dtype=torch.float64)
secont condition:: tensor(8.3356e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.1674e-11, dtype=torch.float64)
secont condition:: tensor(1.1674e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.0121e-11, dtype=torch.float64)
secont condition:: tensor(1.0121e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.7953e-12, dtype=torch.float64)
secont condition:: tensor(6.7953e-12, dtype=torch.float64)
curr_secont condition:: tensor(6.2145e-12, dtype=torch.float64)
secont condition:: tensor(6.2145e-12, dtype=torch.float64)
curr_secont condition:: tensor(7.4692e-12, dtype=torch.float64)
secont condition:: tensor(7.4692e-12, dtype=torch.float64)
curr_secont condition:: tensor(6.1885e-10, dtype=torch.float64)
secont condition:: tensor(6.1885e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.6602e-10, dtype=torch.float64)
secont condition:: tensor(4.6602e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.6133e-10, dtype=torch.float64)
secont condition:: tensor(4.6133e-10, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(4.2347e-10, dtype=torch.float64)
secont condition:: tensor(4.2347e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.8575e-10, dtype=torch.float64)
secont condition:: tensor(2.8575e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.8601e-10, dtype=torch.float64)
secont condition:: tensor(2.8601e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.1913e-10, dtype=torch.float64)
secont condition:: tensor(2.1913e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.8358e-10, dtype=torch.float64)
secont condition:: tensor(1.8358e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.7515e-10, dtype=torch.float64)
secont condition:: tensor(1.7515e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.5746e-10, dtype=torch.float64)
secont condition:: tensor(1.5746e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.3776e-10, dtype=torch.float64)
secont condition:: tensor(1.3776e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.1449e-10, dtype=torch.float64)
secont condition:: tensor(1.1449e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.4091e-10, dtype=torch.float64)
secont condition:: tensor(1.4091e-10, dtype=torch.float64)
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(6.6591e-10, dtype=torch.float64)
secont condition:: tensor(6.6591e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.8296e-10, dtype=torch.float64)
secont condition:: tensor(4.8296e-10, dtype=torch.float64)
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(9.9141e-10, dtype=torch.float64)
explicit_evaluation epoch:: 67
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(6.9820e-10, dtype=torch.float64)
secont condition:: tensor(6.9820e-10, dtype=torch.float64)
curr_secont condition:: tensor(6.4864e-10, dtype=torch.float64)
secont condition:: tensor(6.4864e-10, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(7.3719e-10, dtype=torch.float64)
secont condition:: tensor(7.3719e-10, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(1.7187e-09, dtype=torch.float64)
secont condition:: tensor(1.7187e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.3270e-09, dtype=torch.float64)
secont condition:: tensor(1.3270e-09, dtype=torch.float64)
epoch  8
	calling Sampler:__iter__
curr_secont condition:: tensor(1.8096e-09, dtype=torch.float64)
secont condition:: tensor(1.8096e-09, dtype=torch.float64)
epoch  9
	calling Sampler:__iter__
curr_secont condition:: tensor(1.5691e-09, dtype=torch.float64)
secont condition:: tensor(1.5691e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.6513e-09, dtype=torch.float64)
secont condition:: tensor(1.6513e-09, dtype=torch.float64)
epoch  10
	calling Sampler:__iter__
curr_secont condition:: tensor(1.4690e-09, dtype=torch.float64)
secont condition:: tensor(1.4690e-09, dtype=torch.float64)
epoch  11
	calling Sampler:__iter__
curr_secont condition:: tensor(2.0620e-09, dtype=torch.float64)
secont condition:: tensor(2.0620e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.9314e-09, dtype=torch.float64)
secont condition:: tensor(1.9314e-09, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 30.467145919799805
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.3573e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(6.8890e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(8.5403e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.2117e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(6.9619e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000023, Accuracy: 0.731800
repetition 4
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 12
delta_size:: 1
max_epoch:: 12
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
training time is 74.9614086151123
time_baseline:: 74.9942376613617
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.3770e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.0136e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000023, Accuracy: 0.734300
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 12
delta_size:: 1
max_epoch:: 12
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(1.3308e-10, dtype=torch.float64)
secont condition:: tensor(1.3308e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.2797e-10, dtype=torch.float64)
secont condition:: tensor(1.2797e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.0815e-10, dtype=torch.float64)
secont condition:: tensor(1.0815e-10, dtype=torch.float64)
curr_secont condition:: tensor(7.9632e-11, dtype=torch.float64)
secont condition:: tensor(7.9632e-11, dtype=torch.float64)
curr_secont condition:: tensor(7.8057e-11, dtype=torch.float64)
secont condition:: tensor(7.8057e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.4956e-11, dtype=torch.float64)
secont condition:: tensor(6.4956e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.0256e-11, dtype=torch.float64)
secont condition:: tensor(6.0256e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.6375e-11, dtype=torch.float64)
secont condition:: tensor(4.6375e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.2661e-11, dtype=torch.float64)
secont condition:: tensor(4.2661e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.9141e-11, dtype=torch.float64)
secont condition:: tensor(3.9141e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.8951e-11, dtype=torch.float64)
secont condition:: tensor(2.8951e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.2237e-11, dtype=torch.float64)
secont condition:: tensor(4.2237e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.8008e-11, dtype=torch.float64)
secont condition:: tensor(1.8008e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.6914e-11, dtype=torch.float64)
secont condition:: tensor(1.6914e-11, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(2.6372e-11, dtype=torch.float64)
secont condition:: tensor(2.6372e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.8338e-11, dtype=torch.float64)
secont condition:: tensor(1.8338e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.6675e-11, dtype=torch.float64)
secont condition:: tensor(2.6675e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.7240e-11, dtype=torch.float64)
secont condition:: tensor(2.7240e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.3509e-10, dtype=torch.float64)
secont condition:: tensor(2.3509e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.2348e-10, dtype=torch.float64)
secont condition:: tensor(2.2348e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.7984e-10, dtype=torch.float64)
secont condition:: tensor(1.7984e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.5860e-10, dtype=torch.float64)
secont condition:: tensor(1.5860e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.0767e-10, dtype=torch.float64)
secont condition:: tensor(1.0767e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.0652e-10, dtype=torch.float64)
secont condition:: tensor(1.0652e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.0992e-10, dtype=torch.float64)
secont condition:: tensor(1.0992e-10, dtype=torch.float64)
curr_secont condition:: tensor(9.1163e-11, dtype=torch.float64)
secont condition:: tensor(9.1163e-11, dtype=torch.float64)
curr_secont condition:: tensor(9.3807e-11, dtype=torch.float64)
secont condition:: tensor(9.3807e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.4639e-11, dtype=torch.float64)
secont condition:: tensor(6.4639e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.7218e-11, dtype=torch.float64)
secont condition:: tensor(3.7218e-11, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(5.8998e-11, dtype=torch.float64)
secont condition:: tensor(5.8998e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.8822e-10, dtype=torch.float64)
secont condition:: tensor(3.8822e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.6619e-10, dtype=torch.float64)
secont condition:: tensor(3.6619e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.6702e-10, dtype=torch.float64)
secont condition:: tensor(2.6702e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.8645e-10, dtype=torch.float64)
secont condition:: tensor(2.8645e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.5564e-10, dtype=torch.float64)
secont condition:: tensor(2.5564e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.9562e-10, dtype=torch.float64)
secont condition:: tensor(1.9562e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.0870e-10, dtype=torch.float64)
secont condition:: tensor(2.0870e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.8067e-10, dtype=torch.float64)
secont condition:: tensor(1.8067e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.6039e-10, dtype=torch.float64)
secont condition:: tensor(1.6039e-10, dtype=torch.float64)
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(2.6911e-10, dtype=torch.float64)
secont condition:: tensor(2.6911e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.2566e-10, dtype=torch.float64)
secont condition:: tensor(2.2566e-10, dtype=torch.float64)
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(2.4068e-10, dtype=torch.float64)
explicit_evaluation epoch:: 61
curr_secont condition:: tensor(1.9677e-10, dtype=torch.float64)
secont condition:: tensor(1.9677e-10, dtype=torch.float64)
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(5.3815e-10, dtype=torch.float64)
explicit_evaluation epoch:: 75
curr_secont condition:: tensor(8.8333e-10, dtype=torch.float64)
secont condition:: tensor(8.8333e-10, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(6.0381e-10, dtype=torch.float64)
secont condition:: tensor(6.0381e-10, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(9.5912e-10, dtype=torch.float64)
secont condition:: tensor(9.5912e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.7543e-10, dtype=torch.float64)
secont condition:: tensor(4.7543e-10, dtype=torch.float64)
epoch  8
	calling Sampler:__iter__
curr_secont condition:: tensor(1.5072e-09, dtype=torch.float64)
secont condition:: tensor(1.5072e-09, dtype=torch.float64)
epoch  9
	calling Sampler:__iter__
curr_secont condition:: tensor(1.0255e-09, dtype=torch.float64)
secont condition:: tensor(1.0255e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.2096e-09, dtype=torch.float64)
secont condition:: tensor(1.2096e-09, dtype=torch.float64)
epoch  10
	calling Sampler:__iter__
curr_secont condition:: tensor(1.1108e-09, dtype=torch.float64)
secont condition:: tensor(1.1108e-09, dtype=torch.float64)
epoch  11
	calling Sampler:__iter__
curr_secont condition:: tensor(1.5671e-09, dtype=torch.float64)
secont condition:: tensor(1.5671e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.3557e-09, dtype=torch.float64)
secont condition:: tensor(2.3557e-09, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 30.917253494262695
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.2078e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(7.1718e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(9.7899e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.3031e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.9428e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000023, Accuracy: 0.731000
repetition 5
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 12
delta_size:: 1
max_epoch:: 12
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
training time is 74.62451601028442
time_baseline:: 74.65484404563904
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.3842e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(8.4445e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000023, Accuracy: 0.734300
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 12
delta_size:: 1
max_epoch:: 12
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(6.0348e-33, dtype=torch.float64)
secont condition:: tensor(6.0348e-33, dtype=torch.float64)
curr_secont condition:: tensor(1.4958e-32, dtype=torch.float64)
secont condition:: tensor(1.4958e-32, dtype=torch.float64)
curr_secont condition:: tensor(2.6747e-32, dtype=torch.float64)
secont condition:: tensor(2.6747e-32, dtype=torch.float64)
curr_secont condition:: tensor(4.1223e-32, dtype=torch.float64)
secont condition:: tensor(4.1223e-32, dtype=torch.float64)
curr_secont condition:: tensor(5.8547e-32, dtype=torch.float64)
secont condition:: tensor(5.8547e-32, dtype=torch.float64)
curr_secont condition:: tensor(7.8546e-32, dtype=torch.float64)
secont condition:: tensor(7.8546e-32, dtype=torch.float64)
curr_secont condition:: tensor(1.0110e-31, dtype=torch.float64)
secont condition:: tensor(1.0110e-31, dtype=torch.float64)
curr_secont condition:: tensor(1.2650e-31, dtype=torch.float64)
secont condition:: tensor(1.2650e-31, dtype=torch.float64)
curr_secont condition:: tensor(1.5452e-31, dtype=torch.float64)
secont condition:: tensor(1.5452e-31, dtype=torch.float64)
curr_secont condition:: tensor(1.8514e-31, dtype=torch.float64)
secont condition:: tensor(1.8514e-31, dtype=torch.float64)
curr_secont condition:: tensor(2.1853e-31, dtype=torch.float64)
secont condition:: tensor(2.1853e-31, dtype=torch.float64)
curr_secont condition:: tensor(2.5430e-31, dtype=torch.float64)
secont condition:: tensor(2.5430e-31, dtype=torch.float64)
curr_secont condition:: tensor(2.9246e-31, dtype=torch.float64)
secont condition:: tensor(2.9246e-31, dtype=torch.float64)
curr_secont condition:: tensor(3.3360e-31, dtype=torch.float64)
secont condition:: tensor(3.3360e-31, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(7.4234e-10, dtype=torch.float64)
secont condition:: tensor(7.4234e-10, dtype=torch.float64)
curr_secont condition:: tensor(6.0792e-10, dtype=torch.float64)
secont condition:: tensor(6.0792e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.5086e-10, dtype=torch.float64)
secont condition:: tensor(4.5086e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.7715e-10, dtype=torch.float64)
secont condition:: tensor(4.7715e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.1129e-10, dtype=torch.float64)
secont condition:: tensor(3.1129e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.2487e-10, dtype=torch.float64)
secont condition:: tensor(2.2487e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.8305e-10, dtype=torch.float64)
secont condition:: tensor(1.8305e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.8858e-10, dtype=torch.float64)
secont condition:: tensor(1.8858e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.4267e-10, dtype=torch.float64)
secont condition:: tensor(1.4267e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.5231e-10, dtype=torch.float64)
secont condition:: tensor(1.5231e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.1950e-10, dtype=torch.float64)
secont condition:: tensor(1.1950e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.1482e-10, dtype=torch.float64)
secont condition:: tensor(1.1482e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.2095e-10, dtype=torch.float64)
secont condition:: tensor(1.2095e-10, dtype=torch.float64)
curr_secont condition:: tensor(7.3209e-11, dtype=torch.float64)
secont condition:: tensor(7.3209e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.6730e-10, dtype=torch.float64)
secont condition:: tensor(1.6730e-10, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(6.2090e-10, dtype=torch.float64)
secont condition:: tensor(6.2090e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.6189e-10, dtype=torch.float64)
secont condition:: tensor(4.6189e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.2112e-10, dtype=torch.float64)
secont condition:: tensor(4.2112e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.6840e-10, dtype=torch.float64)
secont condition:: tensor(3.6840e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.0811e-10, dtype=torch.float64)
secont condition:: tensor(3.0811e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.0432e-10, dtype=torch.float64)
secont condition:: tensor(2.0432e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.0839e-10, dtype=torch.float64)
secont condition:: tensor(2.0839e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.4283e-10, dtype=torch.float64)
secont condition:: tensor(2.4283e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.8654e-10, dtype=torch.float64)
secont condition:: tensor(1.8654e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.5156e-10, dtype=torch.float64)
secont condition:: tensor(2.5156e-10, dtype=torch.float64)
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(2.1315e-10, dtype=torch.float64)
secont condition:: tensor(2.1315e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.2692e-10, dtype=torch.float64)
secont condition:: tensor(2.2692e-10, dtype=torch.float64)
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(7.2792e-10, dtype=torch.float64)
explicit_evaluation epoch:: 61
curr_secont condition:: tensor(3.2675e-10, dtype=torch.float64)
explicit_evaluation epoch:: 62
curr_secont condition:: tensor(3.8204e-10, dtype=torch.float64)
secont condition:: tensor(3.8204e-10, dtype=torch.float64)
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(4.0038e-10, dtype=torch.float64)
secont condition:: tensor(4.0038e-10, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(5.5386e-10, dtype=torch.float64)
secont condition:: tensor(5.5386e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.1290e-09, dtype=torch.float64)
secont condition:: tensor(1.1290e-09, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(1.3037e-09, dtype=torch.float64)
secont condition:: tensor(1.3037e-09, dtype=torch.float64)
epoch  8
	calling Sampler:__iter__
curr_secont condition:: tensor(1.9370e-09, dtype=torch.float64)
secont condition:: tensor(1.9370e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.3468e-09, dtype=torch.float64)
secont condition:: tensor(1.3468e-09, dtype=torch.float64)
epoch  9
	calling Sampler:__iter__
curr_secont condition:: tensor(1.4991e-09, dtype=torch.float64)
secont condition:: tensor(1.4991e-09, dtype=torch.float64)
epoch  10
	calling Sampler:__iter__
curr_secont condition:: tensor(1.6510e-09, dtype=torch.float64)
secont condition:: tensor(1.6510e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.0568e-09, dtype=torch.float64)
secont condition:: tensor(1.0568e-09, dtype=torch.float64)
epoch  11
	calling Sampler:__iter__
curr_secont condition:: tensor(1.2552e-09, dtype=torch.float64)
secont condition:: tensor(1.2552e-09, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 30.80547022819519
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.3395e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(6.8167e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.1854e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.3482e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.9853e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000023, Accuracy: 0.732000
epochs:: 15
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type MSELoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
cuda:2
Train - Epoch 0, Batch: 0, Loss: 0.091544
Train - Epoch 0, Batch: 10, Loss: 0.090281
Test Avg. Loss: 0.000027, Accuracy: 0.326300
Train - Epoch 1, Batch: 0, Loss: 0.089719
Train - Epoch 1, Batch: 10, Loss: 0.089165
Test Avg. Loss: 0.000026, Accuracy: 0.574700
Train - Epoch 2, Batch: 0, Loss: 0.088606
Train - Epoch 2, Batch: 10, Loss: 0.088131
Test Avg. Loss: 0.000026, Accuracy: 0.640000
Train - Epoch 3, Batch: 0, Loss: 0.087413
Train - Epoch 3, Batch: 10, Loss: 0.086967
Test Avg. Loss: 0.000025, Accuracy: 0.657200
Train - Epoch 4, Batch: 0, Loss: 0.086142
Train - Epoch 4, Batch: 10, Loss: 0.085224
Test Avg. Loss: 0.000025, Accuracy: 0.662700
Train - Epoch 5, Batch: 0, Loss: 0.085057
Train - Epoch 5, Batch: 10, Loss: 0.084319
Test Avg. Loss: 0.000025, Accuracy: 0.676800
Train - Epoch 6, Batch: 0, Loss: 0.083921
Train - Epoch 6, Batch: 10, Loss: 0.083097
Test Avg. Loss: 0.000024, Accuracy: 0.690200
Train - Epoch 7, Batch: 0, Loss: 0.083321
Train - Epoch 7, Batch: 10, Loss: 0.082739
Test Avg. Loss: 0.000024, Accuracy: 0.701300
Train - Epoch 8, Batch: 0, Loss: 0.082079
Train - Epoch 8, Batch: 10, Loss: 0.081350
Test Avg. Loss: 0.000024, Accuracy: 0.705200
Train - Epoch 9, Batch: 0, Loss: 0.081480
Train - Epoch 9, Batch: 10, Loss: 0.080960
Test Avg. Loss: 0.000023, Accuracy: 0.722000
Train - Epoch 10, Batch: 0, Loss: 0.080599
Train - Epoch 10, Batch: 10, Loss: 0.079949
Test Avg. Loss: 0.000023, Accuracy: 0.729700
Train - Epoch 11, Batch: 0, Loss: 0.079676
Train - Epoch 11, Batch: 10, Loss: 0.079518
Test Avg. Loss: 0.000023, Accuracy: 0.734300
Train - Epoch 12, Batch: 0, Loss: 0.079569
Train - Epoch 12, Batch: 10, Loss: 0.079139
Test Avg. Loss: 0.000022, Accuracy: 0.742500
Train - Epoch 13, Batch: 0, Loss: 0.078575
Train - Epoch 13, Batch: 10, Loss: 0.078437
Test Avg. Loss: 0.000022, Accuracy: 0.747200
Train - Epoch 14, Batch: 0, Loss: 0.078190
Train - Epoch 14, Batch: 10, Loss: 0.077907
Test Avg. Loss: 0.000022, Accuracy: 0.751100
training_time:: 117.57944774627686
repetition 1
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 15
delta_size:: 1
max_epoch:: 15
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
training time is 94.28697037696838
time_baseline:: 94.32307124137878
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(2.0185e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.3896e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000022, Accuracy: 0.751100
incremental updates::
max_epoch:: 15
delta_size:: 1
max_epoch:: 15
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(6.0348e-33, dtype=torch.float64)
secont condition:: tensor(6.0348e-33, dtype=torch.float64)
curr_secont condition:: tensor(1.4958e-32, dtype=torch.float64)
secont condition:: tensor(1.4958e-32, dtype=torch.float64)
curr_secont condition:: tensor(2.6747e-32, dtype=torch.float64)
secont condition:: tensor(2.6747e-32, dtype=torch.float64)
curr_secont condition:: tensor(4.1223e-32, dtype=torch.float64)
secont condition:: tensor(4.1223e-32, dtype=torch.float64)
curr_secont condition:: tensor(5.8547e-32, dtype=torch.float64)
secont condition:: tensor(5.8547e-32, dtype=torch.float64)
curr_secont condition:: tensor(7.8546e-32, dtype=torch.float64)
secont condition:: tensor(7.8546e-32, dtype=torch.float64)
curr_secont condition:: tensor(1.0110e-31, dtype=torch.float64)
secont condition:: tensor(1.0110e-31, dtype=torch.float64)
curr_secont condition:: tensor(1.2650e-31, dtype=torch.float64)
secont condition:: tensor(1.2650e-31, dtype=torch.float64)
curr_secont condition:: tensor(1.5452e-31, dtype=torch.float64)
secont condition:: tensor(1.5452e-31, dtype=torch.float64)
curr_secont condition:: tensor(1.8517e-31, dtype=torch.float64)
secont condition:: tensor(1.8517e-31, dtype=torch.float64)
curr_secont condition:: tensor(4.6129e-10, dtype=torch.float64)
secont condition:: tensor(4.6129e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.2647e-10, dtype=torch.float64)
secont condition:: tensor(3.2647e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.2264e-10, dtype=torch.float64)
secont condition:: tensor(2.2264e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.7168e-10, dtype=torch.float64)
secont condition:: tensor(1.7168e-10, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(1.5353e-10, dtype=torch.float64)
secont condition:: tensor(1.5353e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.2254e-10, dtype=torch.float64)
secont condition:: tensor(1.2254e-10, dtype=torch.float64)
curr_secont condition:: tensor(9.6843e-11, dtype=torch.float64)
secont condition:: tensor(9.6843e-11, dtype=torch.float64)
curr_secont condition:: tensor(8.5027e-11, dtype=torch.float64)
secont condition:: tensor(8.5027e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.1684e-11, dtype=torch.float64)
secont condition:: tensor(6.1684e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.2512e-11, dtype=torch.float64)
secont condition:: tensor(5.2512e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.6911e-11, dtype=torch.float64)
secont condition:: tensor(4.6911e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.8737e-11, dtype=torch.float64)
secont condition:: tensor(3.8737e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.1576e-10, dtype=torch.float64)
secont condition:: tensor(1.1576e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.2209e-10, dtype=torch.float64)
secont condition:: tensor(1.2209e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.0937e-10, dtype=torch.float64)
secont condition:: tensor(1.0937e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.0412e-10, dtype=torch.float64)
secont condition:: tensor(1.0412e-10, dtype=torch.float64)
curr_secont condition:: tensor(8.3249e-11, dtype=torch.float64)
secont condition:: tensor(8.3249e-11, dtype=torch.float64)
curr_secont condition:: tensor(8.4156e-11, dtype=torch.float64)
secont condition:: tensor(8.4156e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.8039e-11, dtype=torch.float64)
secont condition:: tensor(6.8039e-11, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(1.0442e-10, dtype=torch.float64)
secont condition:: tensor(1.0442e-10, dtype=torch.float64)
curr_secont condition:: tensor(-2.3355e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.8537e-09, dtype=torch.float64)
secont condition:: tensor(1.8537e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.1964e-09, dtype=torch.float64)
secont condition:: tensor(1.1964e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.0186e-09, dtype=torch.float64)
secont condition:: tensor(1.0186e-09, dtype=torch.float64)
curr_secont condition:: tensor(7.5930e-10, dtype=torch.float64)
secont condition:: tensor(7.5930e-10, dtype=torch.float64)
curr_secont condition:: tensor(6.5876e-10, dtype=torch.float64)
secont condition:: tensor(6.5876e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.7910e-10, dtype=torch.float64)
secont condition:: tensor(2.7910e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.4463e-10, dtype=torch.float64)
secont condition:: tensor(3.4463e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.6486e-10, dtype=torch.float64)
secont condition:: tensor(3.6486e-10, dtype=torch.float64)
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(-1.1195e-08, dtype=torch.float64)
explicit_evaluation epoch:: 46
curr_secont condition:: tensor(3.6704e-09, dtype=torch.float64)
secont condition:: tensor(3.6704e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.0520e-09, dtype=torch.float64)
secont condition:: tensor(2.0520e-09, dtype=torch.float64)
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(2.5117e-09, dtype=torch.float64)
secont condition:: tensor(2.5117e-09, dtype=torch.float64)
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(1.2402e-09, dtype=torch.float64)
secont condition:: tensor(1.2402e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.4042e-09, dtype=torch.float64)
secont condition:: tensor(1.4042e-09, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(2.6823e-09, dtype=torch.float64)
secont condition:: tensor(2.6823e-09, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(2.1484e-09, dtype=torch.float64)
secont condition:: tensor(2.1484e-09, dtype=torch.float64)
curr_secont condition:: tensor(4.5635e-09, dtype=torch.float64)
secont condition:: tensor(4.5635e-09, dtype=torch.float64)
epoch  8
	calling Sampler:__iter__
curr_secont condition:: tensor(4.9043e-09, dtype=torch.float64)
secont condition:: tensor(4.9043e-09, dtype=torch.float64)
epoch  9
	calling Sampler:__iter__
curr_secont condition:: tensor(4.7139e-09, dtype=torch.float64)
secont condition:: tensor(4.7139e-09, dtype=torch.float64)
curr_secont condition:: tensor(3.9203e-09, dtype=torch.float64)
secont condition:: tensor(3.9203e-09, dtype=torch.float64)
epoch  10
	calling Sampler:__iter__
curr_secont condition:: tensor(2.2355e-09, dtype=torch.float64)
secont condition:: tensor(2.2355e-09, dtype=torch.float64)
epoch  11
	calling Sampler:__iter__
curr_secont condition:: tensor(3.7301e-09, dtype=torch.float64)
secont condition:: tensor(3.7301e-09, dtype=torch.float64)
curr_secont condition:: tensor(3.0500e-09, dtype=torch.float64)
secont condition:: tensor(3.0500e-09, dtype=torch.float64)
epoch  12
	calling Sampler:__iter__
curr_secont condition:: tensor(2.3481e-09, dtype=torch.float64)
secont condition:: tensor(2.3481e-09, dtype=torch.float64)
epoch  13
	calling Sampler:__iter__
curr_secont condition:: tensor(2.6013e-09, dtype=torch.float64)
secont condition:: tensor(2.6013e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.5288e-09, dtype=torch.float64)
secont condition:: tensor(2.5288e-09, dtype=torch.float64)
epoch  14
	calling Sampler:__iter__
curr_secont condition:: tensor(2.0967e-09, dtype=torch.float64)
secont condition:: tensor(2.0967e-09, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 34.394365310668945
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.7732e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(7.0176e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)

curr_diff: 1 tensor(1.6466e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(2.0076e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000022, Accuracy: 0.750800
repetition 2
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 15
delta_size:: 1
max_epoch:: 15
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
training time is 93.29539680480957
time_baseline:: 93.32857012748718
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.3742e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(7.2727e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000022, Accuracy: 0.751100
incremental updates::
max_epoch:: 15
delta_size:: 1
max_epoch:: 15
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(6.0348e-33, dtype=torch.float64)
secont condition:: tensor(6.0348e-33, dtype=torch.float64)
curr_secont condition:: tensor(1.4958e-32, dtype=torch.float64)
secont condition:: tensor(1.4958e-32, dtype=torch.float64)
curr_secont condition:: tensor(2.6747e-32, dtype=torch.float64)
secont condition:: tensor(2.6747e-32, dtype=torch.float64)
curr_secont condition:: tensor(4.1223e-32, dtype=torch.float64)
secont condition:: tensor(4.1223e-32, dtype=torch.float64)
curr_secont condition:: tensor(5.8547e-32, dtype=torch.float64)
secont condition:: tensor(5.8547e-32, dtype=torch.float64)
curr_secont condition:: tensor(7.8546e-32, dtype=torch.float64)
secont condition:: tensor(7.8546e-32, dtype=torch.float64)
curr_secont condition:: tensor(1.0110e-31, dtype=torch.float64)
secont condition:: tensor(1.0110e-31, dtype=torch.float64)
curr_secont condition:: tensor(1.2650e-31, dtype=torch.float64)
secont condition:: tensor(1.2650e-31, dtype=torch.float64)
curr_secont condition:: tensor(1.5452e-31, dtype=torch.float64)
secont condition:: tensor(1.5452e-31, dtype=torch.float64)
curr_secont condition:: tensor(1.8514e-31, dtype=torch.float64)
secont condition:: tensor(1.8514e-31, dtype=torch.float64)
curr_secont condition:: tensor(2.1853e-31, dtype=torch.float64)
secont condition:: tensor(2.1853e-31, dtype=torch.float64)
curr_secont condition:: tensor(2.5428e-31, dtype=torch.float64)
secont condition:: tensor(2.5428e-31, dtype=torch.float64)
curr_secont condition:: tensor(4.2726e-11, dtype=torch.float64)
secont condition:: tensor(4.2726e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.3038e-11, dtype=torch.float64)
secont condition:: tensor(3.3038e-11, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(3.0172e-11, dtype=torch.float64)
secont condition:: tensor(3.0172e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.0630e-11, dtype=torch.float64)
secont condition:: tensor(2.0630e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.2094e-11, dtype=torch.float64)
secont condition:: tensor(2.2094e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.1300e-11, dtype=torch.float64)
secont condition:: tensor(2.1300e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.8101e-11, dtype=torch.float64)
secont condition:: tensor(1.8101e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.9327e-11, dtype=torch.float64)
secont condition:: tensor(1.9327e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.7170e-11, dtype=torch.float64)
secont condition:: tensor(1.7170e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.4497e-11, dtype=torch.float64)
secont condition:: tensor(1.4497e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.3834e-11, dtype=torch.float64)
secont condition:: tensor(1.3834e-11, dtype=torch.float64)
curr_secont condition:: tensor(9.1663e-12, dtype=torch.float64)
secont condition:: tensor(9.1663e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.0810e-11, dtype=torch.float64)
secont condition:: tensor(1.0810e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.2679e-11, dtype=torch.float64)
secont condition:: tensor(1.2679e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.0955e-11, dtype=torch.float64)
secont condition:: tensor(1.0955e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.1635e-11, dtype=torch.float64)
secont condition:: tensor(1.1635e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.9691e-11, dtype=torch.float64)
secont condition:: tensor(2.9691e-11, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(5.0842e-12, dtype=torch.float64)
secont condition:: tensor(5.0842e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.7663e-11, dtype=torch.float64)
secont condition:: tensor(1.7663e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.5112e-11, dtype=torch.float64)
secont condition:: tensor(1.5112e-11, dtype=torch.float64)
curr_secont condition:: tensor(7.8252e-12, dtype=torch.float64)
secont condition:: tensor(7.8252e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.3885e-11, dtype=torch.float64)
secont condition:: tensor(1.3885e-11, dtype=torch.float64)
curr_secont condition:: tensor(9.5110e-12, dtype=torch.float64)
secont condition:: tensor(9.5110e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.1684e-11, dtype=torch.float64)
secont condition:: tensor(1.1684e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.5683e-11, dtype=torch.float64)
secont condition:: tensor(1.5683e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.4397e-11, dtype=torch.float64)
secont condition:: tensor(1.4397e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.2255e-11, dtype=torch.float64)
secont condition:: tensor(1.2255e-11, dtype=torch.float64)
curr_secont condition:: tensor(9.2458e-12, dtype=torch.float64)
explicit_evaluation epoch:: 40
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(5.7880e-11, dtype=torch.float64)
secont condition:: tensor(5.7880e-11, dtype=torch.float64)
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(2.9627e-11, dtype=torch.float64)
secont condition:: tensor(2.9627e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.6459e-11, dtype=torch.float64)
explicit_evaluation epoch:: 62
curr_secont condition:: tensor(5.9454e-11, dtype=torch.float64)
explicit_evaluation epoch:: 64
curr_secont condition:: tensor(1.8146e-10, dtype=torch.float64)
explicit_evaluation epoch:: 70
curr_secont condition:: tensor(6.4455e-10, dtype=torch.float64)
explicit_evaluation epoch:: 71
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(2.0450e-10, dtype=torch.float64)
secont condition:: tensor(2.0450e-10, dtype=torch.float64)
curr_secont condition:: tensor(9.9443e-11, dtype=torch.float64)
explicit_evaluation epoch:: 86
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(1.4139e-10, dtype=torch.float64)
secont condition:: tensor(1.4139e-10, dtype=torch.float64)
curr_secont condition:: tensor(-1.5639e-09, dtype=torch.float64)
explicit_evaluation epoch:: 102
curr_secont condition:: tensor(1.1879e-09, dtype=torch.float64)
secont condition:: tensor(1.1879e-09, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(7.5689e-10, dtype=torch.float64)
secont condition:: tensor(7.5689e-10, dtype=torch.float64)
epoch  8
	calling Sampler:__iter__
curr_secont condition:: tensor(3.2494e-10, dtype=torch.float64)
secont condition:: tensor(3.2494e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.7170e-10, dtype=torch.float64)
secont condition:: tensor(2.7170e-10, dtype=torch.float64)
epoch  9
	calling Sampler:__iter__
curr_secont condition:: tensor(3.1175e-10, dtype=torch.float64)
secont condition:: tensor(3.1175e-10, dtype=torch.float64)
epoch  10
	calling Sampler:__iter__
curr_secont condition:: tensor(3.2430e-10, dtype=torch.float64)
secont condition:: tensor(3.2430e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.7827e-10, dtype=torch.float64)
secont condition:: tensor(2.7827e-10, dtype=torch.float64)
epoch  11
	calling Sampler:__iter__
curr_secont condition:: tensor(5.8959e-10, dtype=torch.float64)
secont condition:: tensor(5.8959e-10, dtype=torch.float64)
epoch  12
	calling Sampler:__iter__
curr_secont condition:: tensor(2.5476e-09, dtype=torch.float64)
secont condition:: tensor(2.5476e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.3829e-09, dtype=torch.float64)
secont condition:: tensor(1.3829e-09, dtype=torch.float64)
epoch  13
	calling Sampler:__iter__
curr_secont condition:: tensor(9.0762e-10, dtype=torch.float64)
secont condition:: tensor(9.0762e-10, dtype=torch.float64)
epoch  14
	calling Sampler:__iter__
curr_secont condition:: tensor(8.0152e-10, dtype=torch.float64)
secont condition:: tensor(8.0152e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.3306e-09, dtype=torch.float64)
secont condition:: tensor(1.3306e-09, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 36.37902045249939/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)

curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.1303e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(4.5870e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(5.5793e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.1180e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.2620e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000022, Accuracy: 0.752200
repetition 3
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 15
delta_size:: 1
max_epoch:: 15
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
training time is 93.18614554405212
time_baseline:: 93.22166895866394
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(2.1691e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(6.5977e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000022, Accuracy: 0.751200
incremental updates::
max_epoch:: 15
delta_size:: 1
max_epoch:: 15
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(6.0348e-33, dtype=torch.float64)
secont condition:: tensor(6.0348e-33, dtype=torch.float64)
curr_secont condition:: tensor(1.4958e-32, dtype=torch.float64)
secont condition:: tensor(1.4958e-32, dtype=torch.float64)
curr_secont condition:: tensor(2.6747e-32, dtype=torch.float64)
secont condition:: tensor(2.6747e-32, dtype=torch.float64)
curr_secont condition:: tensor(4.1223e-32, dtype=torch.float64)
secont condition:: tensor(4.1223e-32, dtype=torch.float64)
curr_secont condition:: tensor(5.8547e-32, dtype=torch.float64)
secont condition:: tensor(5.8547e-32, dtype=torch.float64)
curr_secont condition:: tensor(7.8546e-32, dtype=torch.float64)
secont condition:: tensor(7.8546e-32, dtype=torch.float64)
curr_secont condition:: tensor(1.0110e-31, dtype=torch.float64)
secont condition:: tensor(1.0110e-31, dtype=torch.float64)
curr_secont condition:: tensor(1.2650e-31, dtype=torch.float64)
secont condition:: tensor(1.2650e-31, dtype=torch.float64)
curr_secont condition:: tensor(1.5452e-31, dtype=torch.float64)
secont condition:: tensor(1.5452e-31, dtype=torch.float64)
curr_secont condition:: tensor(1.8516e-31, dtype=torch.float64)
secont condition:: tensor(1.8516e-31, dtype=torch.float64)
curr_secont condition:: tensor(5.4799e-10, dtype=torch.float64)
secont condition:: tensor(5.4799e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.4996e-10, dtype=torch.float64)
secont condition:: tensor(3.4996e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.2052e-10, dtype=torch.float64)
secont condition:: tensor(3.2052e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.0181e-10, dtype=torch.float64)
secont condition:: tensor(3.0181e-10, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(2.0353e-10, dtype=torch.float64)
secont condition:: tensor(2.0353e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.8015e-10, dtype=torch.float64)
secont condition:: tensor(1.8015e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.7332e-10, dtype=torch.float64)
secont condition:: tensor(1.7332e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.1041e-10, dtype=torch.float64)
secont condition:: tensor(1.1041e-10, dtype=torch.float64)
curr_secont condition:: tensor(7.9275e-11, dtype=torch.float64)
secont condition:: tensor(7.9275e-11, dtype=torch.float64)
curr_secont condition:: tensor(7.7135e-11, dtype=torch.float64)
secont condition:: tensor(7.7135e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.8396e-11, dtype=torch.float64)
secont condition:: tensor(6.8396e-11, dtype=torch.float64)
curr_secont condition:: tensor(9.8808e-11, dtype=torch.float64)
secont condition:: tensor(9.8808e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.8534e-11, dtype=torch.float64)
secont condition:: tensor(4.8534e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.5505e-11, dtype=torch.float64)
secont condition:: tensor(6.5505e-11, dtype=torch.float64)
curr_secont condition:: tensor(8.5709e-11, dtype=torch.float64)
secont condition:: tensor(8.5709e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.4608e-11, dtype=torch.float64)
secont condition:: tensor(6.4608e-11, dtype=torch.float64)
curr_secont condition:: tensor(7.5461e-11, dtype=torch.float64)
secont condition:: tensor(7.5461e-11, dtype=torch.float64)
curr_secont condition:: tensor(9.8477e-11, dtype=torch.float64)
secont condition:: tensor(9.8477e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.3659e-11, dtype=torch.float64)
secont condition:: tensor(6.3659e-11, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(5.7463e-11, dtype=torch.float64)
secont condition:: tensor(5.7463e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.9251e-11, dtype=torch.float64)
secont condition:: tensor(2.9251e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.0926e-11, dtype=torch.float64)
secont condition:: tensor(2.0926e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.9433e-11, dtype=torch.float64)
secont condition:: tensor(2.9433e-11, dtype=torch.float64)
curr_secont condition:: tensor(9.1732e-11, dtype=torch.float64)
secont condition:: tensor(9.1732e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.7786e-11, dtype=torch.float64)
secont condition:: tensor(6.7786e-11, dtype=torch.float64)
curr_secont condition:: tensor(-9.0334e-12, dtype=torch.float64)
curr_secont condition:: tensor(3.4873e-11, dtype=torch.float64)
secont condition:: tensor(3.4873e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.6996e-11, dtype=torch.float64)
secont condition:: tensor(3.6996e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.9639e-11, dtype=torch.float64)
secont condition:: tensor(2.9639e-11, dtype=torch.float64)
curr_secont condition:: tensor(-6.2459e-10, dtype=torch.float64)
explicit_evaluation epoch:: 40
curr_secont condition:: tensor(1.0800e-09, dtype=torch.float64)
secont condition:: tensor(1.0800e-09, dtype=torch.float64)
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(9.9816e-10, dtype=torch.float64)
secont condition:: tensor(9.9816e-10, dtype=torch.float64)
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(4.2091e-10, dtype=torch.float64)
secont condition:: tensor(4.2091e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.3209e-10, dtype=torch.float64)
secont condition:: tensor(4.3209e-10, dtype=torch.float64)
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(4.8256e-10, dtype=torch.float64)
secont condition:: tensor(4.8256e-10, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(7.2987e-10, dtype=torch.float64)
secont condition:: tensor(7.2987e-10, dtype=torch.float64)
curr_secont condition:: tensor(5.3238e-10, dtype=torch.float64)
secont condition:: tensor(5.3238e-10, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(6.7753e-10, dtype=torch.float64)
secont condition:: tensor(6.7753e-10, dtype=torch.float64)
epoch  8
	calling Sampler:__iter__
curr_secont condition:: tensor(6.1435e-10, dtype=torch.float64)
secont condition:: tensor(6.1435e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.1585e-08, dtype=torch.float64)
explicit_evaluation epoch:: 126
curr_secont condition:: tensor(2.4441e-09, dtype=torch.float64)
explicit_evaluation epoch:: 127
epoch  9
	calling Sampler:__iter__
curr_secont condition:: tensor(1.7327e-09, dtype=torch.float64)
secont condition:: tensor(1.7327e-09, dtype=torch.float64)
curr_secont condition:: tensor(3.9601e-09, dtype=torch.float64)
secont condition:: tensor(3.9601e-09, dtype=torch.float64)
epoch  10
	calling Sampler:__iter__
curr_secont condition:: tensor(1.5109e-09, dtype=torch.float64)
secont condition:: tensor(1.5109e-09, dtype=torch.float64)
epoch  11
	calling Sampler:__iter__
curr_secont condition:: tensor(1.4348e-09, dtype=torch.float64)
secont condition:: tensor(1.4348e-09, dtype=torch.float64)
curr_secont condition:: tensor(3.0039e-09, dtype=torch.float64)
secont condition:: tensor(3.0039e-09, dtype=torch.float64)
epoch  12
	calling Sampler:__iter__
curr_secont condition:: tensor(2.6341e-09, dtype=torch.float64)
secont condition:: tensor(2.6341e-09, dtype=torch.float64)
epoch  13
	calling Sampler:__iter__
curr_secont condition:: tensor(3.4500e-09, dtype=torch.float64)
secont condition:: tensor(3.4500e-09, dtype=torch.float64)
curr_secont condition:: tensor(3.5006e-09, dtype=torch.float64)
secont condition:: tensor(3.5006e-09, dtype=torch.float64)
epoch  14
	calling Sampler:__iter__
curr_secont condition:: tensor(2.2328e-09, dtype=torch.float64)
secont condition:: tensor(2.2328e-09, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 35.350916385650635
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.7378e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(9.0948e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.1129e-05, dtype=torch.float64, grad_fn=<NormBackward0>)/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)

tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.5330e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(6.0962e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000022, Accuracy: 0.749500
repetition 4
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 15
delta_size:: 1
max_epoch:: 15
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
training time is 93.56082558631897
time_baseline:: 93.59820771217346
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.5841e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.0459e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000022, Accuracy: 0.751100
incremental updates::
max_epoch:: 15
delta_size:: 1
max_epoch:: 15
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(6.0348e-33, dtype=torch.float64)
secont condition:: tensor(6.0348e-33, dtype=torch.float64)
curr_secont condition:: tensor(1.4958e-32, dtype=torch.float64)
secont condition:: tensor(1.4958e-32, dtype=torch.float64)
curr_secont condition:: tensor(2.6747e-32, dtype=torch.float64)
secont condition:: tensor(2.6747e-32, dtype=torch.float64)
curr_secont condition:: tensor(4.1223e-32, dtype=torch.float64)
secont condition:: tensor(4.1223e-32, dtype=torch.float64)
curr_secont condition:: tensor(5.8547e-32, dtype=torch.float64)
secont condition:: tensor(5.8547e-32, dtype=torch.float64)
curr_secont condition:: tensor(7.8546e-32, dtype=torch.float64)
secont condition:: tensor(7.8546e-32, dtype=torch.float64)
curr_secont condition:: tensor(1.0110e-31, dtype=torch.float64)
secont condition:: tensor(1.0110e-31, dtype=torch.float64)
curr_secont condition:: tensor(1.2650e-31, dtype=torch.float64)
secont condition:: tensor(1.2650e-31, dtype=torch.float64)
curr_secont condition:: tensor(1.5455e-31, dtype=torch.float64)
secont condition:: tensor(1.5455e-31, dtype=torch.float64)
curr_secont condition:: tensor(8.1511e-12, dtype=torch.float64)
secont condition:: tensor(8.1511e-12, dtype=torch.float64)
curr_secont condition:: tensor(7.0751e-12, dtype=torch.float64)
secont condition:: tensor(7.0751e-12, dtype=torch.float64)
curr_secont condition:: tensor(4.6527e-12, dtype=torch.float64)
secont condition:: tensor(4.6527e-12, dtype=torch.float64)
curr_secont condition:: tensor(4.7195e-12, dtype=torch.float64)
secont condition:: tensor(4.7195e-12, dtype=torch.float64)
curr_secont condition:: tensor(2.2491e-12, dtype=torch.float64)
secont condition:: tensor(2.2491e-12, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(2.8546e-12, dtype=torch.float64)
secont condition:: tensor(2.8546e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.9293e-12, dtype=torch.float64)
secont condition:: tensor(1.9293e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.9683e-12, dtype=torch.float64)
secont condition:: tensor(1.9683e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.7916e-12, dtype=torch.float64)
secont condition:: tensor(1.7916e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.3075e-13, dtype=torch.float64)
secont condition:: tensor(1.3075e-13, dtype=torch.float64)
curr_secont condition:: tensor(5.9422e-10, dtype=torch.float64)
secont condition:: tensor(5.9422e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.5695e-10, dtype=torch.float64)
secont condition:: tensor(4.5695e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.9586e-10, dtype=torch.float64)
secont condition:: tensor(3.9586e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.8350e-10, dtype=torch.float64)
secont condition:: tensor(2.8350e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.7167e-10, dtype=torch.float64)
secont condition:: tensor(1.7167e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.9045e-10, dtype=torch.float64)
secont condition:: tensor(1.9045e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.4461e-10, dtype=torch.float64)
secont condition:: tensor(1.4461e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.6087e-10, dtype=torch.float64)
secont condition:: tensor(1.6087e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.1903e-10, dtype=torch.float64)
secont condition:: tensor(1.1903e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.1542e-10, dtype=torch.float64)
secont condition:: tensor(1.1542e-10, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(8.7857e-11, dtype=torch.float64)
secont condition:: tensor(8.7857e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.0720e-10, dtype=torch.float64)
secont condition:: tensor(1.0720e-10, dtype=torch.float64)
curr_secont condition:: tensor(7.6276e-11, dtype=torch.float64)
secont condition:: tensor(7.6276e-11, dtype=torch.float64)
curr_secont condition:: tensor(9.5133e-11, dtype=torch.float64)
secont condition:: tensor(9.5133e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.0280e-10, dtype=torch.float64)
secont condition:: tensor(1.0280e-10, dtype=torch.float64)
curr_secont condition:: tensor(8.8260e-11, dtype=torch.float64)
secont condition:: tensor(8.8260e-11, dtype=torch.float64)
curr_secont condition:: tensor(9.6521e-11, dtype=torch.float64)
secont condition:: tensor(9.6521e-11, dtype=torch.float64)
curr_secont condition:: tensor(9.7684e-11, dtype=torch.float64)
secont condition:: tensor(9.7684e-11, dtype=torch.float64)
curr_secont condition:: tensor(8.7629e-11, dtype=torch.float64)
secont condition:: tensor(8.7629e-11, dtype=torch.float64)
curr_secont condition:: tensor(8.1994e-11, dtype=torch.float64)
secont condition:: tensor(8.1994e-11, dtype=torch.float64)
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(6.4591e-11, dtype=torch.float64)
secont condition:: tensor(6.4591e-11, dtype=torch.float64)
curr_secont condition:: tensor(9.1259e-11, dtype=torch.float64)
explicit_evaluation epoch:: 51
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(2.3318e-10, dtype=torch.float64)
secont condition:: tensor(2.3318e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.9978e-08, dtype=torch.float64)
explicit_evaluation epoch:: 62
curr_secont condition:: tensor(1.7010e-09, dtype=torch.float64)
explicit_evaluation epoch:: 63
curr_secont condition:: tensor(7.2992e-10, dtype=torch.float64)
secont condition:: tensor(7.2992e-10, dtype=torch.float64)
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(7.6232e-10, dtype=torch.float64)
secont condition:: tensor(7.6232e-10, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(7.6219e-10, dtype=torch.float64)
secont condition:: tensor(7.6219e-10, dtype=torch.float64)
curr_secont condition:: tensor(7.1859e-10, dtype=torch.float64)
secont condition:: tensor(7.1859e-10, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(8.3791e-10, dtype=torch.float64)
secont condition:: tensor(8.3791e-10, dtype=torch.float64)
epoch  8
	calling Sampler:__iter__
curr_secont condition:: tensor(4.3232e-10, dtype=torch.float64)
secont condition:: tensor(4.3232e-10, dtype=torch.float64)
curr_secont condition:: tensor(6.3222e-10, dtype=torch.float64)
secont condition:: tensor(6.3222e-10, dtype=torch.float64)
epoch  9
	calling Sampler:__iter__
curr_secont condition:: tensor(3.4426e-10, dtype=torch.float64)
secont condition:: tensor(3.4426e-10, dtype=torch.float64)
epoch  10
	calling Sampler:__iter__
curr_secont condition:: tensor(4.8278e-10, dtype=torch.float64)
secont condition:: tensor(4.8278e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.8409e-10, dtype=torch.float64)
secont condition:: tensor(4.8409e-10, dtype=torch.float64)
epoch  11
	calling Sampler:__iter__
curr_secont condition:: tensor(1.1120e-09, dtype=torch.float64)
secont condition:: tensor(1.1120e-09, dtype=torch.float64)
epoch  12
	calling Sampler:__iter__
curr_secont condition:: tensor(7.6625e-10, dtype=torch.float64)
secont condition:: tensor(7.6625e-10, dtype=torch.float64)
curr_secont condition:: tensor(6.5963e-10, dtype=torch.float64)
secont condition:: tensor(6.5963e-10, dtype=torch.float64)
epoch  13
	calling Sampler:__iter__
curr_secont condition:: tensor(5.5502e-10, dtype=torch.float64)
secont condition:: tensor(5.5502e-10, dtype=torch.float64)
epoch  14
	calling Sampler:__iter__
curr_secont condition:: tensor(4.4892e-10, dtype=torch.float64)
secont condition:: tensor(4.4892e-10, dtype=torch.float64)
curr_secont condition:: tensor(8.8326e-10, dtype=torch.float64)
explicit_evaluation epoch:: 217
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 34.558876752853394
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.3332e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(6.1593e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(7.2681e-06, dtype=torch.float64, grad_fn=<NormBackward0>)/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)

tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.3110e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.7519e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000022, Accuracy: 0.750400
repetition 5
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 15
delta_size:: 1
max_epoch:: 15
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
training time is 93.83945751190186
time_baseline:: 93.87972974777222
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.7749e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.5971e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000022, Accuracy: 0.751100
incremental updates::
max_epoch:: 15
delta_size:: 1
max_epoch:: 15
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(6.0348e-33, dtype=torch.float64)
secont condition:: tensor(6.0348e-33, dtype=torch.float64)
curr_secont condition:: tensor(1.4958e-32, dtype=torch.float64)
secont condition:: tensor(1.4958e-32, dtype=torch.float64)
curr_secont condition:: tensor(2.6747e-32, dtype=torch.float64)
secont condition:: tensor(2.6747e-32, dtype=torch.float64)
curr_secont condition:: tensor(4.1223e-32, dtype=torch.float64)
secont condition:: tensor(4.1223e-32, dtype=torch.float64)
curr_secont condition:: tensor(5.8547e-32, dtype=torch.float64)
secont condition:: tensor(5.8547e-32, dtype=torch.float64)
curr_secont condition:: tensor(7.8546e-32, dtype=torch.float64)
secont condition:: tensor(7.8546e-32, dtype=torch.float64)
curr_secont condition:: tensor(1.0110e-31, dtype=torch.float64)
secont condition:: tensor(1.0110e-31, dtype=torch.float64)
curr_secont condition:: tensor(1.2650e-31, dtype=torch.float64)
secont condition:: tensor(1.2650e-31, dtype=torch.float64)
curr_secont condition:: tensor(1.5452e-31, dtype=torch.float64)
secont condition:: tensor(1.5452e-31, dtype=torch.float64)
curr_secont condition:: tensor(1.8514e-31, dtype=torch.float64)
secont condition:: tensor(1.8514e-31, dtype=torch.float64)
curr_secont condition:: tensor(2.1853e-31, dtype=torch.float64)
secont condition:: tensor(2.1853e-31, dtype=torch.float64)
curr_secont condition:: tensor(2.5430e-31, dtype=torch.float64)
secont condition:: tensor(2.5430e-31, dtype=torch.float64)
curr_secont condition:: tensor(2.9254e-31, dtype=torch.float64)
secont condition:: tensor(2.9254e-31, dtype=torch.float64)
curr_secont condition:: tensor(9.8246e-12, dtype=torch.float64)
secont condition:: tensor(9.8246e-12, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(7.8652e-12, dtype=torch.float64)
secont condition:: tensor(7.8652e-12, dtype=torch.float64)
curr_secont condition:: tensor(6.6198e-12, dtype=torch.float64)
secont condition:: tensor(6.6198e-12, dtype=torch.float64)
curr_secont condition:: tensor(5.9228e-12, dtype=torch.float64)
secont condition:: tensor(5.9228e-12, dtype=torch.float64)
curr_secont condition:: tensor(4.6767e-12, dtype=torch.float64)
secont condition:: tensor(4.6767e-12, dtype=torch.float64)
curr_secont condition:: tensor(4.1163e-12, dtype=torch.float64)
secont condition:: tensor(4.1163e-12, dtype=torch.float64)
curr_secont condition:: tensor(3.7487e-11, dtype=torch.float64)
secont condition:: tensor(3.7487e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.9310e-11, dtype=torch.float64)
secont condition:: tensor(2.9310e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.5248e-11, dtype=torch.float64)
secont condition:: tensor(2.5248e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.0868e-11, dtype=torch.float64)
secont condition:: tensor(2.0868e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.5652e-11, dtype=torch.float64)
secont condition:: tensor(1.5652e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.4667e-11, dtype=torch.float64)
secont condition:: tensor(1.4667e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.2997e-11, dtype=torch.float64)
secont condition:: tensor(1.2997e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.2074e-11, dtype=torch.float64)
secont condition:: tensor(1.2074e-11, dtype=torch.float64)
curr_secont condition:: tensor(7.9863e-12, dtype=torch.float64)
secont condition:: tensor(7.9863e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.0096e-11, dtype=torch.float64)
secont condition:: tensor(1.0096e-11, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(4.2662e-12, dtype=torch.float64)
secont condition:: tensor(4.2662e-12, dtype=torch.float64)
curr_secont condition:: tensor(8.5843e-12, dtype=torch.float64)
secont condition:: tensor(8.5843e-12, dtype=torch.float64)
curr_secont condition:: tensor(5.8522e-12, dtype=torch.float64)
secont condition:: tensor(5.8522e-12, dtype=torch.float64)
curr_secont condition:: tensor(7.4058e-12, dtype=torch.float64)
secont condition:: tensor(7.4058e-12, dtype=torch.float64)
curr_secont condition:: tensor(5.4327e-12, dtype=torch.float64)
secont condition:: tensor(5.4327e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.2608e-11, dtype=torch.float64)
secont condition:: tensor(1.2608e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.6023e-12, dtype=torch.float64)
secont condition:: tensor(5.6023e-12, dtype=torch.float64)
curr_secont condition:: tensor(5.9121e-12, dtype=torch.float64)
secont condition:: tensor(5.9121e-12, dtype=torch.float64)
curr_secont condition:: tensor(5.9172e-12, dtype=torch.float64)
secont condition:: tensor(5.9172e-12, dtype=torch.float64)
curr_secont condition:: tensor(6.4525e-12, dtype=torch.float64)
secont condition:: tensor(6.4525e-12, dtype=torch.float64)
curr_secont condition:: tensor(3.8629e-10, dtype=torch.float64)
explicit_evaluation epoch:: 43
curr_secont condition:: tensor(9.5108e-10, dtype=torch.float64)
explicit_evaluation epoch:: 44
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(1.6490e-10, dtype=torch.float64)
secont condition:: tensor(1.6490e-10, dtype=torch.float64)
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(5.2588e-10, dtype=torch.float64)
secont condition:: tensor(5.2588e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.8305e-10, dtype=torch.float64)
secont condition:: tensor(3.8305e-10, dtype=torch.float64)
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(7.4099e-10, dtype=torch.float64)
secont condition:: tensor(7.4099e-10, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(6.2893e-10, dtype=torch.float64)
secont condition:: tensor(6.2893e-10, dtype=torch.float64)
curr_secont condition:: tensor(7.1281e-10, dtype=torch.float64)
secont condition:: tensor(7.1281e-10, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(3.8364e-10, dtype=torch.float64)
explicit_evaluation epoch:: 105
curr_secont condition:: tensor(3.7631e-10, dtype=torch.float64)
secont condition:: tensor(3.7631e-10, dtype=torch.float64)
epoch  8
	calling Sampler:__iter__
curr_secont condition:: tensor(3.2746e-10, dtype=torch.float64)
secont condition:: tensor(3.2746e-10, dtype=torch.float64)
epoch  9
	calling Sampler:__iter__
curr_secont condition:: tensor(2.5442e-10, dtype=torch.float64)
secont condition:: tensor(2.5442e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.5205e-10, dtype=torch.float64)
secont condition:: tensor(2.5205e-10, dtype=torch.float64)
curr_secont condition:: tensor(-4.7754e-09, dtype=torch.float64)
explicit_evaluation epoch:: 149
epoch  10
	calling Sampler:__iter__
curr_secont condition:: tensor(1.5567e-09, dtype=torch.float64)
secont condition:: tensor(1.5567e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.5164e-09, dtype=torch.float64)
secont condition:: tensor(1.5164e-09, dtype=torch.float64)
epoch  11
	calling Sampler:__iter__
curr_secont condition:: tensor(8.2979e-10, dtype=torch.float64)
secont condition:: tensor(8.2979e-10, dtype=torch.float64)
epoch  12
	calling Sampler:__iter__
curr_secont condition:: tensor(5.5755e-10, dtype=torch.float64)
secont condition:: tensor(5.5755e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.0334e-10, dtype=torch.float64)
secont condition:: tensor(4.0334e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.3201e-08, dtype=torch.float64)
explicit_evaluation epoch:: 194
epoch  13
	calling Sampler:__iter__
curr_secont condition:: tensor(3.3851e-09, dtype=torch.float64)
explicit_evaluation epoch:: 195
curr_secont condition:: tensor(1.4482e-09, dtype=torch.float64)
secont condition:: tensor(1.4482e-09, dtype=torch.float64)
epoch  14
	calling Sampler:__iter__
curr_secont condition:: tensor(1.0890e-09, dtype=torch.float64)
secont condition:: tensor(1.0890e-09, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 35.391618967056274
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff:/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
 1 tensor(1.1944e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(6.3350e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(9.7840e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.5959e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(2.5464e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000022, Accuracy: 0.749300
epochs:: 20
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type MSELoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
cuda:2
Train - Epoch 0, Batch: 0, Loss: 0.091544
Train - Epoch 0, Batch: 10, Loss: 0.090281
Test Avg. Loss: 0.000027, Accuracy: 0.326300
Train - Epoch 1, Batch: 0, Loss: 0.089719
Train - Epoch 1, Batch: 10, Loss: 0.089165
Test Avg. Loss: 0.000026, Accuracy: 0.574700
Train - Epoch 2, Batch: 0, Loss: 0.088606
Train - Epoch 2, Batch: 10, Loss: 0.088131
Test Avg. Loss: 0.000026, Accuracy: 0.640000
Train - Epoch 3, Batch: 0, Loss: 0.087413
Train - Epoch 3, Batch: 10, Loss: 0.086967
Test Avg. Loss: 0.000025, Accuracy: 0.657200
Train - Epoch 4, Batch: 0, Loss: 0.086142
Train - Epoch 4, Batch: 10, Loss: 0.085224
Test Avg. Loss: 0.000025, Accuracy: 0.662700
Train - Epoch 5, Batch: 0, Loss: 0.085057
Train - Epoch 5, Batch: 10, Loss: 0.084319
Test Avg. Loss: 0.000025, Accuracy: 0.676800
Train - Epoch 6, Batch: 0, Loss: 0.083921
Train - Epoch 6, Batch: 10, Loss: 0.083097
Test Avg. Loss: 0.000024, Accuracy: 0.690200
Train - Epoch 7, Batch: 0, Loss: 0.083321
Train - Epoch 7, Batch: 10, Loss: 0.082739
Test Avg. Loss: 0.000024, Accuracy: 0.701300
Train - Epoch 8, Batch: 0, Loss: 0.082079
Train - Epoch 8, Batch: 10, Loss: 0.081350
Test Avg. Loss: 0.000024, Accuracy: 0.705200
Train - Epoch 9, Batch: 0, Loss: 0.081480
Train - Epoch 9, Batch: 10, Loss: 0.080960
Test Avg. Loss: 0.000023, Accuracy: 0.722000
Train - Epoch 10, Batch: 0, Loss: 0.080599
Train - Epoch 10, Batch: 10, Loss: 0.079949
Test Avg. Loss: 0.000023, Accuracy: 0.729700
Train - Epoch 11, Batch: 0, Loss: 0.079676
Train - Epoch 11, Batch: 10, Loss: 0.079518
Test Avg. Loss: 0.000023, Accuracy: 0.734300
Train - Epoch 12, Batch: 0, Loss: 0.079569
Train - Epoch 12, Batch: 10, Loss: 0.079139
Test Avg. Loss: 0.000022, Accuracy: 0.742500
Train - Epoch 13, Batch: 0, Loss: 0.078575
Train - Epoch 13, Batch: 10, Loss: 0.078437
Test Avg. Loss: 0.000022, Accuracy: 0.747200
Train - Epoch 14, Batch: 0, Loss: 0.078190
Train - Epoch 14, Batch: 10, Loss: 0.077907
Test Avg. Loss: 0.000022, Accuracy: 0.751100
Train - Epoch 15, Batch: 0, Loss: 0.076895
Train - Epoch 15, Batch: 10, Loss: 0.077564
Test Avg. Loss: 0.000021, Accuracy: 0.757800
Train - Epoch 16, Batch: 0, Loss: 0.076701
Train - Epoch 16, Batch: 10, Loss: 0.076258
Test Avg. Loss: 0.000021, Accuracy: 0.763800
Train - Epoch 17, Batch: 0, Loss: 0.076185
Train - Epoch 17, Batch: 10, Loss: 0.076633
Test Avg. Loss: 0.000021, Accuracy: 0.766900
Train - Epoch 18, Batch: 0, Loss: 0.076035
Train - Epoch 18, Batch: 10, Loss: 0.076243
Test Avg. Loss: 0.000021, Accuracy: 0.768500
Train - Epoch 19, Batch: 0, Loss: 0.076075
Train - Epoch 19, Batch: 10, Loss: 0.075865
Test Avg. Loss: 0.000021, Accuracy: 0.771900
training_time:: 158.78855895996094
repetition 1
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 20
delta_size:: 1
max_epoch:: 20
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
training time is 124.68078827857971
time_baseline:: 124.74147176742554
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.6374e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(2.6976e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000021, Accuracy: 0.771900
incremental updates::
max_epoch:: 20
delta_size:: 1
max_epoch:: 20
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(6.0348e-33, dtype=torch.float64)
secont condition:: tensor(6.0348e-33, dtype=torch.float64)
curr_secont condition:: tensor(1.4958e-32, dtype=torch.float64)
secont condition:: tensor(1.4958e-32, dtype=torch.float64)
curr_secont condition:: tensor(2.6747e-32, dtype=torch.float64)
secont condition:: tensor(2.6747e-32, dtype=torch.float64)
curr_secont condition:: tensor(4.1223e-32, dtype=torch.float64)
secont condition:: tensor(4.1223e-32, dtype=torch.float64)
curr_secont condition:: tensor(5.8547e-32, dtype=torch.float64)
secont condition:: tensor(5.8547e-32, dtype=torch.float64)
curr_secont condition:: tensor(7.8612e-32, dtype=torch.float64)
secont condition:: tensor(7.8612e-32, dtype=torch.float64)
curr_secont condition:: tensor(1.0005e-09, dtype=torch.float64)
secont condition:: tensor(1.0005e-09, dtype=torch.float64)
curr_secont condition:: tensor(7.2589e-10, dtype=torch.float64)
secont condition:: tensor(7.2589e-10, dtype=torch.float64)
curr_secont condition:: tensor(6.7090e-10, dtype=torch.float64)
secont condition:: tensor(6.7090e-10, dtype=torch.float64)
curr_secont condition:: tensor(5.5749e-10, dtype=torch.float64)
secont condition:: tensor(5.5749e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.8009e-10, dtype=torch.float64)
secont condition:: tensor(3.8009e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.3541e-10, dtype=torch.float64)
secont condition:: tensor(3.3541e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.4500e-10, dtype=torch.float64)
secont condition:: tensor(2.4500e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.3066e-10, dtype=torch.float64)
secont condition:: tensor(2.3066e-10, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(1.9530e-10, dtype=torch.float64)
secont condition:: tensor(1.9530e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.8242e-10, dtype=torch.float64)
secont condition:: tensor(1.8242e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.1318e-10, dtype=torch.float64)
secont condition:: tensor(2.1318e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.6959e-10, dtype=torch.float64)
secont condition:: tensor(1.6959e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.4122e-10, dtype=torch.float64)
secont condition:: tensor(1.4122e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.2377e-10, dtype=torch.float64)
secont condition:: tensor(1.2377e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.4073e-10, dtype=torch.float64)
secont condition:: tensor(1.4073e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.2595e-10, dtype=torch.float64)
secont condition:: tensor(1.2595e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.3596e-10, dtype=torch.float64)
secont condition:: tensor(1.3596e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.0433e-10, dtype=torch.float64)
secont condition:: tensor(1.0433e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.1803e-10, dtype=torch.float64)
secont condition:: tensor(1.1803e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.0550e-10, dtype=torch.float64)
secont condition:: tensor(1.0550e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.1161e-10, dtype=torch.float64)
secont condition:: tensor(1.1161e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.2070e-10, dtype=torch.float64)
secont condition:: tensor(1.2070e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.1548e-10, dtype=torch.float64)
secont condition:: tensor(1.1548e-10, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(1.2307e-10, dtype=torch.float64)
secont condition:: tensor(1.2307e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.1374e-10, dtype=torch.float64)
secont condition:: tensor(1.1374e-10, dtype=torch.float64)
curr_secont condition:: tensor(7.7818e-11, dtype=torch.float64)
secont condition:: tensor(7.7818e-11, dtype=torch.float64)
curr_secont condition:: tensor(9.1756e-11, dtype=torch.float64)
secont condition:: tensor(9.1756e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.7564e-11, dtype=torch.float64)
secont condition:: tensor(5.7564e-11, dtype=torch.float64)
curr_secont condition:: tensor(9.4372e-11, dtype=torch.float64)
secont condition:: tensor(9.4372e-11, dtype=torch.float64)
curr_secont condition:: tensor(9.8011e-11, dtype=torch.float64)
secont condition:: tensor(9.8011e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.1987e-10, dtype=torch.float64)
secont condition:: tensor(1.1987e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.6306e-10, dtype=torch.float64)
secont condition:: tensor(2.6306e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.8913e-10, dtype=torch.float64)
secont condition:: tensor(1.8913e-10, dtype=torch.float64)
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(2.0601e-10, dtype=torch.float64)
secont condition:: tensor(2.0601e-10, dtype=torch.float64)
curr_secont condition:: tensor(6.6591e-10, dtype=torch.float64)
secont condition:: tensor(6.6591e-10, dtype=torch.float64)
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(6.2146e-10, dtype=torch.float64)
secont condition:: tensor(6.2146e-10, dtype=torch.float64)
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(6.1844e-10, dtype=torch.float64)
secont condition:: tensor(6.1844e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.2641e-09, dtype=torch.float64)
secont condition:: tensor(1.2641e-09, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(8.4223e-10, dtype=torch.float64)
secont condition:: tensor(8.4223e-10, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(5.7197e-10, dtype=torch.float64)
secont condition:: tensor(5.7197e-10, dtype=torch.float64)
curr_secont condition:: tensor(7.7867e-10, dtype=torch.float64)
secont condition:: tensor(7.7867e-10, dtype=torch.float64)
epoch  8
	calling Sampler:__iter__
curr_secont condition:: tensor(8.0521e-10, dtype=torch.float64)
secont condition:: tensor(8.0521e-10, dtype=torch.float64)
epoch  9
	calling Sampler:__iter__
curr_secont condition:: tensor(4.6948e-10, dtype=torch.float64)
secont condition:: tensor(4.6948e-10, dtype=torch.float64)
curr_secont condition:: tensor(5.3022e-10, dtype=torch.float64)
secont condition:: tensor(5.3022e-10, dtype=torch.float64)
epoch  10
	calling Sampler:__iter__
curr_secont condition:: tensor(8.9922e-10, dtype=torch.float64)
secont condition:: tensor(8.9922e-10, dtype=torch.float64)
epoch  11
	calling Sampler:__iter__
curr_secont condition:: tensor(8.7057e-10, dtype=torch.float64)
secont condition:: tensor(8.7057e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.9701e-09, dtype=torch.float64)
secont condition:: tensor(1.9701e-09, dtype=torch.float64)
epoch  12
	calling Sampler:__iter__
curr_secont condition:: tensor(1.4906e-09, dtype=torch.float64)
secont condition:: tensor(1.4906e-09, dtype=torch.float64)
epoch  13
	calling Sampler:__iter__
curr_secont condition:: tensor(6.8605e-10, dtype=torch.float64)
secont condition:: tensor(6.8605e-10, dtype=torch.float64)
curr_secont condition:: tensor(9.1585e-10, dtype=torch.float64)
secont condition:: tensor(9.1585e-10, dtype=torch.float64)
epoch  14
	calling Sampler:__iter__
curr_secont condition:: tensor(8.5752e-10, dtype=torch.float64)
secont condition:: tensor(8.5752e-10, dtype=torch.float64)
epoch  15
	calling Sampler:__iter__
curr_secont condition:: tensor(7.0140e-10, dtype=torch.float64)
secont condition:: tensor(7.0140e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.5418e-09, dtype=torch.float64)
secont condition:: tensor(1.5418e-09, dtype=torch.float64)
epoch  16
	calling Sampler:__iter__
curr_secont condition:: tensor(2.3907e-09, dtype=torch.float64)
secont condition:: tensor(2.3907e-09, dtype=torch.float64)
epoch  17
	calling Sampler:__iter__
curr_secont condition:: tensor(1.7476e-09, dtype=torch.float64)
secont condition:: tensor(1.7476e-09, dtype=torch.float64)
curr_secont condition::/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
 tensor(1.3501e-09, dtype=torch.float64)
secont condition:: tensor(1.3501e-09, dtype=torch.float64)
epoch  18
	calling Sampler:__iter__
curr_secont condition:: tensor(1.7762e-09, dtype=torch.float64)
secont condition:: tensor(1.7762e-09, dtype=torch.float64)
epoch  19
	calling Sampler:__iter__
curr_secont condition:: tensor(1.2471e-09, dtype=torch.float64)
secont condition:: tensor(1.2471e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.0797e-09, dtype=torch.float64)
secont condition:: tensor(2.0797e-09, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 38.4488308429718
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.4192e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(7.2994e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(9.5167e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.2076e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(8.8831e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000021, Accuracy: 0.773300
repetition 2
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 20
delta_size:: 1
max_epoch:: 20
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
training time is 125.10419249534607
time_baseline:: 125.15069103240967
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.0539e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(9.4318e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(9.1807e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000021, Accuracy: 0.771900
incremental updates::
max_epoch:: 20
delta_size:: 1
max_epoch:: 20
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(6.0348e-33, dtype=torch.float64)
secont condition:: tensor(6.0348e-33, dtype=torch.float64)
curr_secont condition:: tensor(1.4958e-32, dtype=torch.float64)
secont condition:: tensor(1.4958e-32, dtype=torch.float64)
curr_secont condition:: tensor(2.6747e-32, dtype=torch.float64)
secont condition:: tensor(2.6747e-32, dtype=torch.float64)
curr_secont condition:: tensor(4.1223e-32, dtype=torch.float64)
secont condition:: tensor(4.1223e-32, dtype=torch.float64)
curr_secont condition:: tensor(5.8547e-32, dtype=torch.float64)
secont condition:: tensor(5.8547e-32, dtype=torch.float64)
curr_secont condition:: tensor(7.8546e-32, dtype=torch.float64)
secont condition:: tensor(7.8546e-32, dtype=torch.float64)
curr_secont condition:: tensor(1.0112e-31, dtype=torch.float64)
secont condition:: tensor(1.0112e-31, dtype=torch.float64)
curr_secont condition:: tensor(2.5100e-12, dtype=torch.float64)
secont condition:: tensor(2.5100e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.8654e-12, dtype=torch.float64)
secont condition:: tensor(1.8654e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.2714e-12, dtype=torch.float64)
secont condition:: tensor(1.2714e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.1863e-12, dtype=torch.float64)
secont condition:: tensor(1.1863e-12, dtype=torch.float64)
curr_secont condition:: tensor(8.5412e-13, dtype=torch.float64)
secont condition:: tensor(8.5412e-13, dtype=torch.float64)
curr_secont condition:: tensor(6.8555e-13, dtype=torch.float64)
secont condition:: tensor(6.8555e-13, dtype=torch.float64)
curr_secont condition:: tensor(4.1065e-13, dtype=torch.float64)
secont condition:: tensor(4.1065e-13, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(6.1556e-13, dtype=torch.float64)
secont condition:: tensor(6.1556e-13, dtype=torch.float64)
curr_secont condition:: tensor(4.9264e-13, dtype=torch.float64)
secont condition:: tensor(4.9264e-13, dtype=torch.float64)
curr_secont condition:: tensor(4.0945e-13, dtype=torch.float64)
secont condition:: tensor(4.0945e-13, dtype=torch.float64)
curr_secont condition:: tensor(3.5724e-13, dtype=torch.float64)
secont condition:: tensor(3.5724e-13, dtype=torch.float64)
curr_secont condition:: tensor(3.1231e-13, dtype=torch.float64)
secont condition:: tensor(3.1231e-13, dtype=torch.float64)
curr_secont condition:: tensor(-7.6583e-14, dtype=torch.float64)
curr_secont condition:: tensor(1.4840e-11, dtype=torch.float64)
secont condition:: tensor(1.4840e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.2058e-11, dtype=torch.float64)
secont condition:: tensor(1.2058e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.0292e-11, dtype=torch.float64)
secont condition:: tensor(1.0292e-11, dtype=torch.float64)
curr_secont condition:: tensor(8.8639e-12, dtype=torch.float64)
secont condition:: tensor(8.8639e-12, dtype=torch.float64)
curr_secont condition:: tensor(7.3830e-12, dtype=torch.float64)
secont condition:: tensor(7.3830e-12, dtype=torch.float64)
curr_secont condition:: tensor(6.3965e-12, dtype=torch.float64)
secont condition:: tensor(6.3965e-12, dtype=torch.float64)
curr_secont condition:: tensor(5.4642e-12, dtype=torch.float64)
secont condition:: tensor(5.4642e-12, dtype=torch.float64)
curr_secont condition:: tensor(6.1170e-12, dtype=torch.float64)
secont condition:: tensor(6.1170e-12, dtype=torch.float64)
curr_secont condition:: tensor(4.1166e-12, dtype=torch.float64)
secont condition:: tensor(4.1166e-12, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(3.2950e-12, dtype=torch.float64)
secont condition:: tensor(3.2950e-12, dtype=torch.float64)
curr_secont condition:: tensor(4.3557e-12, dtype=torch.float64)
secont condition:: tensor(4.3557e-12, dtype=torch.float64)
curr_secont condition:: tensor(4.3655e-12, dtype=torch.float64)
secont condition:: tensor(4.3655e-12, dtype=torch.float64)
curr_secont condition:: tensor(-2.4588e-12, dtype=torch.float64)
curr_secont condition:: tensor(2.2856e-11, dtype=torch.float64)
secont condition:: tensor(2.2856e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.0312e-11, dtype=torch.float64)
secont condition:: tensor(2.0312e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.6064e-11, dtype=torch.float64)
secont condition:: tensor(1.6064e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.1044e-11, dtype=torch.float64)
secont condition:: tensor(2.1044e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.2363e-11, dtype=torch.float64)
secont condition:: tensor(1.2363e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.1855e-11, dtype=torch.float64)
secont condition:: tensor(1.1855e-11, dtype=torch.float64)
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(1.0853e-11, dtype=torch.float64)
secont condition:: tensor(1.0853e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.3718e-12, dtype=torch.float64)
explicit_evaluation epoch:: 50
curr_secont condition:: tensor(1.2221e-11, dtype=torch.float64)
explicit_evaluation epoch:: 51
curr_secont condition:: tensor(8.7714e-12, dtype=torch.float64)
explicit_evaluation epoch:: 52
curr_secont condition:: tensor(9.8168e-12, dtype=torch.float64)
explicit_evaluation epoch:: 53
curr_secont condition:: tensor(4.3849e-10, dtype=torch.float64)
explicit_evaluation epoch:: 54
curr_secont condition:: tensor(1.6796e-11, dtype=torch.float64)
explicit_evaluation epoch:: 55
curr_secont condition:: tensor(1.4339e-11, dtype=torch.float64)
explicit_evaluation epoch:: 56
curr_secont condition:: tensor(1.1505e-11, dtype=torch.float64)
explicit_evaluation epoch:: 59
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(4.2612e-11, dtype=torch.float64)
secont condition:: tensor(4.2612e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.5244e-11, dtype=torch.float64)
explicit_evaluation epoch:: 70
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(2.0962e-11, dtype=torch.float64)
secont condition:: tensor(2.0962e-11, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(5.5648e-11, dtype=torch.float64)
secont condition:: tensor(5.5648e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.7005e-11, dtype=torch.float64)
explicit_evaluation epoch:: 97
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(9.1091e-11, dtype=torch.float64)
secont condition:: tensor(9.1091e-11, dtype=torch.float64)
curr_secont condition:: tensor(8.7768e-11, dtype=torch.float64)
secont condition:: tensor(8.7768e-11, dtype=torch.float64)
epoch  8
	calling Sampler:__iter__
curr_secont condition:: tensor(7.3205e-11, dtype=torch.float64)
secont condition:: tensor(7.3205e-11, dtype=torch.float64)
epoch  9
	calling Sampler:__iter__
curr_secont condition:: tensor(1.7102e-10, dtype=torch.float64)
secont condition:: tensor(1.7102e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.8261e-10, dtype=torch.float64)
explicit_evaluation epoch:: 140
epoch  10
	calling Sampler:__iter__
curr_secont condition:: tensor(3.0351e-10, dtype=torch.float64)
secont condition:: tensor(3.0351e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.2064e-10, dtype=torch.float64)
secont condition:: tensor(3.2064e-10, dtype=torch.float64)
epoch  11
	calling Sampler:__iter__
curr_secont condition:: tensor(3.3821e-10, dtype=torch.float64)
secont condition:: tensor(3.3821e-10, dtype=torch.float64)
epoch  12
	calling Sampler:__iter__
curr_secont condition:: tensor(3.1131e-10, dtype=torch.float64)
secont condition:: tensor(3.1131e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.3115e-10, dtype=torch.float64)
secont condition:: tensor(3.3115e-10, dtype=torch.float64)
epoch  13
	calling Sampler:__iter__
curr_secont condition:: tensor(2.9408e-10, dtype=torch.float64)
secont condition:: tensor(2.9408e-10, dtype=torch.float64)
epoch  14
	calling Sampler:__iter__
curr_secont condition:: tensor(2.8505e-10, dtype=torch.float64)
secont condition:: tensor(2.8505e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.1277e-10, dtype=torch.float64)/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)

secont condition:: tensor(3.1277e-10, dtype=torch.float64)
epoch  15
	calling Sampler:__iter__
curr_secont condition:: tensor(2.8406e-10, dtype=torch.float64)
secont condition:: tensor(2.8406e-10, dtype=torch.float64)
epoch  16
	calling Sampler:__iter__
curr_secont condition:: tensor(3.6417e-10, dtype=torch.float64)
secont condition:: tensor(3.6417e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.4885e-10, dtype=torch.float64)
secont condition:: tensor(2.4885e-10, dtype=torch.float64)
epoch  17
	calling Sampler:__iter__
curr_secont condition:: tensor(3.0004e-10, dtype=torch.float64)
secont condition:: tensor(3.0004e-10, dtype=torch.float64)
epoch  18
	calling Sampler:__iter__
curr_secont condition:: tensor(2.8580e-10, dtype=torch.float64)
secont condition:: tensor(2.8580e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.3283e-10, dtype=torch.float64)
secont condition:: tensor(3.3283e-10, dtype=torch.float64)
epoch  19
	calling Sampler:__iter__
curr_secont condition:: tensor(2.5704e-10, dtype=torch.float64)
secont condition:: tensor(2.5704e-10, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 43.75703406333923
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(9.8916e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(3.8347e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(6.3886e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(8.3650e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.5314e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000021, Accuracy: 0.771500
repetition 3
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 20
delta_size:: 1
max_epoch:: 20
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
training time is 124.42963910102844
time_baseline:: 124.47473430633545
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(2.3031e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.2504e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000021, Accuracy: 0.771900
incremental updates::
max_epoch:: 20
delta_size:: 1
max_epoch:: 20
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(6.0472e-33, dtype=torch.float64)
secont condition:: tensor(6.0472e-33, dtype=torch.float64)
curr_secont condition:: tensor(3.2213e-10, dtype=torch.float64)
secont condition:: tensor(3.2213e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.5181e-10, dtype=torch.float64)
secont condition:: tensor(3.5181e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.3666e-10, dtype=torch.float64)
secont condition:: tensor(2.3666e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.3961e-10, dtype=torch.float64)
secont condition:: tensor(2.3961e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.0856e-10, dtype=torch.float64)
secont condition:: tensor(2.0856e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.4806e-10, dtype=torch.float64)
secont condition:: tensor(1.4806e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.2230e-10, dtype=torch.float64)
secont condition:: tensor(1.2230e-10, dtype=torch.float64)
curr_secont condition:: tensor(7.3805e-11, dtype=torch.float64)
secont condition:: tensor(7.3805e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.2793e-10, dtype=torch.float64)
secont condition:: tensor(1.2793e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.0574e-10, dtype=torch.float64)
secont condition:: tensor(1.0574e-10, dtype=torch.float64)
curr_secont condition:: tensor(6.7761e-11, dtype=torch.float64)
secont condition:: tensor(6.7761e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.0806e-10, dtype=torch.float64)
secont condition:: tensor(1.0806e-10, dtype=torch.float64)
curr_secont condition:: tensor(8.3650e-11, dtype=torch.float64)
secont condition:: tensor(8.3650e-11, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(7.0393e-11, dtype=torch.float64)
secont condition:: tensor(7.0393e-11, dtype=torch.float64)
curr_secont condition:: tensor(7.7234e-11, dtype=torch.float64)
secont condition:: tensor(7.7234e-11, dtype=torch.float64)
curr_secont condition:: tensor(8.2339e-11, dtype=torch.float64)
secont condition:: tensor(8.2339e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.9253e-11, dtype=torch.float64)
secont condition:: tensor(5.9253e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.1501e-11, dtype=torch.float64)
secont condition:: tensor(5.1501e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.2210e-11, dtype=torch.float64)
secont condition:: tensor(6.2210e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.4106e-11, dtype=torch.float64)
secont condition:: tensor(6.4106e-11, dtype=torch.float64)
curr_secont condition:: tensor(-1.9525e-12, dtype=torch.float64)
curr_secont condition:: tensor(4.7541e-11, dtype=torch.float64)
secont condition:: tensor(4.7541e-11, dtype=torch.float64)
curr_secont condition:: tensor(7.1513e-11, dtype=torch.float64)
secont condition:: tensor(7.1513e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.2381e-11, dtype=torch.float64)
secont condition:: tensor(4.2381e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.7028e-10, dtype=torch.float64)
secont condition:: tensor(6.7028e-10, dtype=torch.float64)
curr_secont condition:: tensor(5.1257e-10, dtype=torch.float64)
secont condition:: tensor(5.1257e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.1957e-10, dtype=torch.float64)
secont condition:: tensor(4.1957e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.4243e-10, dtype=torch.float64)
secont condition:: tensor(3.4243e-10, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(2.9984e-10, dtype=torch.float64)
secont condition:: tensor(2.9984e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.9445e-10, dtype=torch.float64)
secont condition:: tensor(2.9445e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.0209e-10, dtype=torch.float64)
secont condition:: tensor(2.0209e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.0006e-10, dtype=torch.float64)
secont condition:: tensor(2.0006e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.6066e-10, dtype=torch.float64)
secont condition:: tensor(2.6066e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.3944e-10, dtype=torch.float64)
secont condition:: tensor(1.3944e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.7608e-10, dtype=torch.float64)
secont condition:: tensor(2.7608e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.7630e-10, dtype=torch.float64)
secont condition:: tensor(2.7630e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.6614e-10, dtype=torch.float64)
secont condition:: tensor(1.6614e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.7085e-10, dtype=torch.float64)
secont condition:: tensor(1.7085e-10, dtype=torch.float64)
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(3.2936e-10, dtype=torch.float64)
explicit_evaluation epoch:: 47
curr_secont condition:: tensor(8.6051e-10, dtype=torch.float64)
secont condition:: tensor(8.6051e-10, dtype=torch.float64)
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(-9.8436e-10, dtype=torch.float64)
explicit_evaluation epoch:: 66
curr_secont condition:: tensor(4.6199e-09, dtype=torch.float64)
secont condition:: tensor(4.6199e-09, dtype=torch.float64)
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(1.4806e-09, dtype=torch.float64)
secont condition:: tensor(1.4806e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.6667e-09, dtype=torch.float64)
secont condition:: tensor(1.6667e-09, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(8.8766e-10, dtype=torch.float64)
secont condition:: tensor(8.8766e-10, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(2.3279e-09, dtype=torch.float64)
explicit_evaluation epoch:: 106
curr_secont condition:: tensor(1.5767e-09, dtype=torch.float64)
secont condition:: tensor(1.5767e-09, dtype=torch.float64)
epoch  8
	calling Sampler:__iter__
curr_secont condition:: tensor(1.2883e-09, dtype=torch.float64)
secont condition:: tensor(1.2883e-09, dtype=torch.float64)
epoch  9
	calling Sampler:__iter__
curr_secont condition:: tensor(2.8190e-09, dtype=torch.float64)
secont condition:: tensor(2.8190e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.4001e-09, dtype=torch.float64)
secont condition:: tensor(2.4001e-09, dtype=torch.float64)
epoch  10
	calling Sampler:__iter__
curr_secont condition:: tensor(3.8260e-09, dtype=torch.float64)
secont condition:: tensor(3.8260e-09, dtype=torch.float64)
epoch  11
	calling Sampler:__iter__
curr_secont condition:: tensor(2.9945e-09, dtype=torch.float64)
secont condition:: tensor(2.9945e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.2017e-09, dtype=torch.float64)
secont condition:: tensor(2.2017e-09, dtype=torch.float64)
epoch  12
	calling Sampler:__iter__
curr_secont condition:: tensor(2.2893e-09, dtype=torch.float64)
secont condition:: tensor(2.2893e-09, dtype=torch.float64)
epoch  13
	calling Sampler:__iter__
curr_secont condition:: tensor(1.5981e-09, dtype=torch.float64)
secont condition:: tensor(1.5981e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.8097e-09, dtype=torch.float64)
secont condition:: tensor(2.8097e-09, dtype=torch.float64)
epoch  14
	calling Sampler:__iter__
curr_secont condition:: tensor(2.4737e-09, dtype=torch.float64)
secont condition:: tensor(2.4737e-09, dtype=torch.float64)
epoch  15
	calling Sampler:__iter__
curr_secont condition:: tensor(3.8558e-09, dtype=torch.float64)
secont condition:: tensor(3.8558e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.0807e-09, dtype=torch.float64)
secont condition:: tensor(2.0807e-09, dtype=torch.float64)
curr_secont condition:: tensor(-3.6883e-08, dtype=torch.float64)
explicit_evaluation epoch:: 239
epoch  16
	calling Sampler:__iter__
curr_secont condition:: tensor(7.5415e-09, dtype=torch.float64)
secont condition:: tensor(7.5415e-09, dtype=torch.float64)
curr_secont condition:: tensor(6.3040e-09, dtype=torch.float64)
secont condition:: tensor(6.3040e-09, dtype=torch.float64)/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)

epoch  17
	calling Sampler:__iter__
curr_secont condition:: tensor(4.0465e-09, dtype=torch.float64)
secont condition:: tensor(4.0465e-09, dtype=torch.float64)
epoch  18
	calling Sampler:__iter__
curr_secont condition:: tensor(5.5693e-09, dtype=torch.float64)
secont condition:: tensor(5.5693e-09, dtype=torch.float64)
curr_secont condition:: tensor(5.3559e-09, dtype=torch.float64)
secont condition:: tensor(5.3559e-09, dtype=torch.float64)
epoch  19
	calling Sampler:__iter__
curr_secont condition:: tensor(2.7421e-09, dtype=torch.float64)
secont condition:: tensor(2.7421e-09, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 39.404167890548706
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(2.1876e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(5.7207e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(2.1028e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.1981e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000021, Accuracy: 0.772700
repetition 4
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 20
delta_size:: 1
max_epoch:: 20
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
training time is 125.39544486999512
time_baseline:: 125.4411563873291
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(2.6653e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(2.8995e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000021, Accuracy: 0.771900
incremental updates::
max_epoch:: 20
delta_size:: 1
max_epoch:: 20
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(6.0348e-33, dtype=torch.float64)
secont condition:: tensor(6.0348e-33, dtype=torch.float64)
curr_secont condition:: tensor(1.4997e-32, dtype=torch.float64)
secont condition:: tensor(1.4997e-32, dtype=torch.float64)
curr_secont condition:: tensor(1.6116e-10, dtype=torch.float64)
secont condition:: tensor(1.6116e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.0593e-10, dtype=torch.float64)
secont condition:: tensor(1.0593e-10, dtype=torch.float64)
curr_secont condition:: tensor(7.5505e-11, dtype=torch.float64)
secont condition:: tensor(7.5505e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.0718e-11, dtype=torch.float64)
secont condition:: tensor(6.0718e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.0580e-11, dtype=torch.float64)
secont condition:: tensor(6.0580e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.1485e-11, dtype=torch.float64)
secont condition:: tensor(4.1485e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.7131e-11, dtype=torch.float64)
secont condition:: tensor(3.7131e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.7104e-11, dtype=torch.float64)
secont condition:: tensor(3.7104e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.6760e-11, dtype=torch.float64)
secont condition:: tensor(2.6760e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.5401e-11, dtype=torch.float64)
secont condition:: tensor(2.5401e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.9913e-11, dtype=torch.float64)
secont condition:: tensor(1.9913e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.3476e-11, dtype=torch.float64)
secont condition:: tensor(2.3476e-11, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(1.7301e-11, dtype=torch.float64)
secont condition:: tensor(1.7301e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.0282e-11, dtype=torch.float64)
secont condition:: tensor(1.0282e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.7556e-11, dtype=torch.float64)
secont condition:: tensor(1.7556e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.7288e-11, dtype=torch.float64)
secont condition:: tensor(1.7288e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.3357e-11, dtype=torch.float64)
secont condition:: tensor(1.3357e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.8390e-11, dtype=torch.float64)
secont condition:: tensor(1.8390e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.3898e-11, dtype=torch.float64)
secont condition:: tensor(2.3898e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.5907e-11, dtype=torch.float64)
secont condition:: tensor(1.5907e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.8102e-11, dtype=torch.float64)
secont condition:: tensor(1.8102e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.5868e-11, dtype=torch.float64)
secont condition:: tensor(1.5868e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.1643e-11, dtype=torch.float64)
secont condition:: tensor(1.1643e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.8713e-11, dtype=torch.float64)
secont condition:: tensor(1.8713e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.0163e-11, dtype=torch.float64)
secont condition:: tensor(2.0163e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.8415e-11, dtype=torch.float64)
secont condition:: tensor(2.8415e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.6324e-11, dtype=torch.float64)
secont condition:: tensor(1.6324e-11, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(7.5073e-10, dtype=torch.float64)
secont condition:: tensor(7.5073e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.5409e-09, dtype=torch.float64)
secont condition:: tensor(2.5409e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.8260e-09, dtype=torch.float64)
secont condition:: tensor(1.8260e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.5096e-09, dtype=torch.float64)
secont condition:: tensor(1.5096e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.4413e-09, dtype=torch.float64)
secont condition:: tensor(1.4413e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.0773e-09, dtype=torch.float64)
secont condition:: tensor(1.0773e-09, dtype=torch.float64)
curr_secont condition:: tensor(7.5021e-10, dtype=torch.float64)
secont condition:: tensor(7.5021e-10, dtype=torch.float64)
curr_secont condition:: tensor(7.1490e-10, dtype=torch.float64)
secont condition:: tensor(7.1490e-10, dtype=torch.float64)
curr_secont condition:: tensor(6.3870e-10, dtype=torch.float64)
secont condition:: tensor(6.3870e-10, dtype=torch.float64)
curr_secont condition:: tensor(5.1304e-10, dtype=torch.float64)
secont condition:: tensor(5.1304e-10, dtype=torch.float64)
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(4.9289e-10, dtype=torch.float64)
secont condition:: tensor(4.9289e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.0047e-08, dtype=torch.float64)
explicit_evaluation epoch:: 58
curr_secont condition:: tensor(2.7989e-09, dtype=torch.float64)
explicit_evaluation epoch:: 59
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(1.8952e-09, dtype=torch.float64)
secont condition:: tensor(1.8952e-09, dtype=torch.float64)
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(4.3241e-09, dtype=torch.float64)
secont condition:: tensor(4.3241e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.0159e-09, dtype=torch.float64)
secont condition:: tensor(2.0159e-09, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(3.2986e-09, dtype=torch.float64)
secont condition:: tensor(3.2986e-09, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(2.2826e-09, dtype=torch.float64)
secont condition:: tensor(2.2826e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.2414e-09, dtype=torch.float64)
secont condition:: tensor(2.2414e-09, dtype=torch.float64)
epoch  8
	calling Sampler:__iter__
curr_secont condition:: tensor(1.2024e-09, dtype=torch.float64)
secont condition:: tensor(1.2024e-09, dtype=torch.float64)
epoch  9
	calling Sampler:__iter__
curr_secont condition:: tensor(3.0357e-09, dtype=torch.float64)
secont condition:: tensor(3.0357e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.8729e-09, dtype=torch.float64)
secont condition:: tensor(1.8729e-09, dtype=torch.float64)
epoch  10
	calling Sampler:__iter__
curr_secont condition:: tensor(3.1576e-09, dtype=torch.float64)
secont condition:: tensor(3.1576e-09, dtype=torch.float64)
epoch  11
	calling Sampler:__iter__
curr_secont condition:: tensor(2.5173e-09, dtype=torch.float64)
secont condition:: tensor(2.5173e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.6422e-09, dtype=torch.float64)
secont condition:: tensor(1.6422e-09, dtype=torch.float64)
epoch  12
	calling Sampler:__iter__
curr_secont condition:: tensor(1.6514e-09, dtype=torch.float64)
secont condition:: tensor(1.6514e-09, dtype=torch.float64)
epoch  13
	calling Sampler:__iter__
curr_secont condition:: tensor(1.4569e-09, dtype=torch.float64)
secont condition:: tensor(1.4569e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.9477e-09, dtype=torch.float64)
secont condition:: tensor(1.9477e-09, dtype=torch.float64)
epoch  14
	calling Sampler:__iter__
curr_secont condition:: tensor(2.1654e-09, dtype=torch.float64)
secont condition:: tensor(2.1654e-09, dtype=torch.float64)
epoch  15
	calling Sampler:__iter__
curr_secont condition:: tensor(2.5303e-09, dtype=torch.float64)
secont condition:: tensor(2.5303e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.2768e-09, dtype=torch.float64)
secont condition:: tensor(2.2768e-09, dtype=torch.float64)
epoch  16
	calling Sampler:__iter__
curr_secont condition:: tensor(1.8817e-09, dtype=torch.float64)
secont condition:: tensor(1.8817e-09, dtype=torch.float64)
epoch  17
	calling Sampler:__iter__
curr_secont condition:: tensor(2.3239e-09, dtype=torch.float64)/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)

secont condition:: tensor(2.3239e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.9992e-09, dtype=torch.float64)
secont condition:: tensor(1.9992e-09, dtype=torch.float64)
epoch  18
	calling Sampler:__iter__
curr_secont condition:: tensor(2.8289e-09, dtype=torch.float64)
secont condition:: tensor(2.8289e-09, dtype=torch.float64)
epoch  19
	calling Sampler:__iter__
curr_secont condition:: tensor(1.7857e-09, dtype=torch.float64)
secont condition:: tensor(1.7857e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.4077e-09, dtype=torch.float64)
secont condition:: tensor(2.4077e-09, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 38.89488410949707
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(2.1973e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(3.4817e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.7648e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(2.8902e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000021, Accuracy: 0.773300
repetition 5
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 20
delta_size:: 1
max_epoch:: 20
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
training time is 121.60074305534363
time_baseline:: 121.64323854446411
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(2.8937e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(2.7583e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000021, Accuracy: 0.771900
incremental updates::
max_epoch:: 20
delta_size:: 1
max_epoch:: 20
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(6.0348e-33, dtype=torch.float64)
secont condition:: tensor(6.0348e-33, dtype=torch.float64)
curr_secont condition:: tensor(1.4958e-32, dtype=torch.float64)
secont condition:: tensor(1.4958e-32, dtype=torch.float64)
curr_secont condition:: tensor(2.6747e-32, dtype=torch.float64)
secont condition:: tensor(2.6747e-32, dtype=torch.float64)
curr_secont condition:: tensor(4.1223e-32, dtype=torch.float64)
secont condition:: tensor(4.1223e-32, dtype=torch.float64)
curr_secont condition:: tensor(5.8547e-32, dtype=torch.float64)
secont condition:: tensor(5.8547e-32, dtype=torch.float64)
curr_secont condition:: tensor(7.8546e-32, dtype=torch.float64)
secont condition:: tensor(7.8546e-32, dtype=torch.float64)
curr_secont condition:: tensor(1.0110e-31, dtype=torch.float64)
secont condition:: tensor(1.0110e-31, dtype=torch.float64)
curr_secont condition:: tensor(1.2650e-31, dtype=torch.float64)
secont condition:: tensor(1.2650e-31, dtype=torch.float64)
curr_secont condition:: tensor(1.5452e-31, dtype=torch.float64)
secont condition:: tensor(1.5452e-31, dtype=torch.float64)
curr_secont condition:: tensor(1.8514e-31, dtype=torch.float64)
secont condition:: tensor(1.8514e-31, dtype=torch.float64)
curr_secont condition:: tensor(2.1853e-31, dtype=torch.float64)
secont condition:: tensor(2.1853e-31, dtype=torch.float64)
curr_secont condition:: tensor(2.5430e-31, dtype=torch.float64)
secont condition:: tensor(2.5430e-31, dtype=torch.float64)
curr_secont condition:: tensor(2.9246e-31, dtype=torch.float64)
secont condition:: tensor(2.9246e-31, dtype=torch.float64)
curr_secont condition:: tensor(3.3360e-31, dtype=torch.float64)
secont condition:: tensor(3.3360e-31, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(5.4534e-10, dtype=torch.float64)
secont condition:: tensor(5.4534e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.3709e-10, dtype=torch.float64)
secont condition:: tensor(4.3709e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.0898e-10, dtype=torch.float64)
secont condition:: tensor(3.0898e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.0063e-10, dtype=torch.float64)
secont condition:: tensor(3.0063e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.1946e-10, dtype=torch.float64)
secont condition:: tensor(2.1946e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.0998e-10, dtype=torch.float64)
secont condition:: tensor(2.0998e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.5621e-10, dtype=torch.float64)
secont condition:: tensor(1.5621e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.3787e-10, dtype=torch.float64)
secont condition:: tensor(1.3787e-10, dtype=torch.float64)
curr_secont condition:: tensor(9.4551e-11, dtype=torch.float64)
secont condition:: tensor(9.4551e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.1072e-10, dtype=torch.float64)
secont condition:: tensor(3.1072e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.0885e-10, dtype=torch.float64)
secont condition:: tensor(3.0885e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.9073e-10, dtype=torch.float64)
secont condition:: tensor(1.9073e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.2061e-10, dtype=torch.float64)
secont condition:: tensor(2.2061e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.8837e-10, dtype=torch.float64)
secont condition:: tensor(1.8837e-10, dtype=torch.float64)
curr_secont condition:: tensor(9.8008e-11, dtype=torch.float64)
secont condition:: tensor(9.8008e-11, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(1.3053e-10, dtype=torch.float64)
secont condition:: tensor(1.3053e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.1025e-10, dtype=torch.float64)
secont condition:: tensor(1.1025e-10, dtype=torch.float64)
curr_secont condition:: tensor(7.3997e-11, dtype=torch.float64)
secont condition:: tensor(7.3997e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.2171e-10, dtype=torch.float64)
secont condition:: tensor(1.2171e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.1927e-10, dtype=torch.float64)
secont condition:: tensor(1.1927e-10, dtype=torch.float64)
curr_secont condition:: tensor(5.4521e-11, dtype=torch.float64)
secont condition:: tensor(5.4521e-11, dtype=torch.float64)
curr_secont condition:: tensor(7.4841e-11, dtype=torch.float64)
secont condition:: tensor(7.4841e-11, dtype=torch.float64)
curr_secont condition:: tensor(9.2322e-11, dtype=torch.float64)
secont condition:: tensor(9.2322e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.5391e-11, dtype=torch.float64)
secont condition:: tensor(6.5391e-11, dtype=torch.float64)
curr_secont condition:: tensor(9.7964e-11, dtype=torch.float64)
secont condition:: tensor(9.7964e-11, dtype=torch.float64)
curr_secont condition:: tensor(8.8678e-11, dtype=torch.float64)
explicit_evaluation epoch:: 41
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(1.2492e-10, dtype=torch.float64)
secont condition:: tensor(1.2492e-10, dtype=torch.float64)
curr_secont condition:: tensor(6.7288e-11, dtype=torch.float64)
explicit_evaluation epoch:: 54
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(1.0014e-09, dtype=torch.float64)
explicit_evaluation epoch:: 60
curr_secont condition:: tensor(5.0047e-10, dtype=torch.float64)
secont condition:: tensor(5.0047e-10, dtype=torch.float64)
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(8.5326e-10, dtype=torch.float64)
secont condition:: tensor(8.5326e-10, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(5.2376e-10, dtype=torch.float64)
secont condition:: tensor(5.2376e-10, dtype=torch.float64)
curr_secont condition:: tensor(5.9074e-10, dtype=torch.float64)
secont condition:: tensor(5.9074e-10, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(1.0035e-09, dtype=torch.float64)
secont condition:: tensor(1.0035e-09, dtype=torch.float64)
epoch  8
	calling Sampler:__iter__
curr_secont condition:: tensor(7.3441e-10, dtype=torch.float64)
secont condition:: tensor(7.3441e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.8187e-09, dtype=torch.float64)
secont condition:: tensor(1.8187e-09, dtype=torch.float64)
epoch  9
	calling Sampler:__iter__
curr_secont condition:: tensor(7.3169e-10, dtype=torch.float64)
secont condition:: tensor(7.3169e-10, dtype=torch.float64)
epoch  10
	calling Sampler:__iter__
curr_secont condition:: tensor(8.7895e-10, dtype=torch.float64)
secont condition:: tensor(8.7895e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.8819e-09, dtype=torch.float64)
secont condition:: tensor(1.8819e-09, dtype=torch.float64)
epoch  11
	calling Sampler:__iter__
curr_secont condition:: tensor(1.1222e-09, dtype=torch.float64)
secont condition:: tensor(1.1222e-09, dtype=torch.float64)
epoch  12
	calling Sampler:__iter__
curr_secont condition:: tensor(9.0576e-10, dtype=torch.float64)
secont condition:: tensor(9.0576e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.3606e-09, dtype=torch.float64)
secont condition:: tensor(1.3606e-09, dtype=torch.float64)
epoch  13
	calling Sampler:__iter__
curr_secont condition:: tensor(1.0378e-09, dtype=torch.float64)
secont condition:: tensor(1.0378e-09, dtype=torch.float64)
epoch  14
	calling Sampler:__iter__
curr_secont condition:: tensor(1.3765e-09, dtype=torch.float64)
secont condition:: tensor(1.3765e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.5367e-09, dtype=torch.float64)
secont condition:: tensor(1.5367e-09, dtype=torch.float64)
epoch  15
	calling Sampler:__iter__
curr_secont condition:: tensor(1.0461e-09, dtype=torch.float64)
secont condition:: tensor(1.0461e-09, dtype=torch.float64)
epoch  16
	calling Sampler:__iter__
curr_secont condition:: tensor(1.4276e-09, dtype=torch.float64)
secont condition:: tensor(1.4276e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.1142e-09, dtype=torch.float64)
secont condition:: tensor(1.1142e-09, dtype=torch.float64)
epoch /home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
 17
	calling Sampler:__iter__
curr_secont condition:: tensor(1.8548e-09, dtype=torch.float64)
secont condition:: tensor(1.8548e-09, dtype=torch.float64)
epoch  18
	calling Sampler:__iter__
curr_secont condition:: tensor(1.6058e-09, dtype=torch.float64)
secont condition:: tensor(1.6058e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.2317e-09, dtype=torch.float64)
secont condition:: tensor(2.2317e-09, dtype=torch.float64)
epoch  19
	calling Sampler:__iter__
curr_secont condition:: tensor(2.0315e-09, dtype=torch.float64)
secont condition:: tensor(2.0315e-09, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 38.143099784851074
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(2.0127e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(7.9178e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.2913e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(2.3336e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(4.0434e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000021, Accuracy: 0.770800
varied l2 norm::
l2 norm:: 0.0001
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type MSELoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
cuda:2
Train - Epoch 0, Batch: 0, Loss: 0.091544
Train - Epoch 0, Batch: 10, Loss: 0.090321
Test Avg. Loss: 0.000027, Accuracy: 0.328200
Train - Epoch 1, Batch: 0, Loss: 0.089751
Train - Epoch 1, Batch: 10, Loss: 0.089176
Test Avg. Loss: 0.000026, Accuracy: 0.577600
Train - Epoch 2, Batch: 0, Loss: 0.088566
Train - Epoch 2, Batch: 10, Loss: 0.088017
Test Avg. Loss: 0.000026, Accuracy: 0.641600
Train - Epoch 3, Batch: 0, Loss: 0.087171
Train - Epoch 3, Batch: 10, Loss: 0.086594
Test Avg. Loss: 0.000025, Accuracy: 0.661600
Train - Epoch 4, Batch: 0, Loss: 0.085614
Train - Epoch 4, Batch: 10, Loss: 0.084423
Test Avg. Loss: 0.000025, Accuracy: 0.675500
Train - Epoch 5, Batch: 0, Loss: 0.084178
Train - Epoch 5, Batch: 10, Loss: 0.083226
Test Avg. Loss: 0.000024, Accuracy: 0.693300
Train - Epoch 6, Batch: 0, Loss: 0.082741
Train - Epoch 6, Batch: 10, Loss: 0.081707
Test Avg. Loss: 0.000024, Accuracy: 0.710500
Train - Epoch 7, Batch: 0, Loss: 0.081959
Train - Epoch 7, Batch: 10, Loss: 0.081229
Test Avg. Loss: 0.000023, Accuracy: 0.726200
training_time:: 59.252838134765625
repetition 1
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 46.88794445991516
time_baseline:: 46.90585398674011
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.2249e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(3.4505e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000023, Accuracy: 0.726200
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(2.9482e-34, dtype=torch.float64)
secont condition:: tensor(2.9482e-34, dtype=torch.float64)
curr_secont condition:: tensor(6.6450e-34, dtype=torch.float64)
secont condition:: tensor(6.6450e-34, dtype=torch.float64)
curr_secont condition:: tensor(1.1544e-33, dtype=torch.float64)
secont condition:: tensor(1.1544e-33, dtype=torch.float64)
curr_secont condition:: tensor(1.7194e-33, dtype=torch.float64)
secont condition:: tensor(1.7194e-33, dtype=torch.float64)
curr_secont condition:: tensor(2.2898e-33, dtype=torch.float64)
secont condition:: tensor(2.2898e-33, dtype=torch.float64)
curr_secont condition:: tensor(3.5214e-11, dtype=torch.float64)
secont condition:: tensor(3.5214e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.2914e-11, dtype=torch.float64)
secont condition:: tensor(2.2914e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.6965e-11, dtype=torch.float64)
secont condition:: tensor(1.6965e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.5586e-11, dtype=torch.float64)
secont condition:: tensor(1.5586e-11, dtype=torch.float64)
curr_secont condition:: tensor(7.7260e-12, dtype=torch.float64)
secont condition:: tensor(7.7260e-12, dtype=torch.float64)
curr_secont condition:: tensor(6.9929e-12, dtype=torch.float64)
secont condition:: tensor(6.9929e-12, dtype=torch.float64)
curr_secont condition:: tensor(6.3559e-12, dtype=torch.float64)
secont condition:: tensor(6.3559e-12, dtype=torch.float64)
curr_secont condition:: tensor(3.2288e-12, dtype=torch.float64)
secont condition:: tensor(3.2288e-12, dtype=torch.float64)
curr_secont condition:: tensor(3.3013e-12, dtype=torch.float64)
secont condition:: tensor(3.3013e-12, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(3.6905e-12, dtype=torch.float64)
secont condition:: tensor(3.6905e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.9752e-12, dtype=torch.float64)
secont condition:: tensor(1.9752e-12, dtype=torch.float64)
curr_secont condition:: tensor(2.1410e-12, dtype=torch.float64)
secont condition:: tensor(2.1410e-12, dtype=torch.float64)
curr_secont condition:: tensor(2.6392e-12, dtype=torch.float64)
secont condition:: tensor(2.6392e-12, dtype=torch.float64)
curr_secont condition:: tensor(2.4824e-12, dtype=torch.float64)
secont condition:: tensor(2.4824e-12, dtype=torch.float64)
curr_secont condition:: tensor(-4.5663e-13, dtype=torch.float64)
curr_secont condition:: tensor(1.9229e-12, dtype=torch.float64)
secont condition:: tensor(1.9229e-12, dtype=torch.float64)
curr_secont condition:: tensor(3.8572e-13, dtype=torch.float64)
secont condition:: tensor(3.8572e-13, dtype=torch.float64)
curr_secont condition:: tensor(-7.5901e-14, dtype=torch.float64)
curr_secont condition:: tensor(7.5049e-12, dtype=torch.float64)
secont condition:: tensor(7.5049e-12, dtype=torch.float64)
curr_secont condition:: tensor(5.3796e-12, dtype=torch.float64)
secont condition:: tensor(5.3796e-12, dtype=torch.float64)
curr_secont condition:: tensor(4.6204e-12, dtype=torch.float64)
secont condition:: tensor(4.6204e-12, dtype=torch.float64)
curr_secont condition:: tensor(3.2410e-12, dtype=torch.float64)
secont condition:: tensor(3.2410e-12, dtype=torch.float64)
curr_secont condition:: tensor(3.3453e-12, dtype=torch.float64)
secont condition:: tensor(3.3453e-12, dtype=torch.float64)
curr_secont condition:: tensor(-2.8401e-12, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(1.2100e-12, dtype=torch.float64)
secont condition:: tensor(1.2100e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.7098e-12, dtype=torch.float64)
secont condition:: tensor(1.7098e-12, dtype=torch.float64)
curr_secont condition:: tensor(2.5976e-11, dtype=torch.float64)
secont condition:: tensor(2.5976e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.4061e-12, dtype=torch.float64)
secont condition:: tensor(2.4061e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.7525e-12, dtype=torch.float64)
secont condition:: tensor(1.7525e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.7996e-12, dtype=torch.float64)
secont condition:: tensor(1.7996e-12, dtype=torch.float64)
curr_secont condition:: tensor(2.7495e-12, dtype=torch.float64)
secont condition:: tensor(2.7495e-12, dtype=torch.float64)
curr_secont condition:: tensor(6.5056e-12, dtype=torch.float64)
secont condition:: tensor(6.5056e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.0224e-12, dtype=torch.float64)
secont condition:: tensor(1.0224e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.1404e-11, dtype=torch.float64)
secont condition:: tensor(1.1404e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.0935e-13, dtype=torch.float64)
explicit_evaluation epoch:: 40
curr_secont condition:: tensor(1.2058e-11, dtype=torch.float64)
explicit_evaluation epoch:: 41
curr_secont condition:: tensor(5.6922e-12, dtype=torch.float64)
explicit_evaluation epoch:: 42
curr_secont condition:: tensor(1.5110e-11, dtype=torch.float64)
explicit_evaluation epoch:: 44
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(-1.7433e-10, dtype=torch.float64)
explicit_evaluation epoch:: 48
curr_secont condition:: tensor(4.7501e-10, dtype=torch.float64)
secont condition:: tensor(4.7501e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.6393e-10, dtype=torch.float64)
secont condition:: tensor(3.6393e-10, dtype=torch.float64)
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(2.0626e-10, dtype=torch.float64)
secont condition:: tensor(2.0626e-10, dtype=torch.float64)
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(1.2978e-08, dtype=torch.float64)
explicit_evaluation epoch:: 77
curr_secont condition:: tensor(1.1566e-09, dtype=torch.float64)
explicit_evaluation epoch:: 78
curr_secont condition:: tensor(5.9769e-10, dtype=torch.float64)
secont condition:: tensor(5.9769e-10, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(-8.1407e-09, dtype=torch.float64)
explicit_evaluation epoch:: 95
curr_secont condition:: tensor(2.9903e-09, dtype=torch.float64)
secont condition:: tensor(2.9903e-09, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(1.1496e-09, dtype=torch.float64)
secont condition:: tensor(1.1496e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.3379e-09, dtype=torch.float64)
secont condition:: tensor(1.3379e-09, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 27.75908398628235
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(9.2646e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(4.8511e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(6.4254e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.0143e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(9.6057e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000023, Accuracy: 0.724900
repetition 2
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 47.319836378097534
time_baseline:: 47.33718419075012
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.9171e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(6.8116e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000023, Accuracy: 0.726100
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(2.9482e-34, dtype=torch.float64)
secont condition:: tensor(2.9482e-34, dtype=torch.float64)
curr_secont condition:: tensor(6.6450e-34, dtype=torch.float64)
secont condition:: tensor(6.6450e-34, dtype=torch.float64)
curr_secont condition:: tensor(1.1544e-33, dtype=torch.float64)
secont condition:: tensor(1.1544e-33, dtype=torch.float64)
curr_secont condition:: tensor(1.7194e-33, dtype=torch.float64)
secont condition:: tensor(1.7194e-33, dtype=torch.float64)
curr_secont condition:: tensor(2.3207e-33, dtype=torch.float64)
secont condition:: tensor(2.3207e-33, dtype=torch.float64)
curr_secont condition:: tensor(3.0400e-33, dtype=torch.float64)
secont condition:: tensor(3.0400e-33, dtype=torch.float64)
curr_secont condition:: tensor(3.8343e-33, dtype=torch.float64)
secont condition:: tensor(3.8343e-33, dtype=torch.float64)
curr_secont condition:: tensor(4.5995e-33, dtype=torch.float64)
secont condition:: tensor(4.5995e-33, dtype=torch.float64)
curr_secont condition:: tensor(5.6353e-33, dtype=torch.float64)
secont condition:: tensor(5.6353e-33, dtype=torch.float64)
curr_secont condition:: tensor(5.3560e-10, dtype=torch.float64)
secont condition:: tensor(5.3560e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.5532e-10, dtype=torch.float64)
secont condition:: tensor(3.5532e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.5411e-10, dtype=torch.float64)
secont condition:: tensor(2.5411e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.0829e-10, dtype=torch.float64)
secont condition:: tensor(2.0829e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.6532e-10, dtype=torch.float64)
secont condition:: tensor(1.6532e-10, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(1.4312e-10, dtype=torch.float64)
secont condition:: tensor(1.4312e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.0817e-10, dtype=torch.float64)
secont condition:: tensor(1.0817e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.1127e-10, dtype=torch.float64)
secont condition:: tensor(1.1127e-10, dtype=torch.float64)
curr_secont condition:: tensor(6.2723e-11, dtype=torch.float64)
secont condition:: tensor(6.2723e-11, dtype=torch.float64)
curr_secont condition:: tensor(7.4828e-11, dtype=torch.float64)
secont condition:: tensor(7.4828e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.9432e-11, dtype=torch.float64)
secont condition:: tensor(5.9432e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.9919e-11, dtype=torch.float64)
secont condition:: tensor(3.9919e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.8911e-11, dtype=torch.float64)
secont condition:: tensor(2.8911e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.1401e-11, dtype=torch.float64)
secont condition:: tensor(1.1401e-11, dtype=torch.float64)
curr_secont condition:: tensor(-5.3753e-12, dtype=torch.float64)
curr_secont condition:: tensor(5.1456e-11, dtype=torch.float64)
secont condition:: tensor(5.1456e-11, dtype=torch.float64)
curr_secont condition:: tensor(-2.9574e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.6271e-11, dtype=torch.float64)
secont condition:: tensor(2.6271e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.7026e-09, dtype=torch.float64)
secont condition:: tensor(1.7026e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.3722e-09, dtype=torch.float64)
secont condition:: tensor(1.3722e-09, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(8.6130e-10, dtype=torch.float64)
secont condition:: tensor(8.6130e-10, dtype=torch.float64)
curr_secont condition:: tensor(8.9841e-10, dtype=torch.float64)
secont condition:: tensor(8.9841e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.9070e-10, dtype=torch.float64)
secont condition:: tensor(3.9070e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.5468e-10, dtype=torch.float64)
secont condition:: tensor(3.5468e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.5668e-10, dtype=torch.float64)
secont condition:: tensor(3.5668e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.4167e-10, dtype=torch.float64)
secont condition:: tensor(3.4167e-10, dtype=torch.float64)
curr_secont condition:: tensor(5.2548e-10, dtype=torch.float64)
secont condition:: tensor(5.2548e-10, dtype=torch.float64)
curr_secont condition:: tensor(6.0583e-10, dtype=torch.float64)
secont condition:: tensor(6.0583e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.0162e-10, dtype=torch.float64)
secont condition:: tensor(1.0162e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.9978e-10, dtype=torch.float64)
secont condition:: tensor(1.9978e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.6425e-10, dtype=torch.float64)
explicit_evaluation epoch:: 41
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(1.8461e-09, dtype=torch.float64)
secont condition:: tensor(1.8461e-09, dtype=torch.float64)
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(4.6259e-09, dtype=torch.float64)
secont condition:: tensor(4.6259e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.8983e-09, dtype=torch.float64)
secont condition:: tensor(2.8983e-09, dtype=torch.float64)
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(2.8155e-09, dtype=torch.float64)
secont condition:: tensor(2.8155e-09, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(1.7860e-09, dtype=torch.float64)
secont condition:: tensor(1.7860e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.5071e-09, dtype=torch.float64)
secont condition:: tensor(1.5071e-09, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(1.4211e-09, dtype=torch.float64)
secont condition:: tensor(1.4211e-09, dtype=torch.float64)
curr_secont condition:: tensor(-8.9759e-10, dtype=torch.float64)
explicit_evaluation epoch:: 119
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 24.9236798286438
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.5339e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(5.8983e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(4.7101e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.5263e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(9.9820e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000023, Accuracy: 0.727100
repetition 3
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 47.55573844909668
time_baseline:: 47.575000286102295
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.4280e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(8.0622e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000023, Accuracy: 0.726200
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(2.9482e-34, dtype=torch.float64)
secont condition:: tensor(2.9482e-34, dtype=torch.float64)
curr_secont condition:: tensor(6.6450e-34, dtype=torch.float64)
secont condition:: tensor(6.6450e-34, dtype=torch.float64)
curr_secont condition:: tensor(1.1544e-33, dtype=torch.float64)
secont condition:: tensor(1.1544e-33, dtype=torch.float64)
curr_secont condition:: tensor(1.7194e-33, dtype=torch.float64)
secont condition:: tensor(1.7194e-33, dtype=torch.float64)
curr_secont condition:: tensor(2.3207e-33, dtype=torch.float64)
secont condition:: tensor(2.3207e-33, dtype=torch.float64)
curr_secont condition:: tensor(3.0400e-33, dtype=torch.float64)
secont condition:: tensor(3.0400e-33, dtype=torch.float64)
curr_secont condition:: tensor(3.8343e-33, dtype=torch.float64)
secont condition:: tensor(3.8343e-33, dtype=torch.float64)
curr_secont condition:: tensor(4.5416e-33, dtype=torch.float64)
secont condition:: tensor(4.5416e-33, dtype=torch.float64)
curr_secont condition:: tensor(7.0156e-12, dtype=torch.float64)
secont condition:: tensor(7.0156e-12, dtype=torch.float64)
curr_secont condition:: tensor(5.1796e-12, dtype=torch.float64)
secont condition:: tensor(5.1796e-12, dtype=torch.float64)
curr_secont condition:: tensor(3.9929e-12, dtype=torch.float64)
secont condition:: tensor(3.9929e-12, dtype=torch.float64)
curr_secont condition:: tensor(3.1668e-12, dtype=torch.float64)
secont condition:: tensor(3.1668e-12, dtype=torch.float64)
curr_secont condition:: tensor(2.0626e-12, dtype=torch.float64)
secont condition:: tensor(2.0626e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.9666e-12, dtype=torch.float64)
secont condition:: tensor(1.9666e-12, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(1.7859e-12, dtype=torch.float64)
secont condition:: tensor(1.7859e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.5654e-12, dtype=torch.float64)
secont condition:: tensor(1.5654e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.1795e-12, dtype=torch.float64)
secont condition:: tensor(1.1795e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.1238e-12, dtype=torch.float64)
secont condition:: tensor(1.1238e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.0537e-12, dtype=torch.float64)
secont condition:: tensor(1.0537e-12, dtype=torch.float64)
curr_secont condition:: tensor(2.1210e-10, dtype=torch.float64)
secont condition:: tensor(2.1210e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.6503e-10, dtype=torch.float64)
secont condition:: tensor(1.6503e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.2696e-10, dtype=torch.float64)
secont condition:: tensor(1.2696e-10, dtype=torch.float64)
curr_secont condition:: tensor(9.2102e-11, dtype=torch.float64)
secont condition:: tensor(9.2102e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.5739e-11, dtype=torch.float64)
secont condition:: tensor(4.5739e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.3785e-11, dtype=torch.float64)
secont condition:: tensor(4.3785e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.0693e-11, dtype=torch.float64)
secont condition:: tensor(2.0693e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.9996e-11, dtype=torch.float64)
secont condition:: tensor(3.9996e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.8639e-11, dtype=torch.float64)
secont condition:: tensor(1.8639e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.2674e-11, dtype=torch.float64)
secont condition:: tensor(1.2674e-11, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(6.9188e-12, dtype=torch.float64)
secont condition:: tensor(6.9188e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.2285e-11, dtype=torch.float64)
secont condition:: tensor(1.2285e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.5130e-11, dtype=torch.float64)
secont condition:: tensor(3.5130e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.8065e-11, dtype=torch.float64)
secont condition:: tensor(1.8065e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.1188e-12, dtype=torch.float64)
secont condition:: tensor(2.1188e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.2284e-11, dtype=torch.float64)
secont condition:: tensor(1.2284e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.1758e-11, dtype=torch.float64)
secont condition:: tensor(1.1758e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.9767e-11, dtype=torch.float64)
secont condition:: tensor(1.9767e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.5805e-11, dtype=torch.float64)
secont condition:: tensor(1.5805e-11, dtype=torch.float64)
curr_secont condition:: tensor(-1.7039e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.2461e-11, dtype=torch.float64)
secont condition:: tensor(1.2461e-11, dtype=torch.float64)
curr_secont condition:: tensor(-9.0412e-12, dtype=torch.float64)
explicit_evaluation epoch:: 43
curr_secont condition:: tensor(1.0343e-11, dtype=torch.float64)
secont condition:: tensor(1.0343e-11, dtype=torch.float64)
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(-5.4506e-11, dtype=torch.float64)
explicit_evaluation epoch:: 45
curr_secont condition:: tensor(-1.2050e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.5573e-11, dtype=torch.float64)
secont condition:: tensor(1.5573e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.0983e-11, dtype=torch.float64)
explicit_evaluation epoch:: 48
curr_secont condition:: tensor(6.6004e-11, dtype=torch.float64)
explicit_evaluation epoch:: 49
curr_secont condition:: tensor(1.3547e-09, dtype=torch.float64)
explicit_evaluation epoch:: 50
curr_secont condition:: tensor(4.0943e-10, dtype=torch.float64)
explicit_evaluation epoch:: 51
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(3.8826e-10, dtype=torch.float64)
secont condition:: tensor(3.8826e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.5676e-10, dtype=torch.float64)
explicit_evaluation epoch:: 68
curr_secont condition:: tensor(1.1459e-09, dtype=torch.float64)
explicit_evaluation epoch:: 69
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(9.5441e-10, dtype=torch.float64)
secont condition:: tensor(9.5441e-10, dtype=torch.float64)
curr_secont condition:: tensor(5.9967e-08, dtype=torch.float64)
explicit_evaluation epoch:: 88
curr_secont condition:: tensor(2.4400e-09, dtype=torch.float64)
explicit_evaluation epoch:: 89
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(-1.4711e-08, dtype=torch.float64)
explicit_evaluation epoch:: 97
curr_secont condition:: tensor(3.9230e-09, dtype=torch.float64)
secont condition:: tensor(3.9230e-09, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(1.3261e-09, dtype=torch.float64)
secont condition:: tensor(1.3261e-09, dtype=torch.float64)
curr_secont condition:: tensor(9.4989e-10, dtype=torch.float64)
secont condition:: tensor(9.4989e-10, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 29.960558891296387
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(8.6515e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(4.7441e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(7.4373e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.3524e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.5395e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000023, Accuracy: 0.729000
repetition 4
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 47.314215421676636
time_baseline:: 47.33203911781311
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.2456e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(7.7350e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000023, Accuracy: 0.726100
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(2.9482e-34, dtype=torch.float64)
secont condition:: tensor(2.9482e-34, dtype=torch.float64)
curr_secont condition:: tensor(6.6450e-34, dtype=torch.float64)
secont condition:: tensor(6.6450e-34, dtype=torch.float64)
curr_secont condition:: tensor(1.1544e-33, dtype=torch.float64)
secont condition:: tensor(1.1544e-33, dtype=torch.float64)
curr_secont condition:: tensor(1.7194e-33, dtype=torch.float64)
secont condition:: tensor(1.7194e-33, dtype=torch.float64)
curr_secont condition:: tensor(2.3207e-33, dtype=torch.float64)
secont condition:: tensor(2.3207e-33, dtype=torch.float64)
curr_secont condition:: tensor(3.0400e-33, dtype=torch.float64)
secont condition:: tensor(3.0400e-33, dtype=torch.float64)
curr_secont condition:: tensor(3.8025e-33, dtype=torch.float64)
secont condition:: tensor(3.8025e-33, dtype=torch.float64)
curr_secont condition:: tensor(2.6397e-11, dtype=torch.float64)
secont condition:: tensor(2.6397e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.4874e-11, dtype=torch.float64)
secont condition:: tensor(2.4874e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.6502e-11, dtype=torch.float64)
secont condition:: tensor(1.6502e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.4187e-12, dtype=torch.float64)
secont condition:: tensor(4.4187e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.0167e-11, dtype=torch.float64)
secont condition:: tensor(1.0167e-11, dtype=torch.float64)
curr_secont condition:: tensor(8.1497e-12, dtype=torch.float64)
secont condition:: tensor(8.1497e-12, dtype=torch.float64)
curr_secont condition:: tensor(6.2474e-12, dtype=torch.float64)
secont condition:: tensor(6.2474e-12, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(4.8197e-12, dtype=torch.float64)
secont condition:: tensor(4.8197e-12, dtype=torch.float64)
curr_secont condition:: tensor(3.1903e-12, dtype=torch.float64)
secont condition:: tensor(3.1903e-12, dtype=torch.float64)
curr_secont condition:: tensor(3.1303e-12, dtype=torch.float64)
secont condition:: tensor(3.1303e-12, dtype=torch.float64)
curr_secont condition:: tensor(3.4405e-12, dtype=torch.float64)
secont condition:: tensor(3.4405e-12, dtype=torch.float64)
curr_secont condition:: tensor(3.1775e-12, dtype=torch.float64)
secont condition:: tensor(3.1775e-12, dtype=torch.float64)
curr_secont condition:: tensor(2.9269e-12, dtype=torch.float64)
secont condition:: tensor(2.9269e-12, dtype=torch.float64)
curr_secont condition:: tensor(2.3902e-12, dtype=torch.float64)
secont condition:: tensor(2.3902e-12, dtype=torch.float64)
curr_secont condition:: tensor(2.0614e-12, dtype=torch.float64)
secont condition:: tensor(2.0614e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.5453e-12, dtype=torch.float64)
secont condition:: tensor(1.5453e-12, dtype=torch.float64)
curr_secont condition:: tensor(5.8225e-12, dtype=torch.float64)
secont condition:: tensor(5.8225e-12, dtype=torch.float64)
curr_secont condition:: tensor(8.5178e-13, dtype=torch.float64)
secont condition:: tensor(8.5178e-13, dtype=torch.float64)
curr_secont condition:: tensor(1.2420e-12, dtype=torch.float64)
secont condition:: tensor(1.2420e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.0965e-12, dtype=torch.float64)
secont condition:: tensor(1.0965e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.5262e-12, dtype=torch.float64)
secont condition:: tensor(1.5262e-12, dtype=torch.float64)
curr_secont condition:: tensor(-2.2656e-12, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(4.1918e-11, dtype=torch.float64)
secont condition:: tensor(4.1918e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.1103e-11, dtype=torch.float64)
secont condition:: tensor(4.1103e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.2928e-11, dtype=torch.float64)
secont condition:: tensor(3.2928e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.3038e-11, dtype=torch.float64)
secont condition:: tensor(2.3038e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.4320e-11, dtype=torch.float64)
secont condition:: tensor(1.4320e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.2244e-11, dtype=torch.float64)
secont condition:: tensor(1.2244e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.7662e-11, dtype=torch.float64)
secont condition:: tensor(1.7662e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.3670e-11, dtype=torch.float64)
secont condition:: tensor(1.3670e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.3774e-11, dtype=torch.float64)
secont condition:: tensor(1.3774e-11, dtype=torch.float64)
curr_secont condition:: tensor(9.3783e-12, dtype=torch.float64)
secont condition:: tensor(9.3783e-12, dtype=torch.float64)
curr_secont condition:: tensor(7.9891e-11, dtype=torch.float64)
explicit_evaluation epoch:: 43
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(-1.7549e-10, dtype=torch.float64)
explicit_evaluation epoch:: 52
curr_secont condition:: tensor(2.2969e-09, dtype=torch.float64)
secont condition:: tensor(2.2969e-09, dtype=torch.float64)
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(1.0427e-09, dtype=torch.float64)
secont condition:: tensor(1.0427e-09, dtype=torch.float64)
curr_secont condition:: tensor(5.9868e-10, dtype=torch.float64)
secont condition:: tensor(5.9868e-10, dtype=torch.float64)
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(5.6995e-10, dtype=torch.float64)
secont condition:: tensor(5.6995e-10, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(6.2200e-10, dtype=torch.float64)
secont condition:: tensor(6.2200e-10, dtype=torch.float64)
curr_secont condition:: tensor(5.2701e-10, dtype=torch.float64)
secont condition:: tensor(5.2701e-10, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(3.7920e-10, dtype=torch.float64)
secont condition:: tensor(3.7920e-10, dtype=torch.float64)
curr_secont condition:: tensor(-5.8864e-09, dtype=torch.float64)
explicit_evaluation epoch:: 115
curr_secont condition:: tensor(2.3171e-09, dtype=torch.float64)
secont condition:: tensor(2.3171e-09, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.87324023246765
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(8.0724e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(4.0845e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(5.3336e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.0759e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.2479e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000023, Accuracy: 0.725100
repetition 5
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 47.39285373687744
time_baseline:: 47.4109001159668
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.7216e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.3886e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000023, Accuracy: 0.726200
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(9.0737e-12, dtype=torch.float64)
secont condition:: tensor(9.0737e-12, dtype=torch.float64)
curr_secont condition:: tensor(4.8837e-12, dtype=torch.float64)
secont condition:: tensor(4.8837e-12, dtype=torch.float64)
curr_secont condition:: tensor(4.0664e-12, dtype=torch.float64)
secont condition:: tensor(4.0664e-12, dtype=torch.float64)
curr_secont condition:: tensor(3.0544e-12, dtype=torch.float64)
secont condition:: tensor(3.0544e-12, dtype=torch.float64)
curr_secont condition:: tensor(2.3506e-12, dtype=torch.float64)
secont condition:: tensor(2.3506e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.8440e-12, dtype=torch.float64)
secont condition:: tensor(1.8440e-12, dtype=torch.float64)
curr_secont condition:: tensor(6.4736e-13, dtype=torch.float64)
secont condition:: tensor(6.4736e-13, dtype=torch.float64)
curr_secont condition:: tensor(1.3463e-12, dtype=torch.float64)
secont condition:: tensor(1.3463e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.0437e-12, dtype=torch.float64)
secont condition:: tensor(1.0437e-12, dtype=torch.float64)
curr_secont condition:: tensor(8.8315e-13, dtype=torch.float64)
secont condition:: tensor(8.8315e-13, dtype=torch.float64)
curr_secont condition:: tensor(7.9781e-13, dtype=torch.float64)
secont condition:: tensor(7.9781e-13, dtype=torch.float64)
curr_secont condition:: tensor(6.2379e-13, dtype=torch.float64)
secont condition:: tensor(6.2379e-13, dtype=torch.float64)
curr_secont condition:: tensor(5.5363e-13, dtype=torch.float64)
secont condition:: tensor(5.5363e-13, dtype=torch.float64)
curr_secont condition:: tensor(6.0707e-13, dtype=torch.float64)
secont condition:: tensor(6.0707e-13, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(5.1271e-13, dtype=torch.float64)
secont condition:: tensor(5.1271e-13, dtype=torch.float64)
curr_secont condition:: tensor(4.0745e-13, dtype=torch.float64)
secont condition:: tensor(4.0745e-13, dtype=torch.float64)
curr_secont condition:: tensor(3.4950e-11, dtype=torch.float64)
secont condition:: tensor(3.4950e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.5348e-11, dtype=torch.float64)
secont condition:: tensor(2.5348e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.8000e-11, dtype=torch.float64)
secont condition:: tensor(1.8000e-11, dtype=torch.float64)
curr_secont condition:: tensor(9.8296e-12, dtype=torch.float64)
secont condition:: tensor(9.8296e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.1137e-11, dtype=torch.float64)
secont condition:: tensor(1.1137e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.6557e-12, dtype=torch.float64)
secont condition:: tensor(5.6557e-12, dtype=torch.float64)
curr_secont condition:: tensor(7.1360e-12, dtype=torch.float64)
secont condition:: tensor(7.1360e-12, dtype=torch.float64)
curr_secont condition:: tensor(5.7118e-12, dtype=torch.float64)
secont condition:: tensor(5.7118e-12, dtype=torch.float64)
curr_secont condition:: tensor(3.8720e-12, dtype=torch.float64)
secont condition:: tensor(3.8720e-12, dtype=torch.float64)
curr_secont condition:: tensor(2.3718e-12, dtype=torch.float64)
secont condition:: tensor(2.3718e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.3310e-11, dtype=torch.float64)
secont condition:: tensor(1.3310e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.6699e-12, dtype=torch.float64)
secont condition:: tensor(2.6699e-12, dtype=torch.float64)
curr_secont condition:: tensor(-1.7589e-12, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(1.7320e-11, dtype=torch.float64)
secont condition:: tensor(1.7320e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.7583e-12, dtype=torch.float64)
secont condition:: tensor(3.7583e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.9677e-12, dtype=torch.float64)
secont condition:: tensor(1.9677e-12, dtype=torch.float64)
curr_secont condition:: tensor(3.3916e-13, dtype=torch.float64)
secont condition:: tensor(3.3916e-13, dtype=torch.float64)
curr_secont condition:: tensor(1.3621e-12, dtype=torch.float64)
secont condition:: tensor(1.3621e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.6809e-12, dtype=torch.float64)
secont condition:: tensor(1.6809e-12, dtype=torch.float64)
curr_secont condition:: tensor(2.1710e-12, dtype=torch.float64)
secont condition:: tensor(2.1710e-12, dtype=torch.float64)
curr_secont condition:: tensor(3.8498e-13, dtype=torch.float64)
secont condition:: tensor(3.8498e-13, dtype=torch.float64)
curr_secont condition:: tensor(2.4620e-12, dtype=torch.float64)
secont condition:: tensor(2.4620e-12, dtype=torch.float64)
curr_secont condition:: tensor(4.1619e-13, dtype=torch.float64)
secont condition:: tensor(4.1619e-13, dtype=torch.float64)
curr_secont condition:: tensor(1.5785e-12, dtype=torch.float64)
explicit_evaluation epoch:: 40
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(7.4120e-11, dtype=torch.float64)
secont condition:: tensor(7.4120e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.2826e-10, dtype=torch.float64)
explicit_evaluation epoch:: 52
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(2.9871e-11, dtype=torch.float64)
secont condition:: tensor(2.9871e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.9361e-11, dtype=torch.float64)
explicit_evaluation epoch:: 64
curr_secont condition:: tensor(-3.8272e-11, dtype=torch.float64)
explicit_evaluation epoch:: 70
curr_secont condition:: tensor(1.0378e-09, dtype=torch.float64)
secont condition:: tensor(1.0378e-09, dtype=torch.float64)
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(4.0920e-10, dtype=torch.float64)
secont condition:: tensor(4.0920e-10, dtype=torch.float64)
curr_secont condition:: tensor(-3.6225e-09, dtype=torch.float64)
explicit_evaluation epoch:: 86
curr_secont condition:: tensor(2.0744e-09, dtype=torch.float64)
secont condition:: tensor(2.0744e-09, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(1.1059e-09, dtype=torch.float64)
secont condition:: tensor(1.1059e-09, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(1.1744e-09, dtype=torch.float64)
secont condition:: tensor(1.1744e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.5751e-09, dtype=torch.float64)
secont condition:: tensor(1.5751e-09, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 26.237044095993042
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(9.8246e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(3.3638e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(3.9727e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.5830e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.7808e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000023, Accuracy: 0.727500
l2 norm:: 0.0002
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type MSELoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
cuda:2
Train - Epoch 0, Batch: 0, Loss: 0.091544
Train - Epoch 0, Batch: 10, Loss: 0.090319
Test Avg. Loss: 0.000027, Accuracy: 0.327900
Train - Epoch 1, Batch: 0, Loss: 0.089749
Train - Epoch 1, Batch: 10, Loss: 0.089175
Test Avg. Loss: 0.000026, Accuracy: 0.577700
Train - Epoch 2, Batch: 0, Loss: 0.088568
Train - Epoch 2, Batch: 10, Loss: 0.088022
Test Avg. Loss: 0.000026, Accuracy: 0.641200
Train - Epoch 3, Batch: 0, Loss: 0.087184
Train - Epoch 3, Batch: 10, Loss: 0.086613
Test Avg. Loss: 0.000025, Accuracy: 0.661400
Train - Epoch 4, Batch: 0, Loss: 0.085641
Train - Epoch 4, Batch: 10, Loss: 0.084464
Test Avg. Loss: 0.000025, Accuracy: 0.674500
Train - Epoch 5, Batch: 0, Loss: 0.084223
Train - Epoch 5, Batch: 10, Loss: 0.083281
Test Avg. Loss: 0.000024, Accuracy: 0.692300
Train - Epoch 6, Batch: 0, Loss: 0.082801
Train - Epoch 6, Batch: 10, Loss: 0.081776
Test Avg. Loss: 0.000024, Accuracy: 0.709700
Train - Epoch 7, Batch: 0, Loss: 0.082026
Train - Epoch 7, Batch: 10, Loss: 0.081303
Test Avg. Loss: 0.000023, Accuracy: 0.724200
training_time:: 59.43972706794739
repetition 1
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 47.034308433532715
time_baseline:: 47.05236053466797
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.3054e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(2.0288e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000023, Accuracy: 0.724200
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(7.0824e-34, dtype=torch.float64)
secont condition:: tensor(7.0824e-34, dtype=torch.float64)
curr_secont condition:: tensor(1.2748e-11, dtype=torch.float64)
secont condition:: tensor(1.2748e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.9563e-12, dtype=torch.float64)
secont condition:: tensor(6.9563e-12, dtype=torch.float64)
curr_secont condition:: tensor(8.5022e-12, dtype=torch.float64)
secont condition:: tensor(8.5022e-12, dtype=torch.float64)
curr_secont condition:: tensor(5.3237e-12, dtype=torch.float64)
secont condition:: tensor(5.3237e-12, dtype=torch.float64)
curr_secont condition:: tensor(-2.2382e-12, dtype=torch.float64)
curr_secont condition:: tensor(5.2712e-12, dtype=torch.float64)
secont condition:: tensor(5.2712e-12, dtype=torch.float64)
curr_secont condition:: tensor(5.0783e-12, dtype=torch.float64)
secont condition:: tensor(5.0783e-12, dtype=torch.float64)
curr_secont condition:: tensor(3.5666e-12, dtype=torch.float64)
secont condition:: tensor(3.5666e-12, dtype=torch.float64)
curr_secont condition:: tensor(3.3207e-12, dtype=torch.float64)
secont condition:: tensor(3.3207e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.0270e-12, dtype=torch.float64)
secont condition:: tensor(1.0270e-12, dtype=torch.float64)
curr_secont condition:: tensor(2.2924e-12, dtype=torch.float64)
secont condition:: tensor(2.2924e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.8613e-12, dtype=torch.float64)
secont condition:: tensor(1.8613e-12, dtype=torch.float64)
curr_secont condition:: tensor(2.1543e-12, dtype=torch.float64)
secont condition:: tensor(2.1543e-12, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(1.8406e-12, dtype=torch.float64)
secont condition:: tensor(1.8406e-12, dtype=torch.float64)
curr_secont condition:: tensor(2.6299e-12, dtype=torch.float64)
secont condition:: tensor(2.6299e-12, dtype=torch.float64)
curr_secont condition:: tensor(8.4578e-13, dtype=torch.float64)
secont condition:: tensor(8.4578e-13, dtype=torch.float64)
curr_secont condition:: tensor(8.8422e-13, dtype=torch.float64)
secont condition:: tensor(8.8422e-13, dtype=torch.float64)
curr_secont condition:: tensor(4.2703e-13, dtype=torch.float64)
secont condition:: tensor(4.2703e-13, dtype=torch.float64)
curr_secont condition:: tensor(1.1314e-12, dtype=torch.float64)
secont condition:: tensor(1.1314e-12, dtype=torch.float64)
curr_secont condition:: tensor(3.5519e-13, dtype=torch.float64)
secont condition:: tensor(3.5519e-13, dtype=torch.float64)
curr_secont condition:: tensor(6.6573e-13, dtype=torch.float64)
secont condition:: tensor(6.6573e-13, dtype=torch.float64)
curr_secont condition:: tensor(-5.6167e-13, dtype=torch.float64)
curr_secont condition:: tensor(3.9804e-13, dtype=torch.float64)
secont condition:: tensor(3.9804e-13, dtype=torch.float64)
curr_secont condition:: tensor(8.7408e-13, dtype=torch.float64)
secont condition:: tensor(8.7408e-13, dtype=torch.float64)
curr_secont condition:: tensor(7.7661e-13, dtype=torch.float64)
secont condition:: tensor(7.7661e-13, dtype=torch.float64)
curr_secont condition:: tensor(8.2067e-13, dtype=torch.float64)
secont condition:: tensor(8.2067e-13, dtype=torch.float64)
curr_secont condition:: tensor(7.2755e-13, dtype=torch.float64)
secont condition:: tensor(7.2755e-13, dtype=torch.float64)
curr_secont condition:: tensor(3.1110e-10, dtype=torch.float64)
secont condition:: tensor(3.1110e-10, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(1.8730e-10, dtype=torch.float64)
secont condition:: tensor(1.8730e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.4570e-10, dtype=torch.float64)
secont condition:: tensor(1.4570e-10, dtype=torch.float64)
curr_secont condition:: tensor(7.3633e-11, dtype=torch.float64)
secont condition:: tensor(7.3633e-11, dtype=torch.float64)
curr_secont condition:: tensor(7.6858e-11, dtype=torch.float64)
secont condition:: tensor(7.6858e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.1894e-11, dtype=torch.float64)
secont condition:: tensor(4.1894e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.2246e-09, dtype=torch.float64)
secont condition:: tensor(1.2246e-09, dtype=torch.float64)
curr_secont condition:: tensor(8.0052e-10, dtype=torch.float64)
secont condition:: tensor(8.0052e-10, dtype=torch.float64)
curr_secont condition:: tensor(6.8181e-10, dtype=torch.float64)
secont condition:: tensor(6.8181e-10, dtype=torch.float64)
curr_secont condition:: tensor(5.2438e-10, dtype=torch.float64)
secont condition:: tensor(5.2438e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.1423e-10, dtype=torch.float64)
secont condition:: tensor(3.1423e-10, dtype=torch.float64)
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(8.9318e-10, dtype=torch.float64)
explicit_evaluation epoch:: 47
curr_secont condition:: tensor(2.2048e-09, dtype=torch.float64)
secont condition:: tensor(2.2048e-09, dtype=torch.float64)
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(2.0504e-09, dtype=torch.float64)
secont condition:: tensor(2.0504e-09, dtype=torch.float64)
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(1.3892e-09, dtype=torch.float64)
secont condition:: tensor(1.3892e-09, dtype=torch.float64)
curr_secont condition:: tensor(9.6148e-10, dtype=torch.float64)
secont condition:: tensor(9.6148e-10, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(8.0967e-10, dtype=torch.float64)
secont condition:: tensor(8.0967e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.9239e-09, dtype=torch.float64)
explicit_evaluation epoch:: 101
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(1.3068e-09, dtype=torch.float64)
secont condition:: tensor(1.3068e-09, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 24.53708291053772
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.3169e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(5.7780e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(8.7851e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.1329e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(8.9323e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000023, Accuracy: 0.714900
repetition 2
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 47.04255485534668
time_baseline:: 47.06203889846802
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.3610e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.6351e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000023, Accuracy: 0.724200
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(7.0064e-34, dtype=torch.float64)
secont condition:: tensor(7.0064e-34, dtype=torch.float64)
curr_secont condition:: tensor(1.7734e-33, dtype=torch.float64)
secont condition:: tensor(1.7734e-33, dtype=torch.float64)
curr_secont condition:: tensor(3.2329e-33, dtype=torch.float64)
secont condition:: tensor(3.2329e-33, dtype=torch.float64)
curr_secont condition:: tensor(4.9895e-33, dtype=torch.float64)
secont condition:: tensor(4.9895e-33, dtype=torch.float64)
curr_secont condition:: tensor(7.2580e-33, dtype=torch.float64)
secont condition:: tensor(7.2580e-33, dtype=torch.float64)
curr_secont condition:: tensor(9.7903e-33, dtype=torch.float64)
secont condition:: tensor(9.7903e-33, dtype=torch.float64)
curr_secont condition:: tensor(1.1272e-10, dtype=torch.float64)
secont condition:: tensor(1.1272e-10, dtype=torch.float64)
curr_secont condition:: tensor(7.5559e-11, dtype=torch.float64)
secont condition:: tensor(7.5559e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.8451e-11, dtype=torch.float64)
secont condition:: tensor(6.8451e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.6756e-11, dtype=torch.float64)
secont condition:: tensor(3.6756e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.1367e-11, dtype=torch.float64)
secont condition:: tensor(4.1367e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.1715e-11, dtype=torch.float64)
secont condition:: tensor(4.1715e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.5242e-11, dtype=torch.float64)
secont condition:: tensor(2.5242e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.8505e-11, dtype=torch.float64)
secont condition:: tensor(2.8505e-11, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(2.0963e-11, dtype=torch.float64)
secont condition:: tensor(2.0963e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.1887e-11, dtype=torch.float64)
secont condition:: tensor(1.1887e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.1309e-11, dtype=torch.float64)
secont condition:: tensor(1.1309e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.0467e-11, dtype=torch.float64)
secont condition:: tensor(1.0467e-11, dtype=torch.float64)
curr_secont condition:: tensor(-4.2382e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.3592e-11, dtype=torch.float64)
secont condition:: tensor(1.3592e-11, dtype=torch.float64)
curr_secont condition:: tensor(9.1902e-12, dtype=torch.float64)
secont condition:: tensor(9.1902e-12, dtype=torch.float64)
curr_secont condition:: tensor(2.4280e-12, dtype=torch.float64)
secont condition:: tensor(2.4280e-12, dtype=torch.float64)
curr_secont condition:: tensor(5.7871e-12, dtype=torch.float64)
secont condition:: tensor(5.7871e-12, dtype=torch.float64)
curr_secont condition:: tensor(5.4941e-12, dtype=torch.float64)
secont condition:: tensor(5.4941e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.7783e-10, dtype=torch.float64)
secont condition:: tensor(1.7783e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.2104e-10, dtype=torch.float64)
secont condition:: tensor(1.2104e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.0769e-10, dtype=torch.float64)
secont condition:: tensor(1.0769e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.0384e-10, dtype=torch.float64)
secont condition:: tensor(1.0384e-10, dtype=torch.float64)
curr_secont condition:: tensor(6.9163e-11, dtype=torch.float64)
secont condition:: tensor(6.9163e-11, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(5.6640e-11, dtype=torch.float64)
secont condition:: tensor(5.6640e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.6596e-11, dtype=torch.float64)
secont condition:: tensor(6.6596e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.9923e-11, dtype=torch.float64)
secont condition:: tensor(3.9923e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.6662e-11, dtype=torch.float64)
secont condition:: tensor(3.6662e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.3472e-11, dtype=torch.float64)
secont condition:: tensor(2.3472e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.6499e-11, dtype=torch.float64)
secont condition:: tensor(5.6499e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.4827e-11, dtype=torch.float64)
secont condition:: tensor(2.4827e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.3346e-11, dtype=torch.float64)
secont condition:: tensor(2.3346e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.5237e-11, dtype=torch.float64)
secont condition:: tensor(1.5237e-11, dtype=torch.float64)
curr_secont condition:: tensor(-1.0343e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.6887e-10, dtype=torch.float64)
secont condition:: tensor(2.6887e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.5060e-10, dtype=torch.float64)
explicit_evaluation epoch:: 42
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(1.9969e-10, dtype=torch.float64)
secont condition:: tensor(1.9969e-10, dtype=torch.float64)
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(6.4075e-10, dtype=torch.float64)
secont condition:: tensor(6.4075e-10, dtype=torch.float64)
curr_secont condition:: tensor(7.4231e-10, dtype=torch.float64)
secont condition:: tensor(7.4231e-10, dtype=torch.float64)
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(6.0028e-10, dtype=torch.float64)
secont condition:: tensor(6.0028e-10, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(1.0713e-09, dtype=torch.float64)
secont condition:: tensor(1.0713e-09, dtype=torch.float64)
curr_secont condition:: tensor(9.7054e-10, dtype=torch.float64)
secont condition:: tensor(9.7054e-10, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(1.0815e-09, dtype=torch.float64)
secont condition:: tensor(1.0815e-09, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.18129277229309
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(9.1933e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(5.4829e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(8.2505e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.1948e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(2.4524e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000023, Accuracy: 0.715700
repetition 3
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 46.35475540161133
time_baseline:: 46.37402367591858
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.0358e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(5.4965e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000023, Accuracy: 0.724300
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(7.0064e-34, dtype=torch.float64)
secont condition:: tensor(7.0064e-34, dtype=torch.float64)
curr_secont condition:: tensor(1.8208e-33, dtype=torch.float64)
secont condition:: tensor(1.8208e-33, dtype=torch.float64)
curr_secont condition:: tensor(1.1196e-12, dtype=torch.float64)
secont condition:: tensor(1.1196e-12, dtype=torch.float64)
curr_secont condition:: tensor(5.9545e-13, dtype=torch.float64)
secont condition:: tensor(5.9545e-13, dtype=torch.float64)
curr_secont condition:: tensor(4.1278e-13, dtype=torch.float64)
secont condition:: tensor(4.1278e-13, dtype=torch.float64)
curr_secont condition:: tensor(2.6513e-13, dtype=torch.float64)
secont condition:: tensor(2.6513e-13, dtype=torch.float64)
curr_secont condition:: tensor(1.7217e-13, dtype=torch.float64)
secont condition:: tensor(1.7217e-13, dtype=torch.float64)
curr_secont condition:: tensor(1.2438e-13, dtype=torch.float64)
secont condition:: tensor(1.2438e-13, dtype=torch.float64)
curr_secont condition:: tensor(1.0194e-13, dtype=torch.float64)
secont condition:: tensor(1.0194e-13, dtype=torch.float64)
curr_secont condition:: tensor(1.7501e-13, dtype=torch.float64)
secont condition:: tensor(1.7501e-13, dtype=torch.float64)
curr_secont condition:: tensor(5.5418e-14, dtype=torch.float64)
secont condition:: tensor(5.5418e-14, dtype=torch.float64)
curr_secont condition:: tensor(4.9202e-14, dtype=torch.float64)
secont condition:: tensor(4.9202e-14, dtype=torch.float64)
curr_secont condition:: tensor(3.0302e-14, dtype=torch.float64)
secont condition:: tensor(3.0302e-14, dtype=torch.float64)
curr_secont condition:: tensor(2.5091e-14, dtype=torch.float64)
secont condition:: tensor(2.5091e-14, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(-3.4348e-13, dtype=torch.float64)
curr_secont condition:: tensor(3.4975e-14, dtype=torch.float64)
secont condition:: tensor(3.4975e-14, dtype=torch.float64)
curr_secont condition:: tensor(1.3728e-11, dtype=torch.float64)
secont condition:: tensor(1.3728e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.0662e-11, dtype=torch.float64)
secont condition:: tensor(1.0662e-11, dtype=torch.float64)
curr_secont condition:: tensor(8.1542e-12, dtype=torch.float64)
secont condition:: tensor(8.1542e-12, dtype=torch.float64)
curr_secont condition:: tensor(7.4070e-12, dtype=torch.float64)
secont condition:: tensor(7.4070e-12, dtype=torch.float64)
curr_secont condition:: tensor(6.0179e-12, dtype=torch.float64)
secont condition:: tensor(6.0179e-12, dtype=torch.float64)
curr_secont condition:: tensor(3.2868e-12, dtype=torch.float64)
secont condition:: tensor(3.2868e-12, dtype=torch.float64)
curr_secont condition:: tensor(4.1242e-12, dtype=torch.float64)
secont condition:: tensor(4.1242e-12, dtype=torch.float64)
curr_secont condition:: tensor(3.4392e-12, dtype=torch.float64)
secont condition:: tensor(3.4392e-12, dtype=torch.float64)
curr_secont condition:: tensor(3.2599e-12, dtype=torch.float64)
secont condition:: tensor(3.2599e-12, dtype=torch.float64)
curr_secont condition:: tensor(3.6150e-12, dtype=torch.float64)
secont condition:: tensor(3.6150e-12, dtype=torch.float64)
curr_secont condition:: tensor(2.3590e-12, dtype=torch.float64)
secont condition:: tensor(2.3590e-12, dtype=torch.float64)
curr_secont condition:: tensor(2.1315e-12, dtype=torch.float64)
secont condition:: tensor(2.1315e-12, dtype=torch.float64)
curr_secont condition:: tensor(-2.1423e-13, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(1.5936e-12, dtype=torch.float64)
secont condition:: tensor(1.5936e-12, dtype=torch.float64)
curr_secont condition:: tensor(3.7528e-13, dtype=torch.float64)
secont condition:: tensor(3.7528e-13, dtype=torch.float64)
curr_secont condition:: tensor(8.8437e-11, dtype=torch.float64)
secont condition:: tensor(8.8437e-11, dtype=torch.float64)
curr_secont condition:: tensor(9.5040e-11, dtype=torch.float64)
secont condition:: tensor(9.5040e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.0084e-11, dtype=torch.float64)
secont condition:: tensor(6.0084e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.6155e-11, dtype=torch.float64)
secont condition:: tensor(5.6155e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.7763e-11, dtype=torch.float64)
secont condition:: tensor(3.7763e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.0804e-11, dtype=torch.float64)
secont condition:: tensor(4.0804e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.6833e-11, dtype=torch.float64)
secont condition:: tensor(2.6833e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.2071e-11, dtype=torch.float64)
secont condition:: tensor(2.2071e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.8583e-11, dtype=torch.float64)
explicit_evaluation epoch:: 40
curr_secont condition:: tensor(3.8590e-11, dtype=torch.float64)
explicit_evaluation epoch:: 42
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(1.0024e-11, dtype=torch.float64)
secont condition:: tensor(1.0024e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.2526e-11, dtype=torch.float64)
explicit_evaluation epoch:: 53
curr_secont condition:: tensor(1.7442e-11, dtype=torch.float64)
explicit_evaluation epoch:: 55
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(1.1714e-10, dtype=torch.float64)
secont condition:: tensor(1.1714e-10, dtype=torch.float64)
curr_secont condition:: tensor(6.8024e-09, dtype=torch.float64)
explicit_evaluation epoch:: 71
curr_secont condition:: tensor(5.1481e-10, dtype=torch.float64)
explicit_evaluation epoch:: 72
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(3.8366e-10, dtype=torch.float64)
secont condition:: tensor(3.8366e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.6367e-09, dtype=torch.float64)
explicit_evaluation epoch:: 88
curr_secont condition:: tensor(8.4387e-10, dtype=torch.float64)
explicit_evaluation epoch:: 89
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(9.8829e-10, dtype=torch.float64)
secont condition:: tensor(9.8829e-10, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(7.7108e-10, dtype=torch.float64)
secont condition:: tensor(7.7108e-10, dtype=torch.float64)
curr_secont condition:: tensor(6.2294e-10, dtype=torch.float64)
secont condition:: tensor(6.2294e-10, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 27.213316440582275
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(7.8512e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(3.9027e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(6.0919e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(8.8630e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.1271e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000023, Accuracy: 0.725500
repetition 4
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 47.35039710998535
time_baseline:: 47.36890935897827
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.2776e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(5.5377e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000023, Accuracy: 0.724200
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(5.0491e-10, dtype=torch.float64)
secont condition:: tensor(5.0491e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.2660e-10, dtype=torch.float64)
secont condition:: tensor(4.2660e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.0701e-10, dtype=torch.float64)
secont condition:: tensor(3.0701e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.1743e-10, dtype=torch.float64)
secont condition:: tensor(2.1743e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.1135e-10, dtype=torch.float64)
secont condition:: tensor(2.1135e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.4672e-10, dtype=torch.float64)
secont condition:: tensor(1.4672e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.2584e-10, dtype=torch.float64)
secont condition:: tensor(1.2584e-10, dtype=torch.float64)
curr_secont condition:: tensor(9.7063e-11, dtype=torch.float64)
secont condition:: tensor(9.7063e-11, dtype=torch.float64)
curr_secont condition:: tensor(8.3553e-11, dtype=torch.float64)
secont condition:: tensor(8.3553e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.5312e-11, dtype=torch.float64)
secont condition:: tensor(4.5312e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.2398e-11, dtype=torch.float64)
secont condition:: tensor(4.2398e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.0363e-11, dtype=torch.float64)
secont condition:: tensor(5.0363e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.0093e-11, dtype=torch.float64)
secont condition:: tensor(2.0093e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.2066e-11, dtype=torch.float64)
secont condition:: tensor(6.2066e-11, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(2.1429e-11, dtype=torch.float64)
secont condition:: tensor(2.1429e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.5052e-11, dtype=torch.float64)
secont condition:: tensor(2.5052e-11, dtype=torch.float64)
curr_secont condition:: tensor(7.3872e-12, dtype=torch.float64)
secont condition:: tensor(7.3872e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.3036e-11, dtype=torch.float64)
secont condition:: tensor(1.3036e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.1925e-11, dtype=torch.float64)
secont condition:: tensor(2.1925e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.0667e-11, dtype=torch.float64)
secont condition:: tensor(3.0667e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.0263e-11, dtype=torch.float64)
secont condition:: tensor(1.0263e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.6792e-11, dtype=torch.float64)
secont condition:: tensor(4.6792e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.6424e-11, dtype=torch.float64)
secont condition:: tensor(3.6424e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.1902e-11, dtype=torch.float64)
secont condition:: tensor(5.1902e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.9460e-11, dtype=torch.float64)
secont condition:: tensor(1.9460e-11, dtype=torch.float64)
curr_secont condition:: tensor(-1.1085e-11, dtype=torch.float64)
curr_secont condition:: tensor(8.0539e-11, dtype=torch.float64)
secont condition:: tensor(8.0539e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.0832e-11, dtype=torch.float64)
secont condition:: tensor(3.0832e-11, dtype=torch.float64)
curr_secont condition:: tensor(7.0072e-11, dtype=torch.float64)
secont condition:: tensor(7.0072e-11, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(5.3044e-12, dtype=torch.float64)
secont condition:: tensor(5.3044e-12, dtype=torch.float64)
curr_secont condition:: tensor(2.4140e-11, dtype=torch.float64)
secont condition:: tensor(2.4140e-11, dtype=torch.float64)
curr_secont condition:: tensor(-2.1083e-13, dtype=torch.float64)
curr_secont condition:: tensor(-1.1992e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.3846e-11, dtype=torch.float64)
secont condition:: tensor(3.3846e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.2614e-11, dtype=torch.float64)
secont condition:: tensor(3.2614e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.4904e-11, dtype=torch.float64)
secont condition:: tensor(3.4904e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.5233e-11, dtype=torch.float64)
secont condition:: tensor(2.5233e-11, dtype=torch.float64)
curr_secont condition:: tensor(-4.4829e-12, dtype=torch.float64)
curr_secont condition:: tensor(-1.4295e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.4438e-11, dtype=torch.float64)
secont condition:: tensor(5.4438e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.1793e-08, dtype=torch.float64)
explicit_evaluation epoch:: 41
curr_secont condition:: tensor(2.4942e-10, dtype=torch.float64)
explicit_evaluation epoch:: 42
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(5.0896e-10, dtype=torch.float64)
explicit_evaluation epoch:: 46
curr_secont condition:: tensor(1.3710e-09, dtype=torch.float64)
secont condition:: tensor(1.3710e-09, dtype=torch.float64)
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(1.1310e-09, dtype=torch.float64)
secont condition:: tensor(1.1310e-09, dtype=torch.float64)
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(8.7557e-10, dtype=torch.float64)
secont condition:: tensor(8.7557e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.4244e-09, dtype=torch.float64)
secont condition:: tensor(1.4244e-09, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(9.5508e-10, dtype=torch.float64)
secont condition:: tensor(9.5508e-10, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(9.7413e-10, dtype=torch.float64)
secont condition:: tensor(9.7413e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.3230e-09, dtype=torch.float64)
secont condition:: tensor(1.3230e-09, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 26.24738359451294
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.0387e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(5.9214e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(8.0042e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.1939e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.2466e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000023, Accuracy: 0.723300
repetition 5
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 46.7662570476532
time_baseline:: 46.78416895866394
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(8.7206e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(8.2691e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000023, Accuracy: 0.724200
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(7.0064e-34, dtype=torch.float64)
secont condition:: tensor(7.0064e-34, dtype=torch.float64)
curr_secont condition:: tensor(1.7734e-33, dtype=torch.float64)
secont condition:: tensor(1.7734e-33, dtype=torch.float64)
curr_secont condition:: tensor(3.2329e-33, dtype=torch.float64)
secont condition:: tensor(3.2329e-33, dtype=torch.float64)
curr_secont condition:: tensor(4.9895e-33, dtype=torch.float64)
secont condition:: tensor(4.9895e-33, dtype=torch.float64)
curr_secont condition:: tensor(7.2580e-33, dtype=torch.float64)
secont condition:: tensor(7.2580e-33, dtype=torch.float64)
curr_secont condition:: tensor(9.7363e-33, dtype=torch.float64)
secont condition:: tensor(9.7363e-33, dtype=torch.float64)
curr_secont condition:: tensor(1.2675e-32, dtype=torch.float64)
secont condition:: tensor(1.2675e-32, dtype=torch.float64)
curr_secont condition:: tensor(1.6074e-32, dtype=torch.float64)
secont condition:: tensor(1.6074e-32, dtype=torch.float64)
curr_secont condition:: tensor(1.9619e-32, dtype=torch.float64)
secont condition:: tensor(1.9619e-32, dtype=torch.float64)
curr_secont condition:: tensor(2.3754e-32, dtype=torch.float64)
secont condition:: tensor(2.3754e-32, dtype=torch.float64)
curr_secont condition:: tensor(2.8233e-32, dtype=torch.float64)
secont condition:: tensor(2.8233e-32, dtype=torch.float64)
curr_secont condition:: tensor(3.2594e-32, dtype=torch.float64)
secont condition:: tensor(3.2594e-32, dtype=torch.float64)
curr_secont condition:: tensor(3.7808e-32, dtype=torch.float64)
secont condition:: tensor(3.7808e-32, dtype=torch.float64)
curr_secont condition:: tensor(9.5705e-12, dtype=torch.float64)
secont condition:: tensor(9.5705e-12, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(7.1644e-12, dtype=torch.float64)
secont condition:: tensor(7.1644e-12, dtype=torch.float64)
curr_secont condition:: tensor(5.7417e-12, dtype=torch.float64)
secont condition:: tensor(5.7417e-12, dtype=torch.float64)
curr_secont condition:: tensor(3.5369e-12, dtype=torch.float64)
secont condition:: tensor(3.5369e-12, dtype=torch.float64)
curr_secont condition:: tensor(3.2550e-12, dtype=torch.float64)
secont condition:: tensor(3.2550e-12, dtype=torch.float64)
curr_secont condition:: tensor(8.8915e-12, dtype=torch.float64)
secont condition:: tensor(8.8915e-12, dtype=torch.float64)
curr_secont condition:: tensor(7.4972e-12, dtype=torch.float64)
secont condition:: tensor(7.4972e-12, dtype=torch.float64)
curr_secont condition:: tensor(6.6534e-12, dtype=torch.float64)
secont condition:: tensor(6.6534e-12, dtype=torch.float64)
curr_secont condition:: tensor(5.6176e-12, dtype=torch.float64)
secont condition:: tensor(5.6176e-12, dtype=torch.float64)
curr_secont condition:: tensor(4.7232e-12, dtype=torch.float64)
secont condition:: tensor(4.7232e-12, dtype=torch.float64)
curr_secont condition:: tensor(4.1572e-12, dtype=torch.float64)
secont condition:: tensor(4.1572e-12, dtype=torch.float64)
curr_secont condition:: tensor(3.0232e-12, dtype=torch.float64)
secont condition:: tensor(3.0232e-12, dtype=torch.float64)
curr_secont condition:: tensor(2.4092e-12, dtype=torch.float64)
secont condition:: tensor(2.4092e-12, dtype=torch.float64)
curr_secont condition:: tensor(2.5102e-12, dtype=torch.float64)
secont condition:: tensor(2.5102e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.9230e-12, dtype=torch.float64)
secont condition:: tensor(1.9230e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.4969e-12, dtype=torch.float64)
secont condition:: tensor(1.4969e-12, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(7.4477e-13, dtype=torch.float64)
secont condition:: tensor(7.4477e-13, dtype=torch.float64)
curr_secont condition:: tensor(1.8366e-12, dtype=torch.float64)
secont condition:: tensor(1.8366e-12, dtype=torch.float64)
curr_secont condition:: tensor(2.5004e-11, dtype=torch.float64)
secont condition:: tensor(2.5004e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.9094e-11, dtype=torch.float64)
secont condition:: tensor(1.9094e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.5732e-11, dtype=torch.float64)
secont condition:: tensor(1.5732e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.5450e-11, dtype=torch.float64)
secont condition:: tensor(1.5450e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.0728e-11, dtype=torch.float64)
secont condition:: tensor(1.0728e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.1382e-11, dtype=torch.float64)
secont condition:: tensor(1.1382e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.1819e-12, dtype=torch.float64)
secont condition:: tensor(5.1819e-12, dtype=torch.float64)
curr_secont condition:: tensor(6.1511e-12, dtype=torch.float64)
secont condition:: tensor(6.1511e-12, dtype=torch.float64)
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(1.4174e-11, dtype=torch.float64)
secont condition:: tensor(1.4174e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.7187e-10, dtype=torch.float64)
explicit_evaluation epoch:: 50
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(3.1023e-11, dtype=torch.float64)
secont condition:: tensor(3.1023e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.3543e-11, dtype=torch.float64)
explicit_evaluation epoch:: 61
curr_secont condition:: tensor(3.2095e-11, dtype=torch.float64)
secont condition:: tensor(3.2095e-11, dtype=torch.float64)
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(9.5836e-11, dtype=torch.float64)
secont condition:: tensor(9.5836e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.1393e-10, dtype=torch.float64)
explicit_evaluation epoch:: 89
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(1.6326e-10, dtype=torch.float64)
secont condition:: tensor(1.6326e-10, dtype=torch.float64)
curr_secont condition:: tensor(6.4918e-11, dtype=torch.float64)
explicit_evaluation epoch:: 102
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(5.2016e-10, dtype=torch.float64)
secont condition:: tensor(5.2016e-10, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.765761137008667
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(6.3165e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(3.4011e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(5.1273e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(7.4939e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.3218e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000023, Accuracy: 0.715600
l2 norm:: 0.0005
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type MSELoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
cuda:2
Train - Epoch 0, Batch: 0, Loss: 0.091544
Train - Epoch 0, Batch: 10, Loss: 0.090312
Test Avg. Loss: 0.000027, Accuracy: 0.327700
Train - Epoch 1, Batch: 0, Loss: 0.089744
Train - Epoch 1, Batch: 10, Loss: 0.089173
Test Avg. Loss: 0.000026, Accuracy: 0.577300
Train - Epoch 2, Batch: 0, Loss: 0.088573
Train - Epoch 2, Batch: 10, Loss: 0.088039
Test Avg. Loss: 0.000026, Accuracy: 0.641400
Train - Epoch 3, Batch: 0, Loss: 0.087221
Train - Epoch 3, Batch: 10, Loss: 0.086671
Test Avg. Loss: 0.000025, Accuracy: 0.660900
Train - Epoch 4, Batch: 0, Loss: 0.085723
Train - Epoch 4, Batch: 10, Loss: 0.084589
Test Avg. Loss: 0.000025, Accuracy: 0.671900
Train - Epoch 5, Batch: 0, Loss: 0.084359
Train - Epoch 5, Batch: 10, Loss: 0.083450
Test Avg. Loss: 0.000024, Accuracy: 0.689600
Train - Epoch 6, Batch: 0, Loss: 0.082982
Train - Epoch 6, Batch: 10, Loss: 0.081987
Test Avg. Loss: 0.000024, Accuracy: 0.706000
Train - Epoch 7, Batch: 0, Loss: 0.082232
Train - Epoch 7, Batch: 10, Loss: 0.081527
Test Avg. Loss: 0.000023, Accuracy: 0.721000
training_time:: 59.67459416389465
repetition 1
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 46.98927426338196
time_baseline:: 47.00712275505066
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.1910e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(3.4963e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000023, Accuracy: 0.721000
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(1.5260e-33, dtype=torch.float64)
secont condition:: tensor(1.5260e-33, dtype=torch.float64)
curr_secont condition:: tensor(3.7247e-33, dtype=torch.float64)
secont condition:: tensor(3.7247e-33, dtype=torch.float64)
curr_secont condition:: tensor(6.6656e-33, dtype=torch.float64)
secont condition:: tensor(6.6656e-33, dtype=torch.float64)
curr_secont condition:: tensor(1.0212e-32, dtype=torch.float64)
secont condition:: tensor(1.0212e-32, dtype=torch.float64)
curr_secont condition:: tensor(1.4555e-32, dtype=torch.float64)
secont condition:: tensor(1.4555e-32, dtype=torch.float64)
curr_secont condition:: tensor(1.9693e-32, dtype=torch.float64)
secont condition:: tensor(1.9693e-32, dtype=torch.float64)
curr_secont condition:: tensor(2.5208e-32, dtype=torch.float64)
secont condition:: tensor(2.5208e-32, dtype=torch.float64)
curr_secont condition:: tensor(4.1264e-10, dtype=torch.float64)
secont condition:: tensor(4.1264e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.6420e-10, dtype=torch.float64)
secont condition:: tensor(2.6420e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.1160e-10, dtype=torch.float64)
secont condition:: tensor(2.1160e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.9108e-10, dtype=torch.float64)
secont condition:: tensor(1.9108e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.4607e-10, dtype=torch.float64)
secont condition:: tensor(1.4607e-10, dtype=torch.float64)
curr_secont condition:: tensor(8.4677e-11, dtype=torch.float64)
secont condition:: tensor(8.4677e-11, dtype=torch.float64)
curr_secont condition:: tensor(9.8507e-11, dtype=torch.float64)
secont condition:: tensor(9.8507e-11, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(3.6312e-11, dtype=torch.float64)
secont condition:: tensor(3.6312e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.2953e-10, dtype=torch.float64)
secont condition:: tensor(3.2953e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.8319e-10, dtype=torch.float64)
secont condition:: tensor(2.8319e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.7541e-10, dtype=torch.float64)
secont condition:: tensor(1.7541e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.7713e-10, dtype=torch.float64)
secont condition:: tensor(1.7713e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.2675e-10, dtype=torch.float64)
secont condition:: tensor(1.2675e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.0058e-10, dtype=torch.float64)
secont condition:: tensor(1.0058e-10, dtype=torch.float64)
curr_secont condition:: tensor(8.4234e-11, dtype=torch.float64)
secont condition:: tensor(8.4234e-11, dtype=torch.float64)
curr_secont condition:: tensor(7.6746e-11, dtype=torch.float64)
secont condition:: tensor(7.6746e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.8571e-11, dtype=torch.float64)
secont condition:: tensor(5.8571e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.5628e-11, dtype=torch.float64)
secont condition:: tensor(4.5628e-11, dtype=torch.float64)
curr_secont condition:: tensor(8.2978e-11, dtype=torch.float64)
secont condition:: tensor(8.2978e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.3780e-11, dtype=torch.float64)
secont condition:: tensor(5.3780e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.9400e-11, dtype=torch.float64)
secont condition:: tensor(3.9400e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.2273e-11, dtype=torch.float64)
secont condition:: tensor(3.2273e-11, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(4.8561e-11, dtype=torch.float64)
secont condition:: tensor(4.8561e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.5970e-11, dtype=torch.float64)
secont condition:: tensor(2.5970e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.7397e-11, dtype=torch.float64)
secont condition:: tensor(3.7397e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.0439e-11, dtype=torch.float64)
secont condition:: tensor(3.0439e-11, dtype=torch.float64)
curr_secont condition:: tensor(8.2174e-11, dtype=torch.float64)
secont condition:: tensor(8.2174e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.9495e-11, dtype=torch.float64)
secont condition:: tensor(2.9495e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.1990e-10, dtype=torch.float64)
secont condition:: tensor(2.1990e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.5885e-10, dtype=torch.float64)
secont condition:: tensor(2.5885e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.7829e-10, dtype=torch.float64)
secont condition:: tensor(1.7829e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.1070e-10, dtype=torch.float64)
secont condition:: tensor(1.1070e-10, dtype=torch.float64)
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(1.1770e-10, dtype=torch.float64)
secont condition:: tensor(1.1770e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.3120e-09, dtype=torch.float64)
explicit_evaluation epoch:: 53
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(6.6179e-10, dtype=torch.float64)
secont condition:: tensor(6.6179e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.0739e-09, dtype=torch.float64)
secont condition:: tensor(1.0739e-09, dtype=torch.float64)
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(1.2504e-09, dtype=torch.float64)
secont condition:: tensor(1.2504e-09, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(9.5444e-10, dtype=torch.float64)
secont condition:: tensor(9.5444e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.0084e-09, dtype=torch.float64)
secont condition:: tensor(1.0084e-09, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(7.3977e-10, dtype=torch.float64)
secont condition:: tensor(7.3977e-10, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 24.95979619026184
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.1280e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(5.0206e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(6.9949e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(9.6237e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(5.7379e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000023, Accuracy: 0.715200
repetition 2
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 46.956334829330444
time_baseline:: 46.9753098487854
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.0470e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(3.0136e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000023, Accuracy: 0.721000
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(1.5260e-33, dtype=torch.float64)
secont condition:: tensor(1.5260e-33, dtype=torch.float64)
curr_secont condition:: tensor(3.7247e-33, dtype=torch.float64)
secont condition:: tensor(3.7247e-33, dtype=torch.float64)
curr_secont condition:: tensor(6.6540e-33, dtype=torch.float64)
secont condition:: tensor(6.6540e-33, dtype=torch.float64)
curr_secont condition:: tensor(9.4548e-12, dtype=torch.float64)
secont condition:: tensor(9.4548e-12, dtype=torch.float64)
curr_secont condition:: tensor(6.7291e-12, dtype=torch.float64)
secont condition:: tensor(6.7291e-12, dtype=torch.float64)
curr_secont condition:: tensor(7.3943e-13, dtype=torch.float64)
secont condition:: tensor(7.3943e-13, dtype=torch.float64)
curr_secont condition:: tensor(4.8906e-12, dtype=torch.float64)
secont condition:: tensor(4.8906e-12, dtype=torch.float64)
curr_secont condition:: tensor(-9.6640e-12, dtype=torch.float64)
curr_secont condition:: tensor(2.7410e-12, dtype=torch.float64)
secont condition:: tensor(2.7410e-12, dtype=torch.float64)
curr_secont condition:: tensor(-1.4421e-12, dtype=torch.float64)
curr_secont condition:: tensor(2.1651e-12, dtype=torch.float64)
secont condition:: tensor(2.1651e-12, dtype=torch.float64)
curr_secont condition:: tensor(3.0623e-11, dtype=torch.float64)
secont condition:: tensor(3.0623e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.3301e-12, dtype=torch.float64)
secont condition:: tensor(1.3301e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.4772e-12, dtype=torch.float64)
secont condition:: tensor(1.4772e-12, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(7.7016e-13, dtype=torch.float64)
secont condition:: tensor(7.7016e-13, dtype=torch.float64)
curr_secont condition:: tensor(1.3015e-12, dtype=torch.float64)
secont condition:: tensor(1.3015e-12, dtype=torch.float64)
curr_secont condition:: tensor(3.2922e-11, dtype=torch.float64)
secont condition:: tensor(3.2922e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.7026e-11, dtype=torch.float64)
secont condition:: tensor(2.7026e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.3315e-11, dtype=torch.float64)
secont condition:: tensor(2.3315e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.0268e-11, dtype=torch.float64)
secont condition:: tensor(2.0268e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.7021e-11, dtype=torch.float64)
secont condition:: tensor(1.7021e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.7301e-11, dtype=torch.float64)
secont condition:: tensor(1.7301e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.4009e-11, dtype=torch.float64)
secont condition:: tensor(1.4009e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.1672e-11, dtype=torch.float64)
secont condition:: tensor(1.1672e-11, dtype=torch.float64)
curr_secont condition:: tensor(9.7311e-12, dtype=torch.float64)
secont condition:: tensor(9.7311e-12, dtype=torch.float64)
curr_secont condition:: tensor(8.9114e-12, dtype=torch.float64)
secont condition:: tensor(8.9114e-12, dtype=torch.float64)
curr_secont condition:: tensor(5.3061e-12, dtype=torch.float64)
secont condition:: tensor(5.3061e-12, dtype=torch.float64)
curr_secont condition:: tensor(5.9863e-12, dtype=torch.float64)
secont condition:: tensor(5.9863e-12, dtype=torch.float64)
curr_secont condition:: tensor(5.5234e-12, dtype=torch.float64)
secont condition:: tensor(5.5234e-12, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(7.8880e-12, dtype=torch.float64)
secont condition:: tensor(7.8880e-12, dtype=torch.float64)
curr_secont condition:: tensor(4.8424e-12, dtype=torch.float64)
secont condition:: tensor(4.8424e-12, dtype=torch.float64)
curr_secont condition:: tensor(-8.0638e-12, dtype=torch.float64)
curr_secont condition:: tensor(8.7294e-12, dtype=torch.float64)
secont condition:: tensor(8.7294e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.1597e-11, dtype=torch.float64)
secont condition:: tensor(1.1597e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.0838e-11, dtype=torch.float64)
secont condition:: tensor(1.0838e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.5205e-13, dtype=torch.float64)
secont condition:: tensor(2.5205e-13, dtype=torch.float64)
curr_secont condition:: tensor(1.9413e-11, dtype=torch.float64)
secont condition:: tensor(1.9413e-11, dtype=torch.float64)
curr_secont condition:: tensor(-9.0379e-12, dtype=torch.float64)
curr_secont condition:: tensor(5.5302e-12, dtype=torch.float64)
secont condition:: tensor(5.5302e-12, dtype=torch.float64)
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(3.7273e-11, dtype=torch.float64)
explicit_evaluation epoch:: 45
curr_secont condition:: tensor(5.7308e-10, dtype=torch.float64)
explicit_evaluation epoch:: 46
curr_secont condition:: tensor(3.0153e-10, dtype=torch.float64)
secont condition:: tensor(3.0153e-10, dtype=torch.float64)
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(3.2967e-10, dtype=torch.float64)
secont condition:: tensor(3.2967e-10, dtype=torch.float64)
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(3.9709e-10, dtype=torch.float64)
secont condition:: tensor(3.9709e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.1596e-10, dtype=torch.float64)
secont condition:: tensor(3.1596e-10, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(3.4281e-10, dtype=torch.float64)
secont condition:: tensor(3.4281e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.4261e-09, dtype=torch.float64)
explicit_evaluation epoch:: 103
curr_secont condition:: tensor(1.8084e-09, dtype=torch.float64)
explicit_evaluation epoch:: 104
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(5.7899e-10, dtype=torch.float64)
secont condition:: tensor(5.7899e-10, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.349935293197632
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(7.1038e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(2.7148e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(3.7679e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(8.5286e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(6.3720e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000023, Accuracy: 0.723000
repetition 3
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 47.68703103065491
time_baseline:: 47.704927921295166
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.5822e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(2.5326e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000023, Accuracy: 0.721000
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(1.5119e-33, dtype=torch.float64)
secont condition:: tensor(1.5119e-33, dtype=torch.float64)
curr_secont condition:: tensor(7.1984e-11, dtype=torch.float64)
secont condition:: tensor(7.1984e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.9479e-11, dtype=torch.float64)
secont condition:: tensor(5.9479e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.4847e-11, dtype=torch.float64)
secont condition:: tensor(3.4847e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.8840e-11, dtype=torch.float64)
secont condition:: tensor(2.8840e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.4402e-11, dtype=torch.float64)
secont condition:: tensor(2.4402e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.6935e-11, dtype=torch.float64)
secont condition:: tensor(1.6935e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.5280e-12, dtype=torch.float64)
secont condition:: tensor(1.5280e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.2483e-11, dtype=torch.float64)
secont condition:: tensor(1.2483e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.9148e-12, dtype=torch.float64)
secont condition:: tensor(5.9148e-12, dtype=torch.float64)
curr_secont condition:: tensor(7.4064e-12, dtype=torch.float64)
secont condition:: tensor(7.4064e-12, dtype=torch.float64)
curr_secont condition:: tensor(7.7735e-12, dtype=torch.float64)
secont condition:: tensor(7.7735e-12, dtype=torch.float64)
curr_secont condition:: tensor(6.2244e-12, dtype=torch.float64)
secont condition:: tensor(6.2244e-12, dtype=torch.float64)
curr_secont condition:: tensor(-4.2215e-13, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(4.0574e-12, dtype=torch.float64)
secont condition:: tensor(4.0574e-12, dtype=torch.float64)
curr_secont condition:: tensor(4.7412e-12, dtype=torch.float64)
secont condition:: tensor(4.7412e-12, dtype=torch.float64)
curr_secont condition:: tensor(5.3838e-12, dtype=torch.float64)
secont condition:: tensor(5.3838e-12, dtype=torch.float64)
curr_secont condition:: tensor(5.4221e-12, dtype=torch.float64)
secont condition:: tensor(5.4221e-12, dtype=torch.float64)
curr_secont condition:: tensor(5.4675e-12, dtype=torch.float64)
secont condition:: tensor(5.4675e-12, dtype=torch.float64)
curr_secont condition:: tensor(5.7699e-12, dtype=torch.float64)
secont condition:: tensor(5.7699e-12, dtype=torch.float64)
curr_secont condition:: tensor(2.9830e-12, dtype=torch.float64)
secont condition:: tensor(2.9830e-12, dtype=torch.float64)
curr_secont condition:: tensor(4.4993e-12, dtype=torch.float64)
secont condition:: tensor(4.4993e-12, dtype=torch.float64)
curr_secont condition:: tensor(2.6031e-12, dtype=torch.float64)
secont condition:: tensor(2.6031e-12, dtype=torch.float64)
curr_secont condition:: tensor(4.0421e-12, dtype=torch.float64)
secont condition:: tensor(4.0421e-12, dtype=torch.float64)
curr_secont condition:: tensor(3.6375e-12, dtype=torch.float64)
secont condition:: tensor(3.6375e-12, dtype=torch.float64)
curr_secont condition:: tensor(4.5638e-12, dtype=torch.float64)
secont condition:: tensor(4.5638e-12, dtype=torch.float64)
curr_secont condition:: tensor(2.1325e-12, dtype=torch.float64)
secont condition:: tensor(2.1325e-12, dtype=torch.float64)
curr_secont condition:: tensor(4.4372e-11, dtype=torch.float64)
secont condition:: tensor(4.4372e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.2663e-11, dtype=torch.float64)
secont condition:: tensor(3.2663e-11, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(2.5158e-11, dtype=torch.float64)
secont condition:: tensor(2.5158e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.9681e-11, dtype=torch.float64)
secont condition:: tensor(1.9681e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.8681e-11, dtype=torch.float64)
secont condition:: tensor(1.8681e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.6656e-11, dtype=torch.float64)
secont condition:: tensor(1.6656e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.8987e-11, dtype=torch.float64)
secont condition:: tensor(1.8987e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.1679e-11, dtype=torch.float64)
secont condition:: tensor(1.1679e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.4062e-11, dtype=torch.float64)
secont condition:: tensor(1.4062e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.0331e-11, dtype=torch.float64)
secont condition:: tensor(3.0331e-11, dtype=torch.float64)
curr_secont condition:: tensor(7.5856e-12, dtype=torch.float64)
secont condition:: tensor(7.5856e-12, dtype=torch.float64)
curr_secont condition:: tensor(6.7245e-10, dtype=torch.float64)
secont condition:: tensor(6.7245e-10, dtype=torch.float64)
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(5.4750e-10, dtype=torch.float64)
explicit_evaluation epoch:: 45
curr_secont condition:: tensor(3.5574e-10, dtype=torch.float64)
secont condition:: tensor(3.5574e-10, dtype=torch.float64)
curr_secont condition:: tensor(9.5315e-10, dtype=torch.float64)
explicit_evaluation epoch:: 59
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(5.3257e-10, dtype=torch.float64)
secont condition:: tensor(5.3257e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.9386e-08, dtype=torch.float64)
explicit_evaluation epoch:: 74
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(8.3636e-09, dtype=torch.float64)
explicit_evaluation epoch:: 75
curr_secont condition:: tensor(2.1141e-09, dtype=torch.float64)
secont condition:: tensor(2.1141e-09, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(1.5828e-09, dtype=torch.float64)
secont condition:: tensor(1.5828e-09, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(2.7504e-09, dtype=torch.float64)
secont condition:: tensor(2.7504e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.7854e-09, dtype=torch.float64)
secont condition:: tensor(2.7854e-09, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.17236065864563
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.2364e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(5.7362e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(8.5463e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.4838e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(8.3543e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000023, Accuracy: 0.718700
repetition 4
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 48.441715240478516
time_baseline:: 48.46154999732971
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.1454e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(9.2630e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000023, Accuracy: 0.721000
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(1.5260e-33, dtype=torch.float64)
secont condition:: tensor(1.5260e-33, dtype=torch.float64)
curr_secont condition:: tensor(3.7247e-33, dtype=torch.float64)
secont condition:: tensor(3.7247e-33, dtype=torch.float64)
curr_secont condition:: tensor(6.6656e-33, dtype=torch.float64)
secont condition:: tensor(6.6656e-33, dtype=torch.float64)
curr_secont condition:: tensor(1.0179e-32, dtype=torch.float64)
secont condition:: tensor(1.0179e-32, dtype=torch.float64)
curr_secont condition:: tensor(2.0229e-10, dtype=torch.float64)
secont condition:: tensor(2.0229e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.2465e-10, dtype=torch.float64)
secont condition:: tensor(1.2465e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.0557e-10, dtype=torch.float64)
secont condition:: tensor(1.0557e-10, dtype=torch.float64)
curr_secont condition:: tensor(8.0864e-11, dtype=torch.float64)
secont condition:: tensor(8.0864e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.6331e-11, dtype=torch.float64)
secont condition:: tensor(4.6331e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.7527e-11, dtype=torch.float64)
secont condition:: tensor(3.7527e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.7116e-11, dtype=torch.float64)
secont condition:: tensor(3.7116e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.1981e-11, dtype=torch.float64)
secont condition:: tensor(3.1981e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.3098e-11, dtype=torch.float64)
secont condition:: tensor(2.3098e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.8400e-11, dtype=torch.float64)
secont condition:: tensor(1.8400e-11, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(1.5989e-11, dtype=torch.float64)
secont condition:: tensor(1.5989e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.1143e-11, dtype=torch.float64)
secont condition:: tensor(2.1143e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.1928e-11, dtype=torch.float64)
secont condition:: tensor(1.1928e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.6627e-11, dtype=torch.float64)
secont condition:: tensor(1.6627e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.5069e-11, dtype=torch.float64)
secont condition:: tensor(1.5069e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.1098e-11, dtype=torch.float64)
secont condition:: tensor(1.1098e-11, dtype=torch.float64)
curr_secont condition:: tensor(8.1308e-12, dtype=torch.float64)
secont condition:: tensor(8.1308e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.3053e-11, dtype=torch.float64)
secont condition:: tensor(1.3053e-11, dtype=torch.float64)
curr_secont condition:: tensor(9.7729e-12, dtype=torch.float64)
secont condition:: tensor(9.7729e-12, dtype=torch.float64)
curr_secont condition:: tensor(2.8495e-12, dtype=torch.float64)
secont condition:: tensor(2.8495e-12, dtype=torch.float64)
curr_secont condition:: tensor(9.5325e-12, dtype=torch.float64)
secont condition:: tensor(9.5325e-12, dtype=torch.float64)
curr_secont condition:: tensor(8.1150e-12, dtype=torch.float64)
secont condition:: tensor(8.1150e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.6180e-10, dtype=torch.float64)
secont condition:: tensor(1.6180e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.3039e-10, dtype=torch.float64)
secont condition:: tensor(1.3039e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.1384e-10, dtype=torch.float64)
secont condition:: tensor(1.1384e-10, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(1.2479e-10, dtype=torch.float64)
secont condition:: tensor(1.2479e-10, dtype=torch.float64)
curr_secont condition:: tensor(7.9353e-11, dtype=torch.float64)
secont condition:: tensor(7.9353e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.1912e-11, dtype=torch.float64)
secont condition:: tensor(6.1912e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.6243e-11, dtype=torch.float64)
secont condition:: tensor(5.6243e-11, dtype=torch.float64)
curr_secont condition:: tensor(7.7047e-11, dtype=torch.float64)
secont condition:: tensor(7.7047e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.6419e-10, dtype=torch.float64)
secont condition:: tensor(2.6419e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.5075e-10, dtype=torch.float64)
secont condition:: tensor(2.5075e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.9581e-10, dtype=torch.float64)
secont condition:: tensor(1.9581e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.3711e-10, dtype=torch.float64)
secont condition:: tensor(1.3711e-10, dtype=torch.float64)
curr_secont condition:: tensor(7.6638e-11, dtype=torch.float64)
secont condition:: tensor(7.6638e-11, dtype=torch.float64)
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(1.1232e-10, dtype=torch.float64)
secont condition:: tensor(1.1232e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.8349e-10, dtype=torch.float64)
explicit_evaluation epoch:: 51
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(2.1500e-10, dtype=torch.float64)
secont condition:: tensor(2.1500e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.3322e-10, dtype=torch.float64)
explicit_evaluation epoch:: 63
curr_secont condition:: tensor(7.4680e-10, dtype=torch.float64)
secont condition:: tensor(7.4680e-10, dtype=torch.float64)
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(8.2670e-10, dtype=torch.float64)
secont condition:: tensor(8.2670e-10, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(4.0056e-10, dtype=torch.float64)
secont condition:: tensor(4.0056e-10, dtype=torch.float64)
curr_secont condition:: tensor(7.5943e-10, dtype=torch.float64)
secont condition:: tensor(7.5943e-10, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(9.7222e-10, dtype=torch.float64)
secont condition:: tensor(9.7222e-10, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.17786741256714
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(9.8630e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(5.5911e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(8.8178e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.0459e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.8011e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000023, Accuracy: 0.715200
repetition 5
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 47.41759991645813
time_baseline:: 47.43716788291931
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.3378e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(7.5180e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000023, Accuracy: 0.721000
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(1.5260e-33, dtype=torch.float64)
secont condition:: tensor(1.5260e-33, dtype=torch.float64)
curr_secont condition:: tensor(3.7247e-33, dtype=torch.float64)
secont condition:: tensor(3.7247e-33, dtype=torch.float64)
curr_secont condition:: tensor(6.6656e-33, dtype=torch.float64)
secont condition:: tensor(6.6656e-33, dtype=torch.float64)
curr_secont condition:: tensor(1.0212e-32, dtype=torch.float64)
secont condition:: tensor(1.0212e-32, dtype=torch.float64)
curr_secont condition:: tensor(1.4555e-32, dtype=torch.float64)
secont condition:: tensor(1.4555e-32, dtype=torch.float64)
curr_secont condition:: tensor(1.9693e-32, dtype=torch.float64)
secont condition:: tensor(1.9693e-32, dtype=torch.float64)
curr_secont condition:: tensor(2.5272e-32, dtype=torch.float64)
secont condition:: tensor(2.5272e-32, dtype=torch.float64)
curr_secont condition:: tensor(3.1802e-32, dtype=torch.float64)
secont condition:: tensor(3.1802e-32, dtype=torch.float64)
curr_secont condition:: tensor(3.8816e-32, dtype=torch.float64)
secont condition:: tensor(3.8816e-32, dtype=torch.float64)
curr_secont condition:: tensor(2.0323e-11, dtype=torch.float64)
secont condition:: tensor(2.0323e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.4117e-11, dtype=torch.float64)
secont condition:: tensor(1.4117e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.6996e-11, dtype=torch.float64)
secont condition:: tensor(1.6996e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.0934e-11, dtype=torch.float64)
secont condition:: tensor(1.0934e-11, dtype=torch.float64)
curr_secont condition:: tensor(7.2039e-12, dtype=torch.float64)
secont condition:: tensor(7.2039e-12, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(7.6187e-12, dtype=torch.float64)
secont condition:: tensor(7.6187e-12, dtype=torch.float64)
curr_secont condition:: tensor(5.8787e-12, dtype=torch.float64)
secont condition:: tensor(5.8787e-12, dtype=torch.float64)
curr_secont condition:: tensor(3.0589e-12, dtype=torch.float64)
secont condition:: tensor(3.0589e-12, dtype=torch.float64)
curr_secont condition:: tensor(3.3472e-12, dtype=torch.float64)
secont condition:: tensor(3.3472e-12, dtype=torch.float64)
curr_secont condition:: tensor(3.6208e-12, dtype=torch.float64)
secont condition:: tensor(3.6208e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.6979e-12, dtype=torch.float64)
secont condition:: tensor(1.6979e-12, dtype=torch.float64)
curr_secont condition:: tensor(2.3413e-12, dtype=torch.float64)
secont condition:: tensor(2.3413e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.4971e-12, dtype=torch.float64)
secont condition:: tensor(1.4971e-12, dtype=torch.float64)
curr_secont condition:: tensor(2.2350e-12, dtype=torch.float64)
secont condition:: tensor(2.2350e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.7958e-12, dtype=torch.float64)
secont condition:: tensor(1.7958e-12, dtype=torch.float64)
curr_secont condition:: tensor(4.1876e-10, dtype=torch.float64)
secont condition:: tensor(4.1876e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.9301e-10, dtype=torch.float64)
secont condition:: tensor(3.9301e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.2904e-10, dtype=torch.float64)
secont condition:: tensor(3.2904e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.4151e-10, dtype=torch.float64)
secont condition:: tensor(2.4151e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.1036e-10, dtype=torch.float64)
secont condition:: tensor(2.1036e-10, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(2.0999e-10, dtype=torch.float64)
secont condition:: tensor(2.0999e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.7017e-10, dtype=torch.float64)
secont condition:: tensor(1.7017e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.4474e-10, dtype=torch.float64)
secont condition:: tensor(1.4474e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.0980e-10, dtype=torch.float64)
secont condition:: tensor(1.0980e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.1587e-10, dtype=torch.float64)
secont condition:: tensor(1.1587e-10, dtype=torch.float64)
curr_secont condition:: tensor(5.6976e-11, dtype=torch.float64)
secont condition:: tensor(5.6976e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.8781e-11, dtype=torch.float64)
secont condition:: tensor(5.8781e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.5388e-11, dtype=torch.float64)
secont condition:: tensor(6.5388e-11, dtype=torch.float64)
curr_secont condition:: tensor(7.5013e-11, dtype=torch.float64)
secont condition:: tensor(7.5013e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.1008e-11, dtype=torch.float64)
secont condition:: tensor(3.1008e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.0739e-10, dtype=torch.float64)
explicit_evaluation epoch:: 43
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(3.4381e-10, dtype=torch.float64)
secont condition:: tensor(3.4381e-10, dtype=torch.float64)
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(2.4025e-10, dtype=torch.float64)
secont condition:: tensor(2.4025e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.1199e-09, dtype=torch.float64)
secont condition:: tensor(1.1199e-09, dtype=torch.float64)
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(1.1679e-09, dtype=torch.float64)
secont condition:: tensor(1.1679e-09, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(1.1831e-09, dtype=torch.float64)
secont condition:: tensor(1.1831e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.3439e-09, dtype=torch.float64)
secont condition:: tensor(1.3439e-09, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(8.8003e-10, dtype=torch.float64)
secont condition:: tensor(8.8003e-10, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 24.634552001953125
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.0315e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(6.1521e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.0057e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.1490e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.7438e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000023, Accuracy: 0.715200
l2 norm:: 0.001
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type MSELoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
cuda:2
Train - Epoch 0, Batch: 0, Loss: 0.091544
Train - Epoch 0, Batch: 10, Loss: 0.090302
Test Avg. Loss: 0.000027, Accuracy: 0.326900
Train - Epoch 1, Batch: 0, Loss: 0.089735
Train - Epoch 1, Batch: 10, Loss: 0.089169
Test Avg. Loss: 0.000026, Accuracy: 0.576900
Train - Epoch 2, Batch: 0, Loss: 0.088583
Train - Epoch 2, Batch: 10, Loss: 0.088068
Test Avg. Loss: 0.000026, Accuracy: 0.641200
Train - Epoch 3, Batch: 0, Loss: 0.087285
Train - Epoch 3, Batch: 10, Loss: 0.086769
Test Avg. Loss: 0.000025, Accuracy: 0.660000
Train - Epoch 4, Batch: 0, Loss: 0.085862
Train - Epoch 4, Batch: 10, Loss: 0.084800
Test Avg. Loss: 0.000025, Accuracy: 0.668400
Train - Epoch 5, Batch: 0, Loss: 0.084589
Train - Epoch 5, Batch: 10, Loss: 0.083737
Test Avg. Loss: 0.000024, Accuracy: 0.685500
Train - Epoch 6, Batch: 0, Loss: 0.083290
Train - Epoch 6, Batch: 10, Loss: 0.082348
Test Avg. Loss: 0.000024, Accuracy: 0.701200
Train - Epoch 7, Batch: 0, Loss: 0.082587
Train - Epoch 7, Batch: 10, Loss: 0.081918
Test Avg. Loss: 0.000023, Accuracy: 0.715000
training_time:: 59.40687012672424
repetition 1
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 47.720235109329224
time_baseline:: 47.73848271369934
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.1856e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(5.9130e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000023, Accuracy: 0.715000
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(2.2650e-33, dtype=torch.float64)
secont condition:: tensor(2.2650e-33, dtype=torch.float64)
curr_secont condition:: tensor(4.8847e-33, dtype=torch.float64)
secont condition:: tensor(4.8847e-33, dtype=torch.float64)
curr_secont condition:: tensor(7.8650e-33, dtype=torch.float64)
secont condition:: tensor(7.8650e-33, dtype=torch.float64)
curr_secont condition:: tensor(1.1185e-32, dtype=torch.float64)
secont condition:: tensor(1.1185e-32, dtype=torch.float64)
curr_secont condition:: tensor(1.4789e-32, dtype=torch.float64)
secont condition:: tensor(1.4789e-32, dtype=torch.float64)
curr_secont condition:: tensor(1.8871e-32, dtype=torch.float64)
secont condition:: tensor(1.8871e-32, dtype=torch.float64)
curr_secont condition:: tensor(2.3274e-32, dtype=torch.float64)
secont condition:: tensor(2.3274e-32, dtype=torch.float64)
curr_secont condition:: tensor(1.8941e-10, dtype=torch.float64)
secont condition:: tensor(1.8941e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.7707e-10, dtype=torch.float64)
secont condition:: tensor(1.7707e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.3970e-10, dtype=torch.float64)
secont condition:: tensor(1.3970e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.0716e-10, dtype=torch.float64)
secont condition:: tensor(1.0716e-10, dtype=torch.float64)
curr_secont condition:: tensor(7.5883e-11, dtype=torch.float64)
secont condition:: tensor(7.5883e-11, dtype=torch.float64)
curr_secont condition:: tensor(-9.4407e-12, dtype=torch.float64)
curr_secont condition:: tensor(4.3912e-11, dtype=torch.float64)
secont condition:: tensor(4.3912e-11, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(3.7108e-11, dtype=torch.float64)
secont condition:: tensor(3.7108e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.0807e-11, dtype=torch.float64)
secont condition:: tensor(4.0807e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.9702e-11, dtype=torch.float64)
secont condition:: tensor(2.9702e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.6182e-11, dtype=torch.float64)
secont condition:: tensor(4.6182e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.6021e-11, dtype=torch.float64)
secont condition:: tensor(2.6021e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.7461e-12, dtype=torch.float64)
secont condition:: tensor(3.7461e-12, dtype=torch.float64)
curr_secont condition:: tensor(3.4779e-11, dtype=torch.float64)
secont condition:: tensor(3.4779e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.7052e-11, dtype=torch.float64)
secont condition:: tensor(2.7052e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.6101e-11, dtype=torch.float64)
secont condition:: tensor(1.6101e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.0521e-11, dtype=torch.float64)
secont condition:: tensor(3.0521e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.6385e-11, dtype=torch.float64)
secont condition:: tensor(2.6385e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.5346e-11, dtype=torch.float64)
secont condition:: tensor(2.5346e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.1519e-11, dtype=torch.float64)
secont condition:: tensor(2.1519e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.1924e-11, dtype=torch.float64)
secont condition:: tensor(6.1924e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.2685e-11, dtype=torch.float64)
secont condition:: tensor(1.2685e-11, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(3.2866e-11, dtype=torch.float64)
secont condition:: tensor(3.2866e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.1067e-11, dtype=torch.float64)
secont condition:: tensor(1.1067e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.8281e-11, dtype=torch.float64)
secont condition:: tensor(1.8281e-11, dtype=torch.float64)
curr_secont condition:: tensor(-1.7256e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.7874e-11, dtype=torch.float64)
secont condition:: tensor(1.7874e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.0586e-10, dtype=torch.float64)
secont condition:: tensor(1.0586e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.1292e-10, dtype=torch.float64)
secont condition:: tensor(1.1292e-10, dtype=torch.float64)
curr_secont condition:: tensor(9.5329e-11, dtype=torch.float64)
secont condition:: tensor(9.5329e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.7764e-11, dtype=torch.float64)
secont condition:: tensor(6.7764e-11, dtype=torch.float64)
curr_secont condition:: tensor(7.1684e-11, dtype=torch.float64)
secont condition:: tensor(7.1684e-11, dtype=torch.float64)
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(1.3154e-10, dtype=torch.float64)
secont condition:: tensor(1.3154e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.4374e-10, dtype=torch.float64)
secont condition:: tensor(1.4374e-10, dtype=torch.float64)
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(6.9561e-09, dtype=torch.float64)
explicit_evaluation epoch:: 67
curr_secont condition:: tensor(1.2090e-09, dtype=torch.float64)
explicit_evaluation epoch:: 68
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(7.5628e-10, dtype=torch.float64)
secont condition:: tensor(7.5628e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.3315e-09, dtype=torch.float64)
secont condition:: tensor(1.3315e-09, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(8.8934e-10, dtype=torch.float64)
secont condition:: tensor(8.8934e-10, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(1.1347e-09, dtype=torch.float64)
secont condition:: tensor(1.1347e-09, dtype=torch.float64)
curr_secont condition:: tensor(6.2280e-10, dtype=torch.float64)
secont condition:: tensor(6.2280e-10, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 24.438010215759277
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(9.2953e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(3.9693e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(7.3183e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.0792e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.2842e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.717500
repetition 2
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 46.985928535461426
time_baseline:: 47.00546598434448
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.0809e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.2330e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000023, Accuracy: 0.715000
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(2.2650e-33, dtype=torch.float64)
secont condition:: tensor(2.2650e-33, dtype=torch.float64)
curr_secont condition:: tensor(4.8847e-33, dtype=torch.float64)
secont condition:: tensor(4.8847e-33, dtype=torch.float64)
curr_secont condition:: tensor(7.8650e-33, dtype=torch.float64)
secont condition:: tensor(7.8650e-33, dtype=torch.float64)
curr_secont condition:: tensor(1.1185e-32, dtype=torch.float64)
secont condition:: tensor(1.1185e-32, dtype=torch.float64)
curr_secont condition:: tensor(1.4789e-32, dtype=torch.float64)
secont condition:: tensor(1.4789e-32, dtype=torch.float64)
curr_secont condition:: tensor(1.8871e-32, dtype=torch.float64)
secont condition:: tensor(1.8871e-32, dtype=torch.float64)
curr_secont condition:: tensor(2.3276e-32, dtype=torch.float64)
secont condition:: tensor(2.3276e-32, dtype=torch.float64)
curr_secont condition:: tensor(1.5172e-10, dtype=torch.float64)
secont condition:: tensor(1.5172e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.3785e-10, dtype=torch.float64)
secont condition:: tensor(1.3785e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.0498e-10, dtype=torch.float64)
secont condition:: tensor(1.0498e-10, dtype=torch.float64)
curr_secont condition:: tensor(8.6199e-11, dtype=torch.float64)
secont condition:: tensor(8.6199e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.3246e-10, dtype=torch.float64)
secont condition:: tensor(1.3246e-10, dtype=torch.float64)
curr_secont condition:: tensor(6.3101e-11, dtype=torch.float64)
secont condition:: tensor(6.3101e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.1623e-11, dtype=torch.float64)
secont condition:: tensor(6.1623e-11, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(4.2047e-11, dtype=torch.float64)
secont condition:: tensor(4.2047e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.0885e-11, dtype=torch.float64)
secont condition:: tensor(4.0885e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.2313e-11, dtype=torch.float64)
secont condition:: tensor(3.2313e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.8958e-11, dtype=torch.float64)
secont condition:: tensor(2.8958e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.2667e-11, dtype=torch.float64)
secont condition:: tensor(2.2667e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.0654e-11, dtype=torch.float64)
secont condition:: tensor(2.0654e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.7483e-11, dtype=torch.float64)
secont condition:: tensor(2.7483e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.7252e-11, dtype=torch.float64)
secont condition:: tensor(1.7252e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.5605e-11, dtype=torch.float64)
secont condition:: tensor(2.5605e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.1899e-10, dtype=torch.float64)
secont condition:: tensor(2.1899e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.8081e-10, dtype=torch.float64)
secont condition:: tensor(1.8081e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.5436e-10, dtype=torch.float64)
secont condition:: tensor(1.5436e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.3144e-10, dtype=torch.float64)
secont condition:: tensor(1.3144e-10, dtype=torch.float64)
curr_secont condition:: tensor(9.3118e-11, dtype=torch.float64)
secont condition:: tensor(9.3118e-11, dtype=torch.float64)
curr_secont condition:: tensor(8.7072e-11, dtype=torch.float64)
secont condition:: tensor(8.7072e-11, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(7.1526e-11, dtype=torch.float64)
secont condition:: tensor(7.1526e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.7181e-11, dtype=torch.float64)
secont condition:: tensor(5.7181e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.1388e-11, dtype=torch.float64)
secont condition:: tensor(4.1388e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.9445e-11, dtype=torch.float64)
secont condition:: tensor(5.9445e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.3267e-11, dtype=torch.float64)
secont condition:: tensor(5.3267e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.1567e-11, dtype=torch.float64)
secont condition:: tensor(4.1567e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.8239e-11, dtype=torch.float64)
secont condition:: tensor(5.8239e-11, dtype=torch.float64)
curr_secont condition:: tensor(8.8844e-13, dtype=torch.float64)
secont condition:: tensor(8.8844e-13, dtype=torch.float64)
curr_secont condition:: tensor(5.5164e-11, dtype=torch.float64)
secont condition:: tensor(5.5164e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.5239e-11, dtype=torch.float64)
secont condition:: tensor(4.5239e-11, dtype=torch.float64)
curr_secont condition:: tensor(-1.6277e-10, dtype=torch.float64)
explicit_evaluation epoch:: 41
curr_secont condition:: tensor(1.5441e-10, dtype=torch.float64)
secont condition:: tensor(1.5441e-10, dtype=torch.float64)
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(9.3175e-11, dtype=torch.float64)
secont condition:: tensor(9.3175e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.9297e-11, dtype=torch.float64)
explicit_evaluation epoch:: 57
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(1.4538e-10, dtype=torch.float64)
explicit_evaluation epoch:: 60
curr_secont condition:: tensor(3.3886e-10, dtype=torch.float64)
explicit_evaluation epoch:: 62
curr_secont condition:: tensor(2.3925e-10, dtype=torch.float64)
explicit_evaluation epoch:: 63
curr_secont condition:: tensor(1.7389e-10, dtype=torch.float64)
secont condition:: tensor(1.7389e-10, dtype=torch.float64)
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(2.9729e-10, dtype=torch.float64)
explicit_evaluation epoch:: 75
curr_secont condition:: tensor(4.8642e-10, dtype=torch.float64)
secont condition:: tensor(4.8642e-10, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(4.4450e-10, dtype=torch.float64)
secont condition:: tensor(4.4450e-10, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(3.3972e-10, dtype=torch.float64)
secont condition:: tensor(3.3972e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.1934e-10, dtype=torch.float64)
secont condition:: tensor(3.1934e-10, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 26.80321955680847
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(9.5811e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(3.5186e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(5.7538e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.0826e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.7298e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.714200
repetition 3
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 47.08420395851135
time_baseline:: 47.10260987281799
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.3897e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.5759e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000023, Accuracy: 0.715000
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(2.2650e-33, dtype=torch.float64)
secont condition:: tensor(2.2650e-33, dtype=torch.float64)
curr_secont condition:: tensor(4.8847e-33, dtype=torch.float64)
secont condition:: tensor(4.8847e-33, dtype=torch.float64)
curr_secont condition:: tensor(7.8650e-33, dtype=torch.float64)
secont condition:: tensor(7.8650e-33, dtype=torch.float64)
curr_secont condition:: tensor(1.1185e-32, dtype=torch.float64)
secont condition:: tensor(1.1185e-32, dtype=torch.float64)
curr_secont condition:: tensor(1.4789e-32, dtype=torch.float64)
secont condition:: tensor(1.4789e-32, dtype=torch.float64)
curr_secont condition:: tensor(1.8871e-32, dtype=torch.float64)
secont condition:: tensor(1.8871e-32, dtype=torch.float64)
curr_secont condition:: tensor(2.3277e-32, dtype=torch.float64)
secont condition:: tensor(2.3277e-32, dtype=torch.float64)
curr_secont condition:: tensor(3.7436e-12, dtype=torch.float64)
secont condition:: tensor(3.7436e-12, dtype=torch.float64)
curr_secont condition:: tensor(4.9998e-12, dtype=torch.float64)
secont condition:: tensor(4.9998e-12, dtype=torch.float64)
curr_secont condition:: tensor(2.3891e-12, dtype=torch.float64)
secont condition:: tensor(2.3891e-12, dtype=torch.float64)
curr_secont condition:: tensor(2.0836e-12, dtype=torch.float64)
secont condition:: tensor(2.0836e-12, dtype=torch.float64)
curr_secont condition:: tensor(5.7244e-13, dtype=torch.float64)
secont condition:: tensor(5.7244e-13, dtype=torch.float64)
curr_secont condition:: tensor(1.3947e-12, dtype=torch.float64)
secont condition:: tensor(1.3947e-12, dtype=torch.float64)
curr_secont condition:: tensor(2.7927e-12, dtype=torch.float64)
secont condition:: tensor(2.7927e-12, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(1.2074e-12, dtype=torch.float64)
secont condition:: tensor(1.2074e-12, dtype=torch.float64)
curr_secont condition:: tensor(6.4697e-13, dtype=torch.float64)
secont condition:: tensor(6.4697e-13, dtype=torch.float64)
curr_secont condition:: tensor(9.9260e-13, dtype=torch.float64)
secont condition:: tensor(9.9260e-13, dtype=torch.float64)
curr_secont condition:: tensor(8.1776e-13, dtype=torch.float64)
secont condition:: tensor(8.1776e-13, dtype=torch.float64)
curr_secont condition:: tensor(7.8870e-13, dtype=torch.float64)
secont condition:: tensor(7.8870e-13, dtype=torch.float64)
curr_secont condition:: tensor(-8.0725e-13, dtype=torch.float64)
curr_secont condition:: tensor(9.0745e-13, dtype=torch.float64)
secont condition:: tensor(9.0745e-13, dtype=torch.float64)
curr_secont condition:: tensor(8.3120e-13, dtype=torch.float64)
secont condition:: tensor(8.3120e-13, dtype=torch.float64)
curr_secont condition:: tensor(-3.6596e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.3552e-12, dtype=torch.float64)
secont condition:: tensor(1.3552e-12, dtype=torch.float64)
curr_secont condition:: tensor(7.6355e-13, dtype=torch.float64)
secont condition:: tensor(7.6355e-13, dtype=torch.float64)
curr_secont condition:: tensor(7.7368e-13, dtype=torch.float64)
secont condition:: tensor(7.7368e-13, dtype=torch.float64)
curr_secont condition:: tensor(9.7842e-11, dtype=torch.float64)
secont condition:: tensor(9.7842e-11, dtype=torch.float64)
curr_secont condition:: tensor(9.2766e-11, dtype=torch.float64)
secont condition:: tensor(9.2766e-11, dtype=torch.float64)
curr_secont condition:: tensor(7.1510e-11, dtype=torch.float64)
secont condition:: tensor(7.1510e-11, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(2.6527e-11, dtype=torch.float64)
secont condition:: tensor(2.6527e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.7897e-11, dtype=torch.float64)
secont condition:: tensor(3.7897e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.3290e-11, dtype=torch.float64)
secont condition:: tensor(3.3290e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.7493e-11, dtype=torch.float64)
secont condition:: tensor(2.7493e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.5627e-11, dtype=torch.float64)
secont condition:: tensor(1.5627e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.7575e-11, dtype=torch.float64)
secont condition:: tensor(1.7575e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.5142e-11, dtype=torch.float64)
secont condition:: tensor(1.5142e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.4551e-10, dtype=torch.float64)
secont condition:: tensor(2.4551e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.6008e-10, dtype=torch.float64)
secont condition:: tensor(1.6008e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.4462e-10, dtype=torch.float64)
secont condition:: tensor(1.4462e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.8127e-11, dtype=torch.float64)
explicit_evaluation epoch:: 42
curr_secont condition:: tensor(5.1678e-11, dtype=torch.float64)
explicit_evaluation epoch:: 43
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(1.5123e-10, dtype=torch.float64)
secont condition:: tensor(1.5123e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.3717e-08, dtype=torch.float64)
explicit_evaluation epoch:: 59
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(6.4093e-10, dtype=torch.float64)
explicit_evaluation epoch:: 60
curr_secont condition:: tensor(1.5999e-09, dtype=torch.float64)
secont condition:: tensor(1.5999e-09, dtype=torch.float64)
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(8.1974e-10, dtype=torch.float64)
secont condition:: tensor(8.1974e-10, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(1.0260e-09, dtype=torch.float64)
secont condition:: tensor(1.0260e-09, dtype=torch.float64)
curr_secont condition:: tensor(-9.2202e-09, dtype=torch.float64)
explicit_evaluation epoch:: 96
curr_secont condition:: tensor(2.2830e-09, dtype=torch.float64)
secont condition:: tensor(2.2830e-09, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(1.9496e-09, dtype=torch.float64)
secont condition:: tensor(1.9496e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.1042e-09, dtype=torch.float64)
secont condition:: tensor(1.1042e-09, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 26.56195831298828
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(8.6666e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(4.6560e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(7.1518e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.3360e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(2.2792e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.716800
repetition 4
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 47.458617210388184
time_baseline:: 47.476598501205444
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.3917e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(6.0790e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000023, Accuracy: 0.715000
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(2.2650e-33, dtype=torch.float64)
secont condition:: tensor(2.2650e-33, dtype=torch.float64)
curr_secont condition:: tensor(4.8847e-33, dtype=torch.float64)
secont condition:: tensor(4.8847e-33, dtype=torch.float64)
curr_secont condition:: tensor(7.8650e-33, dtype=torch.float64)
secont condition:: tensor(7.8650e-33, dtype=torch.float64)
curr_secont condition:: tensor(1.1185e-32, dtype=torch.float64)
secont condition:: tensor(1.1185e-32, dtype=torch.float64)
curr_secont condition:: tensor(1.4789e-32, dtype=torch.float64)
secont condition:: tensor(1.4789e-32, dtype=torch.float64)
curr_secont condition:: tensor(1.8871e-32, dtype=torch.float64)
secont condition:: tensor(1.8871e-32, dtype=torch.float64)
curr_secont condition:: tensor(2.3254e-32, dtype=torch.float64)
secont condition:: tensor(2.3254e-32, dtype=torch.float64)
curr_secont condition:: tensor(2.8081e-32, dtype=torch.float64)
secont condition:: tensor(2.8081e-32, dtype=torch.float64)
curr_secont condition:: tensor(3.3090e-32, dtype=torch.float64)
secont condition:: tensor(3.3090e-32, dtype=torch.float64)
curr_secont condition:: tensor(3.8452e-32, dtype=torch.float64)
secont condition:: tensor(3.8452e-32, dtype=torch.float64)
curr_secont condition:: tensor(8.1767e-11, dtype=torch.float64)
secont condition:: tensor(8.1767e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.5464e-11, dtype=torch.float64)
secont condition:: tensor(5.5464e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.9356e-11, dtype=torch.float64)
secont condition:: tensor(3.9356e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.7420e-11, dtype=torch.float64)
secont condition:: tensor(2.7420e-11, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(2.7149e-11, dtype=torch.float64)
secont condition:: tensor(2.7149e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.3344e-11, dtype=torch.float64)
secont condition:: tensor(2.3344e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.0912e-11, dtype=torch.float64)
secont condition:: tensor(2.0912e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.5743e-11, dtype=torch.float64)
secont condition:: tensor(1.5743e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.2922e-11, dtype=torch.float64)
secont condition:: tensor(1.2922e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.6902e-12, dtype=torch.float64)
secont condition:: tensor(6.6902e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.0642e-11, dtype=torch.float64)
secont condition:: tensor(1.0642e-11, dtype=torch.float64)
curr_secont condition:: tensor(7.8227e-12, dtype=torch.float64)
secont condition:: tensor(7.8227e-12, dtype=torch.float64)
curr_secont condition:: tensor(6.9521e-12, dtype=torch.float64)
secont condition:: tensor(6.9521e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.2383e-11, dtype=torch.float64)
secont condition:: tensor(1.2383e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.6345e-10, dtype=torch.float64)
secont condition:: tensor(1.6345e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.5752e-10, dtype=torch.float64)
secont condition:: tensor(1.5752e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.0547e-10, dtype=torch.float64)
secont condition:: tensor(1.0547e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.0908e-10, dtype=torch.float64)
secont condition:: tensor(1.0908e-10, dtype=torch.float64)
curr_secont condition:: tensor(7.9467e-11, dtype=torch.float64)
secont condition:: tensor(7.9467e-11, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(1.0160e-10, dtype=torch.float64)
secont condition:: tensor(1.0160e-10, dtype=torch.float64)
curr_secont condition:: tensor(6.1157e-11, dtype=torch.float64)
secont condition:: tensor(6.1157e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.3757e-11, dtype=torch.float64)
secont condition:: tensor(5.3757e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.4278e-11, dtype=torch.float64)
secont condition:: tensor(4.4278e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.6524e-11, dtype=torch.float64)
secont condition:: tensor(2.6524e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.4972e-11, dtype=torch.float64)
secont condition:: tensor(3.4972e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.7224e-11, dtype=torch.float64)
secont condition:: tensor(3.7224e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.2016e-11, dtype=torch.float64)
secont condition:: tensor(4.2016e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.8513e-11, dtype=torch.float64)
secont condition:: tensor(2.8513e-11, dtype=torch.float64)
curr_secont condition:: tensor(7.4171e-10, dtype=torch.float64)
secont condition:: tensor(7.4171e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.5404e-10, dtype=torch.float64)
explicit_evaluation epoch:: 44
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(1.3734e-09, dtype=torch.float64)
secont condition:: tensor(1.3734e-09, dtype=torch.float64)
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(8.6381e-10, dtype=torch.float64)
secont condition:: tensor(8.6381e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.0499e-09, dtype=torch.float64)
secont condition:: tensor(1.0499e-09, dtype=torch.float64)
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(1.6421e-09, dtype=torch.float64)
explicit_evaluation epoch:: 80
curr_secont condition:: tensor(-1.4179e-08, dtype=torch.float64)
explicit_evaluation epoch:: 85
curr_secont condition:: tensor(4.0629e-09, dtype=torch.float64)
secont condition:: tensor(4.0629e-09, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(4.8693e-09, dtype=torch.float64)
secont condition:: tensor(4.8693e-09, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(2.1905e-09, dtype=torch.float64)
secont condition:: tensor(2.1905e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.5922e-09, dtype=torch.float64)
secont condition:: tensor(1.5922e-09, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.02678656578064
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.2162e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(5.5438e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(7.5899e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.2594e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(3.0594e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.714300
repetition 5
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 46.583887577056885
time_baseline:: 46.601937770843506
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.6990e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(8.3874e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000023, Accuracy: 0.715000
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(2.2650e-33, dtype=torch.float64)
secont condition:: tensor(2.2650e-33, dtype=torch.float64)
curr_secont condition:: tensor(4.8847e-33, dtype=torch.float64)
secont condition:: tensor(4.8847e-33, dtype=torch.float64)
curr_secont condition:: tensor(7.8650e-33, dtype=torch.float64)
secont condition:: tensor(7.8650e-33, dtype=torch.float64)
curr_secont condition:: tensor(1.1185e-32, dtype=torch.float64)
secont condition:: tensor(1.1185e-32, dtype=torch.float64)
curr_secont condition:: tensor(1.4789e-32, dtype=torch.float64)
secont condition:: tensor(1.4789e-32, dtype=torch.float64)
curr_secont condition:: tensor(1.8875e-32, dtype=torch.float64)
secont condition:: tensor(1.8875e-32, dtype=torch.float64)
curr_secont condition:: tensor(1.8383e-11, dtype=torch.float64)
secont condition:: tensor(1.8383e-11, dtype=torch.float64)
curr_secont condition:: tensor(8.8264e-12, dtype=torch.float64)
secont condition:: tensor(8.8264e-12, dtype=torch.float64)
curr_secont condition:: tensor(-3.9936e-12, dtype=torch.float64)
curr_secont condition:: tensor(4.8916e-12, dtype=torch.float64)
secont condition:: tensor(4.8916e-12, dtype=torch.float64)
curr_secont condition:: tensor(5.1776e-12, dtype=torch.float64)
secont condition:: tensor(5.1776e-12, dtype=torch.float64)
curr_secont condition:: tensor(3.3325e-12, dtype=torch.float64)
secont condition:: tensor(3.3325e-12, dtype=torch.float64)
curr_secont condition:: tensor(2.6781e-12, dtype=torch.float64)
secont condition:: tensor(2.6781e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.4840e-12, dtype=torch.float64)
secont condition:: tensor(1.4840e-12, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(2.1244e-12, dtype=torch.float64)
secont condition:: tensor(2.1244e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.5188e-12, dtype=torch.float64)
secont condition:: tensor(1.5188e-12, dtype=torch.float64)
curr_secont condition:: tensor(2.1267e-12, dtype=torch.float64)
secont condition:: tensor(2.1267e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.6427e-12, dtype=torch.float64)
secont condition:: tensor(1.6427e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.8561e-12, dtype=torch.float64)
secont condition:: tensor(1.8561e-12, dtype=torch.float64)
curr_secont condition:: tensor(-2.6521e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.3388e-12, dtype=torch.float64)
secont condition:: tensor(1.3388e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.6121e-12, dtype=torch.float64)
secont condition:: tensor(1.6121e-12, dtype=torch.float64)
curr_secont condition:: tensor(9.5296e-13, dtype=torch.float64)
secont condition:: tensor(9.5296e-13, dtype=torch.float64)
curr_secont condition:: tensor(1.0678e-12, dtype=torch.float64)
secont condition:: tensor(1.0678e-12, dtype=torch.float64)
curr_secont condition:: tensor(7.0119e-12, dtype=torch.float64)
secont condition:: tensor(7.0119e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.7576e-12, dtype=torch.float64)
secont condition:: tensor(1.7576e-12, dtype=torch.float64)
curr_secont condition:: tensor(2.3125e-12, dtype=torch.float64)
secont condition:: tensor(2.3125e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.4687e-09, dtype=torch.float64)
secont condition:: tensor(1.4687e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.0015e-09, dtype=torch.float64)
secont condition:: tensor(1.0015e-09, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(7.7490e-10, dtype=torch.float64)
secont condition:: tensor(7.7490e-10, dtype=torch.float64)
curr_secont condition:: tensor(6.3592e-10, dtype=torch.float64)
secont condition:: tensor(6.3592e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.2569e-09, dtype=torch.float64)
secont condition:: tensor(1.2569e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.1224e-09, dtype=torch.float64)
secont condition:: tensor(1.1224e-09, dtype=torch.float64)
curr_secont condition:: tensor(9.7197e-10, dtype=torch.float64)
secont condition:: tensor(9.7197e-10, dtype=torch.float64)
curr_secont condition:: tensor(7.5473e-10, dtype=torch.float64)
secont condition:: tensor(7.5473e-10, dtype=torch.float64)
curr_secont condition:: tensor(5.0266e-10, dtype=torch.float64)
secont condition:: tensor(5.0266e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.4406e-10, dtype=torch.float64)
secont condition:: tensor(4.4406e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.6343e-10, dtype=torch.float64)
secont condition:: tensor(3.6343e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.6371e-10, dtype=torch.float64)
secont condition:: tensor(2.6371e-10, dtype=torch.float64)
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(2.6112e-10, dtype=torch.float64)
secont condition:: tensor(2.6112e-10, dtype=torch.float64)
curr_secont condition:: tensor(-8.4658e-09, dtype=torch.float64)
explicit_evaluation epoch:: 50
curr_secont condition:: tensor(3.5253e-09, dtype=torch.float64)
secont condition:: tensor(3.5253e-09, dtype=torch.float64)
curr_secont condition:: tensor(9.9972e-10, dtype=torch.float64)
explicit_evaluation epoch:: 58
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(1.1484e-09, dtype=torch.float64)
secont condition:: tensor(1.1484e-09, dtype=torch.float64)
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(3.0883e-09, dtype=torch.float64)
secont condition:: tensor(3.0883e-09, dtype=torch.float64)
curr_secont condition:: tensor(2.1456e-09, dtype=torch.float64)
secont condition:: tensor(2.1456e-09, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(2.0819e-09, dtype=torch.float64)
secont condition:: tensor(2.0819e-09, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(1.3430e-09, dtype=torch.float64)
secont condition:: tensor(1.3430e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.1177e-09, dtype=torch.float64)
secont condition:: tensor(1.1177e-09, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.671651124954224
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.3109e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(5.0807e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(5.1306e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.3239e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(4.7361e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.716300
l2 norm:: 0.002
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type MSELoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
cuda:2
Train - Epoch 0, Batch: 0, Loss: 0.091544
Train - Epoch 0, Batch: 10, Loss: 0.090281
Test Avg. Loss: 0.000027, Accuracy: 0.326300
Train - Epoch 1, Batch: 0, Loss: 0.089719
Train - Epoch 1, Batch: 10, Loss: 0.089165
Test Avg. Loss: 0.000026, Accuracy: 0.574700
Train - Epoch 2, Batch: 0, Loss: 0.088606
Train - Epoch 2, Batch: 10, Loss: 0.088131
Test Avg. Loss: 0.000026, Accuracy: 0.640000
Train - Epoch 3, Batch: 0, Loss: 0.087413
Train - Epoch 3, Batch: 10, Loss: 0.086967
Test Avg. Loss: 0.000025, Accuracy: 0.657200
Train - Epoch 4, Batch: 0, Loss: 0.086142
Train - Epoch 4, Batch: 10, Loss: 0.085224
Test Avg. Loss: 0.000025, Accuracy: 0.662700
Train - Epoch 5, Batch: 0, Loss: 0.085057
Train - Epoch 5, Batch: 10, Loss: 0.084319
Test Avg. Loss: 0.000025, Accuracy: 0.676800
Train - Epoch 6, Batch: 0, Loss: 0.083921
Train - Epoch 6, Batch: 10, Loss: 0.083097
Test Avg. Loss: 0.000024, Accuracy: 0.690200
Train - Epoch 7, Batch: 0, Loss: 0.083321
Train - Epoch 7, Batch: 10, Loss: 0.082739
Test Avg. Loss: 0.000024, Accuracy: 0.701300
training_time:: 59.9551146030426
repetition 1
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 47.18003559112549
time_baseline:: 47.198604106903076
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.1976e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(7.5517e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.701200
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(4.1886e-10, dtype=torch.float64)
secont condition:: tensor(4.1886e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.2770e-10, dtype=torch.float64)
secont condition:: tensor(3.2770e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.5529e-10, dtype=torch.float64)
secont condition:: tensor(2.5529e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.8999e-10, dtype=torch.float64)
secont condition:: tensor(1.8999e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.9035e-10, dtype=torch.float64)
secont condition:: tensor(1.9035e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.3308e-10, dtype=torch.float64)
secont condition:: tensor(1.3308e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.2197e-10, dtype=torch.float64)
secont condition:: tensor(1.2197e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.1534e-10, dtype=torch.float64)
secont condition:: tensor(1.1534e-10, dtype=torch.float64)
curr_secont condition:: tensor(6.3857e-11, dtype=torch.float64)
secont condition:: tensor(6.3857e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.9237e-11, dtype=torch.float64)
secont condition:: tensor(5.9237e-11, dtype=torch.float64)
curr_secont condition:: tensor(7.8972e-11, dtype=torch.float64)
secont condition:: tensor(7.8972e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.6181e-11, dtype=torch.float64)
secont condition:: tensor(5.6181e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.6780e-11, dtype=torch.float64)
secont condition:: tensor(4.6780e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.6510e-11, dtype=torch.float64)
secont condition:: tensor(6.6510e-11, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(6.3961e-11, dtype=torch.float64)
secont condition:: tensor(6.3961e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.0428e-11, dtype=torch.float64)
secont condition:: tensor(1.0428e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.7440e-11, dtype=torch.float64)
secont condition:: tensor(4.7440e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.7029e-11, dtype=torch.float64)
secont condition:: tensor(4.7029e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.3861e-11, dtype=torch.float64)
secont condition:: tensor(4.3861e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.0150e-11, dtype=torch.float64)
secont condition:: tensor(6.0150e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.9712e-11, dtype=torch.float64)
secont condition:: tensor(1.9712e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.8971e-11, dtype=torch.float64)
secont condition:: tensor(4.8971e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.8207e-11, dtype=torch.float64)
secont condition:: tensor(3.8207e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.5545e-11, dtype=torch.float64)
secont condition:: tensor(3.5545e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.6266e-11, dtype=torch.float64)
secont condition:: tensor(4.6266e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.5364e-11, dtype=torch.float64)
secont condition:: tensor(3.5364e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.4881e-11, dtype=torch.float64)
secont condition:: tensor(3.4881e-11, dtype=torch.float64)
curr_secont condition:: tensor(7.4702e-11, dtype=torch.float64)
secont condition:: tensor(7.4702e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.8852e-11, dtype=torch.float64)
secont condition:: tensor(3.8852e-11, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(5.4033e-11, dtype=torch.float64)
secont condition:: tensor(5.4033e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.7792e-11, dtype=torch.float64)
secont condition:: tensor(4.7792e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.0840e-11, dtype=torch.float64)
secont condition:: tensor(5.0840e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.6507e-11, dtype=torch.float64)
secont condition:: tensor(2.6507e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.6350e-11, dtype=torch.float64)
secont condition:: tensor(5.6350e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.1761e-11, dtype=torch.float64)
secont condition:: tensor(2.1761e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.0045e-11, dtype=torch.float64)
secont condition:: tensor(4.0045e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.8334e-11, dtype=torch.float64)
secont condition:: tensor(2.8334e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.9119e-11, dtype=torch.float64)
secont condition:: tensor(3.9119e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.8135e-11, dtype=torch.float64)
secont condition:: tensor(4.8135e-11, dtype=torch.float64)
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(1.3921e-11, dtype=torch.float64)
secont condition:: tensor(1.3921e-11, dtype=torch.float64)
curr_secont condition:: tensor(7.9981e-11, dtype=torch.float64)
explicit_evaluation epoch:: 50
curr_secont condition:: tensor(7.4684e-11, dtype=torch.float64)
explicit_evaluation epoch:: 51
curr_secont condition:: tensor(3.7101e-11, dtype=torch.float64)
explicit_evaluation epoch:: 53
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(9.7386e-11, dtype=torch.float64)
secont condition:: tensor(9.7386e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.6958e-08, dtype=torch.float64)
explicit_evaluation epoch:: 64
curr_secont condition:: tensor(4.3545e-10, dtype=torch.float64)
explicit_evaluation epoch:: 65
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(4.4237e-10, dtype=torch.float64)
secont condition:: tensor(4.4237e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.6597e-10, dtype=torch.float64)
secont condition:: tensor(3.6597e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.6452e-08, dtype=torch.float64)
explicit_evaluation epoch:: 89
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(1.8277e-09, dtype=torch.float64)
explicit_evaluation epoch:: 90
curr_secont condition:: tensor(1.1860e-09, dtype=torch.float64)
secont condition:: tensor(1.1860e-09, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(1.9005e-09, dtype=torch.float64)
secont condition:: tensor(1.9005e-09, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 26.86766219139099
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(9.0672e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(6.1695e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.0350e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.1296e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.7677e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.695900
repetition 2
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 47.02628564834595
time_baseline:: 47.04633545875549
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.4389e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.4257e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.701300
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(6.0348e-33, dtype=torch.float64)
secont condition:: tensor(6.0348e-33, dtype=torch.float64)
curr_secont condition:: tensor(1.4958e-32, dtype=torch.float64)
secont condition:: tensor(1.4958e-32, dtype=torch.float64)
curr_secont condition:: tensor(2.6747e-32, dtype=torch.float64)
secont condition:: tensor(2.6747e-32, dtype=torch.float64)
curr_secont condition:: tensor(4.1223e-32, dtype=torch.float64)
secont condition:: tensor(4.1223e-32, dtype=torch.float64)
curr_secont condition:: tensor(5.8547e-32, dtype=torch.float64)
secont condition:: tensor(5.8547e-32, dtype=torch.float64)
curr_secont condition:: tensor(7.8546e-32, dtype=torch.float64)
secont condition:: tensor(7.8546e-32, dtype=torch.float64)
curr_secont condition:: tensor(1.0110e-31, dtype=torch.float64)
secont condition:: tensor(1.0110e-31, dtype=torch.float64)
curr_secont condition:: tensor(1.2650e-31, dtype=torch.float64)
secont condition:: tensor(1.2650e-31, dtype=torch.float64)
curr_secont condition:: tensor(1.5452e-31, dtype=torch.float64)
secont condition:: tensor(1.5452e-31, dtype=torch.float64)
curr_secont condition:: tensor(1.8514e-31, dtype=torch.float64)
secont condition:: tensor(1.8514e-31, dtype=torch.float64)
curr_secont condition:: tensor(2.1853e-31, dtype=torch.float64)
secont condition:: tensor(2.1853e-31, dtype=torch.float64)
curr_secont condition:: tensor(2.5430e-31, dtype=torch.float64)
secont condition:: tensor(2.5430e-31, dtype=torch.float64)
curr_secont condition:: tensor(2.9246e-31, dtype=torch.float64)
secont condition:: tensor(2.9246e-31, dtype=torch.float64)
curr_secont condition:: tensor(3.3361e-31, dtype=torch.float64)
secont condition:: tensor(3.3361e-31, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(3.5165e-10, dtype=torch.float64)
secont condition:: tensor(3.5165e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.1331e-10, dtype=torch.float64)
secont condition:: tensor(3.1331e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.5565e-10, dtype=torch.float64)
secont condition:: tensor(2.5565e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.2635e-10, dtype=torch.float64)
secont condition:: tensor(2.2635e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.6436e-10, dtype=torch.float64)
secont condition:: tensor(1.6436e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.2706e-10, dtype=torch.float64)
secont condition:: tensor(1.2706e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.4278e-10, dtype=torch.float64)
secont condition:: tensor(1.4278e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.0190e-10, dtype=torch.float64)
secont condition:: tensor(1.0190e-10, dtype=torch.float64)
curr_secont condition:: tensor(8.2975e-11, dtype=torch.float64)
secont condition:: tensor(8.2975e-11, dtype=torch.float64)
curr_secont condition:: tensor(9.2106e-11, dtype=torch.float64)
secont condition:: tensor(9.2106e-11, dtype=torch.float64)
curr_secont condition:: tensor(8.5249e-11, dtype=torch.float64)
secont condition:: tensor(8.5249e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.8252e-11, dtype=torch.float64)
secont condition:: tensor(6.8252e-11, dtype=torch.float64)
curr_secont condition:: tensor(8.8123e-11, dtype=torch.float64)
secont condition:: tensor(8.8123e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.6799e-11, dtype=torch.float64)
secont condition:: tensor(6.6799e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.1340e-10, dtype=torch.float64)
secont condition:: tensor(2.1340e-10, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(1.5786e-10, dtype=torch.float64)
secont condition:: tensor(1.5786e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.9068e-10, dtype=torch.float64)
secont condition:: tensor(1.9068e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.2999e-10, dtype=torch.float64)
secont condition:: tensor(1.2999e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.3424e-10, dtype=torch.float64)
secont condition:: tensor(1.3424e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.1982e-10, dtype=torch.float64)
secont condition:: tensor(1.1982e-10, dtype=torch.float64)
curr_secont condition:: tensor(8.8289e-11, dtype=torch.float64)
secont condition:: tensor(8.8289e-11, dtype=torch.float64)
curr_secont condition:: tensor(7.3956e-11, dtype=torch.float64)
secont condition:: tensor(7.3956e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.1557e-10, dtype=torch.float64)
secont condition:: tensor(1.1557e-10, dtype=torch.float64)
curr_secont condition:: tensor(8.5971e-11, dtype=torch.float64)
secont condition:: tensor(8.5971e-11, dtype=torch.float64)
curr_secont condition:: tensor(7.2742e-11, dtype=torch.float64)
secont condition:: tensor(7.2742e-11, dtype=torch.float64)
curr_secont condition:: tensor(8.6534e-09, dtype=torch.float64)
explicit_evaluation epoch:: 44
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(6.5388e-10, dtype=torch.float64)
explicit_evaluation epoch:: 45
curr_secont condition:: tensor(6.4362e-10, dtype=torch.float64)
secont condition:: tensor(6.4362e-10, dtype=torch.float64)
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(1.3145e-09, dtype=torch.float64)
secont condition:: tensor(1.3145e-09, dtype=torch.float64)
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(8.4390e-10, dtype=torch.float64)
secont condition:: tensor(8.4390e-10, dtype=torch.float64)
curr_secont condition:: tensor(8.0039e-10, dtype=torch.float64)
secont condition:: tensor(8.0039e-10, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(6.2487e-10, dtype=torch.float64)
secont condition:: tensor(6.2487e-10, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(6.1647e-10, dtype=torch.float64)
secont condition:: tensor(6.1647e-10, dtype=torch.float64)
curr_secont condition:: tensor(7.0597e-10, dtype=torch.float64)
secont condition:: tensor(7.0597e-10, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 24.75390362739563
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(8.6310e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(5.1046e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(7.9677e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.4399e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(2.2097e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.699800
repetition 3
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 47.672178506851196
time_baseline:: 47.69147753715515
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.0906e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(3.1626e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.701300
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(6.0348e-33, dtype=torch.float64)
secont condition:: tensor(6.0348e-33, dtype=torch.float64)
curr_secont condition:: tensor(1.4958e-32, dtype=torch.float64)
secont condition:: tensor(1.4958e-32, dtype=torch.float64)
curr_secont condition:: tensor(2.6747e-32, dtype=torch.float64)
secont condition:: tensor(2.6747e-32, dtype=torch.float64)
curr_secont condition:: tensor(4.1223e-32, dtype=torch.float64)
secont condition:: tensor(4.1223e-32, dtype=torch.float64)
curr_secont condition:: tensor(5.8547e-32, dtype=torch.float64)
secont condition:: tensor(5.8547e-32, dtype=torch.float64)
curr_secont condition:: tensor(7.8546e-32, dtype=torch.float64)
secont condition:: tensor(7.8546e-32, dtype=torch.float64)
curr_secont condition:: tensor(1.0110e-31, dtype=torch.float64)
secont condition:: tensor(1.0110e-31, dtype=torch.float64)
curr_secont condition:: tensor(1.2650e-31, dtype=torch.float64)
secont condition:: tensor(1.2650e-31, dtype=torch.float64)
curr_secont condition:: tensor(1.5452e-31, dtype=torch.float64)
secont condition:: tensor(1.5452e-31, dtype=torch.float64)
curr_secont condition:: tensor(1.8514e-31, dtype=torch.float64)
secont condition:: tensor(1.8514e-31, dtype=torch.float64)
curr_secont condition:: tensor(2.1853e-31, dtype=torch.float64)
secont condition:: tensor(2.1853e-31, dtype=torch.float64)
curr_secont condition:: tensor(2.5430e-31, dtype=torch.float64)
secont condition:: tensor(2.5430e-31, dtype=torch.float64)
curr_secont condition:: tensor(2.9253e-31, dtype=torch.float64)
secont condition:: tensor(2.9253e-31, dtype=torch.float64)
curr_secont condition:: tensor(1.9262e-10, dtype=torch.float64)
secont condition:: tensor(1.9262e-10, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(1.4985e-10, dtype=torch.float64)
secont condition:: tensor(1.4985e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.3544e-10, dtype=torch.float64)
secont condition:: tensor(1.3544e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.1498e-10, dtype=torch.float64)
secont condition:: tensor(1.1498e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.0299e-10, dtype=torch.float64)
secont condition:: tensor(1.0299e-10, dtype=torch.float64)
curr_secont condition:: tensor(7.8390e-11, dtype=torch.float64)
secont condition:: tensor(7.8390e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.1021e-11, dtype=torch.float64)
secont condition:: tensor(6.1021e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.1624e-11, dtype=torch.float64)
secont condition:: tensor(6.1624e-11, dtype=torch.float64)
curr_secont condition:: tensor(7.5860e-11, dtype=torch.float64)
secont condition:: tensor(7.5860e-11, dtype=torch.float64)
curr_secont condition:: tensor(7.1128e-11, dtype=torch.float64)
secont condition:: tensor(7.1128e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.4258e-11, dtype=torch.float64)
secont condition:: tensor(6.4258e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.8730e-11, dtype=torch.float64)
secont condition:: tensor(4.8730e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.4877e-11, dtype=torch.float64)
secont condition:: tensor(4.4877e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.0583e-11, dtype=torch.float64)
secont condition:: tensor(6.0583e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.4423e-11, dtype=torch.float64)
secont condition:: tensor(4.4423e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.9696e-11, dtype=torch.float64)
secont condition:: tensor(2.9696e-11, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(3.9281e-11, dtype=torch.float64)
secont condition:: tensor(3.9281e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.4601e-11, dtype=torch.float64)
secont condition:: tensor(3.4601e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.4724e-11, dtype=torch.float64)
secont condition:: tensor(3.4724e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.9968e-11, dtype=torch.float64)
secont condition:: tensor(4.9968e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.2325e-11, dtype=torch.float64)
secont condition:: tensor(3.2325e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.7657e-11, dtype=torch.float64)
secont condition:: tensor(3.7657e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.9864e-11, dtype=torch.float64)
secont condition:: tensor(2.9864e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.2574e-11, dtype=torch.float64)
secont condition:: tensor(3.2574e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.9171e-11, dtype=torch.float64)
secont condition:: tensor(2.9171e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.2890e-11, dtype=torch.float64)
secont condition:: tensor(2.2890e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.1455e-08, dtype=torch.float64)
explicit_evaluation epoch:: 42
curr_secont condition:: tensor(1.0810e-09, dtype=torch.float64)
explicit_evaluation epoch:: 43
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(9.6556e-10, dtype=torch.float64)
secont condition:: tensor(9.6556e-10, dtype=torch.float64)
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(9.6682e-10, dtype=torch.float64)
secont condition:: tensor(9.6682e-10, dtype=torch.float64)
curr_secont condition:: tensor(7.0220e-10, dtype=torch.float64)
secont condition:: tensor(7.0220e-10, dtype=torch.float64)
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(5.0859e-10, dtype=torch.float64)
secont condition:: tensor(5.0859e-10, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(6.7373e-10, dtype=torch.float64)
secont condition:: tensor(6.7373e-10, dtype=torch.float64)
curr_secont condition:: tensor(5.2395e-10, dtype=torch.float64)
secont condition:: tensor(5.2395e-10, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(5.5878e-10, dtype=torch.float64)
secont condition:: tensor(5.5878e-10, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.34967017173767
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(7.6916e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(4.2336e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(6.0043e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(8.5022e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(7.6268e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.702800
repetition 4
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 47.818206787109375
time_baseline:: 47.83622097969055
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.0424e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(3.9089e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.701300
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(1.2129e-11, dtype=torch.float64)
secont condition:: tensor(1.2129e-11, dtype=torch.float64)
curr_secont condition:: tensor(8.8794e-12, dtype=torch.float64)
secont condition:: tensor(8.8794e-12, dtype=torch.float64)
curr_secont condition:: tensor(6.5729e-12, dtype=torch.float64)
secont condition:: tensor(6.5729e-12, dtype=torch.float64)
curr_secont condition:: tensor(4.5878e-12, dtype=torch.float64)
secont condition:: tensor(4.5878e-12, dtype=torch.float64)
curr_secont condition:: tensor(4.0314e-12, dtype=torch.float64)
secont condition:: tensor(4.0314e-12, dtype=torch.float64)
curr_secont condition:: tensor(3.0711e-12, dtype=torch.float64)
secont condition:: tensor(3.0711e-12, dtype=torch.float64)
curr_secont condition:: tensor(2.7747e-12, dtype=torch.float64)
secont condition:: tensor(2.7747e-12, dtype=torch.float64)
curr_secont condition:: tensor(2.2343e-12, dtype=torch.float64)
secont condition:: tensor(2.2343e-12, dtype=torch.float64)
curr_secont condition:: tensor(2.2022e-12, dtype=torch.float64)
secont condition:: tensor(2.2022e-12, dtype=torch.float64)
curr_secont condition:: tensor(2.1688e-12, dtype=torch.float64)
secont condition:: tensor(2.1688e-12, dtype=torch.float64)
curr_secont condition:: tensor(2.0283e-12, dtype=torch.float64)
secont condition:: tensor(2.0283e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.6928e-12, dtype=torch.float64)
secont condition:: tensor(1.6928e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.4924e-12, dtype=torch.float64)
secont condition:: tensor(1.4924e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.4832e-12, dtype=torch.float64)
secont condition:: tensor(1.4832e-12, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(1.4486e-12, dtype=torch.float64)
secont condition:: tensor(1.4486e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.1644e-12, dtype=torch.float64)
secont condition:: tensor(1.1644e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.2346e-12, dtype=torch.float64)
secont condition:: tensor(1.2346e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.3776e-12, dtype=torch.float64)
secont condition:: tensor(1.3776e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.3204e-12, dtype=torch.float64)
secont condition:: tensor(1.3204e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.7671e-12, dtype=torch.float64)
secont condition:: tensor(1.7671e-12, dtype=torch.float64)
curr_secont condition:: tensor(3.3806e-12, dtype=torch.float64)
secont condition:: tensor(3.3806e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.6193e-12, dtype=torch.float64)
secont condition:: tensor(1.6193e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.5646e-12, dtype=torch.float64)
secont condition:: tensor(1.5646e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.6392e-12, dtype=torch.float64)
secont condition:: tensor(1.6392e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.6159e-12, dtype=torch.float64)
secont condition:: tensor(1.6159e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.7396e-12, dtype=torch.float64)
secont condition:: tensor(1.7396e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.4729e-12, dtype=torch.float64)
secont condition:: tensor(1.4729e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.5286e-12, dtype=torch.float64)
secont condition:: tensor(1.5286e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.1562e-10, dtype=torch.float64)
secont condition:: tensor(1.1562e-10, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(8.9407e-11, dtype=torch.float64)
secont condition:: tensor(8.9407e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.0011e-10, dtype=torch.float64)
secont condition:: tensor(1.0011e-10, dtype=torch.float64)
curr_secont condition:: tensor(7.8156e-11, dtype=torch.float64)
secont condition:: tensor(7.8156e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.1432e-11, dtype=torch.float64)
secont condition:: tensor(5.1432e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.5422e-11, dtype=torch.float64)
secont condition:: tensor(5.5422e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.6867e-11, dtype=torch.float64)
secont condition:: tensor(4.6867e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.8220e-11, dtype=torch.float64)
secont condition:: tensor(3.8220e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.5756e-11, dtype=torch.float64)
secont condition:: tensor(3.5756e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.2127e-11, dtype=torch.float64)
secont condition:: tensor(3.2127e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.9487e-11, dtype=torch.float64)
secont condition:: tensor(2.9487e-11, dtype=torch.float64)
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(3.5435e-11, dtype=torch.float64)
secont condition:: tensor(3.5435e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.1559e-11, dtype=torch.float64)
explicit_evaluation epoch:: 50
curr_secont condition:: tensor(6.9050e-11, dtype=torch.float64)
explicit_evaluation epoch:: 52
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(2.7866e-10, dtype=torch.float64)
secont condition:: tensor(2.7866e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.2444e-10, dtype=torch.float64)
secont condition:: tensor(3.2444e-10, dtype=torch.float64)
curr_secont condition:: tensor(8.8601e-09, dtype=torch.float64)
explicit_evaluation epoch:: 74
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(1.6081e-09, dtype=torch.float64)
explicit_evaluation epoch:: 75
curr_secont condition:: tensor(9.9952e-10, dtype=torch.float64)
secont condition:: tensor(9.9952e-10, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(7.1921e-10, dtype=torch.float64)
secont condition:: tensor(7.1921e-10, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(2.3028e-09, dtype=torch.float64)
secont condition:: tensor(2.3028e-09, dtype=torch.float64)
curr_secont condition:: tensor(8.5615e-10, dtype=torch.float64)
secont condition:: tensor(8.5615e-10, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.808438301086426
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(7.1254e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(3.8098e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(5.6122e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.0201e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(9.2141e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.699800
repetition 5
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 47.56705951690674
time_baseline:: 47.5849244594574
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.1052e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(9.8023e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.701300
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(6.0348e-33, dtype=torch.float64)
secont condition:: tensor(6.0348e-33, dtype=torch.float64)
curr_secont condition:: tensor(1.4958e-32, dtype=torch.float64)
secont condition:: tensor(1.4958e-32, dtype=torch.float64)
curr_secont condition:: tensor(2.6780e-32, dtype=torch.float64)
secont condition:: tensor(2.6780e-32, dtype=torch.float64)
curr_secont condition:: tensor(2.1414e-11, dtype=torch.float64)
secont condition:: tensor(2.1414e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.0387e-11, dtype=torch.float64)
secont condition:: tensor(2.0387e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.3707e-11, dtype=torch.float64)
secont condition:: tensor(1.3707e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.3083e-11, dtype=torch.float64)
secont condition:: tensor(1.3083e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.0450e-11, dtype=torch.float64)
secont condition:: tensor(1.0450e-11, dtype=torch.float64)
curr_secont condition:: tensor(7.5581e-12, dtype=torch.float64)
secont condition:: tensor(7.5581e-12, dtype=torch.float64)
curr_secont condition:: tensor(5.0371e-12, dtype=torch.float64)
secont condition:: tensor(5.0371e-12, dtype=torch.float64)
curr_secont condition:: tensor(6.3555e-12, dtype=torch.float64)
secont condition:: tensor(6.3555e-12, dtype=torch.float64)
curr_secont condition:: tensor(8.0744e-12, dtype=torch.float64)
secont condition:: tensor(8.0744e-12, dtype=torch.float64)
curr_secont condition:: tensor(4.3761e-12, dtype=torch.float64)
secont condition:: tensor(4.3761e-12, dtype=torch.float64)
curr_secont condition:: tensor(7.9284e-12, dtype=torch.float64)
secont condition:: tensor(7.9284e-12, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(3.9180e-12, dtype=torch.float64)
secont condition:: tensor(3.9180e-12, dtype=torch.float64)
curr_secont condition:: tensor(4.2791e-12, dtype=torch.float64)
secont condition:: tensor(4.2791e-12, dtype=torch.float64)
curr_secont condition:: tensor(3.7824e-12, dtype=torch.float64)
secont condition:: tensor(3.7824e-12, dtype=torch.float64)
curr_secont condition:: tensor(3.4216e-12, dtype=torch.float64)
secont condition:: tensor(3.4216e-12, dtype=torch.float64)
curr_secont condition:: tensor(3.5820e-12, dtype=torch.float64)
secont condition:: tensor(3.5820e-12, dtype=torch.float64)
curr_secont condition:: tensor(5.2268e-12, dtype=torch.float64)
secont condition:: tensor(5.2268e-12, dtype=torch.float64)
curr_secont condition:: tensor(3.5199e-12, dtype=torch.float64)
secont condition:: tensor(3.5199e-12, dtype=torch.float64)
curr_secont condition:: tensor(3.7305e-12, dtype=torch.float64)
secont condition:: tensor(3.7305e-12, dtype=torch.float64)
curr_secont condition:: tensor(3.5349e-12, dtype=torch.float64)
secont condition:: tensor(3.5349e-12, dtype=torch.float64)
curr_secont condition:: tensor(3.2850e-12, dtype=torch.float64)
secont condition:: tensor(3.2850e-12, dtype=torch.float64)
curr_secont condition:: tensor(3.2923e-12, dtype=torch.float64)
secont condition:: tensor(3.2923e-12, dtype=torch.float64)
curr_secont condition:: tensor(3.8365e-12, dtype=torch.float64)
secont condition:: tensor(3.8365e-12, dtype=torch.float64)
curr_secont condition:: tensor(1.4127e-10, dtype=torch.float64)
secont condition:: tensor(1.4127e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.2227e-10, dtype=torch.float64)
secont condition:: tensor(1.2227e-10, dtype=torch.float64)
curr_secont condition:: tensor(9.4834e-11, dtype=torch.float64)
secont condition:: tensor(9.4834e-11, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(8.5774e-11, dtype=torch.float64)
secont condition:: tensor(8.5774e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.3831e-11, dtype=torch.float64)
secont condition:: tensor(6.3831e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.2372e-11, dtype=torch.float64)
secont condition:: tensor(5.2372e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.0470e-11, dtype=torch.float64)
secont condition:: tensor(4.0470e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.0362e-11, dtype=torch.float64)
secont condition:: tensor(4.0362e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.2353e-11, dtype=torch.float64)
secont condition:: tensor(4.2353e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.8127e-11, dtype=torch.float64)
secont condition:: tensor(2.8127e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.9086e-11, dtype=torch.float64)
secont condition:: tensor(2.9086e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.1491e-11, dtype=torch.float64)
secont condition:: tensor(3.1491e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.6093e-11, dtype=torch.float64)
secont condition:: tensor(2.6093e-11, dtype=torch.float64)
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(4.0892e-11, dtype=torch.float64)
secont condition:: tensor(4.0892e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.7995e-09, dtype=torch.float64)
explicit_evaluation epoch:: 50
curr_secont condition:: tensor(5.1747e-10, dtype=torch.float64)
explicit_evaluation epoch:: 51
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(3.6482e-10, dtype=torch.float64)
secont condition:: tensor(3.6482e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.6573e-10, dtype=torch.float64)
secont condition:: tensor(2.6573e-10, dtype=torch.float64)
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(3.1860e-10, dtype=torch.float64)
secont condition:: tensor(3.1860e-10, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(5.0755e-10, dtype=torch.float64)
secont condition:: tensor(5.0755e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.9496e-10, dtype=torch.float64)
secont condition:: tensor(4.9496e-10, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(3.9999e-10, dtype=torch.float64)
secont condition:: tensor(3.9999e-10, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 26.084142684936523
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(7.0570e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(2.9158e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(4.3593e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(9.5323e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.4078e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000024, Accuracy: 0.701600
l2 norm:: 0.005
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type MSELoss. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
cuda:2
Train - Epoch 0, Batch: 0, Loss: 0.091544
Train - Epoch 0, Batch: 10, Loss: 0.090224
Test Avg. Loss: 0.000027, Accuracy: 0.324100
Train - Epoch 1, Batch: 0, Loss: 0.089678
Train - Epoch 1, Batch: 10, Loss: 0.089168
Test Avg. Loss: 0.000026, Accuracy: 0.570600
Train - Epoch 2, Batch: 0, Loss: 0.088691
Train - Epoch 2, Batch: 10, Loss: 0.088338
Test Avg. Loss: 0.000026, Accuracy: 0.635400
Train - Epoch 3, Batch: 0, Loss: 0.087800
Train - Epoch 3, Batch: 10, Loss: 0.087552
Test Avg. Loss: 0.000026, Accuracy: 0.649100
Train - Epoch 4, Batch: 0, Loss: 0.086958
Train - Epoch 4, Batch: 10, Loss: 0.086433
Test Avg. Loss: 0.000026, Accuracy: 0.649600
Train - Epoch 5, Batch: 0, Loss: 0.086393
Train - Epoch 5, Batch: 10, Loss: 0.085973
Test Avg. Loss: 0.000025, Accuracy: 0.653000
Train - Epoch 6, Batch: 0, Loss: 0.085731
Train - Epoch 6, Batch: 10, Loss: 0.085278
Test Avg. Loss: 0.000025, Accuracy: 0.658900
Train - Epoch 7, Batch: 0, Loss: 0.085468
Train - Epoch 7, Batch: 10, Loss: 0.085192
Test Avg. Loss: 0.000025, Accuracy: 0.662400
training_time:: 59.32111930847168
repetition 1
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 46.92128252983093
time_baseline:: 46.9396014213562
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(6.6047e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(3.7010e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000025, Accuracy: 0.662400
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(8.7486e-33, dtype=torch.float64)
secont condition:: tensor(8.7486e-33, dtype=torch.float64)
curr_secont condition:: tensor(1.7377e-32, dtype=torch.float64)
secont condition:: tensor(1.7377e-32, dtype=torch.float64)
curr_secont condition:: tensor(2.5898e-32, dtype=torch.float64)
secont condition:: tensor(2.5898e-32, dtype=torch.float64)
curr_secont condition:: tensor(3.4213e-32, dtype=torch.float64)
secont condition:: tensor(3.4213e-32, dtype=torch.float64)
curr_secont condition:: tensor(4.2367e-32, dtype=torch.float64)
secont condition:: tensor(4.2367e-32, dtype=torch.float64)
curr_secont condition:: tensor(5.0321e-32, dtype=torch.float64)
secont condition:: tensor(5.0321e-32, dtype=torch.float64)
curr_secont condition:: tensor(5.8348e-32, dtype=torch.float64)
secont condition:: tensor(5.8348e-32, dtype=torch.float64)
curr_secont condition:: tensor(6.6133e-32, dtype=torch.float64)
secont condition:: tensor(6.6133e-32, dtype=torch.float64)
curr_secont condition:: tensor(7.3792e-32, dtype=torch.float64)
secont condition:: tensor(7.3792e-32, dtype=torch.float64)
curr_secont condition:: tensor(8.1350e-32, dtype=torch.float64)
secont condition:: tensor(8.1350e-32, dtype=torch.float64)
curr_secont condition:: tensor(1.4949e-10, dtype=torch.float64)
secont condition:: tensor(1.4949e-10, dtype=torch.float64)
curr_secont condition:: tensor(9.5842e-11, dtype=torch.float64)
secont condition:: tensor(9.5842e-11, dtype=torch.float64)
curr_secont condition:: tensor(7.8761e-11, dtype=torch.float64)
secont condition:: tensor(7.8761e-11, dtype=torch.float64)
curr_secont condition:: tensor(7.4224e-11, dtype=torch.float64)
secont condition:: tensor(7.4224e-11, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(6.8093e-11, dtype=torch.float64)
secont condition:: tensor(6.8093e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.7589e-11, dtype=torch.float64)
secont condition:: tensor(5.7589e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.3980e-11, dtype=torch.float64)
secont condition:: tensor(5.3980e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.6837e-11, dtype=torch.float64)
secont condition:: tensor(4.6837e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.0545e-11, dtype=torch.float64)
secont condition:: tensor(4.0545e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.2304e-11, dtype=torch.float64)
secont condition:: tensor(4.2304e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.6221e-11, dtype=torch.float64)
secont condition:: tensor(3.6221e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.9701e-11, dtype=torch.float64)
secont condition:: tensor(5.9701e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.4300e-11, dtype=torch.float64)
secont condition:: tensor(6.4300e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.7647e-11, dtype=torch.float64)
secont condition:: tensor(5.7647e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.9933e-11, dtype=torch.float64)
secont condition:: tensor(4.9933e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.5195e-11, dtype=torch.float64)
secont condition:: tensor(5.5195e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.8313e-11, dtype=torch.float64)
secont condition:: tensor(2.8313e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.3264e-11, dtype=torch.float64)
secont condition:: tensor(6.3264e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.4676e-11, dtype=torch.float64)
secont condition:: tensor(4.4676e-11, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(4.7612e-11, dtype=torch.float64)
secont condition:: tensor(4.7612e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.6848e-11, dtype=torch.float64)
secont condition:: tensor(4.6848e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.5140e-11, dtype=torch.float64)
secont condition:: tensor(4.5140e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.6941e-11, dtype=torch.float64)
secont condition:: tensor(4.6941e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.5765e-11, dtype=torch.float64)
secont condition:: tensor(4.5765e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.3665e-11, dtype=torch.float64)
secont condition:: tensor(5.3665e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.6404e-11, dtype=torch.float64)
secont condition:: tensor(4.6404e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.3662e-11, dtype=torch.float64)
secont condition:: tensor(4.3662e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.6802e-11, dtype=torch.float64)
secont condition:: tensor(4.6802e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.3080e-11, dtype=torch.float64)
secont condition:: tensor(4.3080e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.7781e-09, dtype=torch.float64)
explicit_evaluation epoch:: 40
curr_secont condition:: tensor(1.0398e-10, dtype=torch.float64)
explicit_evaluation epoch:: 41
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(1.0212e-10, dtype=torch.float64)
secont condition:: tensor(1.0212e-10, dtype=torch.float64)
curr_secont condition:: tensor(8.0113e-10, dtype=torch.float64)
explicit_evaluation epoch:: 59
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(3.7420e-10, dtype=torch.float64)
explicit_evaluation epoch:: 60
curr_secont condition:: tensor(2.9185e-10, dtype=torch.float64)
secont condition:: tensor(2.9185e-10, dtype=torch.float64)
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(3.4800e-10, dtype=torch.float64)
secont condition:: tensor(3.4800e-10, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(3.0879e-10, dtype=torch.float64)
secont condition:: tensor(3.0879e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.1629e-10, dtype=torch.float64)
secont condition:: tensor(3.1629e-10, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(2.8975e-10, dtype=torch.float64)
secont condition:: tensor(2.8975e-10, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 24.926758289337158
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(5.3183e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(3.5498e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(6.4058e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(5.7106e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(8.7454e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000025, Accuracy: 0.657500
repetition 2
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 47.23146986961365
time_baseline:: 47.24940228462219
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.1031e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(4.0499e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000025, Accuracy: 0.662400
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(8.7486e-33, dtype=torch.float64)
secont condition:: tensor(8.7486e-33, dtype=torch.float64)
curr_secont condition:: tensor(1.7377e-32, dtype=torch.float64)
secont condition:: tensor(1.7377e-32, dtype=torch.float64)
curr_secont condition:: tensor(2.5898e-32, dtype=torch.float64)
secont condition:: tensor(2.5898e-32, dtype=torch.float64)
curr_secont condition:: tensor(3.4213e-32, dtype=torch.float64)
secont condition:: tensor(3.4213e-32, dtype=torch.float64)
curr_secont condition:: tensor(4.2367e-32, dtype=torch.float64)
secont condition:: tensor(4.2367e-32, dtype=torch.float64)
curr_secont condition:: tensor(5.0304e-32, dtype=torch.float64)
secont condition:: tensor(5.0304e-32, dtype=torch.float64)
curr_secont condition:: tensor(1.3926e-10, dtype=torch.float64)
secont condition:: tensor(1.3926e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.1653e-10, dtype=torch.float64)
secont condition:: tensor(1.1653e-10, dtype=torch.float64)
curr_secont condition:: tensor(9.5357e-11, dtype=torch.float64)
secont condition:: tensor(9.5357e-11, dtype=torch.float64)
curr_secont condition:: tensor(7.4394e-11, dtype=torch.float64)
secont condition:: tensor(7.4394e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.8929e-11, dtype=torch.float64)
secont condition:: tensor(6.8929e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.6779e-11, dtype=torch.float64)
secont condition:: tensor(5.6779e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.7530e-11, dtype=torch.float64)
secont condition:: tensor(4.7530e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.6123e-11, dtype=torch.float64)
secont condition:: tensor(3.6123e-11, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(4.0770e-11, dtype=torch.float64)
secont condition:: tensor(4.0770e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.9738e-11, dtype=torch.float64)
secont condition:: tensor(3.9738e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.5827e-11, dtype=torch.float64)
secont condition:: tensor(3.5827e-11, dtype=torch.float64)
curr_secont condition:: tensor(4.9842e-10, dtype=torch.float64)
secont condition:: tensor(4.9842e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.8647e-10, dtype=torch.float64)
secont condition:: tensor(3.8647e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.1992e-10, dtype=torch.float64)
secont condition:: tensor(3.1992e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.9498e-10, dtype=torch.float64)
secont condition:: tensor(2.9498e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.2363e-10, dtype=torch.float64)
secont condition:: tensor(2.2363e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.2246e-10, dtype=torch.float64)
secont condition:: tensor(2.2246e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.1636e-10, dtype=torch.float64)
secont condition:: tensor(2.1636e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.8692e-10, dtype=torch.float64)
secont condition:: tensor(1.8692e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.5956e-10, dtype=torch.float64)
secont condition:: tensor(1.5956e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.6039e-10, dtype=torch.float64)
secont condition:: tensor(1.6039e-10, dtype=torch.float64)
curr_secont condition:: tensor(7.5412e-11, dtype=torch.float64)
secont condition:: tensor(7.5412e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.2235e-10, dtype=torch.float64)
secont condition:: tensor(1.2235e-10, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(2.9442e-12, dtype=torch.float64)
secont condition:: tensor(2.9442e-12, dtype=torch.float64)
curr_secont condition:: tensor(2.3157e-10, dtype=torch.float64)
secont condition:: tensor(2.3157e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.4496e-10, dtype=torch.float64)
secont condition:: tensor(1.4496e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.2844e-10, dtype=torch.float64)
secont condition:: tensor(1.2844e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.3999e-10, dtype=torch.float64)
secont condition:: tensor(1.3999e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.2070e-10, dtype=torch.float64)
secont condition:: tensor(1.2070e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.2492e-10, dtype=torch.float64)
secont condition:: tensor(1.2492e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.1014e-10, dtype=torch.float64)
secont condition:: tensor(1.1014e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.0579e-10, dtype=torch.float64)
secont condition:: tensor(1.0579e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.1544e-10, dtype=torch.float64)
secont condition:: tensor(1.1544e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.6095e-10, dtype=torch.float64)
explicit_evaluation epoch:: 41
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(1.9148e-10, dtype=torch.float64)
explicit_evaluation epoch:: 45
curr_secont condition:: tensor(2.4149e-10, dtype=torch.float64)
secont condition:: tensor(2.4149e-10, dtype=torch.float64)
curr_secont condition:: tensor(4.6601e-10, dtype=torch.float64)
explicit_evaluation epoch:: 59
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(3.2294e-10, dtype=torch.float64)
secont condition:: tensor(3.2294e-10, dtype=torch.float64)
curr_secont condition:: tensor(-1.7635e-09, dtype=torch.float64)
explicit_evaluation epoch:: 70
curr_secont condition:: tensor(1.5324e-09, dtype=torch.float64)
secont condition:: tensor(1.5324e-09, dtype=torch.float64)
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(2.3593e-09, dtype=torch.float64)
secont condition:: tensor(2.3593e-09, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(1.0260e-09, dtype=torch.float64)
secont condition:: tensor(1.0260e-09, dtype=torch.float64)
curr_secont condition:: tensor(1.0295e-09, dtype=torch.float64)
secont condition:: tensor(1.0295e-09, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(1.5961e-09, dtype=torch.float64)
secont condition:: tensor(1.5961e-09, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 26.32583236694336
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(9.8252e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(5.8876e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.0263e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(1.1887e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.3195e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000025, Accuracy: 0.663400
repetition 3
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 47.44939112663269
time_baseline:: 47.46699333190918
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(7.9102e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(8.8512e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000025, Accuracy: 0.662600
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(8.7486e-33, dtype=torch.float64)
secont condition:: tensor(8.7486e-33, dtype=torch.float64)
curr_secont condition:: tensor(1.7377e-32, dtype=torch.float64)
secont condition:: tensor(1.7377e-32, dtype=torch.float64)
curr_secont condition:: tensor(2.5898e-32, dtype=torch.float64)
secont condition:: tensor(2.5898e-32, dtype=torch.float64)
curr_secont condition:: tensor(3.4213e-32, dtype=torch.float64)
secont condition:: tensor(3.4213e-32, dtype=torch.float64)
curr_secont condition:: tensor(4.2367e-32, dtype=torch.float64)
secont condition:: tensor(4.2367e-32, dtype=torch.float64)
curr_secont condition:: tensor(5.0321e-32, dtype=torch.float64)
secont condition:: tensor(5.0321e-32, dtype=torch.float64)
curr_secont condition:: tensor(5.8348e-32, dtype=torch.float64)
secont condition:: tensor(5.8348e-32, dtype=torch.float64)
curr_secont condition:: tensor(6.6127e-32, dtype=torch.float64)
secont condition:: tensor(6.6127e-32, dtype=torch.float64)
curr_secont condition:: tensor(1.2569e-10, dtype=torch.float64)
secont condition:: tensor(1.2569e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.0126e-10, dtype=torch.float64)
secont condition:: tensor(1.0126e-10, dtype=torch.float64)
curr_secont condition:: tensor(8.2389e-11, dtype=torch.float64)
secont condition:: tensor(8.2389e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.8193e-11, dtype=torch.float64)
secont condition:: tensor(6.8193e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.0951e-11, dtype=torch.float64)
secont condition:: tensor(5.0951e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.2658e-11, dtype=torch.float64)
secont condition:: tensor(5.2658e-11, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(4.2777e-11, dtype=torch.float64)
secont condition:: tensor(4.2777e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.7954e-11, dtype=torch.float64)
secont condition:: tensor(2.7954e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.9534e-11, dtype=torch.float64)
secont condition:: tensor(3.9534e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.6560e-11, dtype=torch.float64)
secont condition:: tensor(3.6560e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.6896e-11, dtype=torch.float64)
secont condition:: tensor(3.6896e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.3151e-11, dtype=torch.float64)
secont condition:: tensor(3.3151e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.8143e-11, dtype=torch.float64)
secont condition:: tensor(2.8143e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.4748e-11, dtype=torch.float64)
secont condition:: tensor(2.4748e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.0163e-11, dtype=torch.float64)
secont condition:: tensor(3.0163e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.7598e-11, dtype=torch.float64)
secont condition:: tensor(3.7598e-11, dtype=torch.float64)
curr_secont condition:: tensor(3.2539e-11, dtype=torch.float64)
secont condition:: tensor(3.2539e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.3258e-11, dtype=torch.float64)
secont condition:: tensor(2.3258e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.0583e-10, dtype=torch.float64)
secont condition:: tensor(1.0583e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.2267e-10, dtype=torch.float64)
secont condition:: tensor(1.2267e-10, dtype=torch.float64)
curr_secont condition:: tensor(8.7350e-11, dtype=torch.float64)
secont condition:: tensor(8.7350e-11, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(9.4620e-11, dtype=torch.float64)
secont condition:: tensor(9.4620e-11, dtype=torch.float64)
curr_secont condition:: tensor(7.2600e-11, dtype=torch.float64)
secont condition:: tensor(7.2600e-11, dtype=torch.float64)
curr_secont condition:: tensor(8.1113e-11, dtype=torch.float64)
secont condition:: tensor(8.1113e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.4517e-11, dtype=torch.float64)
secont condition:: tensor(6.4517e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.9178e-11, dtype=torch.float64)
secont condition:: tensor(5.9178e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.2018e-11, dtype=torch.float64)
secont condition:: tensor(6.2018e-11, dtype=torch.float64)
curr_secont condition:: tensor(9.7093e-11, dtype=torch.float64)
secont condition:: tensor(9.7093e-11, dtype=torch.float64)
curr_secont condition:: tensor(9.5053e-11, dtype=torch.float64)
secont condition:: tensor(9.5053e-11, dtype=torch.float64)
curr_secont condition:: tensor(8.2096e-11, dtype=torch.float64)
secont condition:: tensor(8.2096e-11, dtype=torch.float64)
curr_secont condition:: tensor(8.9130e-11, dtype=torch.float64)
secont condition:: tensor(8.9130e-11, dtype=torch.float64)
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(6.9931e-11, dtype=torch.float64)
secont condition:: tensor(6.9931e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.1097e-10, dtype=torch.float64)
explicit_evaluation epoch:: 51
curr_secont condition:: tensor(3.2119e-10, dtype=torch.float64)
explicit_evaluation epoch:: 52
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(2.3681e-10, dtype=torch.float64)
secont condition:: tensor(2.3681e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.1783e-10, dtype=torch.float64)
secont condition:: tensor(2.1783e-10, dtype=torch.float64)
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(4.8514e-10, dtype=torch.float64)
secont condition:: tensor(4.8514e-10, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(5.5488e-10, dtype=torch.float64)
secont condition:: tensor(5.5488e-10, dtype=torch.float64)
curr_secont condition:: tensor(6.5276e-10, dtype=torch.float64)
secont condition:: tensor(6.5276e-10, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(3.8705e-10, dtype=torch.float64)
secont condition:: tensor(3.8705e-10, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.216931581497192
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(5.6346e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(4.3543e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(8.4178e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(7.5441e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.7187e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000025, Accuracy: 0.664500
repetition 4
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 47.420499086380005
time_baseline:: 47.43990731239319
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(6.1598e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(8.6412e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(5.3710e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000025, Accuracy: 0.662400
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(8.7486e-33, dtype=torch.float64)
secont condition:: tensor(8.7486e-33, dtype=torch.float64)
curr_secont condition:: tensor(1.7377e-32, dtype=torch.float64)
secont condition:: tensor(1.7377e-32, dtype=torch.float64)
curr_secont condition:: tensor(2.5898e-32, dtype=torch.float64)
secont condition:: tensor(2.5898e-32, dtype=torch.float64)
curr_secont condition:: tensor(3.4213e-32, dtype=torch.float64)
secont condition:: tensor(3.4213e-32, dtype=torch.float64)
curr_secont condition:: tensor(4.2367e-32, dtype=torch.float64)
secont condition:: tensor(4.2367e-32, dtype=torch.float64)
curr_secont condition:: tensor(5.0321e-32, dtype=torch.float64)
secont condition:: tensor(5.0321e-32, dtype=torch.float64)
curr_secont condition:: tensor(5.8348e-32, dtype=torch.float64)
secont condition:: tensor(5.8348e-32, dtype=torch.float64)
curr_secont condition:: tensor(6.6133e-32, dtype=torch.float64)
secont condition:: tensor(6.6133e-32, dtype=torch.float64)
curr_secont condition:: tensor(7.3792e-32, dtype=torch.float64)
secont condition:: tensor(7.3792e-32, dtype=torch.float64)
curr_secont condition:: tensor(8.1348e-32, dtype=torch.float64)
secont condition:: tensor(8.1348e-32, dtype=torch.float64)
curr_secont condition:: tensor(8.8676e-32, dtype=torch.float64)
secont condition:: tensor(8.8676e-32, dtype=torch.float64)
curr_secont condition:: tensor(1.8656e-11, dtype=torch.float64)
secont condition:: tensor(1.8656e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.0563e-11, dtype=torch.float64)
secont condition:: tensor(2.0563e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.3256e-11, dtype=torch.float64)
secont condition:: tensor(1.3256e-11, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(1.1367e-11, dtype=torch.float64)
secont condition:: tensor(1.1367e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.0248e-11, dtype=torch.float64)
secont condition:: tensor(1.0248e-11, dtype=torch.float64)
curr_secont condition:: tensor(9.5501e-12, dtype=torch.float64)
secont condition:: tensor(9.5501e-12, dtype=torch.float64)
curr_secont condition:: tensor(8.5789e-12, dtype=torch.float64)
secont condition:: tensor(8.5789e-12, dtype=torch.float64)
curr_secont condition:: tensor(3.4321e-11, dtype=torch.float64)
secont condition:: tensor(3.4321e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.7666e-11, dtype=torch.float64)
secont condition:: tensor(2.7666e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.7217e-11, dtype=torch.float64)
secont condition:: tensor(2.7217e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.3390e-11, dtype=torch.float64)
secont condition:: tensor(2.3390e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.1590e-11, dtype=torch.float64)
secont condition:: tensor(2.1590e-11, dtype=torch.float64)
curr_secont condition:: tensor(2.1427e-11, dtype=torch.float64)
secont condition:: tensor(2.1427e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.7129e-11, dtype=torch.float64)
secont condition:: tensor(1.7129e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.7925e-11, dtype=torch.float64)
secont condition:: tensor(1.7925e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.7124e-11, dtype=torch.float64)
secont condition:: tensor(1.7124e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.5463e-11, dtype=torch.float64)
secont condition:: tensor(1.5463e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.4788e-11, dtype=torch.float64)
secont condition:: tensor(1.4788e-11, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(1.4371e-11, dtype=torch.float64)
secont condition:: tensor(1.4371e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.0017e-10, dtype=torch.float64)
secont condition:: tensor(1.0017e-10, dtype=torch.float64)
curr_secont condition:: tensor(9.4128e-11, dtype=torch.float64)
secont condition:: tensor(9.4128e-11, dtype=torch.float64)
curr_secont condition:: tensor(7.3866e-11, dtype=torch.float64)
secont condition:: tensor(7.3866e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.6441e-11, dtype=torch.float64)
secont condition:: tensor(6.6441e-11, dtype=torch.float64)
curr_secont condition:: tensor(6.6703e-11, dtype=torch.float64)
secont condition:: tensor(6.6703e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.6321e-11, dtype=torch.float64)
secont condition:: tensor(5.6321e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.2193e-11, dtype=torch.float64)
secont condition:: tensor(5.2193e-11, dtype=torch.float64)
curr_secont condition:: tensor(5.4005e-11, dtype=torch.float64)
secont condition:: tensor(5.4005e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.4828e-10, dtype=torch.float64)
secont condition:: tensor(1.4828e-10, dtype=torch.float64)
curr_secont condition:: tensor(6.9567e-11, dtype=torch.float64)
explicit_evaluation epoch:: 40
curr_secont condition:: tensor(1.0641e-10, dtype=torch.float64)
explicit_evaluation epoch:: 44
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(5.2385e-11, dtype=torch.float64)
explicit_evaluation epoch:: 45
curr_secont condition:: tensor(4.7918e-11, dtype=torch.float64)
secont condition:: tensor(4.7918e-11, dtype=torch.float64)
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(8.1969e-11, dtype=torch.float64)
secont condition:: tensor(8.1969e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.3295e-10, dtype=torch.float64)
explicit_evaluation epoch:: 67
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(1.1716e-10, dtype=torch.float64)
secont condition:: tensor(1.1716e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.6933e-10, dtype=torch.float64)
secont condition:: tensor(1.6933e-10, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(1.2454e-10, dtype=torch.float64)
secont condition:: tensor(1.2454e-10, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(1.6066e-10, dtype=torch.float64)
secont condition:: tensor(1.6066e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.7163e-10, dtype=torch.float64)
explicit_evaluation epoch:: 111
curr_secont condition:: tensor(4.6320e-10, dtype=torch.float64)
explicit_evaluation epoch:: 112
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 26.498088359832764
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(4.8368e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(2.8963e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(5.1775e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(5.8856e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(9.9722e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(9.9668e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000025, Accuracy: 0.657100
repetition 5
baseline::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DNNModel_skipnet. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/usr/lib/python3/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Softmax. It won't be checked for correctness upon loading.
  "type " + obj.__name__ + ". It won't be checked "
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 46.745980739593506
time_baseline:: 46.76461935043335
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(7.5164e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(7.3343e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000025, Accuracy: 0.662500
incremental updates::
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:262: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/Skipnet.py:321: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc4(out)
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
curr_secont condition:: tensor(8.7485e-33, dtype=torch.float64)
secont condition:: tensor(8.7485e-33, dtype=torch.float64)
curr_secont condition:: tensor(3.4052e-10, dtype=torch.float64)
secont condition:: tensor(3.4052e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.1759e-10, dtype=torch.float64)
secont condition:: tensor(3.1759e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.6338e-10, dtype=torch.float64)
secont condition:: tensor(2.6338e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.3127e-10, dtype=torch.float64)
secont condition:: tensor(2.3127e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.7665e-10, dtype=torch.float64)
secont condition:: tensor(1.7665e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.6616e-10, dtype=torch.float64)
secont condition:: tensor(1.6616e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.4963e-10, dtype=torch.float64)
secont condition:: tensor(1.4963e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.1885e-10, dtype=torch.float64)
secont condition:: tensor(1.1885e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.0165e-10, dtype=torch.float64)
secont condition:: tensor(1.0165e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.0946e-10, dtype=torch.float64)
secont condition:: tensor(1.0946e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.1742e-10, dtype=torch.float64)
secont condition:: tensor(1.1742e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.0416e-10, dtype=torch.float64)
secont condition:: tensor(1.0416e-10, dtype=torch.float64)
curr_secont condition:: tensor(9.9752e-11, dtype=torch.float64)
secont condition:: tensor(9.9752e-11, dtype=torch.float64)
epoch  1
	calling Sampler:__iter__
curr_secont condition:: tensor(1.0915e-10, dtype=torch.float64)
secont condition:: tensor(1.0915e-10, dtype=torch.float64)
curr_secont condition:: tensor(9.8816e-11, dtype=torch.float64)
secont condition:: tensor(9.8816e-11, dtype=torch.float64)
curr_secont condition:: tensor(8.6184e-11, dtype=torch.float64)
secont condition:: tensor(8.6184e-11, dtype=torch.float64)
curr_secont condition:: tensor(8.5106e-11, dtype=torch.float64)
secont condition:: tensor(8.5106e-11, dtype=torch.float64)
curr_secont condition:: tensor(8.1309e-11, dtype=torch.float64)
secont condition:: tensor(8.1309e-11, dtype=torch.float64)
curr_secont condition:: tensor(8.9524e-11, dtype=torch.float64)
secont condition:: tensor(8.9524e-11, dtype=torch.float64)
curr_secont condition:: tensor(8.9708e-11, dtype=torch.float64)
secont condition:: tensor(8.9708e-11, dtype=torch.float64)
curr_secont condition:: tensor(9.9625e-11, dtype=torch.float64)
secont condition:: tensor(9.9625e-11, dtype=torch.float64)
curr_secont condition:: tensor(8.1543e-11, dtype=torch.float64)
secont condition:: tensor(8.1543e-11, dtype=torch.float64)
curr_secont condition:: tensor(8.9674e-11, dtype=torch.float64)
secont condition:: tensor(8.9674e-11, dtype=torch.float64)
curr_secont condition:: tensor(8.3261e-11, dtype=torch.float64)
secont condition:: tensor(8.3261e-11, dtype=torch.float64)
curr_secont condition:: tensor(8.4489e-11, dtype=torch.float64)
secont condition:: tensor(8.4489e-11, dtype=torch.float64)
curr_secont condition:: tensor(1.4654e-10, dtype=torch.float64)
secont condition:: tensor(1.4654e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.6107e-10, dtype=torch.float64)
secont condition:: tensor(1.6107e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.8724e-10, dtype=torch.float64)
secont condition:: tensor(1.8724e-10, dtype=torch.float64)
epoch  2
	calling Sampler:__iter__
curr_secont condition:: tensor(1.6184e-10, dtype=torch.float64)
secont condition:: tensor(1.6184e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.5178e-10, dtype=torch.float64)
secont condition:: tensor(1.5178e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.3479e-10, dtype=torch.float64)
secont condition:: tensor(1.3479e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.2328e-10, dtype=torch.float64)
secont condition:: tensor(1.2328e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.1223e-10, dtype=torch.float64)
secont condition:: tensor(1.1223e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.4283e-10, dtype=torch.float64)
secont condition:: tensor(1.4283e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.8244e-10, dtype=torch.float64)
secont condition:: tensor(1.8244e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.2402e-10, dtype=torch.float64)
secont condition:: tensor(1.2402e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.2591e-10, dtype=torch.float64)
secont condition:: tensor(1.2591e-10, dtype=torch.float64)
curr_secont condition:: tensor(1.3807e-10, dtype=torch.float64)
secont condition:: tensor(1.3807e-10, dtype=torch.float64)
epoch  3
	calling Sampler:__iter__
curr_secont condition:: tensor(1.3159e-10, dtype=torch.float64)
secont condition:: tensor(1.3159e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.9218e-10, dtype=torch.float64)
secont condition:: tensor(2.9218e-10, dtype=torch.float64)
epoch  4
	calling Sampler:__iter__
curr_secont condition:: tensor(2.7134e-10, dtype=torch.float64)
secont condition:: tensor(2.7134e-10, dtype=torch.float64)
epoch  5
	calling Sampler:__iter__
curr_secont condition:: tensor(2.6413e-10, dtype=torch.float64)
secont condition:: tensor(2.6413e-10, dtype=torch.float64)
curr_secont condition:: tensor(2.8354e-10, dtype=torch.float64)
secont condition:: tensor(2.8354e-10, dtype=torch.float64)
epoch  6
	calling Sampler:__iter__
curr_secont condition:: tensor(2.0522e-10, dtype=torch.float64)
secont condition:: tensor(2.0522e-10, dtype=torch.float64)
epoch  7
	calling Sampler:__iter__
curr_secont condition:: tensor(3.2906e-10, dtype=torch.float64)
secont condition:: tensor(3.2906e-10, dtype=torch.float64)
curr_secont condition:: tensor(3.1791e-10, dtype=torch.float64)
secont condition:: tensor(3.1791e-10, dtype=torch.float64)
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 23.797468185424805
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(5.8888e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(3.6644e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(6.1688e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 1 tensor(7.5514e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 2 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
curr_diff: 3 tensor(1.3366e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000025, Accuracy: 0.663300
benchmark_mnist_DNN.sh: line 126: 0.005: syntax error: invalid arithmetic operator (error token is ".005")
