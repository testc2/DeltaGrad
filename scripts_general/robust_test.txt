period::
init_iters::
varied deletion rate::
varied number of samples::
python3 generate_dataset_train_test.py Logistic_regression rcv1 16384 200 5
batch size:: 10200
repetition 0
python3 benchmark_exp_lr.py 0.001 10200 200 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression rcv1 0 0.005 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 0.693386
Train - Epoch 1, Batch: 0, Loss: 0.693040
Train - Epoch 2, Batch: 0, Loss: 0.692643
Train - Epoch 3, Batch: 0, Loss: 0.692269
Train - Epoch 4, Batch: 0, Loss: 0.691920
Train - Epoch 5, Batch: 0, Loss: 0.691539
Train - Epoch 6, Batch: 0, Loss: 0.691159
Train - Epoch 7, Batch: 0, Loss: 0.690793
Train - Epoch 8, Batch: 0, Loss: 0.690425
Train - Epoch 9, Batch: 0, Loss: 0.690088
Train - Epoch 10, Batch: 0, Loss: 0.689672
Train - Epoch 11, Batch: 0, Loss: 0.689337
Train - Epoch 12, Batch: 0, Loss: 0.688986
Train - Epoch 13, Batch: 0, Loss: 0.688661
Train - Epoch 14, Batch: 0, Loss: 0.688245
Train - Epoch 15, Batch: 0, Loss: 0.687913
Train - Epoch 16, Batch: 0, Loss: 0.687494
Train - Epoch 17, Batch: 0, Loss: 0.687113
Train - Epoch 18, Batch: 0, Loss: 0.686788
Train - Epoch 19, Batch: 0, Loss: 0.686491
Train - Epoch 20, Batch: 0, Loss: 0.686047
Train - Epoch 21, Batch: 0, Loss: 0.685654
Train - Epoch 22, Batch: 0, Loss: 0.685275
Train - Epoch 23, Batch: 0, Loss: 0.684971
Train - Epoch 24, Batch: 0, Loss: 0.684753
Train - Epoch 25, Batch: 0, Loss: 0.684238
Train - Epoch 26, Batch: 0, Loss: 0.683983
Train - Epoch 27, Batch: 0, Loss: 0.683545
Train - Epoch 28, Batch: 0, Loss: 0.683233
Train - Epoch 29, Batch: 0, Loss: 0.682938
Train - Epoch 30, Batch: 0, Loss: 0.682630
Train - Epoch 31, Batch: 0, Loss: 0.682128
Train - Epoch 32, Batch: 0, Loss: 0.681850
Train - Epoch 33, Batch: 0, Loss: 0.681441
Train - Epoch 34, Batch: 0, Loss: 0.681152
Train - Epoch 35, Batch: 0, Loss: 0.680820
Train - Epoch 36, Batch: 0, Loss: 0.680339
Train - Epoch 37, Batch: 0, Loss: 0.680219
Train - Epoch 38, Batch: 0, Loss: 0.679772
Train - Epoch 39, Batch: 0, Loss: 0.679416
Train - Epoch 40, Batch: 0, Loss: 0.678951
Train - Epoch 41, Batch: 0, Loss: 0.678618
Train - Epoch 42, Batch: 0, Loss: 0.678349
Train - Epoch 43, Batch: 0, Loss: 0.678079
Train - Epoch 44, Batch: 0, Loss: 0.677864
Train - Epoch 45, Batch: 0, Loss: 0.677257
Train - Epoch 46, Batch: 0, Loss: 0.676964
Train - Epoch 47, Batch: 0, Loss: 0.676764
Train - Epoch 48, Batch: 0, Loss: 0.676408
Train - Epoch 49, Batch: 0, Loss: 0.675978
Train - Epoch 50, Batch: 0, Loss: 0.675655
Train - Epoch 51, Batch: 0, Loss: 0.675299
Train - Epoch 52, Batch: 0, Loss: 0.675019
Train - Epoch 53, Batch: 0, Loss: 0.674791
Train - Epoch 54, Batch: 0, Loss: 0.674201
Train - Epoch 55, Batch: 0, Loss: 0.674018
Train - Epoch 56, Batch: 0, Loss: 0.673557
Train - Epoch 57, Batch: 0, Loss: 0.673332
Train - Epoch 58, Batch: 0, Loss: 0.672906
Train - Epoch 59, Batch: 0, Loss: 0.672782
Train - Epoch 60, Batch: 0, Loss: 0.672295
Train - Epoch 61, Batch: 0, Loss: 0.671831
Train - Epoch 62, Batch: 0, Loss: 0.671841
Train - Epoch 63, Batch: 0, Loss: 0.671438
Train - Epoch 64, Batch: 0, Loss: 0.670995
Train - Epoch 65, Batch: 0, Loss: 0.670873
Train - Epoch 66, Batch: 0, Loss: 0.670445
Train - Epoch 67, Batch: 0, Loss: 0.669744
Train - Epoch 68, Batch: 0, Loss: 0.669767
Train - Epoch 69, Batch: 0, Loss: 0.669339
Train - Epoch 70, Batch: 0, Loss: 0.669039
Train - Epoch 71, Batch: 0, Loss: 0.668689
Train - Epoch 72, Batch: 0, Loss: 0.668424
Train - Epoch 73, Batch: 0, Loss: 0.668106
Train - Epoch 74, Batch: 0, Loss: 0.668098
Train - Epoch 75, Batch: 0, Loss: 0.667411
Train - Epoch 76, Batch: 0, Loss: 0.667459
Train - Epoch 77, Batch: 0, Loss: 0.667058
Train - Epoch 78, Batch: 0, Loss: 0.666648
Train - Epoch 79, Batch: 0, Loss: 0.665961
Train - Epoch 80, Batch: 0, Loss: 0.665801
Train - Epoch 81, Batch: 0, Loss: 0.665614
Train - Epoch 82, Batch: 0, Loss: 0.665195
Train - Epoch 83, Batch: 0, Loss: 0.665161
Train - Epoch 84, Batch: 0, Loss: 0.664702
Train - Epoch 85, Batch: 0, Loss: 0.664168
Train - Epoch 86, Batch: 0, Loss: 0.664040
Train - Epoch 87, Batch: 0, Loss: 0.663557
Train - Epoch 88, Batch: 0, Loss: 0.663810
Train - Epoch 89, Batch: 0, Loss: 0.662946
Train - Epoch 90, Batch: 0, Loss: 0.662760
Train - Epoch 91, Batch: 0, Loss: 0.662855
Train - Epoch 92, Batch: 0, Loss: 0.662253
Train - Epoch 93, Batch: 0, Loss: 0.662062
Train - Epoch 94, Batch: 0, Loss: 0.661415
Train - Epoch 95, Batch: 0, Loss: 0.661097
Train - Epoch 96, Batch: 0, Loss: 0.660943
Train - Epoch 97, Batch: 0, Loss: 0.660689
Train - Epoch 98, Batch: 0, Loss: 0.660436
Train - Epoch 99, Batch: 0, Loss: 0.659977
Train - Epoch 100, Batch: 0, Loss: 0.659659
Train - Epoch 101, Batch: 0, Loss: 0.659364
Train - Epoch 102, Batch: 0, Loss: 0.659125
Train - Epoch 103, Batch: 0, Loss: 0.658844
Train - Epoch 104, Batch: 0, Loss: 0.658576
Train - Epoch 105, Batch: 0, Loss: 0.658415
Train - Epoch 106, Batch: 0, Loss: 0.657764
Train - Epoch 107, Batch: 0, Loss: 0.657595
Train - Epoch 108, Batch: 0, Loss: 0.657448
Train - Epoch 109, Batch: 0, Loss: 0.656689
Train - Epoch 110, Batch: 0, Loss: 0.656843
Train - Epoch 111, Batch: 0, Loss: 0.656695
Train - Epoch 112, Batch: 0, Loss: 0.655966
Train - Epoch 113, Batch: 0, Loss: 0.655636
Train - Epoch 114, Batch: 0, Loss: 0.655816
Train - Epoch 115, Batch: 0, Loss: 0.655457
Train - Epoch 116, Batch: 0, Loss: 0.655227
Train - Epoch 117, Batch: 0, Loss: 0.655256
Train - Epoch 118, Batch: 0, Loss: 0.654557
Train - Epoch 119, Batch: 0, Loss: 0.654042
Train - Epoch 120, Batch: 0, Loss: 0.653878
Train - Epoch 121, Batch: 0, Loss: 0.653412
Train - Epoch 122, Batch: 0, Loss: 0.653397
Train - Epoch 123, Batch: 0, Loss: 0.653106
Train - Epoch 124, Batch: 0, Loss: 0.652663
Train - Epoch 125, Batch: 0, Loss: 0.652969
Train - Epoch 126, Batch: 0, Loss: 0.652207
Train - Epoch 127, Batch: 0, Loss: 0.651424
Train - Epoch 128, Batch: 0, Loss: 0.651831
Train - Epoch 129, Batch: 0, Loss: 0.651606
Train - Epoch 130, Batch: 0, Loss: 0.650946
Train - Epoch 131, Batch: 0, Loss: 0.650971
Train - Epoch 132, Batch: 0, Loss: 0.650735
Train - Epoch 133, Batch: 0, Loss: 0.650208
Train - Epoch 134, Batch: 0, Loss: 0.650498
Train - Epoch 135, Batch: 0, Loss: 0.649388
Train - Epoch 136, Batch: 0, Loss: 0.649351
Train - Epoch 137, Batch: 0, Loss: 0.649291
Train - Epoch 138, Batch: 0, Loss: 0.649163
Train - Epoch 139, Batch: 0, Loss: 0.648549
Train - Epoch 140, Batch: 0, Loss: 0.648344
Train - Epoch 141, Batch: 0, Loss: 0.648050
Train - Epoch 142, Batch: 0, Loss: 0.647700
Train - Epoch 143, Batch: 0, Loss: 0.647771
Train - Epoch 144, Batch: 0, Loss: 0.646760
Train - Epoch 145, Batch: 0, Loss: 0.646902
Train - Epoch 146, Batch: 0, Loss: 0.646690
Train - Epoch 147, Batch: 0, Loss: 0.646163
Train - Epoch 148, Batch: 0, Loss: 0.646235
Train - Epoch 149, Batch: 0, Loss: 0.645953
Train - Epoch 150, Batch: 0, Loss: 0.645764
Train - Epoch 151, Batch: 0, Loss: 0.645656
Train - Epoch 152, Batch: 0, Loss: 0.644999
Train - Epoch 153, Batch: 0, Loss: 0.644968
Train - Epoch 154, Batch: 0, Loss: 0.644864
Train - Epoch 155, Batch: 0, Loss: 0.644881
Train - Epoch 156, Batch: 0, Loss: 0.644085
Train - Epoch 157, Batch: 0, Loss: 0.643883
Train - Epoch 158, Batch: 0, Loss: 0.643587
Train - Epoch 159, Batch: 0, Loss: 0.642849
Train - Epoch 160, Batch: 0, Loss: 0.643179
Train - Epoch 161, Batch: 0, Loss: 0.642663
Train - Epoch 162, Batch: 0, Loss: 0.641945
Train - Epoch 163, Batch: 0, Loss: 0.641843
Train - Epoch 164, Batch: 0, Loss: 0.642076
Train - Epoch 165, Batch: 0, Loss: 0.641658
Train - Epoch 166, Batch: 0, Loss: 0.641349
Train - Epoch 167, Batch: 0, Loss: 0.641192
Train - Epoch 168, Batch: 0, Loss: 0.640547
Train - Epoch 169, Batch: 0, Loss: 0.641038
Train - Epoch 170, Batch: 0, Loss: 0.640009
Train - Epoch 171, Batch: 0, Loss: 0.640160
Train - Epoch 172, Batch: 0, Loss: 0.639977
Train - Epoch 173, Batch: 0, Loss: 0.639365
Train - Epoch 174, Batch: 0, Loss: 0.639725
Train - Epoch 175, Batch: 0, Loss: 0.638912
Train - Epoch 176, Batch: 0, Loss: 0.639099
Train - Epoch 177, Batch: 0, Loss: 0.638727
Train - Epoch 178, Batch: 0, Loss: 0.638292
Train - Epoch 179, Batch: 0, Loss: 0.637991
Train - Epoch 180, Batch: 0, Loss: 0.637509
Train - Epoch 181, Batch: 0, Loss: 0.636949
Train - Epoch 182, Batch: 0, Loss: 0.637425
Train - Epoch 183, Batch: 0, Loss: 0.637022
Train - Epoch 184, Batch: 0, Loss: 0.636672
Train - Epoch 185, Batch: 0, Loss: 0.636918
Train - Epoch 186, Batch: 0, Loss: 0.636126/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 187, Batch: 0, Loss: 0.635888
Train - Epoch 188, Batch: 0, Loss: 0.635940
Train - Epoch 189, Batch: 0, Loss: 0.635049
Train - Epoch 190, Batch: 0, Loss: 0.635489
Train - Epoch 191, Batch: 0, Loss: 0.635621
Train - Epoch 192, Batch: 0, Loss: 0.635042
Train - Epoch 193, Batch: 0, Loss: 0.634258
Train - Epoch 194, Batch: 0, Loss: 0.633721
Train - Epoch 195, Batch: 0, Loss: 0.633862
Train - Epoch 196, Batch: 0, Loss: 0.634134
Train - Epoch 197, Batch: 0, Loss: 0.634045
Train - Epoch 198, Batch: 0, Loss: 0.633637
Train - Epoch 199, Batch: 0, Loss: 0.632972
training_time:: 351.32034134864807
training time full:: 351.3204092979431
provenance prepare time:: 0.0
here
start test::
Test Avg. Loss: 0.000063, Accuracy: 0.927428
adding noise deletion rate:: 0.01
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
tensor([0.5245, 0.5727, 0.5163,  ..., 0.5234, 0.5128, 0.5203],
       dtype=torch.float64, grad_fn=<MaxBackward0>)
tensor(0.8685, dtype=torch.float64) tensor(0., dtype=torch.float64)
Train - Epoch 0, Batch: 0, Loss: 0.693127
Train - Epoch 1, Batch: 0, Loss: 0.692826
Train - Epoch 2, Batch: 0, Loss: 0.692513
Train - Epoch 3, Batch: 0, Loss: 0.692152
Train - Epoch 4, Batch: 0, Loss: 0.691846
Train - Epoch 5, Batch: 0, Loss: 0.691525
Train - Epoch 6, Batch: 0, Loss: 0.691245
Train - Epoch 7, Batch: 0, Loss: 0.690878
Train - Epoch 8, Batch: 0, Loss: 0.690592
Train - Epoch 9, Batch: 0, Loss: 0.690280
Train - Epoch 10, Batch: 0, Loss: 0.689924
Train - Epoch 11, Batch: 0, Loss: 0.689662
Train - Epoch 12, Batch: 0, Loss: 0.689321
Train - Epoch 13, Batch: 0, Loss: 0.688975
Train - Epoch 14, Batch: 0, Loss: 0.688737
Train - Epoch 15, Batch: 0, Loss: 0.688375
Train - Epoch 16, Batch: 0, Loss: 0.688046
Train - Epoch 17, Batch: 0, Loss: 0.687727
Train - Epoch 18, Batch: 0, Loss: 0.687488
Train - Epoch 19, Batch: 0, Loss: 0.687151
Train - Epoch 20, Batch: 0, Loss: 0.686842
Train - Epoch 21, Batch: 0, Loss: 0.686554
Train - Epoch 22, Batch: 0, Loss: 0.686174
Train - Epoch 23, Batch: 0, Loss: 0.685872
Train - Epoch 24, Batch: 0, Loss: 0.685651
Train - Epoch 25, Batch: 0, Loss: 0.685284
Train - Epoch 26, Batch: 0, Loss: 0.685001
Train - Epoch 27, Batch: 0, Loss: 0.684617
Train - Epoch 28, Batch: 0, Loss: 0.684291
Train - Epoch 29, Batch: 0, Loss: 0.684158
Train - Epoch 30, Batch: 0, Loss: 0.683848
Train - Epoch 31, Batch: 0, Loss: 0.683426
Train - Epoch 32, Batch: 0, Loss: 0.683231
Train - Epoch 33, Batch: 0, Loss: 0.682878
Train - Epoch 34, Batch: 0, Loss: 0.682587
Train - Epoch 35, Batch: 0, Loss: 0.682210
Train - Epoch 36, Batch: 0, Loss: 0.681891
Train - Epoch 37, Batch: 0, Loss: 0.681833
Train - Epoch 38, Batch: 0, Loss: 0.681422
Train - Epoch 39, Batch: 0, Loss: 0.681098
Train - Epoch 40, Batch: 0, Loss: 0.680765
Train - Epoch 41, Batch: 0, Loss: 0.680590
Train - Epoch 42, Batch: 0, Loss: 0.680269
Train - Epoch 43, Batch: 0, Loss: 0.679923
Train - Epoch 44, Batch: 0, Loss: 0.679627
Train - Epoch 45, Batch: 0, Loss: 0.679299
Train - Epoch 46, Batch: 0, Loss: 0.678883
Train - Epoch 47, Batch: 0, Loss: 0.678834
Train - Epoch 48, Batch: 0, Loss: 0.678425
Train - Epoch 49, Batch: 0, Loss: 0.678227
Train - Epoch 50, Batch: 0, Loss: 0.677831
Train - Epoch 51, Batch: 0, Loss: 0.677422
Train - Epoch 52, Batch: 0, Loss: 0.677378
Train - Epoch 53, Batch: 0, Loss: 0.677068
Train - Epoch 54, Batch: 0, Loss: 0.676510
Train - Epoch 55, Batch: 0, Loss: 0.676468
Train - Epoch 56, Batch: 0, Loss: 0.676116
Train - Epoch 57, Batch: 0, Loss: 0.675762
Train - Epoch 58, Batch: 0, Loss: 0.675510
Train - Epoch 59, Batch: 0, Loss: 0.675335
Train - Epoch 60, Batch: 0, Loss: 0.675155
Train - Epoch 61, Batch: 0, Loss: 0.674594
Train - Epoch 62, Batch: 0, Loss: 0.674534
Train - Epoch 63, Batch: 0, Loss: 0.674193
Train - Epoch 64, Batch: 0, Loss: 0.673749
Train - Epoch 65, Batch: 0, Loss: 0.673714
Train - Epoch 66, Batch: 0, Loss: 0.673187
Train - Epoch 67, Batch: 0, Loss: 0.672814
Train - Epoch 68, Batch: 0, Loss: 0.672690
Train - Epoch 69, Batch: 0, Loss: 0.672341
Train - Epoch 70, Batch: 0, Loss: 0.672185
Train - Epoch 71, Batch: 0, Loss: 0.671872
Train - Epoch 72, Batch: 0, Loss: 0.671399
Train - Epoch 73, Batch: 0, Loss: 0.671202
Train - Epoch 74, Batch: 0, Loss: 0.671270
Train - Epoch 75, Batch: 0, Loss: 0.670709
Train - Epoch 76, Batch: 0, Loss: 0.670830
Train - Epoch 77, Batch: 0, Loss: 0.670159
Train - Epoch 78, Batch: 0, Loss: 0.669789
Train - Epoch 79, Batch: 0, Loss: 0.669566
Train - Epoch 80, Batch: 0, Loss: 0.669337
Train - Epoch 81, Batch: 0, Loss: 0.669171
Train - Epoch 82, Batch: 0, Loss: 0.668824
Train - Epoch 83, Batch: 0, Loss: 0.668771
Train - Epoch 84, Batch: 0, Loss: 0.668354
Train - Epoch 85, Batch: 0, Loss: 0.667856
Train - Epoch 86, Batch: 0, Loss: 0.667946
Train - Epoch 87, Batch: 0, Loss: 0.667309
Train - Epoch 88, Batch: 0, Loss: 0.667473
Train - Epoch 89, Batch: 0, Loss: 0.666594
Train - Epoch 90, Batch: 0, Loss: 0.666654
Train - Epoch 91, Batch: 0, Loss: 0.666772
Train - Epoch 92, Batch: 0, Loss: 0.666468
Train - Epoch 93, Batch: 0, Loss: 0.666114
Train - Epoch 94, Batch: 0, Loss: 0.665677
Train - Epoch 95, Batch: 0, Loss: 0.665388
Train - Epoch 96, Batch: 0, Loss: 0.665285
Train - Epoch 97, Batch: 0, Loss: 0.664958
Train - Epoch 98, Batch: 0, Loss: 0.664398
Train - Epoch 99, Batch: 0, Loss: 0.664262
Train - Epoch 100, Batch: 0, Loss: 0.663985
Train - Epoch 101, Batch: 0, Loss: 0.663873
Train - Epoch 102, Batch: 0, Loss: 0.663463
Train - Epoch 103, Batch: 0, Loss: 0.663163
Train - Epoch 104, Batch: 0, Loss: 0.663105
Train - Epoch 105, Batch: 0, Loss: 0.662951
Train - Epoch 106, Batch: 0, Loss: 0.662693
Train - Epoch 107, Batch: 0, Loss: 0.662010
Train - Epoch 108, Batch: 0, Loss: 0.661879
Train - Epoch 109, Batch: 0, Loss: 0.661547
Train - Epoch 110, Batch: 0, Loss: 0.661451
Train - Epoch 111, Batch: 0, Loss: 0.661286
Train - Epoch 112, Batch: 0, Loss: 0.660953
Train - Epoch 113, Batch: 0, Loss: 0.660675
Train - Epoch 114, Batch: 0, Loss: 0.660562
Train - Epoch 115, Batch: 0, Loss: 0.660457
Train - Epoch 116, Batch: 0, Loss: 0.660032
Train - Epoch 117, Batch: 0, Loss: 0.660012
Train - Epoch 118, Batch: 0, Loss: 0.659556
Train - Epoch 119, Batch: 0, Loss: 0.659308
Train - Epoch 120, Batch: 0, Loss: 0.659224
Train - Epoch 121, Batch: 0, Loss: 0.658693
Train - Epoch 122, Batch: 0, Loss: 0.658406
Train - Epoch 123, Batch: 0, Loss: 0.658331
Train - Epoch 124, Batch: 0, Loss: 0.657930
Train - Epoch 125, Batch: 0, Loss: 0.657865
Train - Epoch 126, Batch: 0, Loss: 0.657734
Train - Epoch 127, Batch: 0, Loss: 0.656832
Train - Epoch 128, Batch: 0, Loss: 0.657033
Train - Epoch 129, Batch: 0, Loss: 0.656759
Train - Epoch 130, Batch: 0, Loss: 0.656399
Train - Epoch 131, Batch: 0, Loss: 0.656307
Train - Epoch 132, Batch: 0, Loss: 0.655877
Train - Epoch 133, Batch: 0, Loss: 0.655543
Train - Epoch 134, Batch: 0, Loss: 0.655501
Train - Epoch 135, Batch: 0, Loss: 0.655264
Train - Epoch 136, Batch: 0, Loss: 0.654961
Train - Epoch 137, Batch: 0, Loss: 0.654698
Train - Epoch 138, Batch: 0, Loss: 0.654973
Train - Epoch 139, Batch: 0, Loss: 0.654428
Train - Epoch 140, Batch: 0, Loss: 0.654181
Train - Epoch 141, Batch: 0, Loss: 0.653884
Train - Epoch 142, Batch: 0, Loss: 0.653591
Train - Epoch 143, Batch: 0, Loss: 0.653794
Train - Epoch 144, Batch: 0, Loss: 0.652918
Train - Epoch 145, Batch: 0, Loss: 0.652814
Train - Epoch 146, Batch: 0, Loss: 0.653143
Train - Epoch 147, Batch: 0, Loss: 0.652126
Train - Epoch 148, Batch: 0, Loss: 0.651972
Train - Epoch 149, Batch: 0, Loss: 0.651994
Train - Epoch 150, Batch: 0, Loss: 0.651969
Train - Epoch 151, Batch: 0, Loss: 0.652000
Train - Epoch 152, Batch: 0, Loss: 0.651232
Train - Epoch 153, Batch: 0, Loss: 0.651068
Train - Epoch 154, Batch: 0, Loss: 0.651232
Train - Epoch 155, Batch: 0, Loss: 0.650847
Train - Epoch 156, Batch: 0, Loss: 0.650546
Train - Epoch 157, Batch: 0, Loss: 0.650196
Train - Epoch 158, Batch: 0, Loss: 0.650118
Train - Epoch 159, Batch: 0, Loss: 0.648956
Train - Epoch 160, Batch: 0, Loss: 0.649359
Train - Epoch 161, Batch: 0, Loss: 0.649219
Train - Epoch 162, Batch: 0, Loss: 0.648543
Train - Epoch 163, Batch: 0, Loss: 0.648520
Train - Epoch 164, Batch: 0, Loss: 0.648644
Train - Epoch 165, Batch: 0, Loss: 0.648376
Train - Epoch 166, Batch: 0, Loss: 0.648108
Train - Epoch 167, Batch: 0, Loss: 0.647886
Train - Epoch 168, Batch: 0, Loss: 0.647393
Train - Epoch 169, Batch: 0, Loss: 0.647381
Train - Epoch 170, Batch: 0, Loss: 0.646784
Train - Epoch 171, Batch: 0, Loss: 0.646880
Train - Epoch 172, Batch: 0, Loss: 0.647242
Train - Epoch 173, Batch: 0, Loss: 0.646108
Train - Epoch 174, Batch: 0, Loss: 0.646506
Train - Epoch 175, Batch: 0, Loss: 0.646013
Train - Epoch 176, Batch: 0, Loss: 0.646152
Train - Epoch 177, Batch: 0, Loss: 0.646123
Train - Epoch 178, Batch: 0, Loss: 0.645177
Train - Epoch 179, Batch: 0, Loss: 0.645066
Train - Epoch 180, Batch: 0, Loss: 0.644850
Train - Epoch 181, Batch: 0, Loss: 0.644830
Train - Epoch 182, Batch: 0, Loss: 0.644484/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 183, Batch: 0, Loss: 0.644558
Train - Epoch 184, Batch: 0, Loss: 0.643890
Train - Epoch 185, Batch: 0, Loss: 0.643672
Train - Epoch 186, Batch: 0, Loss: 0.643616
Train - Epoch 187, Batch: 0, Loss: 0.643628
Train - Epoch 188, Batch: 0, Loss: 0.642923
Train - Epoch 189, Batch: 0, Loss: 0.642596
Train - Epoch 190, Batch: 0, Loss: 0.642646
Train - Epoch 191, Batch: 0, Loss: 0.642417
Train - Epoch 192, Batch: 0, Loss: 0.642820
Train - Epoch 193, Batch: 0, Loss: 0.641617
Train - Epoch 194, Batch: 0, Loss: 0.641329
Train - Epoch 195, Batch: 0, Loss: 0.641422
Train - Epoch 196, Batch: 0, Loss: 0.641401
Train - Epoch 197, Batch: 0, Loss: 0.641143
Train - Epoch 198, Batch: 0, Loss: 0.640983
Train - Epoch 199, Batch: 0, Loss: 0.640393
training_time:: 350.7153902053833
training time full:: 350.71543407440186
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000063, Accuracy: 0.925995
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 202
max_epoch:: 200
batch_size:: 10200
epoch:: 0
tensor([  1,   2,   5,   6,   7,   8,  13,  15,  16,  17,  19,  21,  22,  24,
         26,  37,  40,  42,  44,  45,  46,  48,  49,  53,  55,  56,  61,  62,
         68,  69,  70,  76,  77,  79,  81,  82,  85,  88,  89,  91,  95,  98,
        101, 102, 103, 105, 108, 111, 113, 114])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
epoch:: 60
epoch:: 61
epoch:: 62
epoch:: 63
epoch:: 64
epoch:: 65
epoch:: 66
epoch:: 67
epoch:: 68
epoch:: 69
epoch:: 70
epoch:: 71
epoch:: 72
epoch:: 73
epoch:: 74
epoch:: 75
epoch:: 76
epoch:: 77
epoch:: 78
epoch:: 79
epoch:: 80
epoch:: 81
epoch:: 82
epoch:: 83
epoch:: 84
epoch:: 85
epoch:: 86
epoch:: 87
epoch:: 88
epoch:: 89
epoch:: 90
epoch:: 91
epoch:: 92
epoch:: 93
epoch:: 94
epoch:: 95
epoch:: 96
epoch:: 97
epoch:: 98
epoch:: 99
epoch:: 100
epoch:: 101
epoch:: 102
epoch:: 103
epoch:: 104
epoch:: 105
epoch:: 106
epoch:: 107
epoch:: 108
epoch:: 109
epoch:: 110
epoch:: 111
epoch:: 112
epoch:: 113
epoch:: 114
epoch:: 115
epoch:: 116
epoch:: 117
epoch:: 118
epoch:: 119
epoch:: 120
epoch:: 121
epoch:: 122
epoch:: 123
epoch:: 124
epoch:: 125
epoch:: 126
epoch:: 127
epoch:: 128
epoch:: 129
epoch:: 130
epoch:: 131
epoch:: 132
epoch:: 133
epoch:: 134
epoch:: 135
epoch:: 136
epoch:: 137
epoch:: 138
epoch:: 139
epoch:: 140
epoch:: 141
epoch:: 142
epoch:: 143
epoch:: 144
epoch:: 145
epoch:: 146
epoch:: 147
epoch:: 148
epoch:: 149
epoch:: 150
epoch:: 151
epoch:: 152
epoch:: 153
epoch:: 154
epoch:: 155
epoch:: 156
epoch:: 157
epoch:: 158
epoch:: 159
epoch:: 160
epoch:: 161
epoch:: 162
epoch:: 163
epoch:: 164
epoch:: 165
epoch:: 166
epoch:: 167
epoch:: 168
epoch:: 169
epoch:: 170
epoch:: 171
epoch:: 172
epoch:: 173
epoch:: 174
epoch:: 175
epoch:: 176
epoch:: 177
epoch:: 178
epoch:: 179
epoch:: 180
epoch:: 181
epoch:: 182
epoch:: 183
epoch:: 184
epoch:: 185
epoch:: 186
epoch:: 187
epoch:: 188
epoch:: 189
epoch:: 190
epoch:: 191
epoch:: 192
epoch:: 193
epoch:: 194
epoch:: 195
epoch:: 196
epoch:: 197
epoch:: 198
epoch:: 199
training time is 273.36422395706177
overhead:: 0
overhead2:: 0.3988220691680908
overhead3:: 0
time_baseline:: 273.36426186561584
curr_diff: 0 tensor(0.1319, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1319, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9971, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000063, Accuracy: 0.927725
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 202
max_epoch:: 200
overhead:: 0
overhead2:: 0.021778106689453125
overhead3:: 0.1197214126586914
overhead4:: 32.13325548171997
overhead5:: 0
memory usage:: 26658443264
time_provenance:: 47.524758100509644
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1318, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1318, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9972, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000063, Accuracy: 0.927725
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 202
max_epoch:: 200
overhead:: 0
overhead2:: 0.025542259216308594
overhead3:: 0.1322627067565918
overhead4:: 37.96351623535156
overhead5:: 0
memory usage:: 26652303360
time_provenance:: 55.07406306266785
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1318, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1318, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9972, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000063, Accuracy: 0.927725
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 202
max_epoch:: 200
overhead:: 0
overhead2:: 0.028920412063598633
overhead3:: 0.14034581184387207
overhead4:: 43.75731444358826
overhead5:: 0
memory usage:: 26632904704
time_provenance:: 62.6657440662384
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1318, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1318, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9972, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000063, Accuracy: 0.927725
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 202
max_epoch:: 200
overhead:: 0
overhead2:: 0.03998255729675293
overhead3:: 0.1618189811706543
overhead4:: 57.55215048789978
overhead5:: 0
memory usage:: 26634473472
time_provenance:: 80.66700863838196
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1319, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1319, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9972, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000063, Accuracy: 0.927725
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 202
max_epoch:: 200
overhead:: 0
overhead2:: 0.04042410850524902
overhead3:: 0.1578047275543213
overhead4:: 63.13412022590637
overhead5:: 0
memory usage:: 26642718720
time_provenance:: 87.80660104751587
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1319, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1319, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9972, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000063, Accuracy: 0.927725
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 202
max_epoch:: 200
overhead:: 0
overhead2:: 0.04565238952636719
overhead3:: 0.16921019554138184
overhead4:: 68.89163279533386
overhead5:: 0
memory usage:: 26648166400
time_provenance:: 95.05551838874817
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1319, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1319, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9972, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000063, Accuracy: 0.927725
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 202
max_epoch:: 200
overhead:: 0
overhead2:: 0.09378600120544434
overhead3:: 0.27628254890441895
overhead4:: 138.21587109565735
overhead5:: 0
memory usage:: 26639892480
time_provenance:: 184.60623335838318
curr_diff: 0 tensor(8.9938e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.9938e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1319, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1319, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9971, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000063, Accuracy: 0.927725
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 202
max_epoch:: 200
overhead:: 0
overhead2:: 0.09352850914001465
overhead3:: 0.27834200859069824
overhead4:: 140.97005558013916
overhead5:: 0
memory usage:: 26653966336
time_provenance:: 188.2568600177765
curr_diff: 0 tensor(8.9755e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.9755e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1319, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1319, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9971, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000063, Accuracy: 0.927725
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 202
max_epoch:: 200
overhead:: 0
overhead2:: 0.09694790840148926
overhead3:: 0.28710222244262695
overhead4:: 143.41919493675232
overhead5:: 0
memory usage:: 26638569472
time_provenance:: 191.69854640960693
curr_diff: 0 tensor(9.0021e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.0021e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1319, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1319, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9971, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000063, Accuracy: 0.927725
adding noise deletion rate:: 0.02
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
tensor([0.5234, 0.5606, 0.5147,  ..., 0.5272, 0.5123, 0.5210],
       dtype=torch.float64, grad_fn=<MaxBackward0>)
tensor(0.8685, dtype=torch.float64) tensor(0., dtype=torch.float64)
Train - Epoch 0, Batch: 0, Loss: 0.693284
Train - Epoch 1, Batch: 0, Loss: 0.693022
Train - Epoch 2, Batch: 0, Loss: 0.692786
Train - Epoch 3, Batch: 0, Loss: 0.692530
Train - Epoch 4, Batch: 0, Loss: 0.692234
Train - Epoch 5, Batch: 0, Loss: 0.692009
Train - Epoch 6, Batch: 0, Loss: 0.691763
Train - Epoch 7, Batch: 0, Loss: 0.691495
Train - Epoch 8, Batch: 0, Loss: 0.691237
Train - Epoch 9, Batch: 0, Loss: 0.691044
Train - Epoch 10, Batch: 0, Loss: 0.690700
Train - Epoch 11, Batch: 0, Loss: 0.690460
Train - Epoch 12, Batch: 0, Loss: 0.690257
Train - Epoch 13, Batch: 0, Loss: 0.689940
Train - Epoch 14, Batch: 0, Loss: 0.689766
Train - Epoch 15, Batch: 0, Loss: 0.689493
Train - Epoch 16, Batch: 0, Loss: 0.689284
Train - Epoch 17, Batch: 0, Loss: 0.688938
Train - Epoch 18, Batch: 0, Loss: 0.688752
Train - Epoch 19, Batch: 0, Loss: 0.688472
Train - Epoch 20, Batch: 0, Loss: 0.688280
Train - Epoch 21, Batch: 0, Loss: 0.687980
Train - Epoch 22, Batch: 0, Loss: 0.687723
Train - Epoch 23, Batch: 0, Loss: 0.687488
Train - Epoch 24, Batch: 0, Loss: 0.687286
Train - Epoch 25, Batch: 0, Loss: 0.686957
Train - Epoch 26, Batch: 0, Loss: 0.686787
Train - Epoch 27, Batch: 0, Loss: 0.686454
Train - Epoch 28, Batch: 0, Loss: 0.686239
Train - Epoch 29, Batch: 0, Loss: 0.686148
Train - Epoch 30, Batch: 0, Loss: 0.685774
Train - Epoch 31, Batch: 0, Loss: 0.685479
Train - Epoch 32, Batch: 0, Loss: 0.685269
Train - Epoch 33, Batch: 0, Loss: 0.685102
Train - Epoch 34, Batch: 0, Loss: 0.684789
Train - Epoch 35, Batch: 0, Loss: 0.684557
Train - Epoch 36, Batch: 0, Loss: 0.684285
Train - Epoch 37, Batch: 0, Loss: 0.684056
Train - Epoch 38, Batch: 0, Loss: 0.683785
Train - Epoch 39, Batch: 0, Loss: 0.683611
Train - Epoch 40, Batch: 0, Loss: 0.683328
Train - Epoch 41, Batch: 0, Loss: 0.683287
Train - Epoch 42, Batch: 0, Loss: 0.683005
Train - Epoch 43, Batch: 0, Loss: 0.682765
Train - Epoch 44, Batch: 0, Loss: 0.682402
Train - Epoch 45, Batch: 0, Loss: 0.682190
Train - Epoch 46, Batch: 0, Loss: 0.681851
Train - Epoch 47, Batch: 0, Loss: 0.681714
Train - Epoch 48, Batch: 0, Loss: 0.681519
Train - Epoch 49, Batch: 0, Loss: 0.681247
Train - Epoch 50, Batch: 0, Loss: 0.680961
Train - Epoch 51, Batch: 0, Loss: 0.680752
Train - Epoch 52, Batch: 0, Loss: 0.680678
Train - Epoch 53, Batch: 0, Loss: 0.680326
Train - Epoch 54, Batch: 0, Loss: 0.679914
Train - Epoch 55, Batch: 0, Loss: 0.679953
Train - Epoch 56, Batch: 0, Loss: 0.679618
Train - Epoch 57, Batch: 0, Loss: 0.679461
Train - Epoch 58, Batch: 0, Loss: 0.679135
Train - Epoch 59, Batch: 0, Loss: 0.678833
Train - Epoch 60, Batch: 0, Loss: 0.678755
Train - Epoch 61, Batch: 0, Loss: 0.678346
Train - Epoch 62, Batch: 0, Loss: 0.678283
Train - Epoch 63, Batch: 0, Loss: 0.677897
Train - Epoch 64, Batch: 0, Loss: 0.677700
Train - Epoch 65, Batch: 0, Loss: 0.677757
Train - Epoch 66, Batch: 0, Loss: 0.677362
Train - Epoch 67, Batch: 0, Loss: 0.676934
Train - Epoch 68, Batch: 0, Loss: 0.676718
Train - Epoch 69, Batch: 0, Loss: 0.676470
Train - Epoch 70, Batch: 0, Loss: 0.676369
Train - Epoch 71, Batch: 0, Loss: 0.675997
Train - Epoch 72, Batch: 0, Loss: 0.675842
Train - Epoch 73, Batch: 0, Loss: 0.675697
Train - Epoch 74, Batch: 0, Loss: 0.675627
Train - Epoch 75, Batch: 0, Loss: 0.675376
Train - Epoch 76, Batch: 0, Loss: 0.675175
Train - Epoch 77, Batch: 0, Loss: 0.675008
Train - Epoch 78, Batch: 0, Loss: 0.674384
Train - Epoch 79, Batch: 0, Loss: 0.674488
Train - Epoch 80, Batch: 0, Loss: 0.674172
Train - Epoch 81, Batch: 0, Loss: 0.673763
Train - Epoch 82, Batch: 0, Loss: 0.673718
Train - Epoch 83, Batch: 0, Loss: 0.673461
Train - Epoch 84, Batch: 0, Loss: 0.673288
Train - Epoch 85, Batch: 0, Loss: 0.673129
Train - Epoch 86, Batch: 0, Loss: 0.673066
Train - Epoch 87, Batch: 0, Loss: 0.672491
Train - Epoch 88, Batch: 0, Loss: 0.672566
Train - Epoch 89, Batch: 0, Loss: 0.671857
Train - Epoch 90, Batch: 0, Loss: 0.672098
Train - Epoch 91, Batch: 0, Loss: 0.671861
Train - Epoch 92, Batch: 0, Loss: 0.671727
Train - Epoch 93, Batch: 0, Loss: 0.671348
Train - Epoch 94, Batch: 0, Loss: 0.671280
Train - Epoch 95, Batch: 0, Loss: 0.671040
Train - Epoch 96, Batch: 0, Loss: 0.670751
Train - Epoch 97, Batch: 0, Loss: 0.670449
Train - Epoch 98, Batch: 0, Loss: 0.670134
Train - Epoch 99, Batch: 0, Loss: 0.669920
Train - Epoch 100, Batch: 0, Loss: 0.669622
Train - Epoch 101, Batch: 0, Loss: 0.669650
Train - Epoch 102, Batch: 0, Loss: 0.669356
Train - Epoch 103, Batch: 0, Loss: 0.669266
Train - Epoch 104, Batch: 0, Loss: 0.668910
Train - Epoch 105, Batch: 0, Loss: 0.668837
Train - Epoch 106, Batch: 0, Loss: 0.668823
Train - Epoch 107, Batch: 0, Loss: 0.668490
Train - Epoch 108, Batch: 0, Loss: 0.668194
Train - Epoch 109, Batch: 0, Loss: 0.667826
Train - Epoch 110, Batch: 0, Loss: 0.667845
Train - Epoch 111, Batch: 0, Loss: 0.667435
Train - Epoch 112, Batch: 0, Loss: 0.667375
Train - Epoch 113, Batch: 0, Loss: 0.667098
Train - Epoch 114, Batch: 0, Loss: 0.667137
Train - Epoch 115, Batch: 0, Loss: 0.666817
Train - Epoch 116, Batch: 0, Loss: 0.666703
Train - Epoch 117, Batch: 0, Loss: 0.666505
Train - Epoch 118, Batch: 0, Loss: 0.666057
Train - Epoch 119, Batch: 0, Loss: 0.666168
Train - Epoch 120, Batch: 0, Loss: 0.665832
Train - Epoch 121, Batch: 0, Loss: 0.665359
Train - Epoch 122, Batch: 0, Loss: 0.665299
Train - Epoch 123, Batch: 0, Loss: 0.665569
Train - Epoch 124, Batch: 0, Loss: 0.664987
Train - Epoch 125, Batch: 0, Loss: 0.664748
Train - Epoch 126, Batch: 0, Loss: 0.664624
Train - Epoch 127, Batch: 0, Loss: 0.664276
Train - Epoch 128, Batch: 0, Loss: 0.663886
Train - Epoch 129, Batch: 0, Loss: 0.664120
Train - Epoch 130, Batch: 0, Loss: 0.663860
Train - Epoch 131, Batch: 0, Loss: 0.663923
Train - Epoch 132, Batch: 0, Loss: 0.663354
Train - Epoch 133, Batch: 0, Loss: 0.662944
Train - Epoch 134, Batch: 0, Loss: 0.662645
Train - Epoch 135, Batch: 0, Loss: 0.662557
Train - Epoch 136, Batch: 0, Loss: 0.662613
Train - Epoch 137, Batch: 0, Loss: 0.662393
Train - Epoch 138, Batch: 0, Loss: 0.662526
Train - Epoch 139, Batch: 0, Loss: 0.662516
Train - Epoch 140, Batch: 0, Loss: 0.661844
Train - Epoch 141, Batch: 0, Loss: 0.661723
Train - Epoch 142, Batch: 0, Loss: 0.661152
Train - Epoch 143, Batch: 0, Loss: 0.661501
Train - Epoch 144, Batch: 0, Loss: 0.660779
Train - Epoch 145, Batch: 0, Loss: 0.660981
Train - Epoch 146, Batch: 0, Loss: 0.660786
Train - Epoch 147, Batch: 0, Loss: 0.660586
Train - Epoch 148, Batch: 0, Loss: 0.660115
Train - Epoch 149, Batch: 0, Loss: 0.660097
Train - Epoch 150, Batch: 0, Loss: 0.660080
Train - Epoch 151, Batch: 0, Loss: 0.659897
Train - Epoch 152, Batch: 0, Loss: 0.659325
Train - Epoch 153, Batch: 0, Loss: 0.659136
Train - Epoch 154, Batch: 0, Loss: 0.659566
Train - Epoch 155, Batch: 0, Loss: 0.658890
Train - Epoch 156, Batch: 0, Loss: 0.659050
Train - Epoch 157, Batch: 0, Loss: 0.658616
Train - Epoch 158, Batch: 0, Loss: 0.658358
Train - Epoch 159, Batch: 0, Loss: 0.657856
Train - Epoch 160, Batch: 0, Loss: 0.657892
Train - Epoch 161, Batch: 0, Loss: 0.657563
Train - Epoch 162, Batch: 0, Loss: 0.657488
Train - Epoch 163, Batch: 0, Loss: 0.657359
Train - Epoch 164, Batch: 0, Loss: 0.657193
Train - Epoch 165, Batch: 0, Loss: 0.657104
Train - Epoch 166, Batch: 0, Loss: 0.657302
Train - Epoch 167, Batch: 0, Loss: 0.656957
Train - Epoch 168, Batch: 0, Loss: 0.656563
Train - Epoch 169, Batch: 0, Loss: 0.656255
Train - Epoch 170, Batch: 0, Loss: 0.655804
Train - Epoch 171, Batch: 0, Loss: 0.655945
Train - Epoch 172, Batch: 0, Loss: 0.655929
Train - Epoch 173, Batch: 0, Loss: 0.655363
Train - Epoch 174, Batch: 0, Loss: 0.655266
Train - Epoch 175, Batch: 0, Loss: 0.655232
Train - Epoch 176, Batch: 0, Loss: 0.655113
Train - Epoch 177, Batch: 0, Loss: 0.655413
Train - Epoch 178, Batch: 0, Loss: 0.654519
Train - Epoch 179, Batch: 0, Loss: 0.654345
Train - Epoch 180, Batch: 0, Loss: 0.654538
Train - Epoch 181, Batch: 0, Loss: 0.654159
Train - Epoch 182, Batch: 0, Loss: 0.654128/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 183, Batch: 0, Loss: 0.653978
Train - Epoch 184, Batch: 0, Loss: 0.653329
Train - Epoch 185, Batch: 0, Loss: 0.652901
Train - Epoch 186, Batch: 0, Loss: 0.653048
Train - Epoch 187, Batch: 0, Loss: 0.653314
Train - Epoch 188, Batch: 0, Loss: 0.652603
Train - Epoch 189, Batch: 0, Loss: 0.652720
Train - Epoch 190, Batch: 0, Loss: 0.652570
Train - Epoch 191, Batch: 0, Loss: 0.652197
Train - Epoch 192, Batch: 0, Loss: 0.652503
Train - Epoch 193, Batch: 0, Loss: 0.651393
Train - Epoch 194, Batch: 0, Loss: 0.652005
Train - Epoch 195, Batch: 0, Loss: 0.651902
Train - Epoch 196, Batch: 0, Loss: 0.651442
Train - Epoch 197, Batch: 0, Loss: 0.651373
Train - Epoch 198, Batch: 0, Loss: 0.651240
Train - Epoch 199, Batch: 0, Loss: 0.650703
training_time:: 350.46607756614685
training time full:: 350.46612000465393
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000064, Accuracy: 0.922093
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 404
max_epoch:: 200
batch_size:: 10200
epoch:: 0
tensor([  1,   2,   5,   6,   7,   8,  13,  15,  16,  17,  19,  21,  22,  24,
         26,  37,  40,  42,  44,  45,  46,  48,  49,  53,  55,  56,  61,  68,
         69,  70,  76,  77,  79,  81,  82,  85,  88,  89,  95,  98, 101, 102,
        103, 105, 108, 111, 113, 114, 118, 122])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
epoch:: 60
epoch:: 61
epoch:: 62
epoch:: 63
epoch:: 64
epoch:: 65
epoch:: 66
epoch:: 67
epoch:: 68
epoch:: 69
epoch:: 70
epoch:: 71
epoch:: 72
epoch:: 73
epoch:: 74
epoch:: 75
epoch:: 76
epoch:: 77
epoch:: 78
epoch:: 79
epoch:: 80
epoch:: 81
epoch:: 82
epoch:: 83
epoch:: 84
epoch:: 85
epoch:: 86
epoch:: 87
epoch:: 88
epoch:: 89
epoch:: 90
epoch:: 91
epoch:: 92
epoch:: 93
epoch:: 94
epoch:: 95
epoch:: 96
epoch:: 97
epoch:: 98
epoch:: 99
epoch:: 100
epoch:: 101
epoch:: 102
epoch:: 103
epoch:: 104
epoch:: 105
epoch:: 106
epoch:: 107
epoch:: 108
epoch:: 109
epoch:: 110
epoch:: 111
epoch:: 112
epoch:: 113
epoch:: 114
epoch:: 115
epoch:: 116
epoch:: 117
epoch:: 118
epoch:: 119
epoch:: 120
epoch:: 121
epoch:: 122
epoch:: 123
epoch:: 124
epoch:: 125
epoch:: 126
epoch:: 127
epoch:: 128
epoch:: 129
epoch:: 130
epoch:: 131
epoch:: 132
epoch:: 133
epoch:: 134
epoch:: 135
epoch:: 136
epoch:: 137
epoch:: 138
epoch:: 139
epoch:: 140
epoch:: 141
epoch:: 142
epoch:: 143
epoch:: 144
epoch:: 145
epoch:: 146
epoch:: 147
epoch:: 148
epoch:: 149
epoch:: 150
epoch:: 151
epoch:: 152
epoch:: 153
epoch:: 154
epoch:: 155
epoch:: 156
epoch:: 157
epoch:: 158
epoch:: 159
epoch:: 160
epoch:: 161
epoch:: 162
epoch:: 163
epoch:: 164
epoch:: 165
epoch:: 166
epoch:: 167
epoch:: 168
epoch:: 169
epoch:: 170
epoch:: 171
epoch:: 172
epoch:: 173
epoch:: 174
epoch:: 175
epoch:: 176
epoch:: 177
epoch:: 178
epoch:: 179
epoch:: 180
epoch:: 181
epoch:: 182
epoch:: 183
epoch:: 184
epoch:: 185
epoch:: 186
epoch:: 187
epoch:: 188
epoch:: 189
epoch:: 190
epoch:: 191
epoch:: 192
epoch:: 193
epoch:: 194
epoch:: 195
epoch:: 196
epoch:: 197
epoch:: 198
epoch:: 199
training time is 271.2760224342346
overhead:: 0
overhead2:: 0.425187349319458
overhead3:: 0
time_baseline:: 271.2761073112488
curr_diff: 0 tensor(0.1768, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1768, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9941, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000063, Accuracy: 0.926242
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 0 0.02 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 404
max_epoch:: 200
overhead:: 0
overhead2:: 0.020244121551513672
overhead3:: 0.12115478515625
overhead4:: 32.00218391418457
overhead5:: 0
memory usage:: 26615799808
time_provenance:: 50.496556520462036
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1764, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1764, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9942, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000063, Accuracy: 0.926242
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 0 0.02 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 404
max_epoch:: 200
overhead:: 0
overhead2:: 0.02462291717529297
overhead3:: 0.12658452987670898
overhead4:: 37.68709993362427
overhead5:: 0
memory usage:: 26597965824
time_provenance:: 57.79022145271301
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1764, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1764, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9942, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000063, Accuracy: 0.926242
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 0 0.02 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 404
max_epoch:: 200
overhead:: 0
overhead2:: 0.028543472290039062
overhead3:: 0.14063429832458496
overhead4:: 43.597890853881836
overhead5:: 0
memory usage:: 26621239296
time_provenance:: 65.462486743927
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1764, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1764, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9942, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000063, Accuracy: 0.926242
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 0 0.02 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 404
max_epoch:: 200
overhead:: 0
overhead2:: 0.041085004806518555
overhead3:: 0.16223955154418945
overhead4:: 57.775651931762695
overhead5:: 0
memory usage:: 26600263680
time_provenance:: 83.83620190620422
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1765, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1765, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9942, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000063, Accuracy: 0.926242
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 0 0.02 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 404
max_epoch:: 200
overhead:: 0
overhead2:: 0.042978763580322266
overhead3:: 0.16293716430664062
overhead4:: 62.611567974090576
overhead5:: 0
memory usage:: 26599071744
time_provenance:: 90.15751123428345
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1765, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1765, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9942, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000063, Accuracy: 0.926242
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 0 0.02 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 404
max_epoch:: 200
overhead:: 0
overhead2:: 0.044376373291015625
overhead3:: 0.17314815521240234
overhead4:: 67.63960099220276
overhead5:: 0
memory usage:: 26605735936
time_provenance:: 96.6635332107544
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1765, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1765, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9942, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000063, Accuracy: 0.926242
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 0 0.02 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 404
max_epoch:: 200
overhead:: 0
overhead2:: 0.0931997299194336
overhead3:: 0.27486586570739746
overhead4:: 135.48148488998413
overhead5:: 0
memory usage:: 26600972288
time_provenance:: 184.8817594051361
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1765, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1765, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9942, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000063, Accuracy: 0.926242
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 0 0.02 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 404
max_epoch:: 200
overhead:: 0
overhead2:: 0.09278035163879395
overhead3:: 0.27779197692871094
overhead4:: 138.51549816131592
overhead5:: 0
memory usage:: 26603655168
time_provenance:: 188.8221447467804
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1765, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1765, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9942, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000063, Accuracy: 0.926242
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 0 0.02 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 404
max_epoch:: 200
overhead:: 0
overhead2:: 0.10021734237670898
overhead3:: 0.28757596015930176
overhead4:: 141.41823410987854
overhead5:: 0
memory usage:: 26602725376
time_provenance:: 192.40205216407776
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1766, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1766, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9942, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000063, Accuracy: 0.926242
adding noise deletion rate:: 0.04
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
tensor([0.5192, 0.5481, 0.5134,  ..., 0.5268, 0.5081, 0.5187],
       dtype=torch.float64, grad_fn=<MaxBackward0>)
tensor(0.8685, dtype=torch.float64) tensor(0., dtype=torch.float64)
Train - Epoch 0, Batch: 0, Loss: 0.693124
Train - Epoch 1, Batch: 0, Loss: 0.692953
Train - Epoch 2, Batch: 0, Loss: 0.692763
Train - Epoch 3, Batch: 0, Loss: 0.692578
Train - Epoch 4, Batch: 0, Loss: 0.692422
Train - Epoch 5, Batch: 0, Loss: 0.692238
Train - Epoch 6, Batch: 0, Loss: 0.692043
Train - Epoch 7, Batch: 0, Loss: 0.691874
Train - Epoch 8, Batch: 0, Loss: 0.691700
Train - Epoch 9, Batch: 0, Loss: 0.691540
Train - Epoch 10, Batch: 0, Loss: 0.691334
Train - Epoch 11, Batch: 0, Loss: 0.691181
Train - Epoch 12, Batch: 0, Loss: 0.691002
Train - Epoch 13, Batch: 0, Loss: 0.690822
Train - Epoch 14, Batch: 0, Loss: 0.690690
Train - Epoch 15, Batch: 0, Loss: 0.690462
Train - Epoch 16, Batch: 0, Loss: 0.690368
Train - Epoch 17, Batch: 0, Loss: 0.690094
Train - Epoch 18, Batch: 0, Loss: 0.689958
Train - Epoch 19, Batch: 0, Loss: 0.689789
Train - Epoch 20, Batch: 0, Loss: 0.689693
Train - Epoch 21, Batch: 0, Loss: 0.689403
Train - Epoch 22, Batch: 0, Loss: 0.689199
Train - Epoch 23, Batch: 0, Loss: 0.689069
Train - Epoch 24, Batch: 0, Loss: 0.689045
Train - Epoch 25, Batch: 0, Loss: 0.688768
Train - Epoch 26, Batch: 0, Loss: 0.688631
Train - Epoch 27, Batch: 0, Loss: 0.688406
Train - Epoch 28, Batch: 0, Loss: 0.688245
Train - Epoch 29, Batch: 0, Loss: 0.688123
Train - Epoch 30, Batch: 0, Loss: 0.687932
Train - Epoch 31, Batch: 0, Loss: 0.687697
Train - Epoch 32, Batch: 0, Loss: 0.687571
Train - Epoch 33, Batch: 0, Loss: 0.687389
Train - Epoch 34, Batch: 0, Loss: 0.687251
Train - Epoch 35, Batch: 0, Loss: 0.687247
Train - Epoch 36, Batch: 0, Loss: 0.686888
Train - Epoch 37, Batch: 0, Loss: 0.686789
Train - Epoch 38, Batch: 0, Loss: 0.686521
Train - Epoch 39, Batch: 0, Loss: 0.686447
Train - Epoch 40, Batch: 0, Loss: 0.686113
Train - Epoch 41, Batch: 0, Loss: 0.686161
Train - Epoch 42, Batch: 0, Loss: 0.685981
Train - Epoch 43, Batch: 0, Loss: 0.685756
Train - Epoch 44, Batch: 0, Loss: 0.685585
Train - Epoch 45, Batch: 0, Loss: 0.685441
Train - Epoch 46, Batch: 0, Loss: 0.685171
Train - Epoch 47, Batch: 0, Loss: 0.685030
Train - Epoch 48, Batch: 0, Loss: 0.684992
Train - Epoch 49, Batch: 0, Loss: 0.684623
Train - Epoch 50, Batch: 0, Loss: 0.684557
Train - Epoch 51, Batch: 0, Loss: 0.684507
Train - Epoch 52, Batch: 0, Loss: 0.684352
Train - Epoch 53, Batch: 0, Loss: 0.684224
Train - Epoch 54, Batch: 0, Loss: 0.683988
Train - Epoch 55, Batch: 0, Loss: 0.683855
Train - Epoch 56, Batch: 0, Loss: 0.683502
Train - Epoch 57, Batch: 0, Loss: 0.683506
Train - Epoch 58, Batch: 0, Loss: 0.683494
Train - Epoch 59, Batch: 0, Loss: 0.683373
Train - Epoch 60, Batch: 0, Loss: 0.683021
Train - Epoch 61, Batch: 0, Loss: 0.682834
Train - Epoch 62, Batch: 0, Loss: 0.682870
Train - Epoch 63, Batch: 0, Loss: 0.682411
Train - Epoch 64, Batch: 0, Loss: 0.682482
Train - Epoch 65, Batch: 0, Loss: 0.682335
Train - Epoch 66, Batch: 0, Loss: 0.681986
Train - Epoch 67, Batch: 0, Loss: 0.681924
Train - Epoch 68, Batch: 0, Loss: 0.681718
Train - Epoch 69, Batch: 0, Loss: 0.681464
Train - Epoch 70, Batch: 0, Loss: 0.681395
Train - Epoch 71, Batch: 0, Loss: 0.681364
Train - Epoch 72, Batch: 0, Loss: 0.681054
Train - Epoch 73, Batch: 0, Loss: 0.680889
Train - Epoch 74, Batch: 0, Loss: 0.680853
Train - Epoch 75, Batch: 0, Loss: 0.680791
Train - Epoch 76, Batch: 0, Loss: 0.680355
Train - Epoch 77, Batch: 0, Loss: 0.680530
Train - Epoch 78, Batch: 0, Loss: 0.680300
Train - Epoch 79, Batch: 0, Loss: 0.680183
Train - Epoch 80, Batch: 0, Loss: 0.679978
Train - Epoch 81, Batch: 0, Loss: 0.679642
Train - Epoch 82, Batch: 0, Loss: 0.679612
Train - Epoch 83, Batch: 0, Loss: 0.679424
Train - Epoch 84, Batch: 0, Loss: 0.679337
Train - Epoch 85, Batch: 0, Loss: 0.679119
Train - Epoch 86, Batch: 0, Loss: 0.679035
Train - Epoch 87, Batch: 0, Loss: 0.678775
Train - Epoch 88, Batch: 0, Loss: 0.678834
Train - Epoch 89, Batch: 0, Loss: 0.678375
Train - Epoch 90, Batch: 0, Loss: 0.678412
Train - Epoch 91, Batch: 0, Loss: 0.677986
Train - Epoch 92, Batch: 0, Loss: 0.678083
Train - Epoch 93, Batch: 0, Loss: 0.678004
Train - Epoch 94, Batch: 0, Loss: 0.678205
Train - Epoch 95, Batch: 0, Loss: 0.677537
Train - Epoch 96, Batch: 0, Loss: 0.677740
Train - Epoch 97, Batch: 0, Loss: 0.677596
Train - Epoch 98, Batch: 0, Loss: 0.676910
Train - Epoch 99, Batch: 0, Loss: 0.677075
Train - Epoch 100, Batch: 0, Loss: 0.676932
Train - Epoch 101, Batch: 0, Loss: 0.676831
Train - Epoch 102, Batch: 0, Loss: 0.676589
Train - Epoch 103, Batch: 0, Loss: 0.676352
Train - Epoch 104, Batch: 0, Loss: 0.676472
Train - Epoch 105, Batch: 0, Loss: 0.676198
Train - Epoch 106, Batch: 0, Loss: 0.676439
Train - Epoch 107, Batch: 0, Loss: 0.675890
Train - Epoch 108, Batch: 0, Loss: 0.675768
Train - Epoch 109, Batch: 0, Loss: 0.675409
Train - Epoch 110, Batch: 0, Loss: 0.675632
Train - Epoch 111, Batch: 0, Loss: 0.675197
Train - Epoch 112, Batch: 0, Loss: 0.675189
Train - Epoch 113, Batch: 0, Loss: 0.674912
Train - Epoch 114, Batch: 0, Loss: 0.674946
Train - Epoch 115, Batch: 0, Loss: 0.674679
Train - Epoch 116, Batch: 0, Loss: 0.674735
Train - Epoch 117, Batch: 0, Loss: 0.674334
Train - Epoch 118, Batch: 0, Loss: 0.674516
Train - Epoch 119, Batch: 0, Loss: 0.674362
Train - Epoch 120, Batch: 0, Loss: 0.673778
Train - Epoch 121, Batch: 0, Loss: 0.673965
Train - Epoch 122, Batch: 0, Loss: 0.673790
Train - Epoch 123, Batch: 0, Loss: 0.673783
Train - Epoch 124, Batch: 0, Loss: 0.673755
Train - Epoch 125, Batch: 0, Loss: 0.673650
Train - Epoch 126, Batch: 0, Loss: 0.673494
Train - Epoch 127, Batch: 0, Loss: 0.673012
Train - Epoch 128, Batch: 0, Loss: 0.672530
Train - Epoch 129, Batch: 0, Loss: 0.672864
Train - Epoch 130, Batch: 0, Loss: 0.673034
Train - Epoch 131, Batch: 0, Loss: 0.672845
Train - Epoch 132, Batch: 0, Loss: 0.672276
Train - Epoch 133, Batch: 0, Loss: 0.671828
Train - Epoch 134, Batch: 0, Loss: 0.671704
Train - Epoch 135, Batch: 0, Loss: 0.672114
Train - Epoch 136, Batch: 0, Loss: 0.671799
Train - Epoch 137, Batch: 0, Loss: 0.671768
Train - Epoch 138, Batch: 0, Loss: 0.672208
Train - Epoch 139, Batch: 0, Loss: 0.671854
Train - Epoch 140, Batch: 0, Loss: 0.671516
Train - Epoch 141, Batch: 0, Loss: 0.671140
Train - Epoch 142, Batch: 0, Loss: 0.670981
Train - Epoch 143, Batch: 0, Loss: 0.670734
Train - Epoch 144, Batch: 0, Loss: 0.671031
Train - Epoch 145, Batch: 0, Loss: 0.670612
Train - Epoch 146, Batch: 0, Loss: 0.670645
Train - Epoch 147, Batch: 0, Loss: 0.670714
Train - Epoch 148, Batch: 0, Loss: 0.670075
Train - Epoch 149, Batch: 0, Loss: 0.670202
Train - Epoch 150, Batch: 0, Loss: 0.670174
Train - Epoch 151, Batch: 0, Loss: 0.670087
Train - Epoch 152, Batch: 0, Loss: 0.669847
Train - Epoch 153, Batch: 0, Loss: 0.669533
Train - Epoch 154, Batch: 0, Loss: 0.669511
Train - Epoch 155, Batch: 0, Loss: 0.669442
Train - Epoch 156, Batch: 0, Loss: 0.669490
Train - Epoch 157, Batch: 0, Loss: 0.669523
Train - Epoch 158, Batch: 0, Loss: 0.668713
Train - Epoch 159, Batch: 0, Loss: 0.668456
Train - Epoch 160, Batch: 0, Loss: 0.668544
Train - Epoch 161, Batch: 0, Loss: 0.668236
Train - Epoch 162, Batch: 0, Loss: 0.668546
Train - Epoch 163, Batch: 0, Loss: 0.667869
Train - Epoch 164, Batch: 0, Loss: 0.668133
Train - Epoch 165, Batch: 0, Loss: 0.668123
Train - Epoch 166, Batch: 0, Loss: 0.667906
Train - Epoch 167, Batch: 0, Loss: 0.667771
Train - Epoch 168, Batch: 0, Loss: 0.667781
Train - Epoch 169, Batch: 0, Loss: 0.667824
Train - Epoch 170, Batch: 0, Loss: 0.667392
Train - Epoch 171, Batch: 0, Loss: 0.667392
Train - Epoch 172, Batch: 0, Loss: 0.667119
Train - Epoch 173, Batch: 0, Loss: 0.666590
Train - Epoch 174, Batch: 0, Loss: 0.666908
Train - Epoch 175, Batch: 0, Loss: 0.666508
Train - Epoch 176, Batch: 0, Loss: 0.666627
Train - Epoch 177, Batch: 0, Loss: 0.666359
Train - Epoch 178, Batch: 0, Loss: 0.666017
Train - Epoch 179, Batch: 0, Loss: 0.666206
Train - Epoch 180, Batch: 0, Loss: 0.666307
Train - Epoch 181, Batch: 0, Loss: 0.665882
Train - Epoch 182, Batch: 0, Loss: 0.666147/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 183, Batch: 0, Loss: 0.666102
Train - Epoch 184, Batch: 0, Loss: 0.665698
Train - Epoch 185, Batch: 0, Loss: 0.665392
Train - Epoch 186, Batch: 0, Loss: 0.665694
Train - Epoch 187, Batch: 0, Loss: 0.665358
Train - Epoch 188, Batch: 0, Loss: 0.665152
Train - Epoch 189, Batch: 0, Loss: 0.665135
Train - Epoch 190, Batch: 0, Loss: 0.664918
Train - Epoch 191, Batch: 0, Loss: 0.664825
Train - Epoch 192, Batch: 0, Loss: 0.664802
Train - Epoch 193, Batch: 0, Loss: 0.663914
Train - Epoch 194, Batch: 0, Loss: 0.664928
Train - Epoch 195, Batch: 0, Loss: 0.664763
Train - Epoch 196, Batch: 0, Loss: 0.663779
Train - Epoch 197, Batch: 0, Loss: 0.663967
Train - Epoch 198, Batch: 0, Loss: 0.663983
Train - Epoch 199, Batch: 0, Loss: 0.663724
training_time:: 350.9366104602814
training time full:: 350.93664836883545
provenance prepare time:: 0.0
here
start test::
Test Avg. Loss: 0.000065, Accuracy: 0.916461
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 809
max_epoch:: 200
batch_size:: 10200
epoch:: 0
tensor([  1,   2,   5,   6,   7,   8,  13,  15,  16,  17,  19,  21,  22,  24,
         26,  37,  40,  42,  44,  45,  46,  48,  49,  53,  55,  56,  61,  62,
         68,  69,  70,  76,  77,  79,  81,  82,  85,  88,  89,  91,  95,  98,
        101, 102, 103, 105, 108, 111, 113, 114])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
epoch:: 60
epoch:: 61
epoch:: 62
epoch:: 63
epoch:: 64
epoch:: 65
epoch:: 66
epoch:: 67
epoch:: 68
epoch:: 69
epoch:: 70
epoch:: 71
epoch:: 72
epoch:: 73
epoch:: 74
epoch:: 75
epoch:: 76
epoch:: 77
epoch:: 78
epoch:: 79
epoch:: 80
epoch:: 81
epoch:: 82
epoch:: 83
epoch:: 84
epoch:: 85
epoch:: 86
epoch:: 87
epoch:: 88
epoch:: 89
epoch:: 90
epoch:: 91
epoch:: 92
epoch:: 93
epoch:: 94
epoch:: 95
epoch:: 96
epoch:: 97
epoch:: 98
epoch:: 99
epoch:: 100
epoch:: 101
epoch:: 102
epoch:: 103
epoch:: 104
epoch:: 105
epoch:: 106
epoch:: 107
epoch:: 108
epoch:: 109
epoch:: 110
epoch:: 111
epoch:: 112
epoch:: 113
epoch:: 114
epoch:: 115
epoch:: 116
epoch:: 117
epoch:: 118
epoch:: 119
epoch:: 120
epoch:: 121
epoch:: 122
epoch:: 123
epoch:: 124
epoch:: 125
epoch:: 126
epoch:: 127
epoch:: 128
epoch:: 129
epoch:: 130
epoch:: 131
epoch:: 132
epoch:: 133
epoch:: 134
epoch:: 135
epoch:: 136
epoch:: 137
epoch:: 138
epoch:: 139
epoch:: 140
epoch:: 141
epoch:: 142
epoch:: 143
epoch:: 144
epoch:: 145
epoch:: 146
epoch:: 147
epoch:: 148
epoch:: 149
epoch:: 150
epoch:: 151
epoch:: 152
epoch:: 153
epoch:: 154
epoch:: 155
epoch:: 156
epoch:: 157
epoch:: 158
epoch:: 159
epoch:: 160
epoch:: 161
epoch:: 162
epoch:: 163
epoch:: 164
epoch:: 165
epoch:: 166
epoch:: 167
epoch:: 168
epoch:: 169
epoch:: 170
epoch:: 171
epoch:: 172
epoch:: 173
epoch:: 174
epoch:: 175
epoch:: 176
epoch:: 177
epoch:: 178
epoch:: 179
epoch:: 180
epoch:: 181
epoch:: 182
epoch:: 183
epoch:: 184
epoch:: 185
epoch:: 186
epoch:: 187
epoch:: 188
epoch:: 189
epoch:: 190
epoch:: 191
epoch:: 192
epoch:: 193
epoch:: 194
epoch:: 195
epoch:: 196
epoch:: 197
epoch:: 198
epoch:: 199
training time is 265.37084579467773
overhead:: 0
overhead2:: 0.4179222583770752
overhead3:: 0
time_baseline:: 265.370897769928
curr_diff: 0 tensor(0.2503, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.2503, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9854, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000064, Accuracy: 0.938346
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 0 0.04 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 809
max_epoch:: 200
overhead:: 0
overhead2:: 0.023279190063476562
overhead3:: 0.12993311882019043
overhead4:: 31.272419452667236
overhead5:: 0
memory usage:: 26614747136
time_provenance:: 55.99458932876587
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.2493, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.2493, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9855, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000064, Accuracy: 0.938395
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 0 0.04 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 809
max_epoch:: 200
overhead:: 0
overhead2:: 0.026947021484375
overhead3:: 0.13965845108032227
overhead4:: 37.45602202415466
overhead5:: 0
memory usage:: 26610044928
time_provenance:: 64.0401222705841
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.2493, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.2493, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9855, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000064, Accuracy: 0.938395
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 0 0.04 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 809
max_epoch:: 200
overhead:: 0
overhead2:: 0.03041863441467285
overhead3:: 0.14572548866271973
overhead4:: 42.933231592178345
overhead5:: 0
memory usage:: 26600968192
time_provenance:: 71.03974056243896
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.2493, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.2493, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9855, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000064, Accuracy: 0.938395
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 0 0.04 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 809
max_epoch:: 200
overhead:: 0
overhead2:: 0.041501522064208984
overhead3:: 0.16942381858825684
overhead4:: 56.04358887672424
overhead5:: 0
memory usage:: 26602725376
time_provenance:: 88.15860843658447
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.2494, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.2494, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9855, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000064, Accuracy: 0.938395
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 0 0.04 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 809
max_epoch:: 200
overhead:: 0
overhead2:: 0.042588233947753906
overhead3:: 0.16987991333007812
overhead4:: 61.306060552597046
overhead5:: 0
memory usage:: 26613141504
time_provenance:: 94.8358166217804
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.2494, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.2494, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9855, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000064, Accuracy: 0.938395
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 0 0.04 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 809
max_epoch:: 200
overhead:: 0
overhead2:: 0.04673480987548828
overhead3:: 0.1796247959136963
overhead4:: 66.32691979408264
overhead5:: 0
memory usage:: 26602971136
time_provenance:: 101.39658570289612
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.2494, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.2494, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9855, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000064, Accuracy: 0.938395
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 0 0.04 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 809
max_epoch:: 200
overhead:: 0
overhead2:: 0.09210038185119629
overhead3:: 0.2822740077972412
overhead4:: 132.46832251548767
overhead5:: 0
memory usage:: 26605854720
time_provenance:: 186.88655948638916
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.2498, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.2498, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9855, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000064, Accuracy: 0.938445
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 0 0.04 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 809
max_epoch:: 200
overhead:: 0
overhead2:: 0.09754657745361328
overhead3:: 0.28786158561706543
overhead4:: 135.61568665504456
overhead5:: 0
memory usage:: 26598719488
time_provenance:: 190.92199158668518
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.2498, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.2498, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9855, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000064, Accuracy: 0.938445
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 0 0.04 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 809
max_epoch:: 200
overhead:: 0
overhead2:: 0.0974891185760498
overhead3:: 0.2919650077819824
overhead4:: 139.1059877872467
overhead5:: 0
memory usage:: 26610991104
time_provenance:: 195.32906913757324
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.2498, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.2498, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9855, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000064, Accuracy: 0.938445
adding noise deletion rate:: 0.05
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
tensor([0.5329, 0.5624, 0.5177,  ..., 0.5072, 0.5179, 0.5106],
       dtype=torch.float64, grad_fn=<MaxBackward0>)
tensor(0.8685, dtype=torch.float64) tensor(0., dtype=torch.float64)
Train - Epoch 0, Batch: 0, Loss: 0.693173
Train - Epoch 1, Batch: 0, Loss: 0.693060
Train - Epoch 2, Batch: 0, Loss: 0.692943
Train - Epoch 3, Batch: 0, Loss: 0.692842
Train - Epoch 4, Batch: 0, Loss: 0.692716
Train - Epoch 5, Batch: 0, Loss: 0.692577
Train - Epoch 6, Batch: 0, Loss: 0.692495
Train - Epoch 7, Batch: 0, Loss: 0.692349
Train - Epoch 8, Batch: 0, Loss: 0.692281
Train - Epoch 9, Batch: 0, Loss: 0.692162
Train - Epoch 10, Batch: 0, Loss: 0.692027
Train - Epoch 11, Batch: 0, Loss: 0.691928
Train - Epoch 12, Batch: 0, Loss: 0.691795
Train - Epoch 13, Batch: 0, Loss: 0.691697
Train - Epoch 14, Batch: 0, Loss: 0.691591
Train - Epoch 15, Batch: 0, Loss: 0.691477
Train - Epoch 16, Batch: 0, Loss: 0.691361
Train - Epoch 17, Batch: 0, Loss: 0.691226
Train - Epoch 18, Batch: 0, Loss: 0.691124
Train - Epoch 19, Batch: 0, Loss: 0.690967
Train - Epoch 20, Batch: 0, Loss: 0.690990
Train - Epoch 21, Batch: 0, Loss: 0.690810
Train - Epoch 22, Batch: 0, Loss: 0.690695
Train - Epoch 23, Batch: 0, Loss: 0.690589
Train - Epoch 24, Batch: 0, Loss: 0.690449
Train - Epoch 25, Batch: 0, Loss: 0.690339
Train - Epoch 26, Batch: 0, Loss: 0.690254
Train - Epoch 27, Batch: 0, Loss: 0.690068
Train - Epoch 28, Batch: 0, Loss: 0.690122
Train - Epoch 29, Batch: 0, Loss: 0.689964
Train - Epoch 30, Batch: 0, Loss: 0.689770
Train - Epoch 31, Batch: 0, Loss: 0.689682
Train - Epoch 32, Batch: 0, Loss: 0.689543
Train - Epoch 33, Batch: 0, Loss: 0.689492
Train - Epoch 34, Batch: 0, Loss: 0.689237
Train - Epoch 35, Batch: 0, Loss: 0.689387
Train - Epoch 36, Batch: 0, Loss: 0.689130
Train - Epoch 37, Batch: 0, Loss: 0.689060
Train - Epoch 38, Batch: 0, Loss: 0.688907
Train - Epoch 39, Batch: 0, Loss: 0.688810
Train - Epoch 40, Batch: 0, Loss: 0.688789
Train - Epoch 41, Batch: 0, Loss: 0.688672
Train - Epoch 42, Batch: 0, Loss: 0.688525
Train - Epoch 43, Batch: 0, Loss: 0.688442
Train - Epoch 44, Batch: 0, Loss: 0.688306
Train - Epoch 45, Batch: 0, Loss: 0.688316
Train - Epoch 46, Batch: 0, Loss: 0.688061
Train - Epoch 47, Batch: 0, Loss: 0.687984
Train - Epoch 48, Batch: 0, Loss: 0.687880
Train - Epoch 49, Batch: 0, Loss: 0.687812
Train - Epoch 50, Batch: 0, Loss: 0.687677
Train - Epoch 51, Batch: 0, Loss: 0.687627
Train - Epoch 52, Batch: 0, Loss: 0.687394
Train - Epoch 53, Batch: 0, Loss: 0.687431
Train - Epoch 54, Batch: 0, Loss: 0.687246
Train - Epoch 55, Batch: 0, Loss: 0.687064
Train - Epoch 56, Batch: 0, Loss: 0.687042
Train - Epoch 57, Batch: 0, Loss: 0.687082
Train - Epoch 58, Batch: 0, Loss: 0.686989
Train - Epoch 59, Batch: 0, Loss: 0.686665
Train - Epoch 60, Batch: 0, Loss: 0.686605
Train - Epoch 61, Batch: 0, Loss: 0.686519
Train - Epoch 62, Batch: 0, Loss: 0.686475
Train - Epoch 63, Batch: 0, Loss: 0.686251
Train - Epoch 64, Batch: 0, Loss: 0.686276
Train - Epoch 65, Batch: 0, Loss: 0.686155
Train - Epoch 66, Batch: 0, Loss: 0.686042
Train - Epoch 67, Batch: 0, Loss: 0.685883
Train - Epoch 68, Batch: 0, Loss: 0.685839
Train - Epoch 69, Batch: 0, Loss: 0.685720
Train - Epoch 70, Batch: 0, Loss: 0.685552
Train - Epoch 71, Batch: 0, Loss: 0.685556
Train - Epoch 72, Batch: 0, Loss: 0.685352
Train - Epoch 73, Batch: 0, Loss: 0.685182
Train - Epoch 74, Batch: 0, Loss: 0.685178
Train - Epoch 75, Batch: 0, Loss: 0.685114
Train - Epoch 76, Batch: 0, Loss: 0.685014
Train - Epoch 77, Batch: 0, Loss: 0.684966
Train - Epoch 78, Batch: 0, Loss: 0.684740
Train - Epoch 79, Batch: 0, Loss: 0.684706
Train - Epoch 80, Batch: 0, Loss: 0.684671
Train - Epoch 81, Batch: 0, Loss: 0.684459
Train - Epoch 82, Batch: 0, Loss: 0.684527
Train - Epoch 83, Batch: 0, Loss: 0.684403
Train - Epoch 84, Batch: 0, Loss: 0.684103
Train - Epoch 85, Batch: 0, Loss: 0.684084
Train - Epoch 86, Batch: 0, Loss: 0.683953
Train - Epoch 87, Batch: 0, Loss: 0.683755
Train - Epoch 88, Batch: 0, Loss: 0.683750
Train - Epoch 89, Batch: 0, Loss: 0.683537
Train - Epoch 90, Batch: 0, Loss: 0.683554
Train - Epoch 91, Batch: 0, Loss: 0.683447
Train - Epoch 92, Batch: 0, Loss: 0.683345
Train - Epoch 93, Batch: 0, Loss: 0.683246
Train - Epoch 94, Batch: 0, Loss: 0.683300
Train - Epoch 95, Batch: 0, Loss: 0.683188
Train - Epoch 96, Batch: 0, Loss: 0.682961
Train - Epoch 97, Batch: 0, Loss: 0.682914
Train - Epoch 98, Batch: 0, Loss: 0.682716
Train - Epoch 99, Batch: 0, Loss: 0.682644
Train - Epoch 100, Batch: 0, Loss: 0.682719
Train - Epoch 101, Batch: 0, Loss: 0.682452
Train - Epoch 102, Batch: 0, Loss: 0.682744
Train - Epoch 103, Batch: 0, Loss: 0.682484
Train - Epoch 104, Batch: 0, Loss: 0.682334
Train - Epoch 105, Batch: 0, Loss: 0.682129
Train - Epoch 106, Batch: 0, Loss: 0.682091
Train - Epoch 107, Batch: 0, Loss: 0.682002
Train - Epoch 108, Batch: 0, Loss: 0.681798
Train - Epoch 109, Batch: 0, Loss: 0.681770
Train - Epoch 110, Batch: 0, Loss: 0.681708
Train - Epoch 111, Batch: 0, Loss: 0.681551
Train - Epoch 112, Batch: 0, Loss: 0.681467
Train - Epoch 113, Batch: 0, Loss: 0.681403
Train - Epoch 114, Batch: 0, Loss: 0.681225
Train - Epoch 115, Batch: 0, Loss: 0.681100
Train - Epoch 116, Batch: 0, Loss: 0.680949
Train - Epoch 117, Batch: 0, Loss: 0.681131
Train - Epoch 118, Batch: 0, Loss: 0.680827
Train - Epoch 119, Batch: 0, Loss: 0.680641
Train - Epoch 120, Batch: 0, Loss: 0.680622
Train - Epoch 121, Batch: 0, Loss: 0.680750
Train - Epoch 122, Batch: 0, Loss: 0.680823
Train - Epoch 123, Batch: 0, Loss: 0.680773
Train - Epoch 124, Batch: 0, Loss: 0.680379
Train - Epoch 125, Batch: 0, Loss: 0.680284
Train - Epoch 126, Batch: 0, Loss: 0.680451
Train - Epoch 127, Batch: 0, Loss: 0.680339
Train - Epoch 128, Batch: 0, Loss: 0.680006
Train - Epoch 129, Batch: 0, Loss: 0.680197
Train - Epoch 130, Batch: 0, Loss: 0.679783
Train - Epoch 131, Batch: 0, Loss: 0.680011
Train - Epoch 132, Batch: 0, Loss: 0.679623
Train - Epoch 133, Batch: 0, Loss: 0.679597
Train - Epoch 134, Batch: 0, Loss: 0.679468
Train - Epoch 135, Batch: 0, Loss: 0.679120
Train - Epoch 136, Batch: 0, Loss: 0.679085
Train - Epoch 137, Batch: 0, Loss: 0.679227
Train - Epoch 138, Batch: 0, Loss: 0.679096
Train - Epoch 139, Batch: 0, Loss: 0.679140
Train - Epoch 140, Batch: 0, Loss: 0.679038
Train - Epoch 141, Batch: 0, Loss: 0.678368
Train - Epoch 142, Batch: 0, Loss: 0.678538
Train - Epoch 143, Batch: 0, Loss: 0.678647
Train - Epoch 144, Batch: 0, Loss: 0.678642
Train - Epoch 145, Batch: 0, Loss: 0.678677
Train - Epoch 146, Batch: 0, Loss: 0.678531
Train - Epoch 147, Batch: 0, Loss: 0.678509
Train - Epoch 148, Batch: 0, Loss: 0.678293
Train - Epoch 149, Batch: 0, Loss: 0.678369
Train - Epoch 150, Batch: 0, Loss: 0.678022
Train - Epoch 151, Batch: 0, Loss: 0.678008
Train - Epoch 152, Batch: 0, Loss: 0.678135
Train - Epoch 153, Batch: 0, Loss: 0.678211
Train - Epoch 154, Batch: 0, Loss: 0.677933
Train - Epoch 155, Batch: 0, Loss: 0.677672
Train - Epoch 156, Batch: 0, Loss: 0.678009
Train - Epoch 157, Batch: 0, Loss: 0.677498
Train - Epoch 158, Batch: 0, Loss: 0.677320
Train - Epoch 159, Batch: 0, Loss: 0.677288
Train - Epoch 160, Batch: 0, Loss: 0.677349
Train - Epoch 161, Batch: 0, Loss: 0.676923
Train - Epoch 162, Batch: 0, Loss: 0.677051
Train - Epoch 163, Batch: 0, Loss: 0.676971
Train - Epoch 164, Batch: 0, Loss: 0.676802
Train - Epoch 165, Batch: 0, Loss: 0.676694
Train - Epoch 166, Batch: 0, Loss: 0.676613
Train - Epoch 167, Batch: 0, Loss: 0.676893
Train - Epoch 168, Batch: 0, Loss: 0.676792
Train - Epoch 169, Batch: 0, Loss: 0.676279
Train - Epoch 170, Batch: 0, Loss: 0.676364
Train - Epoch 171, Batch: 0, Loss: 0.676170
Train - Epoch 172, Batch: 0, Loss: 0.675947
Train - Epoch 173, Batch: 0, Loss: 0.675628
Train - Epoch 174, Batch: 0, Loss: 0.676084
Train - Epoch 175, Batch: 0, Loss: 0.676068
Train - Epoch 176, Batch: 0, Loss: 0.675265
Train - Epoch 177, Batch: 0, Loss: 0.676001
Train - Epoch 178, Batch: 0, Loss: 0.675699
Train - Epoch 179, Batch: 0, Loss: 0.675592
Train - Epoch 180, Batch: 0, Loss: 0.675271
Train - Epoch 181, Batch: 0, Loss: 0.675428
Train - Epoch 182, Batch: 0, Loss: 0.675668/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 183, Batch: 0, Loss: 0.675643
Train - Epoch 184, Batch: 0, Loss: 0.675344
Train - Epoch 185, Batch: 0, Loss: 0.674713
Train - Epoch 186, Batch: 0, Loss: 0.674812
Train - Epoch 187, Batch: 0, Loss: 0.675147
Train - Epoch 188, Batch: 0, Loss: 0.674964
Train - Epoch 189, Batch: 0, Loss: 0.674723
Train - Epoch 190, Batch: 0, Loss: 0.674777
Train - Epoch 191, Batch: 0, Loss: 0.674960
Train - Epoch 192, Batch: 0, Loss: 0.674893
Train - Epoch 193, Batch: 0, Loss: 0.674103
Train - Epoch 194, Batch: 0, Loss: 0.674693
Train - Epoch 195, Batch: 0, Loss: 0.674543
Train - Epoch 196, Batch: 0, Loss: 0.674466
Train - Epoch 197, Batch: 0, Loss: 0.674112
Train - Epoch 198, Batch: 0, Loss: 0.673792
Train - Epoch 199, Batch: 0, Loss: 0.673958
training_time:: 352.52867698669434
training time full:: 352.52873706817627
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000066, Accuracy: 0.821115
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1012
max_epoch:: 200
batch_size:: 10200
epoch:: 0
tensor([  2,   5,   6,   7,   8,  13,  15,  16,  17,  19,  21,  22,  24,  26,
         40,  42,  44,  45,  46,  48,  49,  55,  56,  61,  62,  68,  69,  70,
         76,  77,  79,  81,  82,  85,  89,  91,  95,  98, 101, 102, 103, 105,
        108, 111, 113, 114, 118, 122, 126, 128])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
epoch:: 60
epoch:: 61
epoch:: 62
epoch:: 63
epoch:: 64
epoch:: 65
epoch:: 66
epoch:: 67
epoch:: 68
epoch:: 69
epoch:: 70
epoch:: 71
epoch:: 72
epoch:: 73
epoch:: 74
epoch:: 75
epoch:: 76
epoch:: 77
epoch:: 78
epoch:: 79
epoch:: 80
epoch:: 81
epoch:: 82
epoch:: 83
epoch:: 84
epoch:: 85
epoch:: 86
epoch:: 87
epoch:: 88
epoch:: 89
epoch:: 90
epoch:: 91
epoch:: 92
epoch:: 93
epoch:: 94
epoch:: 95
epoch:: 96
epoch:: 97
epoch:: 98
epoch:: 99
epoch:: 100
epoch:: 101
epoch:: 102
epoch:: 103
epoch:: 104
epoch:: 105
epoch:: 106
epoch:: 107
epoch:: 108
epoch:: 109
epoch:: 110
epoch:: 111
epoch:: 112
epoch:: 113
epoch:: 114
epoch:: 115
epoch:: 116
epoch:: 117
epoch:: 118
epoch:: 119
epoch:: 120
epoch:: 121
epoch:: 122
epoch:: 123
epoch:: 124
epoch:: 125
epoch:: 126
epoch:: 127
epoch:: 128
epoch:: 129
epoch:: 130
epoch:: 131
epoch:: 132
epoch:: 133
epoch:: 134
epoch:: 135
epoch:: 136
epoch:: 137
epoch:: 138
epoch:: 139
epoch:: 140
epoch:: 141
epoch:: 142
epoch:: 143
epoch:: 144
epoch:: 145
epoch:: 146
epoch:: 147
epoch:: 148
epoch:: 149
epoch:: 150
epoch:: 151
epoch:: 152
epoch:: 153
epoch:: 154
epoch:: 155
epoch:: 156
epoch:: 157
epoch:: 158
epoch:: 159
epoch:: 160
epoch:: 161
epoch:: 162
epoch:: 163
epoch:: 164
epoch:: 165
epoch:: 166
epoch:: 167
epoch:: 168
epoch:: 169
epoch:: 170
epoch:: 171
epoch:: 172
epoch:: 173
epoch:: 174
epoch:: 175
epoch:: 176
epoch:: 177
epoch:: 178
epoch:: 179
epoch:: 180
epoch:: 181
epoch:: 182
epoch:: 183
epoch:: 184
epoch:: 185
epoch:: 186
epoch:: 187
epoch:: 188
epoch:: 189
epoch:: 190
epoch:: 191
epoch:: 192
epoch:: 193
epoch:: 194
epoch:: 195
epoch:: 196
epoch:: 197
epoch:: 198
epoch:: 199
training time is 262.8959929943085
overhead:: 0
overhead2:: 0.42119765281677246
overhead3:: 0
time_baseline:: 262.8960461616516
curr_diff: 0 tensor(0.2994, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.2994, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9671, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000065, Accuracy: 0.945065
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 0 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1012
max_epoch:: 200
overhead:: 0
overhead2:: 0.021562814712524414
overhead3:: 0.1311652660369873
overhead4:: 31.035106897354126
overhead5:: 0
memory usage:: 26598862848
time_provenance:: 60.136391162872314
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.2981, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.2981, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9673, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000065, Accuracy: 0.945015
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 0 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1012
max_epoch:: 200
overhead:: 0
overhead2:: 0.025147676467895508
overhead3:: 0.1397542953491211
overhead4:: 37.03310203552246
overhead5:: 0
memory usage:: 26602364928
time_provenance:: 67.67934393882751
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.2981, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.2981, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9673, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000065, Accuracy: 0.945015
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 0 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1012
max_epoch:: 200
overhead:: 0
overhead2:: 0.030436277389526367
overhead3:: 0.1492321491241455
overhead4:: 42.614712715148926
overhead5:: 0
memory usage:: 26619363328
time_provenance:: 74.82040071487427
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.2981, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.2981, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9673, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000065, Accuracy: 0.945015
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 0 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1012
max_epoch:: 200
overhead:: 0
overhead2:: 0.04021859169006348
overhead3:: 0.16910195350646973
overhead4:: 55.464009284973145
overhead5:: 0
memory usage:: 26603040768
time_provenance:: 91.76945114135742
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.2982, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.2982, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9673, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000065, Accuracy: 0.945015
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 0 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1012
max_epoch:: 200
overhead:: 0
overhead2:: 0.044203758239746094
overhead3:: 0.17771577835083008
overhead4:: 60.8667356967926
overhead5:: 0
memory usage:: 26605686784
time_provenance:: 98.63949179649353
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.2982, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.2982, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9673, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000065, Accuracy: 0.945015
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 0 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1012
max_epoch:: 200
overhead:: 0
overhead2:: 0.04767251014709473
overhead3:: 0.18445348739624023
overhead4:: 66.13259363174438
overhead5:: 0
memory usage:: 26605268992
time_provenance:: 104.99908423423767
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.2982, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.2982, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9673, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000065, Accuracy: 0.945015
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 0 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1012
max_epoch:: 200
overhead:: 0
overhead2:: 0.08915042877197266
overhead3:: 0.269336462020874
overhead4:: 131.03350734710693
overhead5:: 0
memory usage:: 26605572096
time_provenance:: 188.7909390926361
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.2987, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.2987, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9672, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000065, Accuracy: 0.945015
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 0 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1012
max_epoch:: 200
overhead:: 0
overhead2:: 0.09370684623718262
overhead3:: 0.28530120849609375
overhead4:: 134.27160477638245
overhead5:: 0
memory usage:: 26621804544
time_provenance:: 192.8628752231598
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.2987, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.2987, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9672, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000065, Accuracy: 0.945015
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 0 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1012
max_epoch:: 200
overhead:: 0
overhead2:: 0.09791374206542969
overhead3:: 0.29488372802734375
overhead4:: 137.52627325057983
overhead5:: 0
memory usage:: 26604896256
time_provenance:: 196.90951991081238
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.2987, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.2987, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9672, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000065, Accuracy: 0.945015
adding noise deletion rate:: 0.06
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
tensor([0.5111, 0.5113, 0.5082,  ..., 0.5248, 0.5003, 0.5171],
       dtype=torch.float64, grad_fn=<MaxBackward0>)
tensor(0.8685, dtype=torch.float64) tensor(0., dtype=torch.float64)
Train - Epoch 0, Batch: 0, Loss: 0.693068
Train - Epoch 1, Batch: 0, Loss: 0.692992
Train - Epoch 2, Batch: 0, Loss: 0.692924
Train - Epoch 3, Batch: 0, Loss: 0.692843
Train - Epoch 4, Batch: 0, Loss: 0.692776
Train - Epoch 5, Batch: 0, Loss: 0.692704
Train - Epoch 6, Batch: 0, Loss: 0.692633
Train - Epoch 7, Batch: 0, Loss: 0.692593
Train - Epoch 8, Batch: 0, Loss: 0.692516
Train - Epoch 9, Batch: 0, Loss: 0.692431
Train - Epoch 10, Batch: 0, Loss: 0.692381
Train - Epoch 11, Batch: 0, Loss: 0.692330
Train - Epoch 12, Batch: 0, Loss: 0.692272
Train - Epoch 13, Batch: 0, Loss: 0.692184
Train - Epoch 14, Batch: 0, Loss: 0.692124
Train - Epoch 15, Batch: 0, Loss: 0.692105
Train - Epoch 16, Batch: 0, Loss: 0.692010
Train - Epoch 17, Batch: 0, Loss: 0.691919
Train - Epoch 18, Batch: 0, Loss: 0.691849
Train - Epoch 19, Batch: 0, Loss: 0.691803
Train - Epoch 20, Batch: 0, Loss: 0.691763
Train - Epoch 21, Batch: 0, Loss: 0.691676
Train - Epoch 22, Batch: 0, Loss: 0.691584
Train - Epoch 23, Batch: 0, Loss: 0.691512
Train - Epoch 24, Batch: 0, Loss: 0.691510
Train - Epoch 25, Batch: 0, Loss: 0.691388
Train - Epoch 26, Batch: 0, Loss: 0.691348
Train - Epoch 27, Batch: 0, Loss: 0.691305
Train - Epoch 28, Batch: 0, Loss: 0.691195
Train - Epoch 29, Batch: 0, Loss: 0.691101
Train - Epoch 30, Batch: 0, Loss: 0.691040
Train - Epoch 31, Batch: 0, Loss: 0.690977
Train - Epoch 32, Batch: 0, Loss: 0.690993
Train - Epoch 33, Batch: 0, Loss: 0.690873
Train - Epoch 34, Batch: 0, Loss: 0.690822
Train - Epoch 35, Batch: 0, Loss: 0.690737
Train - Epoch 36, Batch: 0, Loss: 0.690775
Train - Epoch 37, Batch: 0, Loss: 0.690644
Train - Epoch 38, Batch: 0, Loss: 0.690581
Train - Epoch 39, Batch: 0, Loss: 0.690534
Train - Epoch 40, Batch: 0, Loss: 0.690338
Train - Epoch 41, Batch: 0, Loss: 0.690366
Train - Epoch 42, Batch: 0, Loss: 0.690337
Train - Epoch 43, Batch: 0, Loss: 0.690202
Train - Epoch 44, Batch: 0, Loss: 0.690235
Train - Epoch 45, Batch: 0, Loss: 0.690147
Train - Epoch 46, Batch: 0, Loss: 0.690046
Train - Epoch 47, Batch: 0, Loss: 0.689859
Train - Epoch 48, Batch: 0, Loss: 0.689923
Train - Epoch 49, Batch: 0, Loss: 0.689837
Train - Epoch 50, Batch: 0, Loss: 0.689748
Train - Epoch 51, Batch: 0, Loss: 0.689836
Train - Epoch 52, Batch: 0, Loss: 0.689646
Train - Epoch 53, Batch: 0, Loss: 0.689573
Train - Epoch 54, Batch: 0, Loss: 0.689630
Train - Epoch 55, Batch: 0, Loss: 0.689541
Train - Epoch 56, Batch: 0, Loss: 0.689338
Train - Epoch 57, Batch: 0, Loss: 0.689436
Train - Epoch 58, Batch: 0, Loss: 0.689230
Train - Epoch 59, Batch: 0, Loss: 0.689396
Train - Epoch 60, Batch: 0, Loss: 0.689161
Train - Epoch 61, Batch: 0, Loss: 0.689083
Train - Epoch 62, Batch: 0, Loss: 0.689063
Train - Epoch 63, Batch: 0, Loss: 0.689025
Train - Epoch 64, Batch: 0, Loss: 0.688934
Train - Epoch 65, Batch: 0, Loss: 0.688863
Train - Epoch 66, Batch: 0, Loss: 0.688794
Train - Epoch 67, Batch: 0, Loss: 0.688840
Train - Epoch 68, Batch: 0, Loss: 0.688687
Train - Epoch 69, Batch: 0, Loss: 0.688588
Train - Epoch 70, Batch: 0, Loss: 0.688744
Train - Epoch 71, Batch: 0, Loss: 0.688458
Train - Epoch 72, Batch: 0, Loss: 0.688624
Train - Epoch 73, Batch: 0, Loss: 0.688418
Train - Epoch 74, Batch: 0, Loss: 0.688269
Train - Epoch 75, Batch: 0, Loss: 0.688334
Train - Epoch 76, Batch: 0, Loss: 0.687949
Train - Epoch 77, Batch: 0, Loss: 0.688279
Train - Epoch 78, Batch: 0, Loss: 0.688253
Train - Epoch 79, Batch: 0, Loss: 0.688051
Train - Epoch 80, Batch: 0, Loss: 0.688085
Train - Epoch 81, Batch: 0, Loss: 0.687989
Train - Epoch 82, Batch: 0, Loss: 0.687820
Train - Epoch 83, Batch: 0, Loss: 0.687775
Train - Epoch 84, Batch: 0, Loss: 0.687859
Train - Epoch 85, Batch: 0, Loss: 0.687641
Train - Epoch 86, Batch: 0, Loss: 0.687603
Train - Epoch 87, Batch: 0, Loss: 0.687668
Train - Epoch 88, Batch: 0, Loss: 0.687547
Train - Epoch 89, Batch: 0, Loss: 0.687528
Train - Epoch 90, Batch: 0, Loss: 0.687474
Train - Epoch 91, Batch: 0, Loss: 0.687222
Train - Epoch 92, Batch: 0, Loss: 0.687307
Train - Epoch 93, Batch: 0, Loss: 0.687182
Train - Epoch 94, Batch: 0, Loss: 0.687327
Train - Epoch 95, Batch: 0, Loss: 0.687081
Train - Epoch 96, Batch: 0, Loss: 0.686903
Train - Epoch 97, Batch: 0, Loss: 0.687315
Train - Epoch 98, Batch: 0, Loss: 0.687082
Train - Epoch 99, Batch: 0, Loss: 0.687082
Train - Epoch 100, Batch: 0, Loss: 0.686938
Train - Epoch 101, Batch: 0, Loss: 0.686901
Train - Epoch 102, Batch: 0, Loss: 0.686740
Train - Epoch 103, Batch: 0, Loss: 0.686599
Train - Epoch 104, Batch: 0, Loss: 0.686532
Train - Epoch 105, Batch: 0, Loss: 0.686761
Train - Epoch 106, Batch: 0, Loss: 0.686625
Train - Epoch 107, Batch: 0, Loss: 0.686500
Train - Epoch 108, Batch: 0, Loss: 0.686371
Train - Epoch 109, Batch: 0, Loss: 0.686380
Train - Epoch 110, Batch: 0, Loss: 0.686171
Train - Epoch 111, Batch: 0, Loss: 0.686208
Train - Epoch 112, Batch: 0, Loss: 0.686128
Train - Epoch 113, Batch: 0, Loss: 0.686128
Train - Epoch 114, Batch: 0, Loss: 0.686173
Train - Epoch 115, Batch: 0, Loss: 0.686027
Train - Epoch 116, Batch: 0, Loss: 0.685851
Train - Epoch 117, Batch: 0, Loss: 0.685804
Train - Epoch 118, Batch: 0, Loss: 0.685892
Train - Epoch 119, Batch: 0, Loss: 0.685887
Train - Epoch 120, Batch: 0, Loss: 0.685568
Train - Epoch 121, Batch: 0, Loss: 0.685651
Train - Epoch 122, Batch: 0, Loss: 0.685630
Train - Epoch 123, Batch: 0, Loss: 0.685546
Train - Epoch 124, Batch: 0, Loss: 0.685712
Train - Epoch 125, Batch: 0, Loss: 0.685486
Train - Epoch 126, Batch: 0, Loss: 0.685481
Train - Epoch 127, Batch: 0, Loss: 0.685435
Train - Epoch 128, Batch: 0, Loss: 0.685198
Train - Epoch 129, Batch: 0, Loss: 0.685221
Train - Epoch 130, Batch: 0, Loss: 0.685323
Train - Epoch 131, Batch: 0, Loss: 0.685215
Train - Epoch 132, Batch: 0, Loss: 0.685304
Train - Epoch 133, Batch: 0, Loss: 0.685105
Train - Epoch 134, Batch: 0, Loss: 0.684870
Train - Epoch 135, Batch: 0, Loss: 0.685148
Train - Epoch 136, Batch: 0, Loss: 0.684913
Train - Epoch 137, Batch: 0, Loss: 0.684986
Train - Epoch 138, Batch: 0, Loss: 0.684760
Train - Epoch 139, Batch: 0, Loss: 0.684866
Train - Epoch 140, Batch: 0, Loss: 0.684553
Train - Epoch 141, Batch: 0, Loss: 0.684608
Train - Epoch 142, Batch: 0, Loss: 0.684673
Train - Epoch 143, Batch: 0, Loss: 0.684542
Train - Epoch 144, Batch: 0, Loss: 0.684646
Train - Epoch 145, Batch: 0, Loss: 0.684270
Train - Epoch 146, Batch: 0, Loss: 0.684351
Train - Epoch 147, Batch: 0, Loss: 0.684179
Train - Epoch 148, Batch: 0, Loss: 0.684146
Train - Epoch 149, Batch: 0, Loss: 0.684453
Train - Epoch 150, Batch: 0, Loss: 0.684254
Train - Epoch 151, Batch: 0, Loss: 0.684235
Train - Epoch 152, Batch: 0, Loss: 0.684338
Train - Epoch 153, Batch: 0, Loss: 0.684034
Train - Epoch 154, Batch: 0, Loss: 0.683863
Train - Epoch 155, Batch: 0, Loss: 0.683653
Train - Epoch 156, Batch: 0, Loss: 0.683943
Train - Epoch 157, Batch: 0, Loss: 0.683754
Train - Epoch 158, Batch: 0, Loss: 0.683752
Train - Epoch 159, Batch: 0, Loss: 0.683497
Train - Epoch 160, Batch: 0, Loss: 0.683549
Train - Epoch 161, Batch: 0, Loss: 0.683514
Train - Epoch 162, Batch: 0, Loss: 0.683662
Train - Epoch 163, Batch: 0, Loss: 0.683128
Train - Epoch 164, Batch: 0, Loss: 0.683564
Train - Epoch 165, Batch: 0, Loss: 0.683190
Train - Epoch 166, Batch: 0, Loss: 0.683165
Train - Epoch 167, Batch: 0, Loss: 0.683134
Train - Epoch 168, Batch: 0, Loss: 0.683172
Train - Epoch 169, Batch: 0, Loss: 0.683286
Train - Epoch 170, Batch: 0, Loss: 0.683175
Train - Epoch 171, Batch: 0, Loss: 0.682968
Train - Epoch 172, Batch: 0, Loss: 0.683149
Train - Epoch 173, Batch: 0, Loss: 0.682855
Train - Epoch 174, Batch: 0, Loss: 0.682768
Train - Epoch 175, Batch: 0, Loss: 0.682758
Train - Epoch 176, Batch: 0, Loss: 0.682596
Train - Epoch 177, Batch: 0, Loss: 0.682598
Train - Epoch 178, Batch: 0, Loss: 0.682654
Train - Epoch 179, Batch: 0, Loss: 0.682420
Train - Epoch 180, Batch: 0, Loss: 0.682652
Train - Epoch 181, Batch: 0, Loss: 0.682509
Train - Epoch 182, Batch: 0, Loss: 0.682459/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 183, Batch: 0, Loss: 0.682212
Train - Epoch 184, Batch: 0, Loss: 0.682725
Train - Epoch 185, Batch: 0, Loss: 0.682540
Train - Epoch 186, Batch: 0, Loss: 0.682408
Train - Epoch 187, Batch: 0, Loss: 0.682276
Train - Epoch 188, Batch: 0, Loss: 0.682319
Train - Epoch 189, Batch: 0, Loss: 0.682169
Train - Epoch 190, Batch: 0, Loss: 0.682154
Train - Epoch 191, Batch: 0, Loss: 0.682040
Train - Epoch 192, Batch: 0, Loss: 0.681867
Train - Epoch 193, Batch: 0, Loss: 0.681870
Train - Epoch 194, Batch: 0, Loss: 0.681970
Train - Epoch 195, Batch: 0, Loss: 0.682099
Train - Epoch 196, Batch: 0, Loss: 0.681887
Train - Epoch 197, Batch: 0, Loss: 0.681940
Train - Epoch 198, Batch: 0, Loss: 0.681888
Train - Epoch 199, Batch: 0, Loss: 0.681646
training_time:: 352.73114013671875
training time full:: 352.7311818599701
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000067, Accuracy: 0.724187
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1214
max_epoch:: 200
batch_size:: 10200
epoch:: 0
tensor([  1,   2,   5,   6,   7,   8,  13,  16,  17,  19,  21,  22,  24,  26,
         37,  40,  42,  44,  45,  46,  48,  49,  53,  55,  56,  61,  62,  68,
         69,  70,  76,  77,  79,  81,  82,  85,  88,  91,  95, 101, 102, 103,
        105, 108, 111, 113, 114, 118, 122, 126])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
epoch:: 60
epoch:: 61
epoch:: 62
epoch:: 63
epoch:: 64
epoch:: 65
epoch:: 66
epoch:: 67
epoch:: 68
epoch:: 69
epoch:: 70
epoch:: 71
epoch:: 72
epoch:: 73
epoch:: 74
epoch:: 75
epoch:: 76
epoch:: 77
epoch:: 78
epoch:: 79
epoch:: 80
epoch:: 81
epoch:: 82
epoch:: 83
epoch:: 84
epoch:: 85
epoch:: 86
epoch:: 87
epoch:: 88
epoch:: 89
epoch:: 90
epoch:: 91
epoch:: 92
epoch:: 93
epoch:: 94
epoch:: 95
epoch:: 96
epoch:: 97
epoch:: 98
epoch:: 99
epoch:: 100
epoch:: 101
epoch:: 102
epoch:: 103
epoch:: 104
epoch:: 105
epoch:: 106
epoch:: 107
epoch:: 108
epoch:: 109
epoch:: 110
epoch:: 111
epoch:: 112
epoch:: 113
epoch:: 114
epoch:: 115
epoch:: 116
epoch:: 117
epoch:: 118
epoch:: 119
epoch:: 120
epoch:: 121
epoch:: 122
epoch:: 123
epoch:: 124
epoch:: 125
epoch:: 126
epoch:: 127
epoch:: 128
epoch:: 129
epoch:: 130
epoch:: 131
epoch:: 132
epoch:: 133
epoch:: 134
epoch:: 135
epoch:: 136
epoch:: 137
epoch:: 138
epoch:: 139
epoch:: 140
epoch:: 141
epoch:: 142
epoch:: 143
epoch:: 144
epoch:: 145
epoch:: 146
epoch:: 147
epoch:: 148
epoch:: 149
epoch:: 150
epoch:: 151
epoch:: 152
epoch:: 153
epoch:: 154
epoch:: 155
epoch:: 156
epoch:: 157
epoch:: 158
epoch:: 159
epoch:: 160
epoch:: 161
epoch:: 162
epoch:: 163
epoch:: 164
epoch:: 165
epoch:: 166
epoch:: 167
epoch:: 168
epoch:: 169
epoch:: 170
epoch:: 171
epoch:: 172
epoch:: 173
epoch:: 174
epoch:: 175
epoch:: 176
epoch:: 177
epoch:: 178
epoch:: 179
epoch:: 180
epoch:: 181
epoch:: 182
epoch:: 183
epoch:: 184
epoch:: 185
epoch:: 186
epoch:: 187
epoch:: 188
epoch:: 189
epoch:: 190
epoch:: 191
epoch:: 192
epoch:: 193
epoch:: 194
epoch:: 195
epoch:: 196
epoch:: 197
epoch:: 198
epoch:: 199
training time is 260.92715764045715
overhead:: 0
overhead2:: 0.4313466548919678
overhead3:: 0
time_baseline:: 260.9272119998932
curr_diff: 0 tensor(0.2982, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.2982, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9547, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000066, Accuracy: 0.921697
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 0 0.06 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1214
max_epoch:: 200
overhead:: 0
overhead2:: 0.020969152450561523
overhead3:: 0.13224196434020996
overhead4:: 30.860034942626953
overhead5:: 0
memory usage:: 26606280704
time_provenance:: 65.10730791091919
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.2966, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.2966, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9551, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000066, Accuracy: 0.921500
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 0 0.06 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1214
max_epoch:: 200
overhead:: 0
overhead2:: 0.025845050811767578
overhead3:: 0.14235162734985352
overhead4:: 36.41673564910889
overhead5:: 0
memory usage:: 26606231552
time_provenance:: 72.69858860969543
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.2966, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.2966, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9551, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000066, Accuracy: 0.921500
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 0 0.06 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1214
max_epoch:: 200
overhead:: 0
overhead2:: 0.02893829345703125
overhead3:: 0.14867258071899414
overhead4:: 41.78768968582153
overhead5:: 0
memory usage:: 26604675072
time_provenance:: 79.28291177749634
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.2966, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.2966, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9551, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000066, Accuracy: 0.921500
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 0 0.06 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1214
max_epoch:: 200
overhead:: 0
overhead2:: 0.04230833053588867
overhead3:: 0.17074179649353027
overhead4:: 54.691667318344116
overhead5:: 0
memory usage:: 26610884608
time_provenance:: 96.26157665252686
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.2968, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.2968, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9551, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000066, Accuracy: 0.921500
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 0 0.06 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1214
max_epoch:: 200
overhead:: 0
overhead2:: 0.042967796325683594
overhead3:: 0.17823386192321777
overhead4:: 59.50165390968323
overhead5:: 0
memory usage:: 26619998208
time_provenance:: 102.5126805305481
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.2968, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.2968, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9551, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000066, Accuracy: 0.921500
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 0 0.06 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1214
max_epoch:: 200
overhead:: 0
overhead2:: 0.046323537826538086
overhead3:: 0.18399834632873535
overhead4:: 64.59437799453735
overhead5:: 0
memory usage:: 26616360960
time_provenance:: 108.97443175315857
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.2968, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.2968, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9551, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000066, Accuracy: 0.921500
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 0 0.06 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1214
max_epoch:: 200
overhead:: 0
overhead2:: 0.09952545166015625
overhead3:: 0.28865909576416016
overhead4:: 130.331955909729
overhead5:: 0
memory usage:: 26604597248
time_provenance:: 192.4160623550415
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.2974, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.2974, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9549, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000066, Accuracy: 0.921549
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 0 0.06 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1214
max_epoch:: 200
overhead:: 0
overhead2:: 0.0919344425201416
overhead3:: 0.2862284183502197
overhead4:: 132.85850715637207
overhead5:: 0
memory usage:: 26599620608
time_provenance:: 195.78701281547546
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.2974, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.2974, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9549, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000066, Accuracy: 0.921549
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 0 0.06 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1214
max_epoch:: 200
overhead:: 0
overhead2:: 0.09612464904785156
overhead3:: 0.29280900955200195
overhead4:: 136.23927235603333
overhead5:: 0
memory usage:: 26607325184
time_provenance:: 199.93601751327515
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.2974, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.2974, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9549, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000066, Accuracy: 0.921549
adding noise deletion rate:: 0.07
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
tensor([0.5330, 0.5357, 0.5178,  ..., 0.5131, 0.5153, 0.5062],
       dtype=torch.float64, grad_fn=<MaxBackward0>)
tensor(0.8685, dtype=torch.float64) tensor(0., dtype=torch.float64)
Train - Epoch 0, Batch: 0, Loss: 0.693139
Train - Epoch 1, Batch: 0, Loss: 0.693121
Train - Epoch 2, Batch: 0, Loss: 0.693071
Train - Epoch 3, Batch: 0, Loss: 0.693025
Train - Epoch 4, Batch: 0, Loss: 0.693000
Train - Epoch 5, Batch: 0, Loss: 0.692942
Train - Epoch 6, Batch: 0, Loss: 0.692894
Train - Epoch 7, Batch: 0, Loss: 0.692864
Train - Epoch 8, Batch: 0, Loss: 0.692819
Train - Epoch 9, Batch: 0, Loss: 0.692805
Train - Epoch 10, Batch: 0, Loss: 0.692729
Train - Epoch 11, Batch: 0, Loss: 0.692678
Train - Epoch 12, Batch: 0, Loss: 0.692610
Train - Epoch 13, Batch: 0, Loss: 0.692608
Train - Epoch 14, Batch: 0, Loss: 0.692587
Train - Epoch 15, Batch: 0, Loss: 0.692528
Train - Epoch 16, Batch: 0, Loss: 0.692467
Train - Epoch 17, Batch: 0, Loss: 0.692401
Train - Epoch 18, Batch: 0, Loss: 0.692384
Train - Epoch 19, Batch: 0, Loss: 0.692273
Train - Epoch 20, Batch: 0, Loss: 0.692329
Train - Epoch 21, Batch: 0, Loss: 0.692206
Train - Epoch 22, Batch: 0, Loss: 0.692216
Train - Epoch 23, Batch: 0, Loss: 0.692128
Train - Epoch 24, Batch: 0, Loss: 0.692096
Train - Epoch 25, Batch: 0, Loss: 0.692074
Train - Epoch 26, Batch: 0, Loss: 0.692068
Train - Epoch 27, Batch: 0, Loss: 0.691962
Train - Epoch 28, Batch: 0, Loss: 0.692057
Train - Epoch 29, Batch: 0, Loss: 0.691989
Train - Epoch 30, Batch: 0, Loss: 0.691891
Train - Epoch 31, Batch: 0, Loss: 0.691844
Train - Epoch 32, Batch: 0, Loss: 0.691793
Train - Epoch 33, Batch: 0, Loss: 0.691757
Train - Epoch 34, Batch: 0, Loss: 0.691657
Train - Epoch 35, Batch: 0, Loss: 0.691768
Train - Epoch 36, Batch: 0, Loss: 0.691677
Train - Epoch 37, Batch: 0, Loss: 0.691566
Train - Epoch 38, Batch: 0, Loss: 0.691595
Train - Epoch 39, Batch: 0, Loss: 0.691479
Train - Epoch 40, Batch: 0, Loss: 0.691542
Train - Epoch 41, Batch: 0, Loss: 0.691388
Train - Epoch 42, Batch: 0, Loss: 0.691338
Train - Epoch 43, Batch: 0, Loss: 0.691410
Train - Epoch 44, Batch: 0, Loss: 0.691326
Train - Epoch 45, Batch: 0, Loss: 0.691211
Train - Epoch 46, Batch: 0, Loss: 0.691263
Train - Epoch 47, Batch: 0, Loss: 0.691200
Train - Epoch 48, Batch: 0, Loss: 0.691239
Train - Epoch 49, Batch: 0, Loss: 0.691129
Train - Epoch 50, Batch: 0, Loss: 0.691119
Train - Epoch 51, Batch: 0, Loss: 0.691004
Train - Epoch 52, Batch: 0, Loss: 0.690903
Train - Epoch 53, Batch: 0, Loss: 0.691029
Train - Epoch 54, Batch: 0, Loss: 0.690803
Train - Epoch 55, Batch: 0, Loss: 0.690792
Train - Epoch 56, Batch: 0, Loss: 0.690838
Train - Epoch 57, Batch: 0, Loss: 0.690726
Train - Epoch 58, Batch: 0, Loss: 0.690834
Train - Epoch 59, Batch: 0, Loss: 0.690524
Train - Epoch 60, Batch: 0, Loss: 0.690618
Train - Epoch 61, Batch: 0, Loss: 0.690695
Train - Epoch 62, Batch: 0, Loss: 0.690564
Train - Epoch 63, Batch: 0, Loss: 0.690554
Train - Epoch 64, Batch: 0, Loss: 0.690532
Train - Epoch 65, Batch: 0, Loss: 0.690626
Train - Epoch 66, Batch: 0, Loss: 0.690456
Train - Epoch 67, Batch: 0, Loss: 0.690470
Train - Epoch 68, Batch: 0, Loss: 0.690439
Train - Epoch 69, Batch: 0, Loss: 0.690299
Train - Epoch 70, Batch: 0, Loss: 0.690264
Train - Epoch 71, Batch: 0, Loss: 0.690306
Train - Epoch 72, Batch: 0, Loss: 0.690185
Train - Epoch 73, Batch: 0, Loss: 0.690131
Train - Epoch 74, Batch: 0, Loss: 0.690107
Train - Epoch 75, Batch: 0, Loss: 0.690121
Train - Epoch 76, Batch: 0, Loss: 0.690189
Train - Epoch 77, Batch: 0, Loss: 0.690009
Train - Epoch 78, Batch: 0, Loss: 0.689941
Train - Epoch 79, Batch: 0, Loss: 0.689941
Train - Epoch 80, Batch: 0, Loss: 0.689973
Train - Epoch 81, Batch: 0, Loss: 0.689821
Train - Epoch 82, Batch: 0, Loss: 0.690042
Train - Epoch 83, Batch: 0, Loss: 0.689848
Train - Epoch 84, Batch: 0, Loss: 0.689780
Train - Epoch 85, Batch: 0, Loss: 0.689621
Train - Epoch 86, Batch: 0, Loss: 0.689721
Train - Epoch 87, Batch: 0, Loss: 0.689462
Train - Epoch 88, Batch: 0, Loss: 0.689539
Train - Epoch 89, Batch: 0, Loss: 0.689551
Train - Epoch 90, Batch: 0, Loss: 0.689562
Train - Epoch 91, Batch: 0, Loss: 0.689561
Train - Epoch 92, Batch: 0, Loss: 0.689460
Train - Epoch 93, Batch: 0, Loss: 0.689406
Train - Epoch 94, Batch: 0, Loss: 0.689310
Train - Epoch 95, Batch: 0, Loss: 0.689493
Train - Epoch 96, Batch: 0, Loss: 0.689603
Train - Epoch 97, Batch: 0, Loss: 0.689024
Train - Epoch 98, Batch: 0, Loss: 0.689213
Train - Epoch 99, Batch: 0, Loss: 0.689030
Train - Epoch 100, Batch: 0, Loss: 0.689255
Train - Epoch 101, Batch: 0, Loss: 0.689228
Train - Epoch 102, Batch: 0, Loss: 0.689461
Train - Epoch 103, Batch: 0, Loss: 0.689314
Train - Epoch 104, Batch: 0, Loss: 0.688946
Train - Epoch 105, Batch: 0, Loss: 0.689230
Train - Epoch 106, Batch: 0, Loss: 0.689018
Train - Epoch 107, Batch: 0, Loss: 0.688940
Train - Epoch 108, Batch: 0, Loss: 0.688997
Train - Epoch 109, Batch: 0, Loss: 0.688917
Train - Epoch 110, Batch: 0, Loss: 0.688871
Train - Epoch 111, Batch: 0, Loss: 0.689012
Train - Epoch 112, Batch: 0, Loss: 0.688962
Train - Epoch 113, Batch: 0, Loss: 0.688893
Train - Epoch 114, Batch: 0, Loss: 0.688430
Train - Epoch 115, Batch: 0, Loss: 0.688640
Train - Epoch 116, Batch: 0, Loss: 0.688662
Train - Epoch 117, Batch: 0, Loss: 0.688674
Train - Epoch 118, Batch: 0, Loss: 0.688675
Train - Epoch 119, Batch: 0, Loss: 0.688470
Train - Epoch 120, Batch: 0, Loss: 0.688470
Train - Epoch 121, Batch: 0, Loss: 0.688486
Train - Epoch 122, Batch: 0, Loss: 0.688314
Train - Epoch 123, Batch: 0, Loss: 0.688810
Train - Epoch 124, Batch: 0, Loss: 0.688321
Train - Epoch 125, Batch: 0, Loss: 0.688342
Train - Epoch 126, Batch: 0, Loss: 0.688372
Train - Epoch 127, Batch: 0, Loss: 0.688725
Train - Epoch 128, Batch: 0, Loss: 0.688350
Train - Epoch 129, Batch: 0, Loss: 0.688126
Train - Epoch 130, Batch: 0, Loss: 0.688392
Train - Epoch 131, Batch: 0, Loss: 0.688275
Train - Epoch 132, Batch: 0, Loss: 0.687896
Train - Epoch 133, Batch: 0, Loss: 0.688091
Train - Epoch 134, Batch: 0, Loss: 0.688104
Train - Epoch 135, Batch: 0, Loss: 0.687674
Train - Epoch 136, Batch: 0, Loss: 0.688017
Train - Epoch 137, Batch: 0, Loss: 0.688190
Train - Epoch 138, Batch: 0, Loss: 0.688106
Train - Epoch 139, Batch: 0, Loss: 0.687881
Train - Epoch 140, Batch: 0, Loss: 0.687980
Train - Epoch 141, Batch: 0, Loss: 0.687408
Train - Epoch 142, Batch: 0, Loss: 0.687773
Train - Epoch 143, Batch: 0, Loss: 0.687669
Train - Epoch 144, Batch: 0, Loss: 0.687710
Train - Epoch 145, Batch: 0, Loss: 0.687752
Train - Epoch 146, Batch: 0, Loss: 0.687610
Train - Epoch 147, Batch: 0, Loss: 0.687877
Train - Epoch 148, Batch: 0, Loss: 0.687856
Train - Epoch 149, Batch: 0, Loss: 0.687562
Train - Epoch 150, Batch: 0, Loss: 0.687462
Train - Epoch 151, Batch: 0, Loss: 0.687533
Train - Epoch 152, Batch: 0, Loss: 0.687585
Train - Epoch 153, Batch: 0, Loss: 0.687799
Train - Epoch 154, Batch: 0, Loss: 0.687749
Train - Epoch 155, Batch: 0, Loss: 0.687350
Train - Epoch 156, Batch: 0, Loss: 0.687507
Train - Epoch 157, Batch: 0, Loss: 0.687374
Train - Epoch 158, Batch: 0, Loss: 0.687169
Train - Epoch 159, Batch: 0, Loss: 0.687182
Train - Epoch 160, Batch: 0, Loss: 0.687407
Train - Epoch 161, Batch: 0, Loss: 0.687102
Train - Epoch 162, Batch: 0, Loss: 0.687086
Train - Epoch 163, Batch: 0, Loss: 0.687190
Train - Epoch 164, Batch: 0, Loss: 0.687418
Train - Epoch 165, Batch: 0, Loss: 0.687194
Train - Epoch 166, Batch: 0, Loss: 0.687014
Train - Epoch 167, Batch: 0, Loss: 0.687384
Train - Epoch 168, Batch: 0, Loss: 0.687297
Train - Epoch 169, Batch: 0, Loss: 0.686848
Train - Epoch 170, Batch: 0, Loss: 0.687198
Train - Epoch 171, Batch: 0, Loss: 0.686997
Train - Epoch 172, Batch: 0, Loss: 0.686737
Train - Epoch 173, Batch: 0, Loss: 0.686956
Train - Epoch 174, Batch: 0, Loss: 0.686858
Train - Epoch 175, Batch: 0, Loss: 0.686942
Train - Epoch 176, Batch: 0, Loss: 0.686735
Train - Epoch 177, Batch: 0, Loss: 0.686930
Train - Epoch 178, Batch: 0, Loss: 0.686833
Train - Epoch 179, Batch: 0, Loss: 0.686847
Train - Epoch 180, Batch: 0, Loss: 0.686533
Train - Epoch 181, Batch: 0, Loss: 0.686535
Train - Epoch 182, Batch: 0, Loss: 0.686933/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 183, Batch: 0, Loss: 0.687317
Train - Epoch 184, Batch: 0, Loss: 0.686982
Train - Epoch 185, Batch: 0, Loss: 0.686553
Train - Epoch 186, Batch: 0, Loss: 0.685984
Train - Epoch 187, Batch: 0, Loss: 0.686710
Train - Epoch 188, Batch: 0, Loss: 0.686779
Train - Epoch 189, Batch: 0, Loss: 0.686585
Train - Epoch 190, Batch: 0, Loss: 0.686526
Train - Epoch 191, Batch: 0, Loss: 0.686494
Train - Epoch 192, Batch: 0, Loss: 0.686072
Train - Epoch 193, Batch: 0, Loss: 0.686244
Train - Epoch 194, Batch: 0, Loss: 0.686385
Train - Epoch 195, Batch: 0, Loss: 0.686365
Train - Epoch 196, Batch: 0, Loss: 0.686390
Train - Epoch 197, Batch: 0, Loss: 0.686338
Train - Epoch 198, Batch: 0, Loss: 0.686027
Train - Epoch 199, Batch: 0, Loss: 0.686205
training_time:: 351.091002702713
training time full:: 351.0910437107086
provenance prepare time:: 4.76837158203125e-07
here
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.515463
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1416
max_epoch:: 200
batch_size:: 10200
epoch:: 0
tensor([  1,   2,   6,   7,   8,  13,  15,  16,  17,  21,  22,  24,  26,  37,
         42,  44,  45,  46,  48,  49,  53,  55,  56,  61,  62,  68,  69,  70,
         76,  77,  81,  85,  88,  89,  91,  98, 102, 103, 108, 111, 118, 122,
        126, 128, 136, 138, 139, 140, 144, 145])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
epoch:: 60
epoch:: 61
epoch:: 62
epoch:: 63
epoch:: 64
epoch:: 65
epoch:: 66
epoch:: 67
epoch:: 68
epoch:: 69
epoch:: 70
epoch:: 71
epoch:: 72
epoch:: 73
epoch:: 74
epoch:: 75
epoch:: 76
epoch:: 77
epoch:: 78
epoch:: 79
epoch:: 80
epoch:: 81
epoch:: 82
epoch:: 83
epoch:: 84
epoch:: 85
epoch:: 86
epoch:: 87
epoch:: 88
epoch:: 89
epoch:: 90
epoch:: 91
epoch:: 92
epoch:: 93
epoch:: 94
epoch:: 95
epoch:: 96
epoch:: 97
epoch:: 98
epoch:: 99
epoch:: 100
epoch:: 101
epoch:: 102
epoch:: 103
epoch:: 104
epoch:: 105
epoch:: 106
epoch:: 107
epoch:: 108
epoch:: 109
epoch:: 110
epoch:: 111
epoch:: 112
epoch:: 113
epoch:: 114
epoch:: 115
epoch:: 116
epoch:: 117
epoch:: 118
epoch:: 119
epoch:: 120
epoch:: 121
epoch:: 122
epoch:: 123
epoch:: 124
epoch:: 125
epoch:: 126
epoch:: 127
epoch:: 128
epoch:: 129
epoch:: 130
epoch:: 131
epoch:: 132
epoch:: 133
epoch:: 134
epoch:: 135
epoch:: 136
epoch:: 137
epoch:: 138
epoch:: 139
epoch:: 140
epoch:: 141
epoch:: 142
epoch:: 143
epoch:: 144
epoch:: 145
epoch:: 146
epoch:: 147
epoch:: 148
epoch:: 149
epoch:: 150
epoch:: 151
epoch:: 152
epoch:: 153
epoch:: 154
epoch:: 155
epoch:: 156
epoch:: 157
epoch:: 158
epoch:: 159
epoch:: 160
epoch:: 161
epoch:: 162
epoch:: 163
epoch:: 164
epoch:: 165
epoch:: 166
epoch:: 167
epoch:: 168
epoch:: 169
epoch:: 170
epoch:: 171
epoch:: 172
epoch:: 173
epoch:: 174
epoch:: 175
epoch:: 176
epoch:: 177
epoch:: 178
epoch:: 179
epoch:: 180
epoch:: 181
epoch:: 182
epoch:: 183
epoch:: 184
epoch:: 185
epoch:: 186
epoch:: 187
epoch:: 188
epoch:: 189
epoch:: 190
epoch:: 191
epoch:: 192
epoch:: 193
epoch:: 194
epoch:: 195
epoch:: 196
epoch:: 197
epoch:: 198
epoch:: 199
training time is 257.0754611492157
overhead:: 0
overhead2:: 0.452589750289917
overhead3:: 0
time_baseline:: 257.0755310058594
curr_diff: 0 tensor(0.3111, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.3111, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9325, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000067, Accuracy: 0.869232
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 0 0.07 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1416
max_epoch:: 200
overhead:: 0
overhead2:: 0.0229184627532959
overhead3:: 0.13496756553649902
overhead4:: 30.343942165374756
overhead5:: 0
memory usage:: 26599387136
time_provenance:: 68.84961557388306
curr_diff: 0 tensor(0.0019, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0019, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.3092, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.3092, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9332, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000067, Accuracy: 0.868294
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 0 0.07 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1416
max_epoch:: 200
overhead:: 0
overhead2:: 0.026724815368652344
overhead3:: 0.14402151107788086
overhead4:: 36.19073128700256
overhead5:: 0
memory usage:: 26603773952
time_provenance:: 76.54738235473633
curr_diff: 0 tensor(0.0019, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0019, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.3092, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.3092, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9332, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000067, Accuracy: 0.868294
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 0 0.07 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1416
max_epoch:: 200
overhead:: 0
overhead2:: 0.03121352195739746
overhead3:: 0.15235090255737305
overhead4:: 41.56612849235535
overhead5:: 0
memory usage:: 26606022656
time_provenance:: 83.61011290550232
curr_diff: 0 tensor(0.0019, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0019, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.3093, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.3093, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9332, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000067, Accuracy: 0.868294
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 0 0.07 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1416
max_epoch:: 200
overhead:: 0
overhead2:: 0.0422215461730957
overhead3:: 0.17415237426757812
overhead4:: 54.918798208236694
overhead5:: 0
memory usage:: 26601857024
time_provenance:: 100.21527099609375
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.3094, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.3094, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9331, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000067, Accuracy: 0.868244
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 0 0.07 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1416
max_epoch:: 200
overhead:: 0
overhead2:: 0.042970895767211914
overhead3:: 0.18175315856933594
overhead4:: 59.494946241378784
overhead5:: 0
memory usage:: 26610884608
time_provenance:: 106.03758144378662
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.3094, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.3094, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9331, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000067, Accuracy: 0.868294
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 0 0.07 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1416
max_epoch:: 200
overhead:: 0
overhead2:: 0.04686617851257324
overhead3:: 0.1889798641204834
overhead4:: 63.96349120140076
overhead5:: 0
memory usage:: 26606747648
time_provenance:: 111.60506749153137
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.3094, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.3094, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9331, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000067, Accuracy: 0.868294
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 0 0.07 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1416
max_epoch:: 200
overhead:: 0
overhead2:: 0.09560966491699219
overhead3:: 0.2894871234893799
overhead4:: 129.03846335411072
overhead5:: 0
memory usage:: 26602733568
time_provenance:: 194.00615286827087
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.3101, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.3101, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9329, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000067, Accuracy: 0.868738
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 0 0.07 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1416
max_epoch:: 200
overhead:: 0
overhead2:: 0.09233570098876953
overhead3:: 0.28848791122436523
overhead4:: 131.65234756469727
overhead5:: 0
memory usage:: 26610524160
time_provenance:: 197.32946968078613
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.3101, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.3101, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9329, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000067, Accuracy: 0.868738
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 0 0.07 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1416
max_epoch:: 200
overhead:: 0
overhead2:: 0.10042476654052734
overhead3:: 0.30016231536865234
overhead4:: 135.6747488975525
overhead5:: 0
memory usage:: 26654822400
time_provenance:: 202.25842380523682
curr_diff: 0 tensor(0.0011, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0011, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.3101, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.3101, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9329, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000067, Accuracy: 0.868738
adding noise deletion rate:: 0.08
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
tensor([0.5135, 0.5184, 0.5047,  ..., 0.5168, 0.5073, 0.5116],
       dtype=torch.float64, grad_fn=<MaxBackward0>)
tensor(0.8685, dtype=torch.float64) tensor(0., dtype=torch.float64)
Train - Epoch 0, Batch: 0, Loss: 0.693177
Train - Epoch 1, Batch: 0, Loss: 0.693122
Train - Epoch 2, Batch: 0, Loss: 0.693075
Train - Epoch 3, Batch: 0, Loss: 0.693000
Train - Epoch 4, Batch: 0, Loss: 0.692916
Train - Epoch 5, Batch: 0, Loss: 0.692839
Train - Epoch 6, Batch: 0, Loss: 0.692767
Train - Epoch 7, Batch: 0, Loss: 0.692667
Train - Epoch 8, Batch: 0, Loss: 0.692635
Train - Epoch 9, Batch: 0, Loss: 0.692532
Train - Epoch 10, Batch: 0, Loss: 0.692503
Train - Epoch 11, Batch: 0, Loss: 0.692436
Train - Epoch 12, Batch: 0, Loss: 0.692386
Train - Epoch 13, Batch: 0, Loss: 0.692269
Train - Epoch 14, Batch: 0, Loss: 0.692227
Train - Epoch 15, Batch: 0, Loss: 0.692170
Train - Epoch 16, Batch: 0, Loss: 0.692108
Train - Epoch 17, Batch: 0, Loss: 0.692018
Train - Epoch 18, Batch: 0, Loss: 0.691902
Train - Epoch 19, Batch: 0, Loss: 0.691922
Train - Epoch 20, Batch: 0, Loss: 0.691757
Train - Epoch 21, Batch: 0, Loss: 0.691783
Train - Epoch 22, Batch: 0, Loss: 0.691664
Train - Epoch 23, Batch: 0, Loss: 0.691560
Train - Epoch 24, Batch: 0, Loss: 0.691555
Train - Epoch 25, Batch: 0, Loss: 0.691421
Train - Epoch 26, Batch: 0, Loss: 0.691411
Train - Epoch 27, Batch: 0, Loss: 0.691481
Train - Epoch 28, Batch: 0, Loss: 0.691159
Train - Epoch 29, Batch: 0, Loss: 0.691140
Train - Epoch 30, Batch: 0, Loss: 0.691138
Train - Epoch 31, Batch: 0, Loss: 0.690984
Train - Epoch 32, Batch: 0, Loss: 0.691102
Train - Epoch 33, Batch: 0, Loss: 0.690993
Train - Epoch 34, Batch: 0, Loss: 0.690911
Train - Epoch 35, Batch: 0, Loss: 0.690746
Train - Epoch 36, Batch: 0, Loss: 0.690840
Train - Epoch 37, Batch: 0, Loss: 0.690792
Train - Epoch 38, Batch: 0, Loss: 0.690544
Train - Epoch 39, Batch: 0, Loss: 0.690673
Train - Epoch 40, Batch: 0, Loss: 0.690560
Train - Epoch 41, Batch: 0, Loss: 0.690460
Train - Epoch 42, Batch: 0, Loss: 0.690449
Train - Epoch 43, Batch: 0, Loss: 0.690207
Train - Epoch 44, Batch: 0, Loss: 0.690167
Train - Epoch 45, Batch: 0, Loss: 0.690298
Train - Epoch 46, Batch: 0, Loss: 0.690216
Train - Epoch 47, Batch: 0, Loss: 0.689964
Train - Epoch 48, Batch: 0, Loss: 0.690039
Train - Epoch 49, Batch: 0, Loss: 0.689987
Train - Epoch 50, Batch: 0, Loss: 0.689885
Train - Epoch 51, Batch: 0, Loss: 0.689942
Train - Epoch 52, Batch: 0, Loss: 0.689840
Train - Epoch 53, Batch: 0, Loss: 0.689823
Train - Epoch 54, Batch: 0, Loss: 0.689801
Train - Epoch 55, Batch: 0, Loss: 0.689666
Train - Epoch 56, Batch: 0, Loss: 0.689645
Train - Epoch 57, Batch: 0, Loss: 0.689618
Train - Epoch 58, Batch: 0, Loss: 0.689254
Train - Epoch 59, Batch: 0, Loss: 0.689399
Train - Epoch 60, Batch: 0, Loss: 0.689225
Train - Epoch 61, Batch: 0, Loss: 0.689158
Train - Epoch 62, Batch: 0, Loss: 0.689130
Train - Epoch 63, Batch: 0, Loss: 0.689142
Train - Epoch 64, Batch: 0, Loss: 0.689065
Train - Epoch 65, Batch: 0, Loss: 0.689059
Train - Epoch 66, Batch: 0, Loss: 0.688988
Train - Epoch 67, Batch: 0, Loss: 0.688893
Train - Epoch 68, Batch: 0, Loss: 0.688831
Train - Epoch 69, Batch: 0, Loss: 0.688720
Train - Epoch 70, Batch: 0, Loss: 0.688804
Train - Epoch 71, Batch: 0, Loss: 0.688476
Train - Epoch 72, Batch: 0, Loss: 0.688806
Train - Epoch 73, Batch: 0, Loss: 0.688657
Train - Epoch 74, Batch: 0, Loss: 0.688574
Train - Epoch 75, Batch: 0, Loss: 0.688607
Train - Epoch 76, Batch: 0, Loss: 0.688389
Train - Epoch 77, Batch: 0, Loss: 0.688493
Train - Epoch 78, Batch: 0, Loss: 0.688377
Train - Epoch 79, Batch: 0, Loss: 0.688460
Train - Epoch 80, Batch: 0, Loss: 0.688150
Train - Epoch 81, Batch: 0, Loss: 0.688234
Train - Epoch 82, Batch: 0, Loss: 0.687811
Train - Epoch 83, Batch: 0, Loss: 0.687955
Train - Epoch 84, Batch: 0, Loss: 0.688073
Train - Epoch 85, Batch: 0, Loss: 0.688201
Train - Epoch 86, Batch: 0, Loss: 0.687917
Train - Epoch 87, Batch: 0, Loss: 0.687933
Train - Epoch 88, Batch: 0, Loss: 0.687968
Train - Epoch 89, Batch: 0, Loss: 0.687946
Train - Epoch 90, Batch: 0, Loss: 0.687718
Train - Epoch 91, Batch: 0, Loss: 0.687609
Train - Epoch 92, Batch: 0, Loss: 0.687745
Train - Epoch 93, Batch: 0, Loss: 0.687551
Train - Epoch 94, Batch: 0, Loss: 0.687813
Train - Epoch 95, Batch: 0, Loss: 0.687493
Train - Epoch 96, Batch: 0, Loss: 0.686833
Train - Epoch 97, Batch: 0, Loss: 0.687773
Train - Epoch 98, Batch: 0, Loss: 0.687266
Train - Epoch 99, Batch: 0, Loss: 0.687278
Train - Epoch 100, Batch: 0, Loss: 0.686944
Train - Epoch 101, Batch: 0, Loss: 0.687103
Train - Epoch 102, Batch: 0, Loss: 0.686819
Train - Epoch 103, Batch: 0, Loss: 0.686997
Train - Epoch 104, Batch: 0, Loss: 0.687180
Train - Epoch 105, Batch: 0, Loss: 0.686633
Train - Epoch 106, Batch: 0, Loss: 0.686900
Train - Epoch 107, Batch: 0, Loss: 0.686987
Train - Epoch 108, Batch: 0, Loss: 0.686781
Train - Epoch 109, Batch: 0, Loss: 0.686796
Train - Epoch 110, Batch: 0, Loss: 0.686777
Train - Epoch 111, Batch: 0, Loss: 0.686572
Train - Epoch 112, Batch: 0, Loss: 0.686754
Train - Epoch 113, Batch: 0, Loss: 0.686434
Train - Epoch 114, Batch: 0, Loss: 0.686772
Train - Epoch 115, Batch: 0, Loss: 0.686368
Train - Epoch 116, Batch: 0, Loss: 0.686333
Train - Epoch 117, Batch: 0, Loss: 0.686391
Train - Epoch 118, Batch: 0, Loss: 0.686416
Train - Epoch 119, Batch: 0, Loss: 0.686441
Train - Epoch 120, Batch: 0, Loss: 0.686201
Train - Epoch 121, Batch: 0, Loss: 0.686247
Train - Epoch 122, Batch: 0, Loss: 0.686090
Train - Epoch 123, Batch: 0, Loss: 0.685671
Train - Epoch 124, Batch: 0, Loss: 0.686219
Train - Epoch 125, Batch: 0, Loss: 0.685876
Train - Epoch 126, Batch: 0, Loss: 0.686216
Train - Epoch 127, Batch: 0, Loss: 0.685976
Train - Epoch 128, Batch: 0, Loss: 0.685534
Train - Epoch 129, Batch: 0, Loss: 0.685742
Train - Epoch 130, Batch: 0, Loss: 0.685606
Train - Epoch 131, Batch: 0, Loss: 0.685953
Train - Epoch 132, Batch: 0, Loss: 0.685653
Train - Epoch 133, Batch: 0, Loss: 0.685724
Train - Epoch 134, Batch: 0, Loss: 0.685690
Train - Epoch 135, Batch: 0, Loss: 0.685773
Train - Epoch 136, Batch: 0, Loss: 0.685563
Train - Epoch 137, Batch: 0, Loss: 0.685545
Train - Epoch 138, Batch: 0, Loss: 0.684928
Train - Epoch 139, Batch: 0, Loss: 0.685380
Train - Epoch 140, Batch: 0, Loss: 0.685223
Train - Epoch 141, Batch: 0, Loss: 0.685718
Train - Epoch 142, Batch: 0, Loss: 0.685708
Train - Epoch 143, Batch: 0, Loss: 0.685524
Train - Epoch 144, Batch: 0, Loss: 0.685371
Train - Epoch 145, Batch: 0, Loss: 0.684991
Train - Epoch 146, Batch: 0, Loss: 0.685364
Train - Epoch 147, Batch: 0, Loss: 0.684611
Train - Epoch 148, Batch: 0, Loss: 0.684926
Train - Epoch 149, Batch: 0, Loss: 0.684717
Train - Epoch 150, Batch: 0, Loss: 0.684681
Train - Epoch 151, Batch: 0, Loss: 0.684967
Train - Epoch 152, Batch: 0, Loss: 0.684921
Train - Epoch 153, Batch: 0, Loss: 0.684367
Train - Epoch 154, Batch: 0, Loss: 0.684525
Train - Epoch 155, Batch: 0, Loss: 0.684558
Train - Epoch 156, Batch: 0, Loss: 0.684501
Train - Epoch 157, Batch: 0, Loss: 0.684543
Train - Epoch 158, Batch: 0, Loss: 0.684568
Train - Epoch 159, Batch: 0, Loss: 0.684544
Train - Epoch 160, Batch: 0, Loss: 0.684238
Train - Epoch 161, Batch: 0, Loss: 0.684652
Train - Epoch 162, Batch: 0, Loss: 0.684593
Train - Epoch 163, Batch: 0, Loss: 0.684392
Train - Epoch 164, Batch: 0, Loss: 0.684035
Train - Epoch 165, Batch: 0, Loss: 0.684049
Train - Epoch 166, Batch: 0, Loss: 0.683989
Train - Epoch 167, Batch: 0, Loss: 0.684112
Train - Epoch 168, Batch: 0, Loss: 0.683873
Train - Epoch 169, Batch: 0, Loss: 0.684362
Train - Epoch 170, Batch: 0, Loss: 0.683903
Train - Epoch 171, Batch: 0, Loss: 0.684155
Train - Epoch 172, Batch: 0, Loss: 0.684369
Train - Epoch 173, Batch: 0, Loss: 0.684070
Train - Epoch 174, Batch: 0, Loss: 0.683950
Train - Epoch 175, Batch: 0, Loss: 0.683772
Train - Epoch 176, Batch: 0, Loss: 0.683526
Train - Epoch 177, Batch: 0, Loss: 0.684178
Train - Epoch 178, Batch: 0, Loss: 0.683711
Train - Epoch 179, Batch: 0, Loss: 0.683548
Train - Epoch 180, Batch: 0, Loss: 0.683756
Train - Epoch 181, Batch: 0, Loss: 0.683903
Train - Epoch 182, Batch: 0, Loss: 0.683472/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 183, Batch: 0, Loss: 0.683216
Train - Epoch 184, Batch: 0, Loss: 0.683245
Train - Epoch 185, Batch: 0, Loss: 0.683518
Train - Epoch 186, Batch: 0, Loss: 0.683648
Train - Epoch 187, Batch: 0, Loss: 0.682942
Train - Epoch 188, Batch: 0, Loss: 0.683567
Train - Epoch 189, Batch: 0, Loss: 0.683085
Train - Epoch 190, Batch: 0, Loss: 0.683357
Train - Epoch 191, Batch: 0, Loss: 0.683010
Train - Epoch 192, Batch: 0, Loss: 0.683386
Train - Epoch 193, Batch: 0, Loss: 0.683436
Train - Epoch 194, Batch: 0, Loss: 0.683422
Train - Epoch 195, Batch: 0, Loss: 0.683135
Train - Epoch 196, Batch: 0, Loss: 0.683015
Train - Epoch 197, Batch: 0, Loss: 0.683376
Train - Epoch 198, Batch: 0, Loss: 0.683133
Train - Epoch 199, Batch: 0, Loss: 0.683060
training_time:: 352.46775579452515
training time full:: 352.4677953720093
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.585318
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1619
max_epoch:: 200
batch_size:: 10200
epoch:: 0
tensor([  1,   2,   5,   6,   8,  13,  15,  16,  17,  19,  22,  24,  26,  37,
         40,  42,  44,  45,  46,  49,  53,  56,  62,  68,  69,  70,  76,  77,
         79,  81,  82,  85,  88,  89,  91,  95,  98, 101, 102, 105, 108, 113,
        114, 118, 122, 126, 128, 129, 135, 136])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
epoch:: 60
epoch:: 61
epoch:: 62
epoch:: 63
epoch:: 64
epoch:: 65
epoch:: 66
epoch:: 67
epoch:: 68
epoch:: 69
epoch:: 70
epoch:: 71
epoch:: 72
epoch:: 73
epoch:: 74
epoch:: 75
epoch:: 76
epoch:: 77
epoch:: 78
epoch:: 79
epoch:: 80
epoch:: 81
epoch:: 82
epoch:: 83
epoch:: 84
epoch:: 85
epoch:: 86
epoch:: 87
epoch:: 88
epoch:: 89
epoch:: 90
epoch:: 91
epoch:: 92
epoch:: 93
epoch:: 94
epoch:: 95
epoch:: 96
epoch:: 97
epoch:: 98
epoch:: 99
epoch:: 100
epoch:: 101
epoch:: 102
epoch:: 103
epoch:: 104
epoch:: 105
epoch:: 106
epoch:: 107
epoch:: 108
epoch:: 109
epoch:: 110
epoch:: 111
epoch:: 112
epoch:: 113
epoch:: 114
epoch:: 115
epoch:: 116
epoch:: 117
epoch:: 118
epoch:: 119
epoch:: 120
epoch:: 121
epoch:: 122
epoch:: 123
epoch:: 124
epoch:: 125
epoch:: 126
epoch:: 127
epoch:: 128
epoch:: 129
epoch:: 130
epoch:: 131
epoch:: 132
epoch:: 133
epoch:: 134
epoch:: 135
epoch:: 136
epoch:: 137
epoch:: 138
epoch:: 139
epoch:: 140
epoch:: 141
epoch:: 142
epoch:: 143
epoch:: 144
epoch:: 145
epoch:: 146
epoch:: 147
epoch:: 148
epoch:: 149
epoch:: 150
epoch:: 151
epoch:: 152
epoch:: 153
epoch:: 154
epoch:: 155
epoch:: 156
epoch:: 157
epoch:: 158
epoch:: 159
epoch:: 160
epoch:: 161
epoch:: 162
epoch:: 163
epoch:: 164
epoch:: 165
epoch:: 166
epoch:: 167
epoch:: 168
epoch:: 169
epoch:: 170
epoch:: 171
epoch:: 172
epoch:: 173
epoch:: 174
epoch:: 175
epoch:: 176
epoch:: 177
epoch:: 178
epoch:: 179
epoch:: 180
epoch:: 181
epoch:: 182
epoch:: 183
epoch:: 184
epoch:: 185
epoch:: 186
epoch:: 187
epoch:: 188
epoch:: 189
epoch:: 190
epoch:: 191
epoch:: 192
epoch:: 193
epoch:: 194
epoch:: 195
epoch:: 196
epoch:: 197
epoch:: 198
epoch:: 199
training time is 254.20060777664185
overhead:: 0
overhead2:: 0.44092249870300293
overhead3:: 0
time_baseline:: 254.20068049430847
curr_diff: 0 tensor(0.3959, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.3959, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8988, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.750914
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 0 0.08 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1619
max_epoch:: 200
overhead:: 0
overhead2:: 0.02357649803161621
overhead3:: 0.13589882850646973
overhead4:: 30.064346075057983
overhead5:: 0
memory usage:: 26621476864
time_provenance:: 72.1549506187439
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.3930, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.3930, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9004, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.749975
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 0 0.08 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1619
max_epoch:: 200
overhead:: 0
overhead2:: 0.027031898498535156
overhead3:: 0.14609146118164062
overhead4:: 35.13517189025879
overhead5:: 0
memory usage:: 26616524800
time_provenance:: 78.67478370666504
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.3930, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.3930, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9004, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.749975
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 0 0.08 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1619
max_epoch:: 200
overhead:: 0
overhead2:: 0.030298948287963867
overhead3:: 0.15373826026916504
overhead4:: 40.91400170326233
overhead5:: 0
memory usage:: 26599886848
time_provenance:: 86.16003608703613
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.3931, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.3931, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9004, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.749975
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 0 0.08 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1619
max_epoch:: 200
overhead:: 0
overhead2:: 0.04077744483947754
overhead3:: 0.17290830612182617
overhead4:: 53.87924814224243
overhead5:: 0
memory usage:: 26606104576
time_provenance:: 102.91575407981873
curr_diff: 0 tensor(0.0027, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0027, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.3932, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.3932, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9003, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.749975
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 0 0.08 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1619
max_epoch:: 200
overhead:: 0
overhead2:: 0.04550528526306152
overhead3:: 0.18356966972351074
overhead4:: 58.425172328948975
overhead5:: 0
memory usage:: 26602942464
time_provenance:: 108.11838293075562
curr_diff: 0 tensor(0.0027, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0027, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.3932, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.3932, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9003, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.749975
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 0 0.08 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1619
max_epoch:: 200
overhead:: 0
overhead2:: 0.050121307373046875
overhead3:: 0.18895721435546875
overhead4:: 63.33622407913208
overhead5:: 0
memory usage:: 26602889216
time_provenance:: 114.61473250389099
curr_diff: 0 tensor(0.0027, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0027, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.3932, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.3932, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9003, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.749975
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 0 0.08 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1619
max_epoch:: 200
overhead:: 0
overhead2:: 0.09116339683532715
overhead3:: 0.2847316265106201
overhead4:: 126.96321296691895
overhead5:: 0
memory usage:: 26601250816
time_provenance:: 194.74072098731995
curr_diff: 0 tensor(0.0016, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0016, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.3943, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.3943, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8997, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.750469
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 0 0.08 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1619
max_epoch:: 200
overhead:: 0
overhead2:: 0.0974431037902832
overhead3:: 0.2982668876647949
overhead4:: 130.5384247303009
overhead5:: 0
memory usage:: 26612842496
time_provenance:: 199.07007241249084
curr_diff: 0 tensor(0.0016, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0016, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.3943, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.3943, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8997, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.750469
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 0 0.08 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1619
max_epoch:: 200
overhead:: 0
overhead2:: 0.1023552417755127
overhead3:: 0.29883694648742676
overhead4:: 133.89945435523987
overhead5:: 0
memory usage:: 26645037056
time_provenance:: 203.22840285301208
curr_diff: 0 tensor(0.0016, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0016, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.3943, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.3943, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8997, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.750469
adding noise deletion rate:: 0.1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
tensor([0.5230, 0.5299, 0.5100,  ..., 0.5110, 0.5153, 0.5023],
       dtype=torch.float64, grad_fn=<MaxBackward0>)
tensor(0.8685, dtype=torch.float64) tensor(0., dtype=torch.float64)
Train - Epoch 0, Batch: 0, Loss: 0.693224
Train - Epoch 1, Batch: 0, Loss: 0.693129
Train - Epoch 2, Batch: 0, Loss: 0.693008
Train - Epoch 3, Batch: 0, Loss: 0.692874
Train - Epoch 4, Batch: 0, Loss: 0.692745
Train - Epoch 5, Batch: 0, Loss: 0.692608
Train - Epoch 6, Batch: 0, Loss: 0.692535
Train - Epoch 7, Batch: 0, Loss: 0.692402
Train - Epoch 8, Batch: 0, Loss: 0.692267
Train - Epoch 9, Batch: 0, Loss: 0.692191
Train - Epoch 10, Batch: 0, Loss: 0.691949
Train - Epoch 11, Batch: 0, Loss: 0.691949
Train - Epoch 12, Batch: 0, Loss: 0.691808
Train - Epoch 13, Batch: 0, Loss: 0.691720
Train - Epoch 14, Batch: 0, Loss: 0.691565
Train - Epoch 15, Batch: 0, Loss: 0.691461
Train - Epoch 16, Batch: 0, Loss: 0.691323
Train - Epoch 17, Batch: 0, Loss: 0.691193
Train - Epoch 18, Batch: 0, Loss: 0.691182
Train - Epoch 19, Batch: 0, Loss: 0.690985
Train - Epoch 20, Batch: 0, Loss: 0.690942
Train - Epoch 21, Batch: 0, Loss: 0.690794
Train - Epoch 22, Batch: 0, Loss: 0.690715
Train - Epoch 23, Batch: 0, Loss: 0.690515
Train - Epoch 24, Batch: 0, Loss: 0.690509
Train - Epoch 25, Batch: 0, Loss: 0.690351
Train - Epoch 26, Batch: 0, Loss: 0.690350
Train - Epoch 27, Batch: 0, Loss: 0.690028
Train - Epoch 28, Batch: 0, Loss: 0.690046
Train - Epoch 29, Batch: 0, Loss: 0.689954
Train - Epoch 30, Batch: 0, Loss: 0.689810
Train - Epoch 31, Batch: 0, Loss: 0.689796
Train - Epoch 32, Batch: 0, Loss: 0.689476
Train - Epoch 33, Batch: 0, Loss: 0.689624
Train - Epoch 34, Batch: 0, Loss: 0.689408
Train - Epoch 35, Batch: 0, Loss: 0.689301
Train - Epoch 36, Batch: 0, Loss: 0.689122
Train - Epoch 37, Batch: 0, Loss: 0.689052
Train - Epoch 38, Batch: 0, Loss: 0.689063
Train - Epoch 39, Batch: 0, Loss: 0.688785
Train - Epoch 40, Batch: 0, Loss: 0.688697
Train - Epoch 41, Batch: 0, Loss: 0.688551
Train - Epoch 42, Batch: 0, Loss: 0.688625
Train - Epoch 43, Batch: 0, Loss: 0.688469
Train - Epoch 44, Batch: 0, Loss: 0.688239
Train - Epoch 45, Batch: 0, Loss: 0.688183
Train - Epoch 46, Batch: 0, Loss: 0.688283
Train - Epoch 47, Batch: 0, Loss: 0.688101
Train - Epoch 48, Batch: 0, Loss: 0.688034
Train - Epoch 49, Batch: 0, Loss: 0.687845
Train - Epoch 50, Batch: 0, Loss: 0.687775
Train - Epoch 51, Batch: 0, Loss: 0.687399
Train - Epoch 52, Batch: 0, Loss: 0.687576
Train - Epoch 53, Batch: 0, Loss: 0.687355
Train - Epoch 54, Batch: 0, Loss: 0.687218
Train - Epoch 55, Batch: 0, Loss: 0.687362
Train - Epoch 56, Batch: 0, Loss: 0.686990
Train - Epoch 57, Batch: 0, Loss: 0.687074
Train - Epoch 58, Batch: 0, Loss: 0.686808
Train - Epoch 59, Batch: 0, Loss: 0.686535
Train - Epoch 60, Batch: 0, Loss: 0.686633
Train - Epoch 61, Batch: 0, Loss: 0.686732
Train - Epoch 62, Batch: 0, Loss: 0.686389
Train - Epoch 63, Batch: 0, Loss: 0.686401
Train - Epoch 64, Batch: 0, Loss: 0.686271
Train - Epoch 65, Batch: 0, Loss: 0.686151
Train - Epoch 66, Batch: 0, Loss: 0.686298
Train - Epoch 67, Batch: 0, Loss: 0.686014
Train - Epoch 68, Batch: 0, Loss: 0.686000
Train - Epoch 69, Batch: 0, Loss: 0.685849
Train - Epoch 70, Batch: 0, Loss: 0.685904
Train - Epoch 71, Batch: 0, Loss: 0.685799
Train - Epoch 72, Batch: 0, Loss: 0.685355
Train - Epoch 73, Batch: 0, Loss: 0.685301
Train - Epoch 74, Batch: 0, Loss: 0.685093
Train - Epoch 75, Batch: 0, Loss: 0.685220
Train - Epoch 76, Batch: 0, Loss: 0.685607
Train - Epoch 77, Batch: 0, Loss: 0.685069
Train - Epoch 78, Batch: 0, Loss: 0.684847
Train - Epoch 79, Batch: 0, Loss: 0.685017
Train - Epoch 80, Batch: 0, Loss: 0.684765
Train - Epoch 81, Batch: 0, Loss: 0.684831
Train - Epoch 82, Batch: 0, Loss: 0.684934
Train - Epoch 83, Batch: 0, Loss: 0.684408
Train - Epoch 84, Batch: 0, Loss: 0.684761
Train - Epoch 85, Batch: 0, Loss: 0.684675
Train - Epoch 86, Batch: 0, Loss: 0.684410
Train - Epoch 87, Batch: 0, Loss: 0.684124
Train - Epoch 88, Batch: 0, Loss: 0.684296
Train - Epoch 89, Batch: 0, Loss: 0.683758
Train - Epoch 90, Batch: 0, Loss: 0.683838
Train - Epoch 91, Batch: 0, Loss: 0.684409
Train - Epoch 92, Batch: 0, Loss: 0.683665
Train - Epoch 93, Batch: 0, Loss: 0.683831
Train - Epoch 94, Batch: 0, Loss: 0.683358
Train - Epoch 95, Batch: 0, Loss: 0.683802
Train - Epoch 96, Batch: 0, Loss: 0.683907
Train - Epoch 97, Batch: 0, Loss: 0.682996
Train - Epoch 98, Batch: 0, Loss: 0.683106
Train - Epoch 99, Batch: 0, Loss: 0.682964
Train - Epoch 100, Batch: 0, Loss: 0.683239
Train - Epoch 101, Batch: 0, Loss: 0.683043
Train - Epoch 102, Batch: 0, Loss: 0.682928
Train - Epoch 103, Batch: 0, Loss: 0.682953
Train - Epoch 104, Batch: 0, Loss: 0.682904
Train - Epoch 105, Batch: 0, Loss: 0.682730
Train - Epoch 106, Batch: 0, Loss: 0.682613
Train - Epoch 107, Batch: 0, Loss: 0.682384
Train - Epoch 108, Batch: 0, Loss: 0.682530
Train - Epoch 109, Batch: 0, Loss: 0.682669
Train - Epoch 110, Batch: 0, Loss: 0.682521
Train - Epoch 111, Batch: 0, Loss: 0.682335
Train - Epoch 112, Batch: 0, Loss: 0.681947
Train - Epoch 113, Batch: 0, Loss: 0.682136
Train - Epoch 114, Batch: 0, Loss: 0.681656
Train - Epoch 115, Batch: 0, Loss: 0.681811
Train - Epoch 116, Batch: 0, Loss: 0.682219
Train - Epoch 117, Batch: 0, Loss: 0.682184
Train - Epoch 118, Batch: 0, Loss: 0.681429
Train - Epoch 119, Batch: 0, Loss: 0.681507
Train - Epoch 120, Batch: 0, Loss: 0.681937
Train - Epoch 121, Batch: 0, Loss: 0.681519
Train - Epoch 122, Batch: 0, Loss: 0.681316
Train - Epoch 123, Batch: 0, Loss: 0.681507
Train - Epoch 124, Batch: 0, Loss: 0.681437
Train - Epoch 125, Batch: 0, Loss: 0.680836
Train - Epoch 126, Batch: 0, Loss: 0.680815
Train - Epoch 127, Batch: 0, Loss: 0.681261
Train - Epoch 128, Batch: 0, Loss: 0.680989
Train - Epoch 129, Batch: 0, Loss: 0.680885
Train - Epoch 130, Batch: 0, Loss: 0.681135
Train - Epoch 131, Batch: 0, Loss: 0.680853
Train - Epoch 132, Batch: 0, Loss: 0.680643
Train - Epoch 133, Batch: 0, Loss: 0.680581
Train - Epoch 134, Batch: 0, Loss: 0.679879
Train - Epoch 135, Batch: 0, Loss: 0.679939
Train - Epoch 136, Batch: 0, Loss: 0.680357
Train - Epoch 137, Batch: 0, Loss: 0.680604
Train - Epoch 138, Batch: 0, Loss: 0.680211
Train - Epoch 139, Batch: 0, Loss: 0.680361
Train - Epoch 140, Batch: 0, Loss: 0.680142
Train - Epoch 141, Batch: 0, Loss: 0.679986
Train - Epoch 142, Batch: 0, Loss: 0.680069
Train - Epoch 143, Batch: 0, Loss: 0.680612
Train - Epoch 144, Batch: 0, Loss: 0.679869
Train - Epoch 145, Batch: 0, Loss: 0.679910
Train - Epoch 146, Batch: 0, Loss: 0.679283
Train - Epoch 147, Batch: 0, Loss: 0.679669
Train - Epoch 148, Batch: 0, Loss: 0.679806
Train - Epoch 149, Batch: 0, Loss: 0.679024
Train - Epoch 150, Batch: 0, Loss: 0.679115
Train - Epoch 151, Batch: 0, Loss: 0.678939
Train - Epoch 152, Batch: 0, Loss: 0.679313
Train - Epoch 153, Batch: 0, Loss: 0.679622
Train - Epoch 154, Batch: 0, Loss: 0.680123
Train - Epoch 155, Batch: 0, Loss: 0.678799
Train - Epoch 156, Batch: 0, Loss: 0.678874
Train - Epoch 157, Batch: 0, Loss: 0.678167
Train - Epoch 158, Batch: 0, Loss: 0.678776
Train - Epoch 159, Batch: 0, Loss: 0.678273
Train - Epoch 160, Batch: 0, Loss: 0.679067
Train - Epoch 161, Batch: 0, Loss: 0.678279
Train - Epoch 162, Batch: 0, Loss: 0.678874
Train - Epoch 163, Batch: 0, Loss: 0.678826
Train - Epoch 164, Batch: 0, Loss: 0.678808
Train - Epoch 165, Batch: 0, Loss: 0.678267
Train - Epoch 166, Batch: 0, Loss: 0.678135
Train - Epoch 167, Batch: 0, Loss: 0.678464
Train - Epoch 168, Batch: 0, Loss: 0.677849
Train - Epoch 169, Batch: 0, Loss: 0.677995
Train - Epoch 170, Batch: 0, Loss: 0.678205
Train - Epoch 171, Batch: 0, Loss: 0.678273
Train - Epoch 172, Batch: 0, Loss: 0.677440
Train - Epoch 173, Batch: 0, Loss: 0.678369
Train - Epoch 174, Batch: 0, Loss: 0.678010
Train - Epoch 175, Batch: 0, Loss: 0.678626
Train - Epoch 176, Batch: 0, Loss: 0.677789
Train - Epoch 177, Batch: 0, Loss: 0.678323
Train - Epoch 178, Batch: 0, Loss: 0.677283
Train - Epoch 179, Batch: 0, Loss: 0.677659
Train - Epoch 180, Batch: 0, Loss: 0.676997
Train - Epoch 181, Batch: 0, Loss: 0.677029
Train - Epoch 182, Batch: 0, Loss: 0.677349/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 183, Batch: 0, Loss: 0.677639
Train - Epoch 184, Batch: 0, Loss: 0.676678
Train - Epoch 185, Batch: 0, Loss: 0.676720
Train - Epoch 186, Batch: 0, Loss: 0.676295
Train - Epoch 187, Batch: 0, Loss: 0.678121
Train - Epoch 188, Batch: 0, Loss: 0.676896
Train - Epoch 189, Batch: 0, Loss: 0.676876
Train - Epoch 190, Batch: 0, Loss: 0.676664
Train - Epoch 191, Batch: 0, Loss: 0.676696
Train - Epoch 192, Batch: 0, Loss: 0.676888
Train - Epoch 193, Batch: 0, Loss: 0.676291
Train - Epoch 194, Batch: 0, Loss: 0.675941
Train - Epoch 195, Batch: 0, Loss: 0.676823
Train - Epoch 196, Batch: 0, Loss: 0.677020
Train - Epoch 197, Batch: 0, Loss: 0.676317
Train - Epoch 198, Batch: 0, Loss: 0.675760
Train - Epoch 199, Batch: 0, Loss: 0.676591
training_time:: 351.54109597206116
training time full:: 351.541131734848
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000067, Accuracy: 0.483500
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 2024
max_epoch:: 200
batch_size:: 10200
epoch:: 0
tensor([  1,   2,   5,   6,   8,  15,  16,  17,  19,  22,  24,  37,  40,  42,
         44,  45,  46,  49,  53,  56,  62,  68,  69,  70,  76,  77,  79,  81,
         82,  85,  88,  89,  91,  95,  98, 101, 102, 105, 108, 113, 114, 118,
        122, 126, 128, 129, 135, 136, 138, 139])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
epoch:: 60
epoch:: 61
epoch:: 62
epoch:: 63
epoch:: 64
epoch:: 65
epoch:: 66
epoch:: 67
epoch:: 68
epoch:: 69
epoch:: 70
epoch:: 71
epoch:: 72
epoch:: 73
epoch:: 74
epoch:: 75
epoch:: 76
epoch:: 77
epoch:: 78
epoch:: 79
epoch:: 80
epoch:: 81
epoch:: 82
epoch:: 83
epoch:: 84
epoch:: 85
epoch:: 86
epoch:: 87
epoch:: 88
epoch:: 89
epoch:: 90
epoch:: 91
epoch:: 92
epoch:: 93
epoch:: 94
epoch:: 95
epoch:: 96
epoch:: 97
epoch:: 98
epoch:: 99
epoch:: 100
epoch:: 101
epoch:: 102
epoch:: 103
epoch:: 104
epoch:: 105
epoch:: 106
epoch:: 107
epoch:: 108
epoch:: 109
epoch:: 110
epoch:: 111
epoch:: 112
epoch:: 113
epoch:: 114
epoch:: 115
epoch:: 116
epoch:: 117
epoch:: 118
epoch:: 119
epoch:: 120
epoch:: 121
epoch:: 122
epoch:: 123
epoch:: 124
epoch:: 125
epoch:: 126
epoch:: 127
epoch:: 128
epoch:: 129
epoch:: 130
epoch:: 131
epoch:: 132
epoch:: 133
epoch:: 134
epoch:: 135
epoch:: 136
epoch:: 137
epoch:: 138
epoch:: 139
epoch:: 140
epoch:: 141
epoch:: 142
epoch:: 143
epoch:: 144
epoch:: 145
epoch:: 146
epoch:: 147
epoch:: 148
epoch:: 149
epoch:: 150
epoch:: 151
epoch:: 152
epoch:: 153
epoch:: 154
epoch:: 155
epoch:: 156
epoch:: 157
epoch:: 158
epoch:: 159
epoch:: 160
epoch:: 161
epoch:: 162
epoch:: 163
epoch:: 164
epoch:: 165
epoch:: 166
epoch:: 167
epoch:: 168
epoch:: 169
epoch:: 170
epoch:: 171
epoch:: 172
epoch:: 173
epoch:: 174
epoch:: 175
epoch:: 176
epoch:: 177
epoch:: 178
epoch:: 179
epoch:: 180
epoch:: 181
epoch:: 182
epoch:: 183
epoch:: 184
epoch:: 185
epoch:: 186
epoch:: 187
epoch:: 188
epoch:: 189
epoch:: 190
epoch:: 191
epoch:: 192
epoch:: 193
epoch:: 194
epoch:: 195
epoch:: 196
epoch:: 197
epoch:: 198
epoch:: 199
training time is 248.37009286880493
overhead:: 0
overhead2:: 0.456679105758667
overhead3:: 0
time_baseline:: 248.37017226219177
curr_diff: 0 tensor(0.5746, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.5746, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8307, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.776158
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 0 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 2024
max_epoch:: 200
overhead:: 0
overhead2:: 0.023958683013916016
overhead3:: 0.14040136337280273
overhead4:: 29.575258016586304
overhead5:: 0
memory usage:: 26605010944
time_provenance:: 78.47083020210266
curr_diff: 0 tensor(0.0055, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0055, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.5692, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.5692, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8345, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.768600
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 0 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 2024
max_epoch:: 200
overhead:: 0
overhead2:: 0.026381254196166992
overhead3:: 0.14869952201843262
overhead4:: 34.96106791496277
overhead5:: 0
memory usage:: 26600202240
time_provenance:: 85.8662223815918
curr_diff: 0 tensor(0.0055, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0055, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.5692, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.5692, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8345, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.768551
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 0 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 2024
max_epoch:: 200
overhead:: 0
overhead2:: 0.028824329376220703
overhead3:: 0.15511202812194824
overhead4:: 39.94147753715515
overhead5:: 0
memory usage:: 26602635264
time_provenance:: 91.61517095565796
curr_diff: 0 tensor(0.0055, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0055, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.5692, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.5692, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8345, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.768600
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 0 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 2024
max_epoch:: 200
overhead:: 0
overhead2:: 0.03850984573364258
overhead3:: 0.17583632469177246
overhead4:: 53.00153636932373
overhead5:: 0
memory usage:: 26601836544
time_provenance:: 108.48363494873047
curr_diff: 0 tensor(0.0051, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0051, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.5696, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.5696, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8342, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.768847
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 0 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 2024
max_epoch:: 200
overhead:: 0
overhead2:: 0.04352164268493652
overhead3:: 0.17773222923278809
overhead4:: 57.59090828895569
overhead5:: 0
memory usage:: 26604294144
time_provenance:: 113.96616005897522
curr_diff: 0 tensor(0.0051, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0051, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.5696, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.5696, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8342, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.768798
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 0 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 2024
max_epoch:: 200
overhead:: 0
overhead2:: 0.04706430435180664
overhead3:: 0.19039154052734375
overhead4:: 62.131096839904785
overhead5:: 0
memory usage:: 26607308800
time_provenance:: 119.43088173866272
curr_diff: 0 tensor(0.0050, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0050, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.5696, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.5696, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8342, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.768847
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 0 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 2024
max_epoch:: 200
overhead:: 0
overhead2:: 0.1010286808013916
overhead3:: 0.29309606552124023
overhead4:: 125.7213442325592
overhead5:: 0
memory usage:: 26604347392
time_provenance:: 199.14480924606323
curr_diff: 0 tensor(0.0030, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0030, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.5716, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.5716, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8328, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.771465
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 0 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 2024
max_epoch:: 200
overhead:: 0
overhead2:: 0.10123682022094727
overhead3:: 0.30243420600891113
overhead4:: 128.13028407096863
overhead5:: 0
memory usage:: 26621722624
time_provenance:: 202.09052872657776
curr_diff: 0 tensor(0.0030, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0030, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.5716, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.5716, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8328, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.771465
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 0 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 2024
max_epoch:: 200
overhead:: 0
overhead2:: 0.09629154205322266
overhead3:: 0.3016815185546875
overhead4:: 131.81455183029175
overhead5:: 0
memory usage:: 26603347968
time_provenance:: 206.5735547542572
curr_diff: 0 tensor(0.0030, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0030, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.5716, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.5716, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8328, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.771465
repetition 1
python3 benchmark_exp_lr.py 0.001 10200 200 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression rcv1 1 0.005 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 0.693153
Train - Epoch 1, Batch: 0, Loss: 0.693062
Train - Epoch 2, Batch: 0, Loss: 0.692921
Train - Epoch 3, Batch: 0, Loss: 0.692807
Train - Epoch 4, Batch: 0, Loss: 0.692676
Train - Epoch 5, Batch: 0, Loss: 0.692570
Train - Epoch 6, Batch: 0, Loss: 0.692469
Train - Epoch 7, Batch: 0, Loss: 0.692339
Train - Epoch 8, Batch: 0, Loss: 0.692229
Train - Epoch 9, Batch: 0, Loss: 0.692105
Train - Epoch 10, Batch: 0, Loss: 0.691994
Train - Epoch 11, Batch: 0, Loss: 0.691940
Train - Epoch 12, Batch: 0, Loss: 0.691730
Train - Epoch 13, Batch: 0, Loss: 0.691649
Train - Epoch 14, Batch: 0, Loss: 0.691602
Train - Epoch 15, Batch: 0, Loss: 0.691331
Train - Epoch 16, Batch: 0, Loss: 0.691328
Train - Epoch 17, Batch: 0, Loss: 0.691169
Train - Epoch 18, Batch: 0, Loss: 0.691061
Train - Epoch 19, Batch: 0, Loss: 0.690859
Train - Epoch 20, Batch: 0, Loss: 0.690893
Train - Epoch 21, Batch: 0, Loss: 0.690769
Train - Epoch 22, Batch: 0, Loss: 0.690641
Train - Epoch 23, Batch: 0, Loss: 0.690461
Train - Epoch 24, Batch: 0, Loss: 0.690366
Train - Epoch 25, Batch: 0, Loss: 0.690279
Train - Epoch 26, Batch: 0, Loss: 0.690133
Train - Epoch 27, Batch: 0, Loss: 0.690083
Train - Epoch 28, Batch: 0, Loss: 0.689898
Train - Epoch 29, Batch: 0, Loss: 0.689823
Train - Epoch 30, Batch: 0, Loss: 0.689756
Train - Epoch 31, Batch: 0, Loss: 0.689450
Train - Epoch 32, Batch: 0, Loss: 0.689646
Train - Epoch 33, Batch: 0, Loss: 0.689385
Train - Epoch 34, Batch: 0, Loss: 0.689235
Train - Epoch 35, Batch: 0, Loss: 0.689123
Train - Epoch 36, Batch: 0, Loss: 0.689169
Train - Epoch 37, Batch: 0, Loss: 0.688940
Train - Epoch 38, Batch: 0, Loss: 0.688955
Train - Epoch 39, Batch: 0, Loss: 0.688700
Train - Epoch 40, Batch: 0, Loss: 0.688713
Train - Epoch 41, Batch: 0, Loss: 0.688568
Train - Epoch 42, Batch: 0, Loss: 0.688335
Train - Epoch 43, Batch: 0, Loss: 0.688439
Train - Epoch 44, Batch: 0, Loss: 0.688238
Train - Epoch 45, Batch: 0, Loss: 0.688242
Train - Epoch 46, Batch: 0, Loss: 0.687940
Train - Epoch 47, Batch: 0, Loss: 0.687945
Train - Epoch 48, Batch: 0, Loss: 0.687897
Train - Epoch 49, Batch: 0, Loss: 0.687853
Train - Epoch 50, Batch: 0, Loss: 0.687841
Train - Epoch 51, Batch: 0, Loss: 0.687265
Train - Epoch 52, Batch: 0, Loss: 0.687612
Train - Epoch 53, Batch: 0, Loss: 0.687089
Train - Epoch 54, Batch: 0, Loss: 0.687095
Train - Epoch 55, Batch: 0, Loss: 0.687430
Train - Epoch 56, Batch: 0, Loss: 0.687009
Train - Epoch 57, Batch: 0, Loss: 0.686940
Train - Epoch 58, Batch: 0, Loss: 0.686629
Train - Epoch 59, Batch: 0, Loss: 0.686832
Train - Epoch 60, Batch: 0, Loss: 0.686874
Train - Epoch 61, Batch: 0, Loss: 0.686491
Train - Epoch 62, Batch: 0, Loss: 0.686612
Train - Epoch 63, Batch: 0, Loss: 0.686339
Train - Epoch 64, Batch: 0, Loss: 0.686211
Train - Epoch 65, Batch: 0, Loss: 0.686232
Train - Epoch 66, Batch: 0, Loss: 0.686049
Train - Epoch 67, Batch: 0, Loss: 0.685912
Train - Epoch 68, Batch: 0, Loss: 0.686115
Train - Epoch 69, Batch: 0, Loss: 0.685803
Train - Epoch 70, Batch: 0, Loss: 0.685338
Train - Epoch 71, Batch: 0, Loss: 0.685654
Train - Epoch 72, Batch: 0, Loss: 0.685627
Train - Epoch 73, Batch: 0, Loss: 0.685626
Train - Epoch 74, Batch: 0, Loss: 0.685607
Train - Epoch 75, Batch: 0, Loss: 0.685017
Train - Epoch 76, Batch: 0, Loss: 0.685315
Train - Epoch 77, Batch: 0, Loss: 0.684957
Train - Epoch 78, Batch: 0, Loss: 0.685110
Train - Epoch 79, Batch: 0, Loss: 0.685083
Train - Epoch 80, Batch: 0, Loss: 0.684992
Train - Epoch 81, Batch: 0, Loss: 0.684695
Train - Epoch 82, Batch: 0, Loss: 0.684289
Train - Epoch 83, Batch: 0, Loss: 0.684617
Train - Epoch 84, Batch: 0, Loss: 0.684596
Train - Epoch 85, Batch: 0, Loss: 0.684536
Train - Epoch 86, Batch: 0, Loss: 0.684636
Train - Epoch 87, Batch: 0, Loss: 0.684225
Train - Epoch 88, Batch: 0, Loss: 0.684452
Train - Epoch 89, Batch: 0, Loss: 0.684020
Train - Epoch 90, Batch: 0, Loss: 0.683800
Train - Epoch 91, Batch: 0, Loss: 0.683873
Train - Epoch 92, Batch: 0, Loss: 0.684036
Train - Epoch 93, Batch: 0, Loss: 0.683786
Train - Epoch 94, Batch: 0, Loss: 0.683408
Train - Epoch 95, Batch: 0, Loss: 0.683735
Train - Epoch 96, Batch: 0, Loss: 0.683194
Train - Epoch 97, Batch: 0, Loss: 0.683244
Train - Epoch 98, Batch: 0, Loss: 0.682987
Train - Epoch 99, Batch: 0, Loss: 0.683501
Train - Epoch 100, Batch: 0, Loss: 0.683148
Train - Epoch 101, Batch: 0, Loss: 0.682923
Train - Epoch 102, Batch: 0, Loss: 0.682927
Train - Epoch 103, Batch: 0, Loss: 0.682776
Train - Epoch 104, Batch: 0, Loss: 0.682867
Train - Epoch 105, Batch: 0, Loss: 0.682444
Train - Epoch 106, Batch: 0, Loss: 0.682585
Train - Epoch 107, Batch: 0, Loss: 0.682606
Train - Epoch 108, Batch: 0, Loss: 0.682153
Train - Epoch 109, Batch: 0, Loss: 0.682612
Train - Epoch 110, Batch: 0, Loss: 0.682313
Train - Epoch 111, Batch: 0, Loss: 0.682166
Train - Epoch 112, Batch: 0, Loss: 0.681943
Train - Epoch 113, Batch: 0, Loss: 0.681728
Train - Epoch 114, Batch: 0, Loss: 0.682119
Train - Epoch 115, Batch: 0, Loss: 0.681714
Train - Epoch 116, Batch: 0, Loss: 0.682262
Train - Epoch 117, Batch: 0, Loss: 0.681755
Train - Epoch 118, Batch: 0, Loss: 0.681406
Train - Epoch 119, Batch: 0, Loss: 0.682115
Train - Epoch 120, Batch: 0, Loss: 0.681774
Train - Epoch 121, Batch: 0, Loss: 0.681607
Train - Epoch 122, Batch: 0, Loss: 0.681409
Train - Epoch 123, Batch: 0, Loss: 0.681085
Train - Epoch 124, Batch: 0, Loss: 0.681626
Train - Epoch 125, Batch: 0, Loss: 0.681524
Train - Epoch 126, Batch: 0, Loss: 0.681315
Train - Epoch 127, Batch: 0, Loss: 0.681202
Train - Epoch 128, Batch: 0, Loss: 0.680977
Train - Epoch 129, Batch: 0, Loss: 0.680745
Train - Epoch 130, Batch: 0, Loss: 0.680321
Train - Epoch 131, Batch: 0, Loss: 0.680602
Train - Epoch 132, Batch: 0, Loss: 0.680759
Train - Epoch 133, Batch: 0, Loss: 0.680428
Train - Epoch 134, Batch: 0, Loss: 0.680823
Train - Epoch 135, Batch: 0, Loss: 0.680341
Train - Epoch 136, Batch: 0, Loss: 0.680386
Train - Epoch 137, Batch: 0, Loss: 0.680100
Train - Epoch 138, Batch: 0, Loss: 0.680039
Train - Epoch 139, Batch: 0, Loss: 0.679917
Train - Epoch 140, Batch: 0, Loss: 0.680009
Train - Epoch 141, Batch: 0, Loss: 0.679640
Train - Epoch 142, Batch: 0, Loss: 0.679802
Train - Epoch 143, Batch: 0, Loss: 0.680009
Train - Epoch 144, Batch: 0, Loss: 0.679692
Train - Epoch 145, Batch: 0, Loss: 0.679429
Train - Epoch 146, Batch: 0, Loss: 0.679524
Train - Epoch 147, Batch: 0, Loss: 0.679028
Train - Epoch 148, Batch: 0, Loss: 0.679092
Train - Epoch 149, Batch: 0, Loss: 0.679766
Train - Epoch 150, Batch: 0, Loss: 0.678872
Train - Epoch 151, Batch: 0, Loss: 0.679115
Train - Epoch 152, Batch: 0, Loss: 0.679212
Train - Epoch 153, Batch: 0, Loss: 0.679507
Train - Epoch 154, Batch: 0, Loss: 0.679155
Train - Epoch 155, Batch: 0, Loss: 0.678926
Train - Epoch 156, Batch: 0, Loss: 0.678666
Train - Epoch 157, Batch: 0, Loss: 0.678765
Train - Epoch 158, Batch: 0, Loss: 0.678712
Train - Epoch 159, Batch: 0, Loss: 0.678860
Train - Epoch 160, Batch: 0, Loss: 0.677890
Train - Epoch 161, Batch: 0, Loss: 0.678942
Train - Epoch 162, Batch: 0, Loss: 0.678038
Train - Epoch 163, Batch: 0, Loss: 0.678413
Train - Epoch 164, Batch: 0, Loss: 0.678163
Train - Epoch 165, Batch: 0, Loss: 0.677852
Train - Epoch 166, Batch: 0, Loss: 0.678258
Train - Epoch 167, Batch: 0, Loss: 0.678195
Train - Epoch 168, Batch: 0, Loss: 0.677469
Train - Epoch 169, Batch: 0, Loss: 0.678328
Train - Epoch 170, Batch: 0, Loss: 0.678365
Train - Epoch 171, Batch: 0, Loss: 0.677778
Train - Epoch 172, Batch: 0, Loss: 0.677590
Train - Epoch 173, Batch: 0, Loss: 0.677662
Train - Epoch 174, Batch: 0, Loss: 0.678408
Train - Epoch 175, Batch: 0, Loss: 0.676909
Train - Epoch 176, Batch: 0, Loss: 0.677160
Train - Epoch 177, Batch: 0, Loss: 0.678054
Train - Epoch 178, Batch: 0, Loss: 0.677723
Train - Epoch 179, Batch: 0, Loss: 0.677004
Train - Epoch 180, Batch: 0, Loss: 0.677290
Train - Epoch 181, Batch: 0, Loss: 0.677075
Train - Epoch 182, Batch: 0, Loss: 0.677103
Train - Epoch 183, Batch: 0, Loss: 0.677115
Train - Epoch 184, Batch: 0, Loss: 0.677314
Train - Epoch 185, Batch: 0, Loss: 0.677424
Train - Epoch 186, Batch: 0, Loss: 0.677471/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 187, Batch: 0, Loss: 0.676130
Train - Epoch 188, Batch: 0, Loss: 0.676628
Train - Epoch 189, Batch: 0, Loss: 0.676558
Train - Epoch 190, Batch: 0, Loss: 0.676756
Train - Epoch 191, Batch: 0, Loss: 0.676614
Train - Epoch 192, Batch: 0, Loss: 0.676690
Train - Epoch 193, Batch: 0, Loss: 0.676320
Train - Epoch 194, Batch: 0, Loss: 0.675866
Train - Epoch 195, Batch: 0, Loss: 0.676405
Train - Epoch 196, Batch: 0, Loss: 0.675724
Train - Epoch 197, Batch: 0, Loss: 0.676130
Train - Epoch 198, Batch: 0, Loss: 0.675644
Train - Epoch 199, Batch: 0, Loss: 0.676878
training_time:: 352.0040533542633
training time full:: 352.00411915779114
provenance prepare time:: 7.152557373046875e-07
here
start test::
Test Avg. Loss: 0.000067, Accuracy: 0.484982
adding noise deletion rate:: 0.01
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
tensor([0.5259, 0.5366, 0.5105,  ..., 0.5308, 0.5154, 0.5178],
       dtype=torch.float64, grad_fn=<MaxBackward0>)
tensor(0.8685, dtype=torch.float64) tensor(0., dtype=torch.float64)
Train - Epoch 0, Batch: 0, Loss: 0.693087
Train - Epoch 1, Batch: 0, Loss: 0.692992
Train - Epoch 2, Batch: 0, Loss: 0.692938
Train - Epoch 3, Batch: 0, Loss: 0.692866
Train - Epoch 4, Batch: 0, Loss: 0.692762
Train - Epoch 5, Batch: 0, Loss: 0.692708
Train - Epoch 6, Batch: 0, Loss: 0.692602
Train - Epoch 7, Batch: 0, Loss: 0.692513
Train - Epoch 8, Batch: 0, Loss: 0.692465
Train - Epoch 9, Batch: 0, Loss: 0.692387
Train - Epoch 10, Batch: 0, Loss: 0.692328
Train - Epoch 11, Batch: 0, Loss: 0.692319
Train - Epoch 12, Batch: 0, Loss: 0.692190
Train - Epoch 13, Batch: 0, Loss: 0.692142
Train - Epoch 14, Batch: 0, Loss: 0.692118
Train - Epoch 15, Batch: 0, Loss: 0.691900
Train - Epoch 16, Batch: 0, Loss: 0.691904
Train - Epoch 17, Batch: 0, Loss: 0.691823
Train - Epoch 18, Batch: 0, Loss: 0.691766
Train - Epoch 19, Batch: 0, Loss: 0.691597
Train - Epoch 20, Batch: 0, Loss: 0.691666
Train - Epoch 21, Batch: 0, Loss: 0.691619
Train - Epoch 22, Batch: 0, Loss: 0.691454
Train - Epoch 23, Batch: 0, Loss: 0.691454
Train - Epoch 24, Batch: 0, Loss: 0.691275
Train - Epoch 25, Batch: 0, Loss: 0.691185
Train - Epoch 26, Batch: 0, Loss: 0.691127
Train - Epoch 27, Batch: 0, Loss: 0.691078
Train - Epoch 28, Batch: 0, Loss: 0.690964
Train - Epoch 29, Batch: 0, Loss: 0.690928
Train - Epoch 30, Batch: 0, Loss: 0.690900
Train - Epoch 31, Batch: 0, Loss: 0.690696
Train - Epoch 32, Batch: 0, Loss: 0.690828
Train - Epoch 33, Batch: 0, Loss: 0.690666
Train - Epoch 34, Batch: 0, Loss: 0.690544
Train - Epoch 35, Batch: 0, Loss: 0.690415
Train - Epoch 36, Batch: 0, Loss: 0.690498
Train - Epoch 37, Batch: 0, Loss: 0.690401
Train - Epoch 38, Batch: 0, Loss: 0.690381
Train - Epoch 39, Batch: 0, Loss: 0.690199
Train - Epoch 40, Batch: 0, Loss: 0.690248
Train - Epoch 41, Batch: 0, Loss: 0.690143
Train - Epoch 42, Batch: 0, Loss: 0.690059
Train - Epoch 43, Batch: 0, Loss: 0.690087
Train - Epoch 44, Batch: 0, Loss: 0.689915
Train - Epoch 45, Batch: 0, Loss: 0.689878
Train - Epoch 46, Batch: 0, Loss: 0.689810
Train - Epoch 47, Batch: 0, Loss: 0.689800
Train - Epoch 48, Batch: 0, Loss: 0.689754
Train - Epoch 49, Batch: 0, Loss: 0.689600
Train - Epoch 50, Batch: 0, Loss: 0.689679
Train - Epoch 51, Batch: 0, Loss: 0.689141
Train - Epoch 52, Batch: 0, Loss: 0.689588
Train - Epoch 53, Batch: 0, Loss: 0.689189
Train - Epoch 54, Batch: 0, Loss: 0.689125
Train - Epoch 55, Batch: 0, Loss: 0.689245
Train - Epoch 56, Batch: 0, Loss: 0.689171
Train - Epoch 57, Batch: 0, Loss: 0.688995
Train - Epoch 58, Batch: 0, Loss: 0.688950
Train - Epoch 59, Batch: 0, Loss: 0.689079
Train - Epoch 60, Batch: 0, Loss: 0.689091
Train - Epoch 61, Batch: 0, Loss: 0.688825
Train - Epoch 62, Batch: 0, Loss: 0.688903
Train - Epoch 63, Batch: 0, Loss: 0.688703
Train - Epoch 64, Batch: 0, Loss: 0.688652
Train - Epoch 65, Batch: 0, Loss: 0.688635
Train - Epoch 66, Batch: 0, Loss: 0.688508
Train - Epoch 67, Batch: 0, Loss: 0.688511
Train - Epoch 68, Batch: 0, Loss: 0.688507
Train - Epoch 69, Batch: 0, Loss: 0.688312
Train - Epoch 70, Batch: 0, Loss: 0.688137
Train - Epoch 71, Batch: 0, Loss: 0.688295
Train - Epoch 72, Batch: 0, Loss: 0.688318
Train - Epoch 73, Batch: 0, Loss: 0.688341
Train - Epoch 74, Batch: 0, Loss: 0.688195
Train - Epoch 75, Batch: 0, Loss: 0.687912
Train - Epoch 76, Batch: 0, Loss: 0.688084
Train - Epoch 77, Batch: 0, Loss: 0.687900
Train - Epoch 78, Batch: 0, Loss: 0.687890
Train - Epoch 79, Batch: 0, Loss: 0.687940
Train - Epoch 80, Batch: 0, Loss: 0.687968
Train - Epoch 81, Batch: 0, Loss: 0.687669
Train - Epoch 82, Batch: 0, Loss: 0.687355
Train - Epoch 83, Batch: 0, Loss: 0.687575
Train - Epoch 84, Batch: 0, Loss: 0.687793
Train - Epoch 85, Batch: 0, Loss: 0.687416
Train - Epoch 86, Batch: 0, Loss: 0.687326
Train - Epoch 87, Batch: 0, Loss: 0.687297
Train - Epoch 88, Batch: 0, Loss: 0.687605
Train - Epoch 89, Batch: 0, Loss: 0.687466
Train - Epoch 90, Batch: 0, Loss: 0.687087
Train - Epoch 91, Batch: 0, Loss: 0.687007
Train - Epoch 92, Batch: 0, Loss: 0.687174
Train - Epoch 93, Batch: 0, Loss: 0.687039
Train - Epoch 94, Batch: 0, Loss: 0.686629
Train - Epoch 95, Batch: 0, Loss: 0.686909
Train - Epoch 96, Batch: 0, Loss: 0.686583
Train - Epoch 97, Batch: 0, Loss: 0.686748
Train - Epoch 98, Batch: 0, Loss: 0.686547
Train - Epoch 99, Batch: 0, Loss: 0.686868
Train - Epoch 100, Batch: 0, Loss: 0.686645
Train - Epoch 101, Batch: 0, Loss: 0.686582
Train - Epoch 102, Batch: 0, Loss: 0.686469
Train - Epoch 103, Batch: 0, Loss: 0.686462
Train - Epoch 104, Batch: 0, Loss: 0.686521
Train - Epoch 105, Batch: 0, Loss: 0.686027
Train - Epoch 106, Batch: 0, Loss: 0.686158
Train - Epoch 107, Batch: 0, Loss: 0.686340
Train - Epoch 108, Batch: 0, Loss: 0.686046
Train - Epoch 109, Batch: 0, Loss: 0.686196
Train - Epoch 110, Batch: 0, Loss: 0.686350
Train - Epoch 111, Batch: 0, Loss: 0.685909
Train - Epoch 112, Batch: 0, Loss: 0.685880
Train - Epoch 113, Batch: 0, Loss: 0.685790
Train - Epoch 114, Batch: 0, Loss: 0.685771
Train - Epoch 115, Batch: 0, Loss: 0.685649
Train - Epoch 116, Batch: 0, Loss: 0.686058
Train - Epoch 117, Batch: 0, Loss: 0.685826
Train - Epoch 118, Batch: 0, Loss: 0.685694
Train - Epoch 119, Batch: 0, Loss: 0.685884
Train - Epoch 120, Batch: 0, Loss: 0.685906
Train - Epoch 121, Batch: 0, Loss: 0.685802
Train - Epoch 122, Batch: 0, Loss: 0.685499
Train - Epoch 123, Batch: 0, Loss: 0.685340
Train - Epoch 124, Batch: 0, Loss: 0.685571
Train - Epoch 125, Batch: 0, Loss: 0.685561
Train - Epoch 126, Batch: 0, Loss: 0.685351
Train - Epoch 127, Batch: 0, Loss: 0.685395
Train - Epoch 128, Batch: 0, Loss: 0.685390
Train - Epoch 129, Batch: 0, Loss: 0.685060
Train - Epoch 130, Batch: 0, Loss: 0.684735
Train - Epoch 131, Batch: 0, Loss: 0.685131
Train - Epoch 132, Batch: 0, Loss: 0.685193
Train - Epoch 133, Batch: 0, Loss: 0.684890
Train - Epoch 134, Batch: 0, Loss: 0.684847
Train - Epoch 135, Batch: 0, Loss: 0.684669
Train - Epoch 136, Batch: 0, Loss: 0.684695
Train - Epoch 137, Batch: 0, Loss: 0.684633
Train - Epoch 138, Batch: 0, Loss: 0.684604
Train - Epoch 139, Batch: 0, Loss: 0.684469
Train - Epoch 140, Batch: 0, Loss: 0.684621
Train - Epoch 141, Batch: 0, Loss: 0.684158
Train - Epoch 142, Batch: 0, Loss: 0.684395
Train - Epoch 143, Batch: 0, Loss: 0.684707
Train - Epoch 144, Batch: 0, Loss: 0.684355
Train - Epoch 145, Batch: 0, Loss: 0.683982
Train - Epoch 146, Batch: 0, Loss: 0.684141
Train - Epoch 147, Batch: 0, Loss: 0.684040
Train - Epoch 148, Batch: 0, Loss: 0.684097
Train - Epoch 149, Batch: 0, Loss: 0.684161
Train - Epoch 150, Batch: 0, Loss: 0.684008
Train - Epoch 151, Batch: 0, Loss: 0.683942
Train - Epoch 152, Batch: 0, Loss: 0.684174
Train - Epoch 153, Batch: 0, Loss: 0.684104
Train - Epoch 154, Batch: 0, Loss: 0.684217
Train - Epoch 155, Batch: 0, Loss: 0.683779
Train - Epoch 156, Batch: 0, Loss: 0.683482
Train - Epoch 157, Batch: 0, Loss: 0.683674
Train - Epoch 158, Batch: 0, Loss: 0.683594
Train - Epoch 159, Batch: 0, Loss: 0.683523
Train - Epoch 160, Batch: 0, Loss: 0.683218
Train - Epoch 161, Batch: 0, Loss: 0.683805
Train - Epoch 162, Batch: 0, Loss: 0.683575
Train - Epoch 163, Batch: 0, Loss: 0.683697
Train - Epoch 164, Batch: 0, Loss: 0.683324
Train - Epoch 165, Batch: 0, Loss: 0.683324
Train - Epoch 166, Batch: 0, Loss: 0.683576
Train - Epoch 167, Batch: 0, Loss: 0.683304
Train - Epoch 168, Batch: 0, Loss: 0.682843
Train - Epoch 169, Batch: 0, Loss: 0.683366
Train - Epoch 170, Batch: 0, Loss: 0.683581
Train - Epoch 171, Batch: 0, Loss: 0.683117
Train - Epoch 172, Batch: 0, Loss: 0.682992
Train - Epoch 173, Batch: 0, Loss: 0.682959
Train - Epoch 174, Batch: 0, Loss: 0.683620
Train - Epoch 175, Batch: 0, Loss: 0.682220
Train - Epoch 176, Batch: 0, Loss: 0.682532
Train - Epoch 177, Batch: 0, Loss: 0.682963
Train - Epoch 178, Batch: 0, Loss: 0.682893
Train - Epoch 179, Batch: 0, Loss: 0.682715
Train - Epoch 180, Batch: 0, Loss: 0.682797
Train - Epoch 181, Batch: 0, Loss: 0.682814
Train - Epoch 182, Batch: 0, Loss: 0.682443/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 183, Batch: 0, Loss: 0.682440
Train - Epoch 184, Batch: 0, Loss: 0.682939
Train - Epoch 185, Batch: 0, Loss: 0.682862
Train - Epoch 186, Batch: 0, Loss: 0.682773
Train - Epoch 187, Batch: 0, Loss: 0.681916
Train - Epoch 188, Batch: 0, Loss: 0.682259
Train - Epoch 189, Batch: 0, Loss: 0.682560
Train - Epoch 190, Batch: 0, Loss: 0.682095
Train - Epoch 191, Batch: 0, Loss: 0.682238
Train - Epoch 192, Batch: 0, Loss: 0.682515
Train - Epoch 193, Batch: 0, Loss: 0.682111
Train - Epoch 194, Batch: 0, Loss: 0.681696
Train - Epoch 195, Batch: 0, Loss: 0.682416
Train - Epoch 196, Batch: 0, Loss: 0.681892
Train - Epoch 197, Batch: 0, Loss: 0.682500
Train - Epoch 198, Batch: 0, Loss: 0.681539
Train - Epoch 199, Batch: 0, Loss: 0.682284
training_time:: 352.8567261695862
training time full:: 352.85677218437195
provenance prepare time:: 4.76837158203125e-07
here
start test::
Test Avg. Loss: 0.000067, Accuracy: 0.494418
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 202
max_epoch:: 200
batch_size:: 10200
epoch:: 0
tensor([ 0,  1,  5,  7,  8,  9, 10, 12, 17, 19, 22, 23, 25, 26, 32, 34, 36, 37,
        40, 43, 44, 46, 48, 49, 51, 52, 54, 55, 57, 58, 60, 61, 64, 67, 68, 73,
        75, 76, 77, 80, 81, 82, 84, 86, 89, 92, 93, 95, 96, 98])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
epoch:: 60
epoch:: 61
epoch:: 62
epoch:: 63
epoch:: 64
epoch:: 65
epoch:: 66
epoch:: 67
epoch:: 68
epoch:: 69
epoch:: 70
epoch:: 71
epoch:: 72
epoch:: 73
epoch:: 74
epoch:: 75
epoch:: 76
epoch:: 77
epoch:: 78
epoch:: 79
epoch:: 80
epoch:: 81
epoch:: 82
epoch:: 83
epoch:: 84
epoch:: 85
epoch:: 86
epoch:: 87
epoch:: 88
epoch:: 89
epoch:: 90
epoch:: 91
epoch:: 92
epoch:: 93
epoch:: 94
epoch:: 95
epoch:: 96
epoch:: 97
epoch:: 98
epoch:: 99
epoch:: 100
epoch:: 101
epoch:: 102
epoch:: 103
epoch:: 104
epoch:: 105
epoch:: 106
epoch:: 107
epoch:: 108
epoch:: 109
epoch:: 110
epoch:: 111
epoch:: 112
epoch:: 113
epoch:: 114
epoch:: 115
epoch:: 116
epoch:: 117
epoch:: 118
epoch:: 119
epoch:: 120
epoch:: 121
epoch:: 122
epoch:: 123
epoch:: 124
epoch:: 125
epoch:: 126
epoch:: 127
epoch:: 128
epoch:: 129
epoch:: 130
epoch:: 131
epoch:: 132
epoch:: 133
epoch:: 134
epoch:: 135
epoch:: 136
epoch:: 137
epoch:: 138
epoch:: 139
epoch:: 140
epoch:: 141
epoch:: 142
epoch:: 143
epoch:: 144
epoch:: 145
epoch:: 146
epoch:: 147
epoch:: 148
epoch:: 149
epoch:: 150
epoch:: 151
epoch:: 152
epoch:: 153
epoch:: 154
epoch:: 155
epoch:: 156
epoch:: 157
epoch:: 158
epoch:: 159
epoch:: 160
epoch:: 161
epoch:: 162
epoch:: 163
epoch:: 164
epoch:: 165
epoch:: 166
epoch:: 167
epoch:: 168
epoch:: 169
epoch:: 170
epoch:: 171
epoch:: 172
epoch:: 173
epoch:: 174
epoch:: 175
epoch:: 176
epoch:: 177
epoch:: 178
epoch:: 179
epoch:: 180
epoch:: 181
epoch:: 182
epoch:: 183
epoch:: 184
epoch:: 185
epoch:: 186
epoch:: 187
epoch:: 188
epoch:: 189
epoch:: 190
epoch:: 191
epoch:: 192
epoch:: 193
epoch:: 194
epoch:: 195
epoch:: 196
epoch:: 197
epoch:: 198
epoch:: 199
training time is 273.44991278648376
overhead:: 0
overhead2:: 0.40142393112182617
overhead3:: 0
time_baseline:: 273.44995188713074
curr_diff: 0 tensor(0.1153, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1153, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9947, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000067, Accuracy: 0.487551
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 202
max_epoch:: 200
overhead:: 0
overhead2:: 0.02054595947265625
overhead3:: 0.12204957008361816
overhead4:: 32.22559142112732
overhead5:: 0
memory usage:: 26647928832
time_provenance:: 47.6524612903595
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1151, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1151, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9947, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000067, Accuracy: 0.487551
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 202
max_epoch:: 200
overhead:: 0
overhead2:: 0.02583622932434082
overhead3:: 0.13170433044433594
overhead4:: 37.88647985458374
overhead5:: 0
memory usage:: 26635124736
time_provenance:: 54.98711609840393
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1151, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1151, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9947, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000067, Accuracy: 0.487551
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 202
max_epoch:: 200
overhead:: 0
overhead2:: 0.027545452117919922
overhead3:: 0.1322922706604004
overhead4:: 43.48523283004761
overhead5:: 0
memory usage:: 26645643264
time_provenance:: 62.29496121406555
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1151, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1151, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9947, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000067, Accuracy: 0.487551
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 202
max_epoch:: 200
overhead:: 0
overhead2:: 0.03913140296936035
overhead3:: 0.16147184371948242
overhead4:: 57.565948724746704
overhead5:: 0
memory usage:: 26678927360
time_provenance:: 80.64175987243652
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1152, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1152, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9947, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000067, Accuracy: 0.487551
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 202
max_epoch:: 200
overhead:: 0
overhead2:: 0.04177117347717285
overhead3:: 0.1695706844329834
overhead4:: 62.9267463684082
overhead5:: 0
memory usage:: 26634547200
time_provenance:: 87.57201623916626
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1152, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1152, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9947, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000067, Accuracy: 0.487551
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 202
max_epoch:: 200
overhead:: 0
overhead2:: 0.04548072814941406
overhead3:: 0.17375469207763672
overhead4:: 67.96688604354858
overhead5:: 0
memory usage:: 26645823488
time_provenance:: 94.22339582443237
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1152, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1152, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9947, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000067, Accuracy: 0.487551
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 202
max_epoch:: 200
overhead:: 0
overhead2:: 0.09206318855285645
overhead3:: 0.27573251724243164
overhead4:: 136.63906979560852
overhead5:: 0
memory usage:: 26635935744
time_provenance:: 182.9934847354889
curr_diff: 0 tensor(9.8068e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.8068e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1152, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1152, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9947, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000067, Accuracy: 0.487551
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 202
max_epoch:: 200
overhead:: 0
overhead2:: 0.09308838844299316
overhead3:: 0.28063058853149414
overhead4:: 140.46101641654968
overhead5:: 0
memory usage:: 26646077440
time_provenance:: 187.748188495636
curr_diff: 0 tensor(9.8079e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.8079e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1152, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1152, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9947, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000067, Accuracy: 0.487551
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 202
max_epoch:: 200
overhead:: 0
overhead2:: 0.09543323516845703
overhead3:: 0.28409767150878906
overhead4:: 144.285888671875
overhead5:: 0
memory usage:: 26638123008
time_provenance:: 192.50126910209656
curr_diff: 0 tensor(9.8118e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.8118e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1152, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1152, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9947, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000067, Accuracy: 0.487551
adding noise deletion rate:: 0.02
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
tensor([0.5210, 0.5263, 0.5098,  ..., 0.5259, 0.5127, 0.5153],
       dtype=torch.float64, grad_fn=<MaxBackward0>)
tensor(0.8685, dtype=torch.float64) tensor(0., dtype=torch.float64)
Train - Epoch 0, Batch: 0, Loss: 0.693171
Train - Epoch 1, Batch: 0, Loss: 0.693123
Train - Epoch 2, Batch: 0, Loss: 0.693121
Train - Epoch 3, Batch: 0, Loss: 0.693049
Train - Epoch 4, Batch: 0, Loss: 0.693035
Train - Epoch 5, Batch: 0, Loss: 0.692968
Train - Epoch 6, Batch: 0, Loss: 0.692969
Train - Epoch 7, Batch: 0, Loss: 0.692947
Train - Epoch 8, Batch: 0, Loss: 0.692903
Train - Epoch 9, Batch: 0, Loss: 0.692884
Train - Epoch 10, Batch: 0, Loss: 0.692848
Train - Epoch 11, Batch: 0, Loss: 0.692824
Train - Epoch 12, Batch: 0, Loss: 0.692781
Train - Epoch 13, Batch: 0, Loss: 0.692758
Train - Epoch 14, Batch: 0, Loss: 0.692723
Train - Epoch 15, Batch: 0, Loss: 0.692677
Train - Epoch 16, Batch: 0, Loss: 0.692640
Train - Epoch 17, Batch: 0, Loss: 0.692600
Train - Epoch 18, Batch: 0, Loss: 0.692549
Train - Epoch 19, Batch: 0, Loss: 0.692503
Train - Epoch 20, Batch: 0, Loss: 0.692547
Train - Epoch 21, Batch: 0, Loss: 0.692554
Train - Epoch 22, Batch: 0, Loss: 0.692480
Train - Epoch 23, Batch: 0, Loss: 0.692434
Train - Epoch 24, Batch: 0, Loss: 0.692422
Train - Epoch 25, Batch: 0, Loss: 0.692367
Train - Epoch 26, Batch: 0, Loss: 0.692314
Train - Epoch 27, Batch: 0, Loss: 0.692282
Train - Epoch 28, Batch: 0, Loss: 0.692266
Train - Epoch 29, Batch: 0, Loss: 0.692227
Train - Epoch 30, Batch: 0, Loss: 0.692211
Train - Epoch 31, Batch: 0, Loss: 0.692148
Train - Epoch 32, Batch: 0, Loss: 0.692178
Train - Epoch 33, Batch: 0, Loss: 0.692120
Train - Epoch 34, Batch: 0, Loss: 0.692046
Train - Epoch 35, Batch: 0, Loss: 0.692019
Train - Epoch 36, Batch: 0, Loss: 0.692016
Train - Epoch 37, Batch: 0, Loss: 0.692050
Train - Epoch 38, Batch: 0, Loss: 0.691994
Train - Epoch 39, Batch: 0, Loss: 0.691917
Train - Epoch 40, Batch: 0, Loss: 0.691980
Train - Epoch 41, Batch: 0, Loss: 0.691945
Train - Epoch 42, Batch: 0, Loss: 0.691832
Train - Epoch 43, Batch: 0, Loss: 0.691852
Train - Epoch 44, Batch: 0, Loss: 0.691760
Train - Epoch 45, Batch: 0, Loss: 0.691750
Train - Epoch 46, Batch: 0, Loss: 0.691748
Train - Epoch 47, Batch: 0, Loss: 0.691718
Train - Epoch 48, Batch: 0, Loss: 0.691747
Train - Epoch 49, Batch: 0, Loss: 0.691591
Train - Epoch 50, Batch: 0, Loss: 0.691620
Train - Epoch 51, Batch: 0, Loss: 0.691481
Train - Epoch 52, Batch: 0, Loss: 0.691601
Train - Epoch 53, Batch: 0, Loss: 0.691494
Train - Epoch 54, Batch: 0, Loss: 0.691412
Train - Epoch 55, Batch: 0, Loss: 0.691528
Train - Epoch 56, Batch: 0, Loss: 0.691435
Train - Epoch 57, Batch: 0, Loss: 0.691458
Train - Epoch 58, Batch: 0, Loss: 0.691402
Train - Epoch 59, Batch: 0, Loss: 0.691430
Train - Epoch 60, Batch: 0, Loss: 0.691427
Train - Epoch 61, Batch: 0, Loss: 0.691326
Train - Epoch 62, Batch: 0, Loss: 0.691339
Train - Epoch 63, Batch: 0, Loss: 0.691242
Train - Epoch 64, Batch: 0, Loss: 0.691260
Train - Epoch 65, Batch: 0, Loss: 0.691236
Train - Epoch 66, Batch: 0, Loss: 0.691202
Train - Epoch 67, Batch: 0, Loss: 0.691092
Train - Epoch 68, Batch: 0, Loss: 0.691084
Train - Epoch 69, Batch: 0, Loss: 0.691139
Train - Epoch 70, Batch: 0, Loss: 0.690909
Train - Epoch 71, Batch: 0, Loss: 0.691025
Train - Epoch 72, Batch: 0, Loss: 0.690962
Train - Epoch 73, Batch: 0, Loss: 0.691087
Train - Epoch 74, Batch: 0, Loss: 0.691085
Train - Epoch 75, Batch: 0, Loss: 0.690953
Train - Epoch 76, Batch: 0, Loss: 0.690921
Train - Epoch 77, Batch: 0, Loss: 0.690941
Train - Epoch 78, Batch: 0, Loss: 0.690933
Train - Epoch 79, Batch: 0, Loss: 0.690891
Train - Epoch 80, Batch: 0, Loss: 0.690858
Train - Epoch 81, Batch: 0, Loss: 0.690795
Train - Epoch 82, Batch: 0, Loss: 0.690805
Train - Epoch 83, Batch: 0, Loss: 0.690673
Train - Epoch 84, Batch: 0, Loss: 0.690743
Train - Epoch 85, Batch: 0, Loss: 0.690685
Train - Epoch 86, Batch: 0, Loss: 0.690574
Train - Epoch 87, Batch: 0, Loss: 0.690583
Train - Epoch 88, Batch: 0, Loss: 0.690673
Train - Epoch 89, Batch: 0, Loss: 0.690737
Train - Epoch 90, Batch: 0, Loss: 0.690470
Train - Epoch 91, Batch: 0, Loss: 0.690496
Train - Epoch 92, Batch: 0, Loss: 0.690518
Train - Epoch 93, Batch: 0, Loss: 0.690425
Train - Epoch 94, Batch: 0, Loss: 0.690222
Train - Epoch 95, Batch: 0, Loss: 0.690398
Train - Epoch 96, Batch: 0, Loss: 0.690268
Train - Epoch 97, Batch: 0, Loss: 0.690265
Train - Epoch 98, Batch: 0, Loss: 0.690261
Train - Epoch 99, Batch: 0, Loss: 0.690381
Train - Epoch 100, Batch: 0, Loss: 0.690167
Train - Epoch 101, Batch: 0, Loss: 0.690200
Train - Epoch 102, Batch: 0, Loss: 0.690227
Train - Epoch 103, Batch: 0, Loss: 0.690125
Train - Epoch 104, Batch: 0, Loss: 0.690201
Train - Epoch 105, Batch: 0, Loss: 0.690043
Train - Epoch 106, Batch: 0, Loss: 0.690096
Train - Epoch 107, Batch: 0, Loss: 0.690158
Train - Epoch 108, Batch: 0, Loss: 0.689922
Train - Epoch 109, Batch: 0, Loss: 0.689955
Train - Epoch 110, Batch: 0, Loss: 0.690171
Train - Epoch 111, Batch: 0, Loss: 0.689927
Train - Epoch 112, Batch: 0, Loss: 0.689829
Train - Epoch 113, Batch: 0, Loss: 0.689918
Train - Epoch 114, Batch: 0, Loss: 0.689943
Train - Epoch 115, Batch: 0, Loss: 0.689734
Train - Epoch 116, Batch: 0, Loss: 0.689922
Train - Epoch 117, Batch: 0, Loss: 0.689870
Train - Epoch 118, Batch: 0, Loss: 0.689840
Train - Epoch 119, Batch: 0, Loss: 0.689887
Train - Epoch 120, Batch: 0, Loss: 0.689917
Train - Epoch 121, Batch: 0, Loss: 0.689804
Train - Epoch 122, Batch: 0, Loss: 0.689727
Train - Epoch 123, Batch: 0, Loss: 0.689602
Train - Epoch 124, Batch: 0, Loss: 0.689781
Train - Epoch 125, Batch: 0, Loss: 0.689775
Train - Epoch 126, Batch: 0, Loss: 0.689700
Train - Epoch 127, Batch: 0, Loss: 0.689665
Train - Epoch 128, Batch: 0, Loss: 0.689704
Train - Epoch 129, Batch: 0, Loss: 0.689583
Train - Epoch 130, Batch: 0, Loss: 0.689370
Train - Epoch 131, Batch: 0, Loss: 0.689623
Train - Epoch 132, Batch: 0, Loss: 0.689563
Train - Epoch 133, Batch: 0, Loss: 0.689580
Train - Epoch 134, Batch: 0, Loss: 0.689416
Train - Epoch 135, Batch: 0, Loss: 0.689381
Train - Epoch 136, Batch: 0, Loss: 0.689502
Train - Epoch 137, Batch: 0, Loss: 0.689324
Train - Epoch 138, Batch: 0, Loss: 0.689353
Train - Epoch 139, Batch: 0, Loss: 0.689206
Train - Epoch 140, Batch: 0, Loss: 0.689292
Train - Epoch 141, Batch: 0, Loss: 0.689002
Train - Epoch 142, Batch: 0, Loss: 0.689184
Train - Epoch 143, Batch: 0, Loss: 0.689383
Train - Epoch 144, Batch: 0, Loss: 0.689033
Train - Epoch 145, Batch: 0, Loss: 0.689039
Train - Epoch 146, Batch: 0, Loss: 0.688945
Train - Epoch 147, Batch: 0, Loss: 0.689036
Train - Epoch 148, Batch: 0, Loss: 0.689043
Train - Epoch 149, Batch: 0, Loss: 0.688960
Train - Epoch 150, Batch: 0, Loss: 0.688843
Train - Epoch 151, Batch: 0, Loss: 0.689098
Train - Epoch 152, Batch: 0, Loss: 0.689141
Train - Epoch 153, Batch: 0, Loss: 0.688910
Train - Epoch 154, Batch: 0, Loss: 0.689097
Train - Epoch 155, Batch: 0, Loss: 0.688894
Train - Epoch 156, Batch: 0, Loss: 0.688823
Train - Epoch 157, Batch: 0, Loss: 0.688934
Train - Epoch 158, Batch: 0, Loss: 0.688861
Train - Epoch 159, Batch: 0, Loss: 0.688726
Train - Epoch 160, Batch: 0, Loss: 0.688723
Train - Epoch 161, Batch: 0, Loss: 0.688958
Train - Epoch 162, Batch: 0, Loss: 0.688792
Train - Epoch 163, Batch: 0, Loss: 0.688912
Train - Epoch 164, Batch: 0, Loss: 0.688531
Train - Epoch 165, Batch: 0, Loss: 0.688660
Train - Epoch 166, Batch: 0, Loss: 0.688685
Train - Epoch 167, Batch: 0, Loss: 0.688539
Train - Epoch 168, Batch: 0, Loss: 0.688572
Train - Epoch 169, Batch: 0, Loss: 0.688473
Train - Epoch 170, Batch: 0, Loss: 0.688795
Train - Epoch 171, Batch: 0, Loss: 0.688541
Train - Epoch 172, Batch: 0, Loss: 0.688508
Train - Epoch 173, Batch: 0, Loss: 0.688352
Train - Epoch 174, Batch: 0, Loss: 0.688689
Train - Epoch 175, Batch: 0, Loss: 0.688055
Train - Epoch 176, Batch: 0, Loss: 0.688411
Train - Epoch 177, Batch: 0, Loss: 0.688423
Train - Epoch 178, Batch: 0, Loss: 0.688429
Train - Epoch 179, Batch: 0, Loss: 0.688167
Train - Epoch 180, Batch: 0, Loss: 0.688519
Train - Epoch 181, Batch: 0, Loss: 0.688488
Train - Epoch 182, Batch: 0, Loss: 0.688278/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 183, Batch: 0, Loss: 0.688081
Train - Epoch 184, Batch: 0, Loss: 0.688453
Train - Epoch 185, Batch: 0, Loss: 0.688297
Train - Epoch 186, Batch: 0, Loss: 0.688516
Train - Epoch 187, Batch: 0, Loss: 0.688040
Train - Epoch 188, Batch: 0, Loss: 0.688236
Train - Epoch 189, Batch: 0, Loss: 0.688323
Train - Epoch 190, Batch: 0, Loss: 0.687993
Train - Epoch 191, Batch: 0, Loss: 0.688067
Train - Epoch 192, Batch: 0, Loss: 0.688187
Train - Epoch 193, Batch: 0, Loss: 0.688065
Train - Epoch 194, Batch: 0, Loss: 0.687889
Train - Epoch 195, Batch: 0, Loss: 0.688190
Train - Epoch 196, Batch: 0, Loss: 0.687911
Train - Epoch 197, Batch: 0, Loss: 0.688398
Train - Epoch 198, Batch: 0, Loss: 0.688006
Train - Epoch 199, Batch: 0, Loss: 0.688072
training_time:: 351.6026954650879
training time full:: 351.60273575782776
provenance prepare time:: 4.76837158203125e-07
here
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.555973
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 404
max_epoch:: 200
batch_size:: 10200
epoch:: 0
tensor([ 0,  1,  4,  5,  8,  9, 10, 12, 17, 19, 22, 23, 25, 26, 32, 36, 37, 40,
        43, 44, 46, 48, 49, 51, 52, 54, 55, 57, 58, 60, 64, 67, 68, 72, 73, 75,
        76, 77, 78, 80, 81, 82, 84, 86, 89, 92, 93, 95, 96, 98])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
epoch:: 60
epoch:: 61
epoch:: 62
epoch:: 63
epoch:: 64
epoch:: 65
epoch:: 66
epoch:: 67
epoch:: 68
epoch:: 69
epoch:: 70
epoch:: 71
epoch:: 72
epoch:: 73
epoch:: 74
epoch:: 75
epoch:: 76
epoch:: 77
epoch:: 78
epoch:: 79
epoch:: 80
epoch:: 81
epoch:: 82
epoch:: 83
epoch:: 84
epoch:: 85
epoch:: 86
epoch:: 87
epoch:: 88
epoch:: 89
epoch:: 90
epoch:: 91
epoch:: 92
epoch:: 93
epoch:: 94
epoch:: 95
epoch:: 96
epoch:: 97
epoch:: 98
epoch:: 99
epoch:: 100
epoch:: 101
epoch:: 102
epoch:: 103
epoch:: 104
epoch:: 105
epoch:: 106
epoch:: 107
epoch:: 108
epoch:: 109
epoch:: 110
epoch:: 111
epoch:: 112
epoch:: 113
epoch:: 114
epoch:: 115
epoch:: 116
epoch:: 117
epoch:: 118
epoch:: 119
epoch:: 120
epoch:: 121
epoch:: 122
epoch:: 123
epoch:: 124
epoch:: 125
epoch:: 126
epoch:: 127
epoch:: 128
epoch:: 129
epoch:: 130
epoch:: 131
epoch:: 132
epoch:: 133
epoch:: 134
epoch:: 135
epoch:: 136
epoch:: 137
epoch:: 138
epoch:: 139
epoch:: 140
epoch:: 141
epoch:: 142
epoch:: 143
epoch:: 144
epoch:: 145
epoch:: 146
epoch:: 147
epoch:: 148
epoch:: 149
epoch:: 150
epoch:: 151
epoch:: 152
epoch:: 153
epoch:: 154
epoch:: 155
epoch:: 156
epoch:: 157
epoch:: 158
epoch:: 159
epoch:: 160
epoch:: 161
epoch:: 162
epoch:: 163
epoch:: 164
epoch:: 165
epoch:: 166
epoch:: 167
epoch:: 168
epoch:: 169
epoch:: 170
epoch:: 171
epoch:: 172
epoch:: 173
epoch:: 174
epoch:: 175
epoch:: 176
epoch:: 177
epoch:: 178
epoch:: 179
epoch:: 180
epoch:: 181
epoch:: 182
epoch:: 183
epoch:: 184
epoch:: 185
epoch:: 186
epoch:: 187
epoch:: 188
epoch:: 189
epoch:: 190
epoch:: 191
epoch:: 192
epoch:: 193
epoch:: 194
epoch:: 195
epoch:: 196
epoch:: 197
epoch:: 198
epoch:: 199
training time is 271.14286375045776
overhead:: 0
overhead2:: 0.4211750030517578
overhead3:: 0
time_baseline:: 271.14290857315063
curr_diff: 0 tensor(0.1589, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1589, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9842, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.513092
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 1 0.02 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 404
max_epoch:: 200
overhead:: 0
overhead2:: 0.024092435836791992
overhead3:: 0.12610483169555664
overhead4:: 31.847153902053833
overhead5:: 0
memory usage:: 26613903360
time_provenance:: 50.346498012542725
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1586, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1586, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9842, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.513092
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 1 0.02 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 404
max_epoch:: 200
overhead:: 0
overhead2:: 0.02585744857788086
overhead3:: 0.13232898712158203
overhead4:: 37.33668851852417
overhead5:: 0
memory usage:: 26598027264
time_provenance:: 57.30088448524475
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1586, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1586, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9842, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.513092
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 1 0.02 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 404
max_epoch:: 200
overhead:: 0
overhead2:: 0.03079080581665039
overhead3:: 0.14000630378723145
overhead4:: 43.36161541938782
overhead5:: 0
memory usage:: 26619219968
time_provenance:: 65.17987322807312
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1586, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1586, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9842, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.513092
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 1 0.02 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 404
max_epoch:: 200
overhead:: 0
overhead2:: 0.03846335411071777
overhead3:: 0.1581130027770996
overhead4:: 57.125107288360596
overhead5:: 0
memory usage:: 26614853632
time_provenance:: 83.15062618255615
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1587, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1587, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9842, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.513092
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 1 0.02 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 404
max_epoch:: 200
overhead:: 0
overhead2:: 0.042090654373168945
overhead3:: 0.1721172332763672
overhead4:: 62.59632921218872
overhead5:: 0
memory usage:: 26618060800
time_provenance:: 90.17095899581909
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1587, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1587, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9842, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.513092
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 1 0.02 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 404
max_epoch:: 200
overhead:: 0
overhead2:: 0.04624319076538086
overhead3:: 0.1747133731842041
overhead4:: 67.96672010421753
overhead5:: 0
memory usage:: 26615271424
time_provenance:: 96.99774861335754
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1587, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1587, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9842, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.513092
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 1 0.02 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 404
max_epoch:: 200
overhead:: 0
overhead2:: 0.09212470054626465
overhead3:: 0.2760028839111328
overhead4:: 135.98865222930908
overhead5:: 0
memory usage:: 26597847040
time_provenance:: 185.12089371681213
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1587, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1587, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9842, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.513092
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 1 0.02 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 404
max_epoch:: 200
overhead:: 0
overhead2:: 0.09467577934265137
overhead3:: 0.28049778938293457
overhead4:: 138.75778436660767
overhead5:: 0
memory usage:: 26601254912
time_provenance:: 188.93655943870544
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1587, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1587, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9842, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.513092
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 1 0.02 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 404
max_epoch:: 200
overhead:: 0
overhead2:: 0.09682488441467285
overhead3:: 0.28943586349487305
overhead4:: 142.98970675468445
overhead5:: 0
memory usage:: 26609299456
time_provenance:: 193.99168157577515
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1587, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1587, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9842, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.513092
adding noise deletion rate:: 0.04
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
tensor([0.5112, 0.5133, 0.5040,  ..., 0.5177, 0.5066, 0.5090],
       dtype=torch.float64, grad_fn=<MaxBackward0>)
tensor(0.8685, dtype=torch.float64) tensor(0., dtype=torch.float64)
Train - Epoch 0, Batch: 0, Loss: 0.693185
Train - Epoch 1, Batch: 0, Loss: 0.693182
Train - Epoch 2, Batch: 0, Loss: 0.693157
Train - Epoch 3, Batch: 0, Loss: 0.693129
Train - Epoch 4, Batch: 0, Loss: 0.693078
Train - Epoch 5, Batch: 0, Loss: 0.693069
Train - Epoch 6, Batch: 0, Loss: 0.693035
Train - Epoch 7, Batch: 0, Loss: 0.692969
Train - Epoch 8, Batch: 0, Loss: 0.692988
Train - Epoch 9, Batch: 0, Loss: 0.692949
Train - Epoch 10, Batch: 0, Loss: 0.692899
Train - Epoch 11, Batch: 0, Loss: 0.692867
Train - Epoch 12, Batch: 0, Loss: 0.692830
Train - Epoch 13, Batch: 0, Loss: 0.692826
Train - Epoch 14, Batch: 0, Loss: 0.692793
Train - Epoch 15, Batch: 0, Loss: 0.692808
Train - Epoch 16, Batch: 0, Loss: 0.692768
Train - Epoch 17, Batch: 0, Loss: 0.692741
Train - Epoch 18, Batch: 0, Loss: 0.692721
Train - Epoch 19, Batch: 0, Loss: 0.692707
Train - Epoch 20, Batch: 0, Loss: 0.692665
Train - Epoch 21, Batch: 0, Loss: 0.692572
Train - Epoch 22, Batch: 0, Loss: 0.692626
Train - Epoch 23, Batch: 0, Loss: 0.692551
Train - Epoch 24, Batch: 0, Loss: 0.692511
Train - Epoch 25, Batch: 0, Loss: 0.692485
Train - Epoch 26, Batch: 0, Loss: 0.692506
Train - Epoch 27, Batch: 0, Loss: 0.692515
Train - Epoch 28, Batch: 0, Loss: 0.692445
Train - Epoch 29, Batch: 0, Loss: 0.692442
Train - Epoch 30, Batch: 0, Loss: 0.692397
Train - Epoch 31, Batch: 0, Loss: 0.692383
Train - Epoch 32, Batch: 0, Loss: 0.692302
Train - Epoch 33, Batch: 0, Loss: 0.692335
Train - Epoch 34, Batch: 0, Loss: 0.692246
Train - Epoch 35, Batch: 0, Loss: 0.692242
Train - Epoch 36, Batch: 0, Loss: 0.692202
Train - Epoch 37, Batch: 0, Loss: 0.692179
Train - Epoch 38, Batch: 0, Loss: 0.692176
Train - Epoch 39, Batch: 0, Loss: 0.692195
Train - Epoch 40, Batch: 0, Loss: 0.692099
Train - Epoch 41, Batch: 0, Loss: 0.692037
Train - Epoch 42, Batch: 0, Loss: 0.692068
Train - Epoch 43, Batch: 0, Loss: 0.692115
Train - Epoch 44, Batch: 0, Loss: 0.691992
Train - Epoch 45, Batch: 0, Loss: 0.691938
Train - Epoch 46, Batch: 0, Loss: 0.691913
Train - Epoch 47, Batch: 0, Loss: 0.691974
Train - Epoch 48, Batch: 0, Loss: 0.691868
Train - Epoch 49, Batch: 0, Loss: 0.691929
Train - Epoch 50, Batch: 0, Loss: 0.691838
Train - Epoch 51, Batch: 0, Loss: 0.691860
Train - Epoch 52, Batch: 0, Loss: 0.691726
Train - Epoch 53, Batch: 0, Loss: 0.691861
Train - Epoch 54, Batch: 0, Loss: 0.691760
Train - Epoch 55, Batch: 0, Loss: 0.691781
Train - Epoch 56, Batch: 0, Loss: 0.691671
Train - Epoch 57, Batch: 0, Loss: 0.691599
Train - Epoch 58, Batch: 0, Loss: 0.691696
Train - Epoch 59, Batch: 0, Loss: 0.691597
Train - Epoch 60, Batch: 0, Loss: 0.691482
Train - Epoch 61, Batch: 0, Loss: 0.691484
Train - Epoch 62, Batch: 0, Loss: 0.691503
Train - Epoch 63, Batch: 0, Loss: 0.691489
Train - Epoch 64, Batch: 0, Loss: 0.691397
Train - Epoch 65, Batch: 0, Loss: 0.691470
Train - Epoch 66, Batch: 0, Loss: 0.691465
Train - Epoch 67, Batch: 0, Loss: 0.691384
Train - Epoch 68, Batch: 0, Loss: 0.691466
Train - Epoch 69, Batch: 0, Loss: 0.691389
Train - Epoch 70, Batch: 0, Loss: 0.691432
Train - Epoch 71, Batch: 0, Loss: 0.691242
Train - Epoch 72, Batch: 0, Loss: 0.691231
Train - Epoch 73, Batch: 0, Loss: 0.691141
Train - Epoch 74, Batch: 0, Loss: 0.691253
Train - Epoch 75, Batch: 0, Loss: 0.691206
Train - Epoch 76, Batch: 0, Loss: 0.691149
Train - Epoch 77, Batch: 0, Loss: 0.691160
Train - Epoch 78, Batch: 0, Loss: 0.691132
Train - Epoch 79, Batch: 0, Loss: 0.690934
Train - Epoch 80, Batch: 0, Loss: 0.690994
Train - Epoch 81, Batch: 0, Loss: 0.690975
Train - Epoch 82, Batch: 0, Loss: 0.690902
Train - Epoch 83, Batch: 0, Loss: 0.691257
Train - Epoch 84, Batch: 0, Loss: 0.691061
Train - Epoch 85, Batch: 0, Loss: 0.691017
Train - Epoch 86, Batch: 0, Loss: 0.690967
Train - Epoch 87, Batch: 0, Loss: 0.690978
Train - Epoch 88, Batch: 0, Loss: 0.690943
Train - Epoch 89, Batch: 0, Loss: 0.690810
Train - Epoch 90, Batch: 0, Loss: 0.690969
Train - Epoch 91, Batch: 0, Loss: 0.690730
Train - Epoch 92, Batch: 0, Loss: 0.690809
Train - Epoch 93, Batch: 0, Loss: 0.690726
Train - Epoch 94, Batch: 0, Loss: 0.690981
Train - Epoch 95, Batch: 0, Loss: 0.690825
Train - Epoch 96, Batch: 0, Loss: 0.690800
Train - Epoch 97, Batch: 0, Loss: 0.690784
Train - Epoch 98, Batch: 0, Loss: 0.690670
Train - Epoch 99, Batch: 0, Loss: 0.690680
Train - Epoch 100, Batch: 0, Loss: 0.690794
Train - Epoch 101, Batch: 0, Loss: 0.690642
Train - Epoch 102, Batch: 0, Loss: 0.690647
Train - Epoch 103, Batch: 0, Loss: 0.690654
Train - Epoch 104, Batch: 0, Loss: 0.690479
Train - Epoch 105, Batch: 0, Loss: 0.690504
Train - Epoch 106, Batch: 0, Loss: 0.690625
Train - Epoch 107, Batch: 0, Loss: 0.690546
Train - Epoch 108, Batch: 0, Loss: 0.690527
Train - Epoch 109, Batch: 0, Loss: 0.690431
Train - Epoch 110, Batch: 0, Loss: 0.690401
Train - Epoch 111, Batch: 0, Loss: 0.690396
Train - Epoch 112, Batch: 0, Loss: 0.690275
Train - Epoch 113, Batch: 0, Loss: 0.690311
Train - Epoch 114, Batch: 0, Loss: 0.690389
Train - Epoch 115, Batch: 0, Loss: 0.690519
Train - Epoch 116, Batch: 0, Loss: 0.690316
Train - Epoch 117, Batch: 0, Loss: 0.690160
Train - Epoch 118, Batch: 0, Loss: 0.690164
Train - Epoch 119, Batch: 0, Loss: 0.690088
Train - Epoch 120, Batch: 0, Loss: 0.690006
Train - Epoch 121, Batch: 0, Loss: 0.690132
Train - Epoch 122, Batch: 0, Loss: 0.690089
Train - Epoch 123, Batch: 0, Loss: 0.690186
Train - Epoch 124, Batch: 0, Loss: 0.690193
Train - Epoch 125, Batch: 0, Loss: 0.689992
Train - Epoch 126, Batch: 0, Loss: 0.690095
Train - Epoch 127, Batch: 0, Loss: 0.689994
Train - Epoch 128, Batch: 0, Loss: 0.690068
Train - Epoch 129, Batch: 0, Loss: 0.689905
Train - Epoch 130, Batch: 0, Loss: 0.689889
Train - Epoch 131, Batch: 0, Loss: 0.689792
Train - Epoch 132, Batch: 0, Loss: 0.689858
Train - Epoch 133, Batch: 0, Loss: 0.689848
Train - Epoch 134, Batch: 0, Loss: 0.689737
Train - Epoch 135, Batch: 0, Loss: 0.689795
Train - Epoch 136, Batch: 0, Loss: 0.689759
Train - Epoch 137, Batch: 0, Loss: 0.689761
Train - Epoch 138, Batch: 0, Loss: 0.689609
Train - Epoch 139, Batch: 0, Loss: 0.689677
Train - Epoch 140, Batch: 0, Loss: 0.689833
Train - Epoch 141, Batch: 0, Loss: 0.689829
Train - Epoch 142, Batch: 0, Loss: 0.689841
Train - Epoch 143, Batch: 0, Loss: 0.689627
Train - Epoch 144, Batch: 0, Loss: 0.689749
Train - Epoch 145, Batch: 0, Loss: 0.689670
Train - Epoch 146, Batch: 0, Loss: 0.689822
Train - Epoch 147, Batch: 0, Loss: 0.689454
Train - Epoch 148, Batch: 0, Loss: 0.689581
Train - Epoch 149, Batch: 0, Loss: 0.689701
Train - Epoch 150, Batch: 0, Loss: 0.689685
Train - Epoch 151, Batch: 0, Loss: 0.689569
Train - Epoch 152, Batch: 0, Loss: 0.689610
Train - Epoch 153, Batch: 0, Loss: 0.689707
Train - Epoch 154, Batch: 0, Loss: 0.689419
Train - Epoch 155, Batch: 0, Loss: 0.689540
Train - Epoch 156, Batch: 0, Loss: 0.689362
Train - Epoch 157, Batch: 0, Loss: 0.689429
Train - Epoch 158, Batch: 0, Loss: 0.689357
Train - Epoch 159, Batch: 0, Loss: 0.689402
Train - Epoch 160, Batch: 0, Loss: 0.689490
Train - Epoch 161, Batch: 0, Loss: 0.689104
Train - Epoch 162, Batch: 0, Loss: 0.689287
Train - Epoch 163, Batch: 0, Loss: 0.689150
Train - Epoch 164, Batch: 0, Loss: 0.689416
Train - Epoch 165, Batch: 0, Loss: 0.689362
Train - Epoch 166, Batch: 0, Loss: 0.689358
Train - Epoch 167, Batch: 0, Loss: 0.689225
Train - Epoch 168, Batch: 0, Loss: 0.689158
Train - Epoch 169, Batch: 0, Loss: 0.689149
Train - Epoch 170, Batch: 0, Loss: 0.689008
Train - Epoch 171, Batch: 0, Loss: 0.689122
Train - Epoch 172, Batch: 0, Loss: 0.689152
Train - Epoch 173, Batch: 0, Loss: 0.689176
Train - Epoch 174, Batch: 0, Loss: 0.689193
Train - Epoch 175, Batch: 0, Loss: 0.689227
Train - Epoch 176, Batch: 0, Loss: 0.689069
Train - Epoch 177, Batch: 0, Loss: 0.689134
Train - Epoch 178, Batch: 0, Loss: 0.689047
Train - Epoch 179, Batch: 0, Loss: 0.689043
Train - Epoch 180, Batch: 0, Loss: 0.688950
Train - Epoch 181, Batch: 0, Loss: 0.688819
Train - Epoch 182, Batch: 0, Loss: 0.689186/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 183, Batch: 0, Loss: 0.689072
Train - Epoch 184, Batch: 0, Loss: 0.688618
Train - Epoch 185, Batch: 0, Loss: 0.688871
Train - Epoch 186, Batch: 0, Loss: 0.688949
Train - Epoch 187, Batch: 0, Loss: 0.688808
Train - Epoch 188, Batch: 0, Loss: 0.688713
Train - Epoch 189, Batch: 0, Loss: 0.688617
Train - Epoch 190, Batch: 0, Loss: 0.688951
Train - Epoch 191, Batch: 0, Loss: 0.688762
Train - Epoch 192, Batch: 0, Loss: 0.688842
Train - Epoch 193, Batch: 0, Loss: 0.688533
Train - Epoch 194, Batch: 0, Loss: 0.688844
Train - Epoch 195, Batch: 0, Loss: 0.688469
Train - Epoch 196, Batch: 0, Loss: 0.688655
Train - Epoch 197, Batch: 0, Loss: 0.688287
Train - Epoch 198, Batch: 0, Loss: 0.688752
Train - Epoch 199, Batch: 0, Loss: 0.688471
training_time:: 350.7787826061249
training time full:: 350.77882194519043
provenance prepare time:: 0.0
here
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.694299
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 809
max_epoch:: 200
batch_size:: 10200
epoch:: 0
tensor([  0,   1,   4,   5,   7,   8,  10,  12,  17,  19,  22,  23,  25,  32,
         34,  37,  40,  43,  44,  46,  49,  51,  52,  54,  57,  58,  60,  61,
         64,  68,  72,  73,  75,  76,  77,  78,  80,  81,  82,  84,  86,  89,
         92,  93,  95,  96,  98,  99, 102, 103])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
epoch:: 60
epoch:: 61
epoch:: 62
epoch:: 63
epoch:: 64
epoch:: 65
epoch:: 66
epoch:: 67
epoch:: 68
epoch:: 69
epoch:: 70
epoch:: 71
epoch:: 72
epoch:: 73
epoch:: 74
epoch:: 75
epoch:: 76
epoch:: 77
epoch:: 78
epoch:: 79
epoch:: 80
epoch:: 81
epoch:: 82
epoch:: 83
epoch:: 84
epoch:: 85
epoch:: 86
epoch:: 87
epoch:: 88
epoch:: 89
epoch:: 90
epoch:: 91
epoch:: 92
epoch:: 93
epoch:: 94
epoch:: 95
epoch:: 96
epoch:: 97
epoch:: 98
epoch:: 99
epoch:: 100
epoch:: 101
epoch:: 102
epoch:: 103
epoch:: 104
epoch:: 105
epoch:: 106
epoch:: 107
epoch:: 108
epoch:: 109
epoch:: 110
epoch:: 111
epoch:: 112
epoch:: 113
epoch:: 114
epoch:: 115
epoch:: 116
epoch:: 117
epoch:: 118
epoch:: 119
epoch:: 120
epoch:: 121
epoch:: 122
epoch:: 123
epoch:: 124
epoch:: 125
epoch:: 126
epoch:: 127
epoch:: 128
epoch:: 129
epoch:: 130
epoch:: 131
epoch:: 132
epoch:: 133
epoch:: 134
epoch:: 135
epoch:: 136
epoch:: 137
epoch:: 138
epoch:: 139
epoch:: 140
epoch:: 141
epoch:: 142
epoch:: 143
epoch:: 144
epoch:: 145
epoch:: 146
epoch:: 147
epoch:: 148
epoch:: 149
epoch:: 150
epoch:: 151
epoch:: 152
epoch:: 153
epoch:: 154
epoch:: 155
epoch:: 156
epoch:: 157
epoch:: 158
epoch:: 159
epoch:: 160
epoch:: 161
epoch:: 162
epoch:: 163
epoch:: 164
epoch:: 165
epoch:: 166
epoch:: 167
epoch:: 168
epoch:: 169
epoch:: 170
epoch:: 171
epoch:: 172
epoch:: 173
epoch:: 174
epoch:: 175
epoch:: 176
epoch:: 177
epoch:: 178
epoch:: 179
epoch:: 180
epoch:: 181
epoch:: 182
epoch:: 183
epoch:: 184
epoch:: 185
epoch:: 186
epoch:: 187
epoch:: 188
epoch:: 189
epoch:: 190
epoch:: 191
epoch:: 192
epoch:: 193
epoch:: 194
epoch:: 195
epoch:: 196
epoch:: 197
epoch:: 198
epoch:: 199
training time is 265.1762754917145
overhead:: 0
overhead2:: 0.41878390312194824
overhead3:: 0
time_baseline:: 265.1763253211975
curr_diff: 0 tensor(0.2381, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.2381, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9526, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.792165
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 1 0.04 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 809
max_epoch:: 200
overhead:: 0
overhead2:: 0.020967960357666016
overhead3:: 0.12876558303833008
overhead4:: 31.242965698242188
overhead5:: 0
memory usage:: 26600017920
time_provenance:: 55.968993186950684
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.2372, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.2372, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9530, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.791918
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 1 0.04 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 809
max_epoch:: 200
overhead:: 0
overhead2:: 0.02540898323059082
overhead3:: 0.13750910758972168
overhead4:: 37.25036144256592
overhead5:: 0
memory usage:: 26610499584
time_provenance:: 63.715129137039185
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.2372, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.2372, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9530, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.791918
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 1 0.04 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 809
max_epoch:: 200
overhead:: 0
overhead2:: 0.029677867889404297
overhead3:: 0.1468367576599121
overhead4:: 42.57100319862366
overhead5:: 0
memory usage:: 26610720768
time_provenance:: 70.47358441352844
curr_diff: 0 tensor(0.0010, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0010, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.2372, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.2372, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9530, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.791918
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 1 0.04 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 809
max_epoch:: 200
overhead:: 0
overhead2:: 0.03971362113952637
overhead3:: 0.16802072525024414
overhead4:: 56.679665088653564
overhead5:: 0
memory usage:: 26601816064
time_provenance:: 88.90846943855286
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.2373, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.2373, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9529, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.791819
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 1 0.04 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 809
max_epoch:: 200
overhead:: 0
overhead2:: 0.03854680061340332
overhead3:: 0.1578354835510254
overhead4:: 61.47942233085632
overhead5:: 0
memory usage:: 26618114048
time_provenance:: 95.02738833427429
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.2373, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.2373, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9529, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.791819
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 1 0.04 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 809
max_epoch:: 200
overhead:: 0
overhead2:: 0.046068429946899414
overhead3:: 0.1797635555267334
overhead4:: 66.26678967475891
overhead5:: 0
memory usage:: 26600161280
time_provenance:: 101.13240122795105
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.2373, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.2373, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9529, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.791819
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 1 0.04 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 809
max_epoch:: 200
overhead:: 0
overhead2:: 0.09090900421142578
overhead3:: 0.27980971336364746
overhead4:: 132.34401988983154
overhead5:: 0
memory usage:: 26600767488
time_provenance:: 186.89878296852112
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.2376, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.2376, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9528, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.792017
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 1 0.04 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 809
max_epoch:: 200
overhead:: 0
overhead2:: 0.09575176239013672
overhead3:: 0.28468775749206543
overhead4:: 135.0860950946808
overhead5:: 0
memory usage:: 26615570432
time_provenance:: 190.47734999656677
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.2376, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.2376, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9528, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.792017
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 1 0.04 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 809
max_epoch:: 200
overhead:: 0
overhead2:: 0.09567475318908691
overhead3:: 0.2917754650115967
overhead4:: 138.29885721206665
overhead5:: 0
memory usage:: 26619727872
time_provenance:: 194.4996361732483
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.2376, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.2376, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9528, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.792017
adding noise deletion rate:: 0.05
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
tensor([0.5101, 0.5117, 0.5036,  ..., 0.5016, 0.5053, 0.5030],
       dtype=torch.float64, grad_fn=<MaxBackward0>)
tensor(0.8685, dtype=torch.float64) tensor(0., dtype=torch.float64)
Train - Epoch 0, Batch: 0, Loss: 0.693215
Train - Epoch 1, Batch: 0, Loss: 0.693162
Train - Epoch 2, Batch: 0, Loss: 0.693104
Train - Epoch 3, Batch: 0, Loss: 0.693014
Train - Epoch 4, Batch: 0, Loss: 0.692953
Train - Epoch 5, Batch: 0, Loss: 0.692869
Train - Epoch 6, Batch: 0, Loss: 0.692801
Train - Epoch 7, Batch: 0, Loss: 0.692758
Train - Epoch 8, Batch: 0, Loss: 0.692708
Train - Epoch 9, Batch: 0, Loss: 0.692630
Train - Epoch 10, Batch: 0, Loss: 0.692554
Train - Epoch 11, Batch: 0, Loss: 0.692577
Train - Epoch 12, Batch: 0, Loss: 0.692438
Train - Epoch 13, Batch: 0, Loss: 0.692370
Train - Epoch 14, Batch: 0, Loss: 0.692358
Train - Epoch 15, Batch: 0, Loss: 0.692202
Train - Epoch 16, Batch: 0, Loss: 0.692179
Train - Epoch 17, Batch: 0, Loss: 0.692107
Train - Epoch 18, Batch: 0, Loss: 0.692079
Train - Epoch 19, Batch: 0, Loss: 0.692000
Train - Epoch 20, Batch: 0, Loss: 0.691959
Train - Epoch 21, Batch: 0, Loss: 0.691898
Train - Epoch 22, Batch: 0, Loss: 0.691841
Train - Epoch 23, Batch: 0, Loss: 0.691690
Train - Epoch 24, Batch: 0, Loss: 0.691691
Train - Epoch 25, Batch: 0, Loss: 0.691651
Train - Epoch 26, Batch: 0, Loss: 0.691533
Train - Epoch 27, Batch: 0, Loss: 0.691495
Train - Epoch 28, Batch: 0, Loss: 0.691455
Train - Epoch 29, Batch: 0, Loss: 0.691348
Train - Epoch 30, Batch: 0, Loss: 0.691313
Train - Epoch 31, Batch: 0, Loss: 0.691175
Train - Epoch 32, Batch: 0, Loss: 0.691245
Train - Epoch 33, Batch: 0, Loss: 0.691197
Train - Epoch 34, Batch: 0, Loss: 0.691037
Train - Epoch 35, Batch: 0, Loss: 0.690999
Train - Epoch 36, Batch: 0, Loss: 0.691054
Train - Epoch 37, Batch: 0, Loss: 0.690860
Train - Epoch 38, Batch: 0, Loss: 0.690895
Train - Epoch 39, Batch: 0, Loss: 0.690759
Train - Epoch 40, Batch: 0, Loss: 0.690740
Train - Epoch 41, Batch: 0, Loss: 0.690636
Train - Epoch 42, Batch: 0, Loss: 0.690561
Train - Epoch 43, Batch: 0, Loss: 0.690556
Train - Epoch 44, Batch: 0, Loss: 0.690511
Train - Epoch 45, Batch: 0, Loss: 0.690481
Train - Epoch 46, Batch: 0, Loss: 0.690358
Train - Epoch 47, Batch: 0, Loss: 0.690248
Train - Epoch 48, Batch: 0, Loss: 0.690314
Train - Epoch 49, Batch: 0, Loss: 0.690281
Train - Epoch 50, Batch: 0, Loss: 0.690241
Train - Epoch 51, Batch: 0, Loss: 0.689959
Train - Epoch 52, Batch: 0, Loss: 0.690132
Train - Epoch 53, Batch: 0, Loss: 0.689776
Train - Epoch 54, Batch: 0, Loss: 0.689886
Train - Epoch 55, Batch: 0, Loss: 0.689954
Train - Epoch 56, Batch: 0, Loss: 0.689949
Train - Epoch 57, Batch: 0, Loss: 0.689754
Train - Epoch 58, Batch: 0, Loss: 0.689574
Train - Epoch 59, Batch: 0, Loss: 0.689678
Train - Epoch 60, Batch: 0, Loss: 0.689696
Train - Epoch 61, Batch: 0, Loss: 0.689464
Train - Epoch 62, Batch: 0, Loss: 0.689533
Train - Epoch 63, Batch: 0, Loss: 0.689419
Train - Epoch 64, Batch: 0, Loss: 0.689408
Train - Epoch 65, Batch: 0, Loss: 0.689443
Train - Epoch 66, Batch: 0, Loss: 0.689146
Train - Epoch 67, Batch: 0, Loss: 0.689165
Train - Epoch 68, Batch: 0, Loss: 0.689330
Train - Epoch 69, Batch: 0, Loss: 0.689055
Train - Epoch 70, Batch: 0, Loss: 0.688720
Train - Epoch 71, Batch: 0, Loss: 0.689121
Train - Epoch 72, Batch: 0, Loss: 0.688904
Train - Epoch 73, Batch: 0, Loss: 0.689091
Train - Epoch 74, Batch: 0, Loss: 0.688899
Train - Epoch 75, Batch: 0, Loss: 0.688771
Train - Epoch 76, Batch: 0, Loss: 0.688863
Train - Epoch 77, Batch: 0, Loss: 0.688619
Train - Epoch 78, Batch: 0, Loss: 0.688634
Train - Epoch 79, Batch: 0, Loss: 0.688846
Train - Epoch 80, Batch: 0, Loss: 0.688636
Train - Epoch 81, Batch: 0, Loss: 0.688437
Train - Epoch 82, Batch: 0, Loss: 0.688315
Train - Epoch 83, Batch: 0, Loss: 0.688238
Train - Epoch 84, Batch: 0, Loss: 0.688344
Train - Epoch 85, Batch: 0, Loss: 0.688303
Train - Epoch 86, Batch: 0, Loss: 0.688399
Train - Epoch 87, Batch: 0, Loss: 0.688197
Train - Epoch 88, Batch: 0, Loss: 0.688213
Train - Epoch 89, Batch: 0, Loss: 0.688080
Train - Epoch 90, Batch: 0, Loss: 0.687870
Train - Epoch 91, Batch: 0, Loss: 0.687938
Train - Epoch 92, Batch: 0, Loss: 0.688022
Train - Epoch 93, Batch: 0, Loss: 0.688197
Train - Epoch 94, Batch: 0, Loss: 0.687785
Train - Epoch 95, Batch: 0, Loss: 0.687847
Train - Epoch 96, Batch: 0, Loss: 0.687573
Train - Epoch 97, Batch: 0, Loss: 0.687610
Train - Epoch 98, Batch: 0, Loss: 0.687447
Train - Epoch 99, Batch: 0, Loss: 0.687862
Train - Epoch 100, Batch: 0, Loss: 0.687511
Train - Epoch 101, Batch: 0, Loss: 0.687457
Train - Epoch 102, Batch: 0, Loss: 0.687359
Train - Epoch 103, Batch: 0, Loss: 0.687311
Train - Epoch 104, Batch: 0, Loss: 0.687556
Train - Epoch 105, Batch: 0, Loss: 0.687329
Train - Epoch 106, Batch: 0, Loss: 0.687238
Train - Epoch 107, Batch: 0, Loss: 0.687361
Train - Epoch 108, Batch: 0, Loss: 0.687100
Train - Epoch 109, Batch: 0, Loss: 0.687315
Train - Epoch 110, Batch: 0, Loss: 0.687007
Train - Epoch 111, Batch: 0, Loss: 0.687098
Train - Epoch 112, Batch: 0, Loss: 0.686804
Train - Epoch 113, Batch: 0, Loss: 0.686868
Train - Epoch 114, Batch: 0, Loss: 0.687200
Train - Epoch 115, Batch: 0, Loss: 0.686661
Train - Epoch 116, Batch: 0, Loss: 0.686984
Train - Epoch 117, Batch: 0, Loss: 0.686833
Train - Epoch 118, Batch: 0, Loss: 0.686514
Train - Epoch 119, Batch: 0, Loss: 0.687139
Train - Epoch 120, Batch: 0, Loss: 0.686855
Train - Epoch 121, Batch: 0, Loss: 0.686686
Train - Epoch 122, Batch: 0, Loss: 0.686544
Train - Epoch 123, Batch: 0, Loss: 0.686362
Train - Epoch 124, Batch: 0, Loss: 0.686621
Train - Epoch 125, Batch: 0, Loss: 0.686689
Train - Epoch 126, Batch: 0, Loss: 0.686419
Train - Epoch 127, Batch: 0, Loss: 0.686371
Train - Epoch 128, Batch: 0, Loss: 0.686274
Train - Epoch 129, Batch: 0, Loss: 0.686263
Train - Epoch 130, Batch: 0, Loss: 0.686070
Train - Epoch 131, Batch: 0, Loss: 0.686258
Train - Epoch 132, Batch: 0, Loss: 0.686266
Train - Epoch 133, Batch: 0, Loss: 0.686112
Train - Epoch 134, Batch: 0, Loss: 0.686077
Train - Epoch 135, Batch: 0, Loss: 0.686012
Train - Epoch 136, Batch: 0, Loss: 0.686095
Train - Epoch 137, Batch: 0, Loss: 0.686096
Train - Epoch 138, Batch: 0, Loss: 0.685938
Train - Epoch 139, Batch: 0, Loss: 0.685526
Train - Epoch 140, Batch: 0, Loss: 0.685671
Train - Epoch 141, Batch: 0, Loss: 0.685694
Train - Epoch 142, Batch: 0, Loss: 0.685750
Train - Epoch 143, Batch: 0, Loss: 0.685699
Train - Epoch 144, Batch: 0, Loss: 0.685464
Train - Epoch 145, Batch: 0, Loss: 0.685432
Train - Epoch 146, Batch: 0, Loss: 0.685339
Train - Epoch 147, Batch: 0, Loss: 0.685330
Train - Epoch 148, Batch: 0, Loss: 0.685405
Train - Epoch 149, Batch: 0, Loss: 0.685604
Train - Epoch 150, Batch: 0, Loss: 0.685027
Train - Epoch 151, Batch: 0, Loss: 0.685249
Train - Epoch 152, Batch: 0, Loss: 0.685278
Train - Epoch 153, Batch: 0, Loss: 0.685398
Train - Epoch 154, Batch: 0, Loss: 0.685402
Train - Epoch 155, Batch: 0, Loss: 0.685309
Train - Epoch 156, Batch: 0, Loss: 0.685006
Train - Epoch 157, Batch: 0, Loss: 0.684939
Train - Epoch 158, Batch: 0, Loss: 0.684977
Train - Epoch 159, Batch: 0, Loss: 0.685082
Train - Epoch 160, Batch: 0, Loss: 0.684468
Train - Epoch 161, Batch: 0, Loss: 0.685225
Train - Epoch 162, Batch: 0, Loss: 0.684562
Train - Epoch 163, Batch: 0, Loss: 0.684897
Train - Epoch 164, Batch: 0, Loss: 0.684570
Train - Epoch 165, Batch: 0, Loss: 0.684214
Train - Epoch 166, Batch: 0, Loss: 0.684559
Train - Epoch 167, Batch: 0, Loss: 0.684699
Train - Epoch 168, Batch: 0, Loss: 0.684317
Train - Epoch 169, Batch: 0, Loss: 0.684911
Train - Epoch 170, Batch: 0, Loss: 0.684843
Train - Epoch 171, Batch: 0, Loss: 0.684618
Train - Epoch 172, Batch: 0, Loss: 0.684226
Train - Epoch 173, Batch: 0, Loss: 0.684363
Train - Epoch 174, Batch: 0, Loss: 0.684666
Train - Epoch 175, Batch: 0, Loss: 0.683655
Train - Epoch 176, Batch: 0, Loss: 0.684095
Train - Epoch 177, Batch: 0, Loss: 0.684634
Train - Epoch 178, Batch: 0, Loss: 0.684297
Train - Epoch 179, Batch: 0, Loss: 0.684104
Train - Epoch 180, Batch: 0, Loss: 0.684005
Train - Epoch 181, Batch: 0, Loss: 0.684253
Train - Epoch 182, Batch: 0, Loss: 0.683926/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 183, Batch: 0, Loss: 0.683822
Train - Epoch 184, Batch: 0, Loss: 0.684081
Train - Epoch 185, Batch: 0, Loss: 0.684249
Train - Epoch 186, Batch: 0, Loss: 0.684330
Train - Epoch 187, Batch: 0, Loss: 0.683660
Train - Epoch 188, Batch: 0, Loss: 0.683665
Train - Epoch 189, Batch: 0, Loss: 0.683912
Train - Epoch 190, Batch: 0, Loss: 0.683720
Train - Epoch 191, Batch: 0, Loss: 0.683581
Train - Epoch 192, Batch: 0, Loss: 0.683802
Train - Epoch 193, Batch: 0, Loss: 0.683759
Train - Epoch 194, Batch: 0, Loss: 0.683198
Train - Epoch 195, Batch: 0, Loss: 0.683572
Train - Epoch 196, Batch: 0, Loss: 0.683249
Train - Epoch 197, Batch: 0, Loss: 0.683678
Train - Epoch 198, Batch: 0, Loss: 0.683024
Train - Epoch 199, Batch: 0, Loss: 0.684142
training_time:: 350.8115711212158
training time full:: 350.8116102218628
provenance prepare time:: 0.0
here
start test::
Test Avg. Loss: 0.000067, Accuracy: 0.495801
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1012
max_epoch:: 200
batch_size:: 10200
epoch:: 0
tensor([  0,   1,   5,   8,  10,  12,  17,  19,  22,  23,  25,  26,  32,  37,
         40,  43,  44,  46,  49,  51,  52,  54,  57,  58,  60,  64,  68,  73,
         75,  76,  77,  80,  81,  82,  84,  86,  89,  92,  93,  95,  96,  98,
         99, 102, 103, 105, 106, 107, 109, 115])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
epoch:: 60
epoch:: 61
epoch:: 62
epoch:: 63
epoch:: 64
epoch:: 65
epoch:: 66
epoch:: 67
epoch:: 68
epoch:: 69
epoch:: 70
epoch:: 71
epoch:: 72
epoch:: 73
epoch:: 74
epoch:: 75
epoch:: 76
epoch:: 77
epoch:: 78
epoch:: 79
epoch:: 80
epoch:: 81
epoch:: 82
epoch:: 83
epoch:: 84
epoch:: 85
epoch:: 86
epoch:: 87
epoch:: 88
epoch:: 89
epoch:: 90
epoch:: 91
epoch:: 92
epoch:: 93
epoch:: 94
epoch:: 95
epoch:: 96
epoch:: 97
epoch:: 98
epoch:: 99
epoch:: 100
epoch:: 101
epoch:: 102
epoch:: 103
epoch:: 104
epoch:: 105
epoch:: 106
epoch:: 107
epoch:: 108
epoch:: 109
epoch:: 110
epoch:: 111
epoch:: 112
epoch:: 113
epoch:: 114
epoch:: 115
epoch:: 116
epoch:: 117
epoch:: 118
epoch:: 119
epoch:: 120
epoch:: 121
epoch:: 122
epoch:: 123
epoch:: 124
epoch:: 125
epoch:: 126
epoch:: 127
epoch:: 128
epoch:: 129
epoch:: 130
epoch:: 131
epoch:: 132
epoch:: 133
epoch:: 134
epoch:: 135
epoch:: 136
epoch:: 137
epoch:: 138
epoch:: 139
epoch:: 140
epoch:: 141
epoch:: 142
epoch:: 143
epoch:: 144
epoch:: 145
epoch:: 146
epoch:: 147
epoch:: 148
epoch:: 149
epoch:: 150
epoch:: 151
epoch:: 152
epoch:: 153
epoch:: 154
epoch:: 155
epoch:: 156
epoch:: 157
epoch:: 158
epoch:: 159
epoch:: 160
epoch:: 161
epoch:: 162
epoch:: 163
epoch:: 164
epoch:: 165
epoch:: 166
epoch:: 167
epoch:: 168
epoch:: 169
epoch:: 170
epoch:: 171
epoch:: 172
epoch:: 173
epoch:: 174
epoch:: 175
epoch:: 176
epoch:: 177
epoch:: 178
epoch:: 179
epoch:: 180
epoch:: 181
epoch:: 182
epoch:: 183
epoch:: 184
epoch:: 185
epoch:: 186
epoch:: 187
epoch:: 188
epoch:: 189
epoch:: 190
epoch:: 191
epoch:: 192
epoch:: 193
epoch:: 194
epoch:: 195
epoch:: 196
epoch:: 197
epoch:: 198
epoch:: 199
training time is 262.5260248184204
overhead:: 0
overhead2:: 0.422271728515625
overhead3:: 0
time_baseline:: 262.5260775089264
curr_diff: 0 tensor(0.3417, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.3417, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9255, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.682294
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 1 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1012
max_epoch:: 200
overhead:: 0
overhead2:: 0.02135300636291504
overhead3:: 0.13120508193969727
overhead4:: 30.83871030807495
overhead5:: 0
memory usage:: 26599317504
time_provenance:: 59.958208084106445
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.3400, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.3400, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9263, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.680516
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 1 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1012
max_epoch:: 200
overhead:: 0
overhead2:: 0.026710033416748047
overhead3:: 0.14062047004699707
overhead4:: 36.39648962020874
overhead5:: 0
memory usage:: 26599727104
time_provenance:: 67.15333127975464
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.3400, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.3400, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9263, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.680516
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 1 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1012
max_epoch:: 200
overhead:: 0
overhead2:: 0.029392242431640625
overhead3:: 0.14877724647521973
overhead4:: 42.4693968296051
overhead5:: 0
memory usage:: 26620088320
time_provenance:: 74.75453209877014
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.3400, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.3400, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9263, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.680516
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 1 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1012
max_epoch:: 200
overhead:: 0
overhead2:: 0.04093575477600098
overhead3:: 0.1689915657043457
overhead4:: 55.65175414085388
overhead5:: 0
memory usage:: 26610008064
time_provenance:: 92.00305247306824
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.3403, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.3403, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9262, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.680960
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 1 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1012
max_epoch:: 200
overhead:: 0
overhead2:: 0.04081535339355469
overhead3:: 0.17451977729797363
overhead4:: 59.69908332824707
overhead5:: 0
memory usage:: 26603384832
time_provenance:: 96.97644591331482
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.3403, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.3403, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9261, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.680960
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 1 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1012
max_epoch:: 200
overhead:: 0
overhead2:: 0.04774308204650879
overhead3:: 0.18762469291687012
overhead4:: 65.17924094200134
overhead5:: 0
memory usage:: 26598780928
time_provenance:: 104.09769821166992
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.3403, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.3403, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9261, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.680960
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 1 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1012
max_epoch:: 200
overhead:: 0
overhead2:: 0.0930933952331543
overhead3:: 0.28623437881469727
overhead4:: 130.33649969100952
overhead5:: 0
memory usage:: 26615144448
time_provenance:: 188.10779404640198
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.3407, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.3407, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9259, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.681405
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 1 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1012
max_epoch:: 200
overhead:: 0
overhead2:: 0.08777117729187012
overhead3:: 0.2812013626098633
overhead4:: 133.89693117141724
overhead5:: 0
memory usage:: 26614431744
time_provenance:: 192.48915600776672
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.3407, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.3407, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9259, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.681405
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 1 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1012
max_epoch:: 200
overhead:: 0
overhead2:: 0.0995485782623291
overhead3:: 0.29661083221435547
overhead4:: 137.15672039985657
overhead5:: 0
memory usage:: 26600833024
time_provenance:: 196.58322048187256
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.3407, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.3407, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9259, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.681405
adding noise deletion rate:: 0.06
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
tensor([0.5148, 0.5244, 0.5052,  ..., 0.5213, 0.5089, 0.5126],
       dtype=torch.float64, grad_fn=<MaxBackward0>)
tensor(0.8685, dtype=torch.float64) tensor(0., dtype=torch.float64)
Train - Epoch 0, Batch: 0, Loss: 0.693160
Train - Epoch 1, Batch: 0, Loss: 0.693108
Train - Epoch 2, Batch: 0, Loss: 0.693044
Train - Epoch 3, Batch: 0, Loss: 0.693011
Train - Epoch 4, Batch: 0, Loss: 0.692984
Train - Epoch 5, Batch: 0, Loss: 0.692935
Train - Epoch 6, Batch: 0, Loss: 0.692895
Train - Epoch 7, Batch: 0, Loss: 0.692833
Train - Epoch 8, Batch: 0, Loss: 0.692808
Train - Epoch 9, Batch: 0, Loss: 0.692753
Train - Epoch 10, Batch: 0, Loss: 0.692737
Train - Epoch 11, Batch: 0, Loss: 0.692673
Train - Epoch 12, Batch: 0, Loss: 0.692611
Train - Epoch 13, Batch: 0, Loss: 0.692577
Train - Epoch 14, Batch: 0, Loss: 0.692551
Train - Epoch 15, Batch: 0, Loss: 0.692520
Train - Epoch 16, Batch: 0, Loss: 0.692480
Train - Epoch 17, Batch: 0, Loss: 0.692451
Train - Epoch 18, Batch: 0, Loss: 0.692430
Train - Epoch 19, Batch: 0, Loss: 0.692388
Train - Epoch 20, Batch: 0, Loss: 0.692328
Train - Epoch 21, Batch: 0, Loss: 0.692271
Train - Epoch 22, Batch: 0, Loss: 0.692286
Train - Epoch 23, Batch: 0, Loss: 0.692171
Train - Epoch 24, Batch: 0, Loss: 0.692051
Train - Epoch 25, Batch: 0, Loss: 0.692100
Train - Epoch 26, Batch: 0, Loss: 0.692106
Train - Epoch 27, Batch: 0, Loss: 0.692083
Train - Epoch 28, Batch: 0, Loss: 0.691972
Train - Epoch 29, Batch: 0, Loss: 0.691968
Train - Epoch 30, Batch: 0, Loss: 0.691948
Train - Epoch 31, Batch: 0, Loss: 0.691890
Train - Epoch 32, Batch: 0, Loss: 0.691826
Train - Epoch 33, Batch: 0, Loss: 0.691864
Train - Epoch 34, Batch: 0, Loss: 0.691668
Train - Epoch 35, Batch: 0, Loss: 0.691782
Train - Epoch 36, Batch: 0, Loss: 0.691667
Train - Epoch 37, Batch: 0, Loss: 0.691584
Train - Epoch 38, Batch: 0, Loss: 0.691565
Train - Epoch 39, Batch: 0, Loss: 0.691615
Train - Epoch 40, Batch: 0, Loss: 0.691517
Train - Epoch 41, Batch: 0, Loss: 0.691447
Train - Epoch 42, Batch: 0, Loss: 0.691381
Train - Epoch 43, Batch: 0, Loss: 0.691479
Train - Epoch 44, Batch: 0, Loss: 0.691321
Train - Epoch 45, Batch: 0, Loss: 0.691253
Train - Epoch 46, Batch: 0, Loss: 0.691246
Train - Epoch 47, Batch: 0, Loss: 0.691305
Train - Epoch 48, Batch: 0, Loss: 0.691219
Train - Epoch 49, Batch: 0, Loss: 0.691298
Train - Epoch 50, Batch: 0, Loss: 0.691147
Train - Epoch 51, Batch: 0, Loss: 0.691215
Train - Epoch 52, Batch: 0, Loss: 0.690964
Train - Epoch 53, Batch: 0, Loss: 0.691095
Train - Epoch 54, Batch: 0, Loss: 0.691058
Train - Epoch 55, Batch: 0, Loss: 0.690991
Train - Epoch 56, Batch: 0, Loss: 0.690863
Train - Epoch 57, Batch: 0, Loss: 0.690888
Train - Epoch 58, Batch: 0, Loss: 0.690893
Train - Epoch 59, Batch: 0, Loss: 0.690785
Train - Epoch 60, Batch: 0, Loss: 0.690631
Train - Epoch 61, Batch: 0, Loss: 0.690661
Train - Epoch 62, Batch: 0, Loss: 0.690592
Train - Epoch 63, Batch: 0, Loss: 0.690616
Train - Epoch 64, Batch: 0, Loss: 0.690520
Train - Epoch 65, Batch: 0, Loss: 0.690628
Train - Epoch 66, Batch: 0, Loss: 0.690672
Train - Epoch 67, Batch: 0, Loss: 0.690430
Train - Epoch 68, Batch: 0, Loss: 0.690568
Train - Epoch 69, Batch: 0, Loss: 0.690409
Train - Epoch 70, Batch: 0, Loss: 0.690520
Train - Epoch 71, Batch: 0, Loss: 0.690328
Train - Epoch 72, Batch: 0, Loss: 0.690313
Train - Epoch 73, Batch: 0, Loss: 0.690106
Train - Epoch 74, Batch: 0, Loss: 0.690229
Train - Epoch 75, Batch: 0, Loss: 0.690223
Train - Epoch 76, Batch: 0, Loss: 0.690128
Train - Epoch 77, Batch: 0, Loss: 0.690061
Train - Epoch 78, Batch: 0, Loss: 0.690151
Train - Epoch 79, Batch: 0, Loss: 0.689925
Train - Epoch 80, Batch: 0, Loss: 0.689869
Train - Epoch 81, Batch: 0, Loss: 0.689893
Train - Epoch 82, Batch: 0, Loss: 0.689812
Train - Epoch 83, Batch: 0, Loss: 0.690244
Train - Epoch 84, Batch: 0, Loss: 0.689982
Train - Epoch 85, Batch: 0, Loss: 0.690022
Train - Epoch 86, Batch: 0, Loss: 0.689856
Train - Epoch 87, Batch: 0, Loss: 0.689904
Train - Epoch 88, Batch: 0, Loss: 0.689948
Train - Epoch 89, Batch: 0, Loss: 0.689722
Train - Epoch 90, Batch: 0, Loss: 0.689896
Train - Epoch 91, Batch: 0, Loss: 0.689495
Train - Epoch 92, Batch: 0, Loss: 0.689666
Train - Epoch 93, Batch: 0, Loss: 0.689527
Train - Epoch 94, Batch: 0, Loss: 0.689781
Train - Epoch 95, Batch: 0, Loss: 0.689618
Train - Epoch 96, Batch: 0, Loss: 0.689654
Train - Epoch 97, Batch: 0, Loss: 0.689566
Train - Epoch 98, Batch: 0, Loss: 0.689491
Train - Epoch 99, Batch: 0, Loss: 0.689415
Train - Epoch 100, Batch: 0, Loss: 0.689541
Train - Epoch 101, Batch: 0, Loss: 0.689428
Train - Epoch 102, Batch: 0, Loss: 0.689414
Train - Epoch 103, Batch: 0, Loss: 0.689479
Train - Epoch 104, Batch: 0, Loss: 0.689157
Train - Epoch 105, Batch: 0, Loss: 0.689283
Train - Epoch 106, Batch: 0, Loss: 0.689295
Train - Epoch 107, Batch: 0, Loss: 0.689247
Train - Epoch 108, Batch: 0, Loss: 0.689212
Train - Epoch 109, Batch: 0, Loss: 0.689108
Train - Epoch 110, Batch: 0, Loss: 0.689101
Train - Epoch 111, Batch: 0, Loss: 0.689138
Train - Epoch 112, Batch: 0, Loss: 0.689035
Train - Epoch 113, Batch: 0, Loss: 0.688952
Train - Epoch 114, Batch: 0, Loss: 0.689151
Train - Epoch 115, Batch: 0, Loss: 0.689155
Train - Epoch 116, Batch: 0, Loss: 0.689010
Train - Epoch 117, Batch: 0, Loss: 0.688709
Train - Epoch 118, Batch: 0, Loss: 0.688691
Train - Epoch 119, Batch: 0, Loss: 0.688568
Train - Epoch 120, Batch: 0, Loss: 0.688400
Train - Epoch 121, Batch: 0, Loss: 0.688712
Train - Epoch 122, Batch: 0, Loss: 0.688595
Train - Epoch 123, Batch: 0, Loss: 0.688835
Train - Epoch 124, Batch: 0, Loss: 0.688891
Train - Epoch 125, Batch: 0, Loss: 0.688420
Train - Epoch 126, Batch: 0, Loss: 0.688686
Train - Epoch 127, Batch: 0, Loss: 0.688574
Train - Epoch 128, Batch: 0, Loss: 0.688510
Train - Epoch 129, Batch: 0, Loss: 0.688387
Train - Epoch 130, Batch: 0, Loss: 0.688411
Train - Epoch 131, Batch: 0, Loss: 0.688168
Train - Epoch 132, Batch: 0, Loss: 0.688229
Train - Epoch 133, Batch: 0, Loss: 0.688310
Train - Epoch 134, Batch: 0, Loss: 0.688149
Train - Epoch 135, Batch: 0, Loss: 0.688207
Train - Epoch 136, Batch: 0, Loss: 0.688150
Train - Epoch 137, Batch: 0, Loss: 0.688157
Train - Epoch 138, Batch: 0, Loss: 0.687971
Train - Epoch 139, Batch: 0, Loss: 0.688018
Train - Epoch 140, Batch: 0, Loss: 0.688418
Train - Epoch 141, Batch: 0, Loss: 0.688220
Train - Epoch 142, Batch: 0, Loss: 0.688287
Train - Epoch 143, Batch: 0, Loss: 0.688063
Train - Epoch 144, Batch: 0, Loss: 0.688157
Train - Epoch 145, Batch: 0, Loss: 0.688109
Train - Epoch 146, Batch: 0, Loss: 0.688253
Train - Epoch 147, Batch: 0, Loss: 0.687731
Train - Epoch 148, Batch: 0, Loss: 0.687876
Train - Epoch 149, Batch: 0, Loss: 0.688189
Train - Epoch 150, Batch: 0, Loss: 0.687927
Train - Epoch 151, Batch: 0, Loss: 0.687787
Train - Epoch 152, Batch: 0, Loss: 0.687917
Train - Epoch 153, Batch: 0, Loss: 0.688219
Train - Epoch 154, Batch: 0, Loss: 0.687745
Train - Epoch 155, Batch: 0, Loss: 0.687782
Train - Epoch 156, Batch: 0, Loss: 0.687543
Train - Epoch 157, Batch: 0, Loss: 0.687871
Train - Epoch 158, Batch: 0, Loss: 0.687781
Train - Epoch 159, Batch: 0, Loss: 0.687715
Train - Epoch 160, Batch: 0, Loss: 0.687851
Train - Epoch 161, Batch: 0, Loss: 0.687315
Train - Epoch 162, Batch: 0, Loss: 0.687493
Train - Epoch 163, Batch: 0, Loss: 0.687303
Train - Epoch 164, Batch: 0, Loss: 0.687603
Train - Epoch 165, Batch: 0, Loss: 0.687636
Train - Epoch 166, Batch: 0, Loss: 0.687611
Train - Epoch 167, Batch: 0, Loss: 0.687432
Train - Epoch 168, Batch: 0, Loss: 0.687135
Train - Epoch 169, Batch: 0, Loss: 0.687426
Train - Epoch 170, Batch: 0, Loss: 0.687141
Train - Epoch 171, Batch: 0, Loss: 0.687352
Train - Epoch 172, Batch: 0, Loss: 0.687422
Train - Epoch 173, Batch: 0, Loss: 0.687278
Train - Epoch 174, Batch: 0, Loss: 0.687330
Train - Epoch 175, Batch: 0, Loss: 0.687446
Train - Epoch 176, Batch: 0, Loss: 0.687156
Train - Epoch 177, Batch: 0, Loss: 0.687244
Train - Epoch 178, Batch: 0, Loss: 0.687347
Train - Epoch 179, Batch: 0, Loss: 0.687143
Train - Epoch 180, Batch: 0, Loss: 0.686827
Train - Epoch 181, Batch: 0, Loss: 0.686695
Train - Epoch 182, Batch: 0, Loss: 0.687085/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 183, Batch: 0, Loss: 0.687257
Train - Epoch 184, Batch: 0, Loss: 0.686645
Train - Epoch 185, Batch: 0, Loss: 0.687005
Train - Epoch 186, Batch: 0, Loss: 0.686875
Train - Epoch 187, Batch: 0, Loss: 0.686843
Train - Epoch 188, Batch: 0, Loss: 0.686760
Train - Epoch 189, Batch: 0, Loss: 0.686414
Train - Epoch 190, Batch: 0, Loss: 0.686969
Train - Epoch 191, Batch: 0, Loss: 0.686886
Train - Epoch 192, Batch: 0, Loss: 0.686853
Train - Epoch 193, Batch: 0, Loss: 0.686340
Train - Epoch 194, Batch: 0, Loss: 0.686933
Train - Epoch 195, Batch: 0, Loss: 0.686330
Train - Epoch 196, Batch: 0, Loss: 0.686512
Train - Epoch 197, Batch: 0, Loss: 0.686090
Train - Epoch 198, Batch: 0, Loss: 0.686812
Train - Epoch 199, Batch: 0, Loss: 0.686301
training_time:: 351.729777097702
training time full:: 351.7298216819763
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.641982
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1214
max_epoch:: 200
batch_size:: 10200
epoch:: 0
tensor([  0,   1,   5,   8,  10,  12,  17,  19,  22,  23,  25,  26,  32,  37,
         40,  43,  44,  46,  49,  51,  52,  54,  57,  58,  60,  64,  68,  73,
         75,  76,  77,  80,  81,  82,  84,  86,  89,  92,  93,  95,  96,  98,
         99, 102, 103, 105, 106, 107, 109, 115])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
epoch:: 60
epoch:: 61
epoch:: 62
epoch:: 63
epoch:: 64
epoch:: 65
epoch:: 66
epoch:: 67
epoch:: 68
epoch:: 69
epoch:: 70
epoch:: 71
epoch:: 72
epoch:: 73
epoch:: 74
epoch:: 75
epoch:: 76
epoch:: 77
epoch:: 78
epoch:: 79
epoch:: 80
epoch:: 81
epoch:: 82
epoch:: 83
epoch:: 84
epoch:: 85
epoch:: 86
epoch:: 87
epoch:: 88
epoch:: 89
epoch:: 90
epoch:: 91
epoch:: 92
epoch:: 93
epoch:: 94
epoch:: 95
epoch:: 96
epoch:: 97
epoch:: 98
epoch:: 99
epoch:: 100
epoch:: 101
epoch:: 102
epoch:: 103
epoch:: 104
epoch:: 105
epoch:: 106
epoch:: 107
epoch:: 108
epoch:: 109
epoch:: 110
epoch:: 111
epoch:: 112
epoch:: 113
epoch:: 114
epoch:: 115
epoch:: 116
epoch:: 117
epoch:: 118
epoch:: 119
epoch:: 120
epoch:: 121
epoch:: 122
epoch:: 123
epoch:: 124
epoch:: 125
epoch:: 126
epoch:: 127
epoch:: 128
epoch:: 129
epoch:: 130
epoch:: 131
epoch:: 132
epoch:: 133
epoch:: 134
epoch:: 135
epoch:: 136
epoch:: 137
epoch:: 138
epoch:: 139
epoch:: 140
epoch:: 141
epoch:: 142
epoch:: 143
epoch:: 144
epoch:: 145
epoch:: 146
epoch:: 147
epoch:: 148
epoch:: 149
epoch:: 150
epoch:: 151
epoch:: 152
epoch:: 153
epoch:: 154
epoch:: 155
epoch:: 156
epoch:: 157
epoch:: 158
epoch:: 159
epoch:: 160
epoch:: 161
epoch:: 162
epoch:: 163
epoch:: 164
epoch:: 165
epoch:: 166
epoch:: 167
epoch:: 168
epoch:: 169
epoch:: 170
epoch:: 171
epoch:: 172
epoch:: 173
epoch:: 174
epoch:: 175
epoch:: 176
epoch:: 177
epoch:: 178
epoch:: 179
epoch:: 180
epoch:: 181
epoch:: 182
epoch:: 183
epoch:: 184
epoch:: 185
epoch:: 186
epoch:: 187
epoch:: 188
epoch:: 189
epoch:: 190
epoch:: 191
epoch:: 192
epoch:: 193
epoch:: 194
epoch:: 195
epoch:: 196
epoch:: 197
epoch:: 198
epoch:: 199
training time is 260.25394463539124
overhead:: 0
overhead2:: 0.4330143928527832
overhead3:: 0
time_baseline:: 260.25399899482727
curr_diff: 0 tensor(0.4243, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.4243, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8611, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.759757
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 1 0.06 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1214
max_epoch:: 200
overhead:: 0
overhead2:: 0.022551298141479492
overhead3:: 0.1338808536529541
overhead4:: 30.677334308624268
overhead5:: 0
memory usage:: 26604347392
time_provenance:: 65.3553695678711
curr_diff: 0 tensor(0.0025, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0025, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.4218, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.4218, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8627, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.762425
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 1 0.06 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1214
max_epoch:: 200
overhead:: 0
overhead2:: 0.025892019271850586
overhead3:: 0.14107322692871094
overhead4:: 36.47972011566162
overhead5:: 0
memory usage:: 26614816768
time_provenance:: 72.56921792030334
curr_diff: 0 tensor(0.0025, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0025, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.4218, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.4218, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8627, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.762425
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 1 0.06 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1214
max_epoch:: 200
overhead:: 0
overhead2:: 0.0298154354095459
overhead3:: 0.15076613426208496
overhead4:: 41.85255742073059
overhead5:: 0
memory usage:: 26601754624
time_provenance:: 79.36321449279785
curr_diff: 0 tensor(0.0025, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0025, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.4218, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.4218, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8627, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.762425
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 1 0.06 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1214
max_epoch:: 200
overhead:: 0
overhead2:: 0.03888535499572754
overhead3:: 0.17070317268371582
overhead4:: 55.083096504211426
overhead5:: 0
memory usage:: 26609041408
time_provenance:: 96.73676943778992
curr_diff: 0 tensor(0.0020, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0020, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.4223, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.4223, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8625, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.762029
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 1 0.06 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1214
max_epoch:: 200
overhead:: 0
overhead2:: 0.04219865798950195
overhead3:: 0.17682170867919922
overhead4:: 59.968037605285645
overhead5:: 0
memory usage:: 26599583744
time_provenance:: 102.99857330322266
curr_diff: 0 tensor(0.0020, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0020, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.4223, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.4223, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8625, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.761980
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 1 0.06 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1214
max_epoch:: 200
overhead:: 0
overhead2:: 0.0479428768157959
overhead3:: 0.1891953945159912
overhead4:: 64.65799379348755
overhead5:: 0
memory usage:: 26609893376
time_provenance:: 108.8461742401123
curr_diff: 0 tensor(0.0020, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0020, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.4223, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.4223, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8625, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.761980
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 1 0.06 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1214
max_epoch:: 200
overhead:: 0
overhead2:: 0.09170961380004883
overhead3:: 0.2844862937927246
overhead4:: 130.29066848754883
overhead5:: 0
memory usage:: 26606678016
time_provenance:: 192.42677640914917
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.4229, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.4229, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8620, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.761288
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 1 0.06 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1214
max_epoch:: 200
overhead:: 0
overhead2:: 0.09306716918945312
overhead3:: 0.28664302825927734
overhead4:: 133.36556887626648
overhead5:: 0
memory usage:: 26608185344
time_provenance:: 196.2393102645874
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.4229, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.4229, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8620, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.761288
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 1 0.06 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1214
max_epoch:: 200
overhead:: 0
overhead2:: 0.10193276405334473
overhead3:: 0.29860377311706543
overhead4:: 135.9922890663147
overhead5:: 0
memory usage:: 26603511808
time_provenance:: 199.63558959960938
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.4229, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.4229, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8620, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.761288
adding noise deletion rate:: 0.07
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
tensor([0.5167, 0.5186, 0.5067,  ..., 0.5058, 0.5112, 0.5013],
       dtype=torch.float64, grad_fn=<MaxBackward0>)
tensor(0.8685, dtype=torch.float64) tensor(0., dtype=torch.float64)
Train - Epoch 0, Batch: 0, Loss: 0.693251
Train - Epoch 1, Batch: 0, Loss: 0.693162
Train - Epoch 2, Batch: 0, Loss: 0.693079
Train - Epoch 3, Batch: 0, Loss: 0.692967
Train - Epoch 4, Batch: 0, Loss: 0.692895
Train - Epoch 5, Batch: 0, Loss: 0.692769
Train - Epoch 6, Batch: 0, Loss: 0.692707
Train - Epoch 7, Batch: 0, Loss: 0.692581
Train - Epoch 8, Batch: 0, Loss: 0.692501
Train - Epoch 9, Batch: 0, Loss: 0.692417
Train - Epoch 10, Batch: 0, Loss: 0.692335
Train - Epoch 11, Batch: 0, Loss: 0.692327
Train - Epoch 12, Batch: 0, Loss: 0.692164
Train - Epoch 13, Batch: 0, Loss: 0.692086
Train - Epoch 14, Batch: 0, Loss: 0.692062
Train - Epoch 15, Batch: 0, Loss: 0.691860
Train - Epoch 16, Batch: 0, Loss: 0.691818
Train - Epoch 17, Batch: 0, Loss: 0.691723
Train - Epoch 18, Batch: 0, Loss: 0.691641
Train - Epoch 19, Batch: 0, Loss: 0.691487
Train - Epoch 20, Batch: 0, Loss: 0.691525
Train - Epoch 21, Batch: 0, Loss: 0.691406
Train - Epoch 22, Batch: 0, Loss: 0.691318
Train - Epoch 23, Batch: 0, Loss: 0.691108
Train - Epoch 24, Batch: 0, Loss: 0.691128
Train - Epoch 25, Batch: 0, Loss: 0.691017
Train - Epoch 26, Batch: 0, Loss: 0.690946
Train - Epoch 27, Batch: 0, Loss: 0.690903
Train - Epoch 28, Batch: 0, Loss: 0.690792
Train - Epoch 29, Batch: 0, Loss: 0.690639
Train - Epoch 30, Batch: 0, Loss: 0.690662
Train - Epoch 31, Batch: 0, Loss: 0.690433
Train - Epoch 32, Batch: 0, Loss: 0.690518
Train - Epoch 33, Batch: 0, Loss: 0.690348
Train - Epoch 34, Batch: 0, Loss: 0.690200
Train - Epoch 35, Batch: 0, Loss: 0.690144
Train - Epoch 36, Batch: 0, Loss: 0.690208
Train - Epoch 37, Batch: 0, Loss: 0.690020
Train - Epoch 38, Batch: 0, Loss: 0.689999
Train - Epoch 39, Batch: 0, Loss: 0.689784
Train - Epoch 40, Batch: 0, Loss: 0.689738
Train - Epoch 41, Batch: 0, Loss: 0.689644
Train - Epoch 42, Batch: 0, Loss: 0.689545
Train - Epoch 43, Batch: 0, Loss: 0.689595
Train - Epoch 44, Batch: 0, Loss: 0.689486
Train - Epoch 45, Batch: 0, Loss: 0.689427
Train - Epoch 46, Batch: 0, Loss: 0.689234
Train - Epoch 47, Batch: 0, Loss: 0.689186
Train - Epoch 48, Batch: 0, Loss: 0.689151
Train - Epoch 49, Batch: 0, Loss: 0.689192
Train - Epoch 50, Batch: 0, Loss: 0.689217
Train - Epoch 51, Batch: 0, Loss: 0.688656
Train - Epoch 52, Batch: 0, Loss: 0.688895
Train - Epoch 53, Batch: 0, Loss: 0.688476
Train - Epoch 54, Batch: 0, Loss: 0.688664
Train - Epoch 55, Batch: 0, Loss: 0.688830
Train - Epoch 56, Batch: 0, Loss: 0.688572
Train - Epoch 57, Batch: 0, Loss: 0.688435
Train - Epoch 58, Batch: 0, Loss: 0.688229
Train - Epoch 59, Batch: 0, Loss: 0.688291
Train - Epoch 60, Batch: 0, Loss: 0.688320
Train - Epoch 61, Batch: 0, Loss: 0.688072
Train - Epoch 62, Batch: 0, Loss: 0.688076
Train - Epoch 63, Batch: 0, Loss: 0.687931
Train - Epoch 64, Batch: 0, Loss: 0.687834
Train - Epoch 65, Batch: 0, Loss: 0.687935
Train - Epoch 66, Batch: 0, Loss: 0.687628
Train - Epoch 67, Batch: 0, Loss: 0.687650
Train - Epoch 68, Batch: 0, Loss: 0.687718
Train - Epoch 69, Batch: 0, Loss: 0.687503
Train - Epoch 70, Batch: 0, Loss: 0.687024
Train - Epoch 71, Batch: 0, Loss: 0.687464
Train - Epoch 72, Batch: 0, Loss: 0.687369
Train - Epoch 73, Batch: 0, Loss: 0.687456
Train - Epoch 74, Batch: 0, Loss: 0.687205
Train - Epoch 75, Batch: 0, Loss: 0.687043
Train - Epoch 76, Batch: 0, Loss: 0.687241
Train - Epoch 77, Batch: 0, Loss: 0.686934
Train - Epoch 78, Batch: 0, Loss: 0.686967
Train - Epoch 79, Batch: 0, Loss: 0.687135
Train - Epoch 80, Batch: 0, Loss: 0.686960
Train - Epoch 81, Batch: 0, Loss: 0.686652
Train - Epoch 82, Batch: 0, Loss: 0.686326
Train - Epoch 83, Batch: 0, Loss: 0.686474
Train - Epoch 84, Batch: 0, Loss: 0.686569
Train - Epoch 85, Batch: 0, Loss: 0.686572
Train - Epoch 86, Batch: 0, Loss: 0.686637
Train - Epoch 87, Batch: 0, Loss: 0.686360
Train - Epoch 88, Batch: 0, Loss: 0.686449
Train - Epoch 89, Batch: 0, Loss: 0.686225
Train - Epoch 90, Batch: 0, Loss: 0.685813
Train - Epoch 91, Batch: 0, Loss: 0.686073
Train - Epoch 92, Batch: 0, Loss: 0.686100
Train - Epoch 93, Batch: 0, Loss: 0.686156
Train - Epoch 94, Batch: 0, Loss: 0.685714
Train - Epoch 95, Batch: 0, Loss: 0.685966
Train - Epoch 96, Batch: 0, Loss: 0.685450
Train - Epoch 97, Batch: 0, Loss: 0.685587
Train - Epoch 98, Batch: 0, Loss: 0.685385
Train - Epoch 99, Batch: 0, Loss: 0.685798
Train - Epoch 100, Batch: 0, Loss: 0.685532
Train - Epoch 101, Batch: 0, Loss: 0.685273
Train - Epoch 102, Batch: 0, Loss: 0.685174
Train - Epoch 103, Batch: 0, Loss: 0.685183
Train - Epoch 104, Batch: 0, Loss: 0.685248
Train - Epoch 105, Batch: 0, Loss: 0.684949
Train - Epoch 106, Batch: 0, Loss: 0.685020
Train - Epoch 107, Batch: 0, Loss: 0.685085
Train - Epoch 108, Batch: 0, Loss: 0.684640
Train - Epoch 109, Batch: 0, Loss: 0.685052
Train - Epoch 110, Batch: 0, Loss: 0.684755
Train - Epoch 111, Batch: 0, Loss: 0.684810
Train - Epoch 112, Batch: 0, Loss: 0.684468
Train - Epoch 113, Batch: 0, Loss: 0.684393
Train - Epoch 114, Batch: 0, Loss: 0.684719
Train - Epoch 115, Batch: 0, Loss: 0.684330
Train - Epoch 116, Batch: 0, Loss: 0.684787
Train - Epoch 117, Batch: 0, Loss: 0.684306
Train - Epoch 118, Batch: 0, Loss: 0.683966
Train - Epoch 119, Batch: 0, Loss: 0.684726
Train - Epoch 120, Batch: 0, Loss: 0.684412
Train - Epoch 121, Batch: 0, Loss: 0.684261
Train - Epoch 122, Batch: 0, Loss: 0.684040
Train - Epoch 123, Batch: 0, Loss: 0.683769
Train - Epoch 124, Batch: 0, Loss: 0.683985
Train - Epoch 125, Batch: 0, Loss: 0.684265
Train - Epoch 126, Batch: 0, Loss: 0.683996
Train - Epoch 127, Batch: 0, Loss: 0.683914
Train - Epoch 128, Batch: 0, Loss: 0.683821
Train - Epoch 129, Batch: 0, Loss: 0.683591
Train - Epoch 130, Batch: 0, Loss: 0.683335
Train - Epoch 131, Batch: 0, Loss: 0.683516
Train - Epoch 132, Batch: 0, Loss: 0.683738
Train - Epoch 133, Batch: 0, Loss: 0.683366
Train - Epoch 134, Batch: 0, Loss: 0.683566
Train - Epoch 135, Batch: 0, Loss: 0.683270
Train - Epoch 136, Batch: 0, Loss: 0.683375
Train - Epoch 137, Batch: 0, Loss: 0.683281
Train - Epoch 138, Batch: 0, Loss: 0.683049
Train - Epoch 139, Batch: 0, Loss: 0.682827
Train - Epoch 140, Batch: 0, Loss: 0.682909
Train - Epoch 141, Batch: 0, Loss: 0.682762
Train - Epoch 142, Batch: 0, Loss: 0.682856
Train - Epoch 143, Batch: 0, Loss: 0.683019
Train - Epoch 144, Batch: 0, Loss: 0.682851
Train - Epoch 145, Batch: 0, Loss: 0.682508
Train - Epoch 146, Batch: 0, Loss: 0.682506
Train - Epoch 147, Batch: 0, Loss: 0.682114
Train - Epoch 148, Batch: 0, Loss: 0.682251
Train - Epoch 149, Batch: 0, Loss: 0.682848
Train - Epoch 150, Batch: 0, Loss: 0.682163
Train - Epoch 151, Batch: 0, Loss: 0.682311
Train - Epoch 152, Batch: 0, Loss: 0.682377
Train - Epoch 153, Batch: 0, Loss: 0.682471
Train - Epoch 154, Batch: 0, Loss: 0.682228
Train - Epoch 155, Batch: 0, Loss: 0.682069
Train - Epoch 156, Batch: 0, Loss: 0.681892
Train - Epoch 157, Batch: 0, Loss: 0.681847
Train - Epoch 158, Batch: 0, Loss: 0.681985
Train - Epoch 159, Batch: 0, Loss: 0.682032
Train - Epoch 160, Batch: 0, Loss: 0.681223
Train - Epoch 161, Batch: 0, Loss: 0.682237
Train - Epoch 162, Batch: 0, Loss: 0.681368
Train - Epoch 163, Batch: 0, Loss: 0.681568
Train - Epoch 164, Batch: 0, Loss: 0.681565
Train - Epoch 165, Batch: 0, Loss: 0.681133
Train - Epoch 166, Batch: 0, Loss: 0.681607
Train - Epoch 167, Batch: 0, Loss: 0.681747
Train - Epoch 168, Batch: 0, Loss: 0.681103
Train - Epoch 169, Batch: 0, Loss: 0.681813
Train - Epoch 170, Batch: 0, Loss: 0.681609
Train - Epoch 171, Batch: 0, Loss: 0.681406
Train - Epoch 172, Batch: 0, Loss: 0.681076
Train - Epoch 173, Batch: 0, Loss: 0.681100
Train - Epoch 174, Batch: 0, Loss: 0.681691
Train - Epoch 175, Batch: 0, Loss: 0.680521
Train - Epoch 176, Batch: 0, Loss: 0.680677
Train - Epoch 177, Batch: 0, Loss: 0.681297
Train - Epoch 178, Batch: 0, Loss: 0.680982
Train - Epoch 179, Batch: 0, Loss: 0.680791
Train - Epoch 180, Batch: 0, Loss: 0.680553
Train - Epoch 181, Batch: 0, Loss: 0.680818
Train - Epoch 182, Batch: 0, Loss: 0.680827/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 183, Batch: 0, Loss: 0.680916
Train - Epoch 184, Batch: 0, Loss: 0.680849
Train - Epoch 185, Batch: 0, Loss: 0.681007
Train - Epoch 186, Batch: 0, Loss: 0.681170
Train - Epoch 187, Batch: 0, Loss: 0.680081
Train - Epoch 188, Batch: 0, Loss: 0.680386
Train - Epoch 189, Batch: 0, Loss: 0.680365
Train - Epoch 190, Batch: 0, Loss: 0.680434
Train - Epoch 191, Batch: 0, Loss: 0.680241
Train - Epoch 192, Batch: 0, Loss: 0.680448
Train - Epoch 193, Batch: 0, Loss: 0.680113
Train - Epoch 194, Batch: 0, Loss: 0.679779
Train - Epoch 195, Batch: 0, Loss: 0.680182
Train - Epoch 196, Batch: 0, Loss: 0.679790
Train - Epoch 197, Batch: 0, Loss: 0.680008
Train - Epoch 198, Batch: 0, Loss: 0.679612
Train - Epoch 199, Batch: 0, Loss: 0.680633
training_time:: 353.89803862571716
training time full:: 353.89807868003845
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000067, Accuracy: 0.487007
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1416
max_epoch:: 200
batch_size:: 10200
epoch:: 0
tensor([  0,   1,   5,   8,  10,  12,  17,  19,  22,  23,  25,  26,  32,  37,
         40,  43,  44,  46,  49,  51,  52,  54,  57,  58,  60,  64,  68,  73,
         75,  76,  77,  80,  81,  82,  84,  86,  89,  92,  93,  95,  96,  98,
         99, 102, 103, 105, 106, 107, 109, 115])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
epoch:: 60
epoch:: 61
epoch:: 62
epoch:: 63
epoch:: 64
epoch:: 65
epoch:: 66
epoch:: 67
epoch:: 68
epoch:: 69
epoch:: 70
epoch:: 71
epoch:: 72
epoch:: 73
epoch:: 74
epoch:: 75
epoch:: 76
epoch:: 77
epoch:: 78
epoch:: 79
epoch:: 80
epoch:: 81
epoch:: 82
epoch:: 83
epoch:: 84
epoch:: 85
epoch:: 86
epoch:: 87
epoch:: 88
epoch:: 89
epoch:: 90
epoch:: 91
epoch:: 92
epoch:: 93
epoch:: 94
epoch:: 95
epoch:: 96
epoch:: 97
epoch:: 98
epoch:: 99
epoch:: 100
epoch:: 101
epoch:: 102
epoch:: 103
epoch:: 104
epoch:: 105
epoch:: 106
epoch:: 107
epoch:: 108
epoch:: 109
epoch:: 110
epoch:: 111
epoch:: 112
epoch:: 113
epoch:: 114
epoch:: 115
epoch:: 116
epoch:: 117
epoch:: 118
epoch:: 119
epoch:: 120
epoch:: 121
epoch:: 122
epoch:: 123
epoch:: 124
epoch:: 125
epoch:: 126
epoch:: 127
epoch:: 128
epoch:: 129
epoch:: 130
epoch:: 131
epoch:: 132
epoch:: 133
epoch:: 134
epoch:: 135
epoch:: 136
epoch:: 137
epoch:: 138
epoch:: 139
epoch:: 140
epoch:: 141
epoch:: 142
epoch:: 143
epoch:: 144
epoch:: 145
epoch:: 146
epoch:: 147
epoch:: 148
epoch:: 149
epoch:: 150
epoch:: 151
epoch:: 152
epoch:: 153
epoch:: 154
epoch:: 155
epoch:: 156
epoch:: 157
epoch:: 158
epoch:: 159
epoch:: 160
epoch:: 161
epoch:: 162
epoch:: 163
epoch:: 164
epoch:: 165
epoch:: 166
epoch:: 167
epoch:: 168
epoch:: 169
epoch:: 170
epoch:: 171
epoch:: 172
epoch:: 173
epoch:: 174
epoch:: 175
epoch:: 176
epoch:: 177
epoch:: 178
epoch:: 179
epoch:: 180
epoch:: 181
epoch:: 182
epoch:: 183
epoch:: 184
epoch:: 185
epoch:: 186
epoch:: 187
epoch:: 188
epoch:: 189
epoch:: 190
epoch:: 191
epoch:: 192
epoch:: 193
epoch:: 194
epoch:: 195
epoch:: 196
epoch:: 197
epoch:: 198
epoch:: 199
training time is 258.285276889801
overhead:: 0
overhead2:: 0.44371819496154785
overhead3:: 0
time_baseline:: 258.2853512763977
curr_diff: 0 tensor(0.4557, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.4557, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8826, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.682146
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 1 0.07 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1416
max_epoch:: 200
overhead:: 0
overhead2:: 0.021231889724731445
overhead3:: 0.13449764251708984
overhead4:: 30.12443995475769
overhead5:: 0
memory usage:: 26620174336
time_provenance:: 69.00558567047119
curr_diff: 0 tensor(0.0032, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0032, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.4526, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.4526, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8844, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.678984
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 1 0.07 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1416
max_epoch:: 200
overhead:: 0
overhead2:: 0.02536320686340332
overhead3:: 0.14270734786987305
overhead4:: 35.713337659835815
overhead5:: 0
memory usage:: 26663813120
time_provenance:: 75.59045243263245
curr_diff: 0 tensor(0.0032, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0032, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.4526, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.4526, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8844, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.678984
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 1 0.07 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1416
max_epoch:: 200
overhead:: 0
overhead2:: 0.031625986099243164
overhead3:: 0.15436625480651855
overhead4:: 40.98691272735596
overhead5:: 0
memory usage:: 26611585024
time_provenance:: 82.52035808563232
curr_diff: 0 tensor(0.0032, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0032, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.4526, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.4526, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8844, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.678984
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 1 0.07 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1416
max_epoch:: 200
overhead:: 0
overhead2:: 0.04107093811035156
overhead3:: 0.1748065948486328
overhead4:: 54.12597107887268
overhead5:: 0
memory usage:: 26612330496
time_provenance:: 99.42938256263733
curr_diff: 0 tensor(0.0026, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0026, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.4531, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.4531, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8841, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.679429
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 1 0.07 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1416
max_epoch:: 200
overhead:: 0
overhead2:: 0.04368257522583008
overhead3:: 0.18030571937561035
overhead4:: 59.09933257102966
overhead5:: 0
memory usage:: 26610737152
time_provenance:: 105.87024903297424
curr_diff: 0 tensor(0.0026, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0026, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.4531, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.4531, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8841, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.679429
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 1 0.07 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1416
max_epoch:: 200
overhead:: 0
overhead2:: 0.04872584342956543
overhead3:: 0.18864893913269043
overhead4:: 64.15301990509033
overhead5:: 0
memory usage:: 26615033856
time_provenance:: 112.26720809936523
curr_diff: 0 tensor(0.0026, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0026, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.4531, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.4531, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8841, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.679478
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 1 0.07 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1416
max_epoch:: 200
overhead:: 0
overhead2:: 0.10917115211486816
overhead3:: 0.2890911102294922
overhead4:: 127.69968724250793
overhead5:: 0
memory usage:: 26614501376
time_provenance:: 192.6433970928192
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.4539, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.4539, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8836, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.680516
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 1 0.07 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1416
max_epoch:: 200
overhead:: 0
overhead2:: 0.0969705581665039
overhead3:: 0.2929699420928955
overhead4:: 131.25701189041138
overhead5:: 0
memory usage:: 26611335168
time_provenance:: 197.02903819084167
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.4539, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.4539, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8836, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.680516
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 1 0.07 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1416
max_epoch:: 200
overhead:: 0
overhead2:: 0.10222530364990234
overhead3:: 0.29908037185668945
overhead4:: 135.16190648078918
overhead5:: 0
memory usage:: 26600198144
time_provenance:: 201.77346968650818
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.4539, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.4539, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8836, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.680516
adding noise deletion rate:: 0.08
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
tensor([0.5224, 0.5298, 0.5086,  ..., 0.5267, 0.5119, 0.5165],
       dtype=torch.float64, grad_fn=<MaxBackward0>)
tensor(0.8685, dtype=torch.float64) tensor(0., dtype=torch.float64)
Train - Epoch 0, Batch: 0, Loss: 0.693221
Train - Epoch 1, Batch: 0, Loss: 0.693149
Train - Epoch 2, Batch: 0, Loss: 0.693105
Train - Epoch 3, Batch: 0, Loss: 0.693037
Train - Epoch 4, Batch: 0, Loss: 0.692976
Train - Epoch 5, Batch: 0, Loss: 0.692920
Train - Epoch 6, Batch: 0, Loss: 0.692843
Train - Epoch 7, Batch: 0, Loss: 0.692814
Train - Epoch 8, Batch: 0, Loss: 0.692729
Train - Epoch 9, Batch: 0, Loss: 0.692676
Train - Epoch 10, Batch: 0, Loss: 0.692633
Train - Epoch 11, Batch: 0, Loss: 0.692547
Train - Epoch 12, Batch: 0, Loss: 0.692470
Train - Epoch 13, Batch: 0, Loss: 0.692463
Train - Epoch 14, Batch: 0, Loss: 0.692369
Train - Epoch 15, Batch: 0, Loss: 0.692345
Train - Epoch 16, Batch: 0, Loss: 0.692287
Train - Epoch 17, Batch: 0, Loss: 0.692248
Train - Epoch 18, Batch: 0, Loss: 0.692175
Train - Epoch 19, Batch: 0, Loss: 0.692137
Train - Epoch 20, Batch: 0, Loss: 0.692068
Train - Epoch 21, Batch: 0, Loss: 0.691967
Train - Epoch 22, Batch: 0, Loss: 0.691964
Train - Epoch 23, Batch: 0, Loss: 0.691884
Train - Epoch 24, Batch: 0, Loss: 0.691740
Train - Epoch 25, Batch: 0, Loss: 0.691789
Train - Epoch 26, Batch: 0, Loss: 0.691794
Train - Epoch 27, Batch: 0, Loss: 0.691721
Train - Epoch 28, Batch: 0, Loss: 0.691584
Train - Epoch 29, Batch: 0, Loss: 0.691563
Train - Epoch 30, Batch: 0, Loss: 0.691568
Train - Epoch 31, Batch: 0, Loss: 0.691447
Train - Epoch 32, Batch: 0, Loss: 0.691340
Train - Epoch 33, Batch: 0, Loss: 0.691405
Train - Epoch 34, Batch: 0, Loss: 0.691219
Train - Epoch 35, Batch: 0, Loss: 0.691265
Train - Epoch 36, Batch: 0, Loss: 0.691166
Train - Epoch 37, Batch: 0, Loss: 0.691067
Train - Epoch 38, Batch: 0, Loss: 0.691056
Train - Epoch 39, Batch: 0, Loss: 0.691081
Train - Epoch 40, Batch: 0, Loss: 0.690918
Train - Epoch 41, Batch: 0, Loss: 0.690818
Train - Epoch 42, Batch: 0, Loss: 0.690814
Train - Epoch 43, Batch: 0, Loss: 0.690889
Train - Epoch 44, Batch: 0, Loss: 0.690649
Train - Epoch 45, Batch: 0, Loss: 0.690668
Train - Epoch 46, Batch: 0, Loss: 0.690604
Train - Epoch 47, Batch: 0, Loss: 0.690652
Train - Epoch 48, Batch: 0, Loss: 0.690556
Train - Epoch 49, Batch: 0, Loss: 0.690614
Train - Epoch 50, Batch: 0, Loss: 0.690428
Train - Epoch 51, Batch: 0, Loss: 0.690497
Train - Epoch 52, Batch: 0, Loss: 0.690246
Train - Epoch 53, Batch: 0, Loss: 0.690337
Train - Epoch 54, Batch: 0, Loss: 0.690322
Train - Epoch 55, Batch: 0, Loss: 0.690202
Train - Epoch 56, Batch: 0, Loss: 0.690022
Train - Epoch 57, Batch: 0, Loss: 0.690106
Train - Epoch 58, Batch: 0, Loss: 0.690127
Train - Epoch 59, Batch: 0, Loss: 0.689959
Train - Epoch 60, Batch: 0, Loss: 0.689787
Train - Epoch 61, Batch: 0, Loss: 0.689717
Train - Epoch 62, Batch: 0, Loss: 0.689777
Train - Epoch 63, Batch: 0, Loss: 0.689866
Train - Epoch 64, Batch: 0, Loss: 0.689667
Train - Epoch 65, Batch: 0, Loss: 0.689691
Train - Epoch 66, Batch: 0, Loss: 0.689817
Train - Epoch 67, Batch: 0, Loss: 0.689478
Train - Epoch 68, Batch: 0, Loss: 0.689562
Train - Epoch 69, Batch: 0, Loss: 0.689501
Train - Epoch 70, Batch: 0, Loss: 0.689573
Train - Epoch 71, Batch: 0, Loss: 0.689366
Train - Epoch 72, Batch: 0, Loss: 0.689343
Train - Epoch 73, Batch: 0, Loss: 0.689087
Train - Epoch 74, Batch: 0, Loss: 0.689266
Train - Epoch 75, Batch: 0, Loss: 0.689214
Train - Epoch 76, Batch: 0, Loss: 0.689071
Train - Epoch 77, Batch: 0, Loss: 0.688979
Train - Epoch 78, Batch: 0, Loss: 0.689074
Train - Epoch 79, Batch: 0, Loss: 0.688757
Train - Epoch 80, Batch: 0, Loss: 0.688812
Train - Epoch 81, Batch: 0, Loss: 0.688781
Train - Epoch 82, Batch: 0, Loss: 0.688671
Train - Epoch 83, Batch: 0, Loss: 0.689144
Train - Epoch 84, Batch: 0, Loss: 0.688849
Train - Epoch 85, Batch: 0, Loss: 0.688885
Train - Epoch 86, Batch: 0, Loss: 0.688566
Train - Epoch 87, Batch: 0, Loss: 0.688805
Train - Epoch 88, Batch: 0, Loss: 0.688830
Train - Epoch 89, Batch: 0, Loss: 0.688575
Train - Epoch 90, Batch: 0, Loss: 0.688718
Train - Epoch 91, Batch: 0, Loss: 0.688135
Train - Epoch 92, Batch: 0, Loss: 0.688451
Train - Epoch 93, Batch: 0, Loss: 0.688224
Train - Epoch 94, Batch: 0, Loss: 0.688622
Train - Epoch 95, Batch: 0, Loss: 0.688253
Train - Epoch 96, Batch: 0, Loss: 0.688402
Train - Epoch 97, Batch: 0, Loss: 0.688304
Train - Epoch 98, Batch: 0, Loss: 0.688115
Train - Epoch 99, Batch: 0, Loss: 0.688155
Train - Epoch 100, Batch: 0, Loss: 0.688201
Train - Epoch 101, Batch: 0, Loss: 0.688120
Train - Epoch 102, Batch: 0, Loss: 0.688120
Train - Epoch 103, Batch: 0, Loss: 0.688165
Train - Epoch 104, Batch: 0, Loss: 0.687858
Train - Epoch 105, Batch: 0, Loss: 0.687986
Train - Epoch 106, Batch: 0, Loss: 0.687864
Train - Epoch 107, Batch: 0, Loss: 0.687804
Train - Epoch 108, Batch: 0, Loss: 0.687789
Train - Epoch 109, Batch: 0, Loss: 0.687603
Train - Epoch 110, Batch: 0, Loss: 0.687628
Train - Epoch 111, Batch: 0, Loss: 0.687580
Train - Epoch 112, Batch: 0, Loss: 0.687675
Train - Epoch 113, Batch: 0, Loss: 0.687512
Train - Epoch 114, Batch: 0, Loss: 0.687639
Train - Epoch 115, Batch: 0, Loss: 0.687620
Train - Epoch 116, Batch: 0, Loss: 0.687643
Train - Epoch 117, Batch: 0, Loss: 0.687255
Train - Epoch 118, Batch: 0, Loss: 0.687091
Train - Epoch 119, Batch: 0, Loss: 0.686974
Train - Epoch 120, Batch: 0, Loss: 0.686864
Train - Epoch 121, Batch: 0, Loss: 0.687081
Train - Epoch 122, Batch: 0, Loss: 0.687184
Train - Epoch 123, Batch: 0, Loss: 0.687283
Train - Epoch 124, Batch: 0, Loss: 0.687370
Train - Epoch 125, Batch: 0, Loss: 0.686713
Train - Epoch 126, Batch: 0, Loss: 0.687175
Train - Epoch 127, Batch: 0, Loss: 0.686951
Train - Epoch 128, Batch: 0, Loss: 0.686808
Train - Epoch 129, Batch: 0, Loss: 0.686702
Train - Epoch 130, Batch: 0, Loss: 0.686859
Train - Epoch 131, Batch: 0, Loss: 0.686498
Train - Epoch 132, Batch: 0, Loss: 0.686375
Train - Epoch 133, Batch: 0, Loss: 0.686582
Train - Epoch 134, Batch: 0, Loss: 0.686432
Train - Epoch 135, Batch: 0, Loss: 0.686436
Train - Epoch 136, Batch: 0, Loss: 0.686421
Train - Epoch 137, Batch: 0, Loss: 0.686537
Train - Epoch 138, Batch: 0, Loss: 0.686220
Train - Epoch 139, Batch: 0, Loss: 0.686125
Train - Epoch 140, Batch: 0, Loss: 0.686717
Train - Epoch 141, Batch: 0, Loss: 0.686381
Train - Epoch 142, Batch: 0, Loss: 0.686546
Train - Epoch 143, Batch: 0, Loss: 0.686417
Train - Epoch 144, Batch: 0, Loss: 0.686455
Train - Epoch 145, Batch: 0, Loss: 0.686333
Train - Epoch 146, Batch: 0, Loss: 0.686454
Train - Epoch 147, Batch: 0, Loss: 0.685929
Train - Epoch 148, Batch: 0, Loss: 0.685949
Train - Epoch 149, Batch: 0, Loss: 0.686389
Train - Epoch 150, Batch: 0, Loss: 0.686132
Train - Epoch 151, Batch: 0, Loss: 0.685885
Train - Epoch 152, Batch: 0, Loss: 0.686059
Train - Epoch 153, Batch: 0, Loss: 0.686493
Train - Epoch 154, Batch: 0, Loss: 0.685843
Train - Epoch 155, Batch: 0, Loss: 0.685829
Train - Epoch 156, Batch: 0, Loss: 0.685574
Train - Epoch 157, Batch: 0, Loss: 0.685928
Train - Epoch 158, Batch: 0, Loss: 0.685835
Train - Epoch 159, Batch: 0, Loss: 0.685811
Train - Epoch 160, Batch: 0, Loss: 0.685990
Train - Epoch 161, Batch: 0, Loss: 0.685411
Train - Epoch 162, Batch: 0, Loss: 0.685540
Train - Epoch 163, Batch: 0, Loss: 0.685323
Train - Epoch 164, Batch: 0, Loss: 0.685657
Train - Epoch 165, Batch: 0, Loss: 0.685573
Train - Epoch 166, Batch: 0, Loss: 0.685630
Train - Epoch 167, Batch: 0, Loss: 0.685365
Train - Epoch 168, Batch: 0, Loss: 0.684947
Train - Epoch 169, Batch: 0, Loss: 0.685260
Train - Epoch 170, Batch: 0, Loss: 0.685188
Train - Epoch 171, Batch: 0, Loss: 0.685418
Train - Epoch 172, Batch: 0, Loss: 0.685584
Train - Epoch 173, Batch: 0, Loss: 0.685189
Train - Epoch 174, Batch: 0, Loss: 0.685343
Train - Epoch 175, Batch: 0, Loss: 0.685524
Train - Epoch 176, Batch: 0, Loss: 0.685031
Train - Epoch 177, Batch: 0, Loss: 0.685129
Train - Epoch 178, Batch: 0, Loss: 0.685399
Train - Epoch 179, Batch: 0, Loss: 0.684963
Train - Epoch 180, Batch: 0, Loss: 0.684618
Train - Epoch 181, Batch: 0, Loss: 0.684527
Train - Epoch 182, Batch: 0, Loss: 0.684961/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 183, Batch: 0, Loss: 0.685309
Train - Epoch 184, Batch: 0, Loss: 0.684511
Train - Epoch 185, Batch: 0, Loss: 0.684930
Train - Epoch 186, Batch: 0, Loss: 0.684817
Train - Epoch 187, Batch: 0, Loss: 0.684729
Train - Epoch 188, Batch: 0, Loss: 0.684591
Train - Epoch 189, Batch: 0, Loss: 0.684171
Train - Epoch 190, Batch: 0, Loss: 0.684694
Train - Epoch 191, Batch: 0, Loss: 0.684820
Train - Epoch 192, Batch: 0, Loss: 0.684778
Train - Epoch 193, Batch: 0, Loss: 0.684246
Train - Epoch 194, Batch: 0, Loss: 0.684880
Train - Epoch 195, Batch: 0, Loss: 0.684064
Train - Epoch 196, Batch: 0, Loss: 0.684274
Train - Epoch 197, Batch: 0, Loss: 0.683734
Train - Epoch 198, Batch: 0, Loss: 0.684724
Train - Epoch 199, Batch: 0, Loss: 0.684197
training_time:: 351.22090673446655
training time full:: 351.22094774246216
provenance prepare time:: 0.0
here
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.602164
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1619
max_epoch:: 200
batch_size:: 10200
epoch:: 0
tensor([  0,   1,   5,   8,  10,  12,  17,  19,  22,  23,  25,  26,  32,  37,
         40,  43,  44,  46,  49,  51,  52,  54,  57,  58,  60,  64,  68,  73,
         75,  76,  77,  80,  81,  84,  86,  89,  92,  93,  95,  96,  98,  99,
        102, 105, 106, 107, 109, 115, 116, 119])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
epoch:: 60
epoch:: 61
epoch:: 62
epoch:: 63
epoch:: 64
epoch:: 65
epoch:: 66
epoch:: 67
epoch:: 68
epoch:: 69
epoch:: 70
epoch:: 71
epoch:: 72
epoch:: 73
epoch:: 74
epoch:: 75
epoch:: 76
epoch:: 77
epoch:: 78
epoch:: 79
epoch:: 80
epoch:: 81
epoch:: 82
epoch:: 83
epoch:: 84
epoch:: 85
epoch:: 86
epoch:: 87
epoch:: 88
epoch:: 89
epoch:: 90
epoch:: 91
epoch:: 92
epoch:: 93
epoch:: 94
epoch:: 95
epoch:: 96
epoch:: 97
epoch:: 98
epoch:: 99
epoch:: 100
epoch:: 101
epoch:: 102
epoch:: 103
epoch:: 104
epoch:: 105
epoch:: 106
epoch:: 107
epoch:: 108
epoch:: 109
epoch:: 110
epoch:: 111
epoch:: 112
epoch:: 113
epoch:: 114
epoch:: 115
epoch:: 116
epoch:: 117
epoch:: 118
epoch:: 119
epoch:: 120
epoch:: 121
epoch:: 122
epoch:: 123
epoch:: 124
epoch:: 125
epoch:: 126
epoch:: 127
epoch:: 128
epoch:: 129
epoch:: 130
epoch:: 131
epoch:: 132
epoch:: 133
epoch:: 134
epoch:: 135
epoch:: 136
epoch:: 137
epoch:: 138
epoch:: 139
epoch:: 140
epoch:: 141
epoch:: 142
epoch:: 143
epoch:: 144
epoch:: 145
epoch:: 146
epoch:: 147
epoch:: 148
epoch:: 149
epoch:: 150
epoch:: 151
epoch:: 152
epoch:: 153
epoch:: 154
epoch:: 155
epoch:: 156
epoch:: 157
epoch:: 158
epoch:: 159
epoch:: 160
epoch:: 161
epoch:: 162
epoch:: 163
epoch:: 164
epoch:: 165
epoch:: 166
epoch:: 167
epoch:: 168
epoch:: 169
epoch:: 170
epoch:: 171
epoch:: 172
epoch:: 173
epoch:: 174
epoch:: 175
epoch:: 176
epoch:: 177
epoch:: 178
epoch:: 179
epoch:: 180
epoch:: 181
epoch:: 182
epoch:: 183
epoch:: 184
epoch:: 185
epoch:: 186
epoch:: 187
epoch:: 188
epoch:: 189
epoch:: 190
epoch:: 191
epoch:: 192
epoch:: 193
epoch:: 194
epoch:: 195
epoch:: 196
epoch:: 197
epoch:: 198
epoch:: 199
training time is 255.56451439857483
overhead:: 0
overhead2:: 0.4380521774291992
overhead3:: 0
time_baseline:: 255.56458806991577
curr_diff: 0 tensor(0.5359, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.5359, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.7955, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.744936
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 1 0.08 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1619
max_epoch:: 200
overhead:: 0
overhead2:: 0.022294044494628906
overhead3:: 0.13484764099121094
overhead4:: 30.12854552268982
overhead5:: 0
memory usage:: 26615656448
time_provenance:: 72.93295240402222
curr_diff: 0 tensor(0.0042, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0042, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.5318, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.5318, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.7987, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.749531
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 1 0.08 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1619
max_epoch:: 200
overhead:: 0
overhead2:: 0.030788898468017578
overhead3:: 0.14629054069519043
overhead4:: 35.56774616241455
overhead5:: 0
memory usage:: 26606469120
time_provenance:: 79.55596041679382
curr_diff: 0 tensor(0.0042, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0042, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.5318, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.5318, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.7987, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.749481
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 1 0.08 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1619
max_epoch:: 200
overhead:: 0
overhead2:: 0.03203916549682617
overhead3:: 0.15448951721191406
overhead4:: 41.05044412612915
overhead5:: 0
memory usage:: 26615033856
time_provenance:: 85.95217180252075
curr_diff: 0 tensor(0.0042, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0042, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.5318, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.5318, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.7987, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.749481
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 1 0.08 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1619
max_epoch:: 200
overhead:: 0
overhead2:: 0.03773784637451172
overhead3:: 0.17284798622131348
overhead4:: 54.18257927894592
overhead5:: 0
memory usage:: 26599026688
time_provenance:: 103.24930024147034
curr_diff: 0 tensor(0.0035, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0035, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.5324, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.5324, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.7982, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.748740
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 1 0.08 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1619
max_epoch:: 200
overhead:: 0
overhead2:: 0.04193902015686035
overhead3:: 0.18150901794433594
overhead4:: 58.851606607437134
overhead5:: 0
memory usage:: 26599456768
time_provenance:: 108.97693800926208
curr_diff: 0 tensor(0.0035, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0035, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.5324, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.5324, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.7982, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.748740
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 1 0.08 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1619
max_epoch:: 200
overhead:: 0
overhead2:: 0.04914116859436035
overhead3:: 0.1895442008972168
overhead4:: 64.26829648017883
overhead5:: 0
memory usage:: 26620518400
time_provenance:: 115.6986289024353
curr_diff: 0 tensor(0.0035, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0035, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.5325, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.5325, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.7981, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.748740
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 1 0.08 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1619
max_epoch:: 200
overhead:: 0
overhead2:: 0.1193387508392334
overhead3:: 0.2920205593109131
overhead4:: 127.86733031272888
overhead5:: 0
memory usage:: 26608271360
time_provenance:: 195.73785209655762
curr_diff: 0 tensor(0.0023, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0023, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.5336, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.5336, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.7973, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.747703
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 1 0.08 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1619
max_epoch:: 200
overhead:: 0
overhead2:: 0.1003568172454834
overhead3:: 0.2952842712402344
overhead4:: 130.0426344871521
overhead5:: 0
memory usage:: 26621161472
time_provenance:: 198.57194256782532
curr_diff: 0 tensor(0.0023, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0023, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.5336, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.5336, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.7973, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.747703
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 1 0.08 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1619
max_epoch:: 200
overhead:: 0
overhead2:: 0.09736156463623047
overhead3:: 0.29854369163513184
overhead4:: 133.42667293548584
overhead5:: 0
memory usage:: 26601582592
time_provenance:: 202.767648935318
curr_diff: 0 tensor(0.0023, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0023, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.5336, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.5336, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.7973, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.747703
adding noise deletion rate:: 0.1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
tensor([0.5198, 0.5238, 0.5080,  ..., 0.5093, 0.5134, 0.5037],
       dtype=torch.float64, grad_fn=<MaxBackward0>)
tensor(0.8685, dtype=torch.float64) tensor(0., dtype=torch.float64)
Train - Epoch 0, Batch: 0, Loss: 0.693203
Train - Epoch 1, Batch: 0, Loss: 0.693032
Train - Epoch 2, Batch: 0, Loss: 0.692886
Train - Epoch 3, Batch: 0, Loss: 0.692737
Train - Epoch 4, Batch: 0, Loss: 0.692573
Train - Epoch 5, Batch: 0, Loss: 0.692479
Train - Epoch 6, Batch: 0, Loss: 0.692306
Train - Epoch 7, Batch: 0, Loss: 0.692186
Train - Epoch 8, Batch: 0, Loss: 0.692037
Train - Epoch 9, Batch: 0, Loss: 0.691878
Train - Epoch 10, Batch: 0, Loss: 0.691718
Train - Epoch 11, Batch: 0, Loss: 0.691675
Train - Epoch 12, Batch: 0, Loss: 0.691438
Train - Epoch 13, Batch: 0, Loss: 0.691325
Train - Epoch 14, Batch: 0, Loss: 0.691260
Train - Epoch 15, Batch: 0, Loss: 0.690963
Train - Epoch 16, Batch: 0, Loss: 0.690923
Train - Epoch 17, Batch: 0, Loss: 0.690754
Train - Epoch 18, Batch: 0, Loss: 0.690686
Train - Epoch 19, Batch: 0, Loss: 0.690424
Train - Epoch 20, Batch: 0, Loss: 0.690401
Train - Epoch 21, Batch: 0, Loss: 0.690248
Train - Epoch 22, Batch: 0, Loss: 0.690066
Train - Epoch 23, Batch: 0, Loss: 0.689848
Train - Epoch 24, Batch: 0, Loss: 0.689741
Train - Epoch 25, Batch: 0, Loss: 0.689654
Train - Epoch 26, Batch: 0, Loss: 0.689503
Train - Epoch 27, Batch: 0, Loss: 0.689418
Train - Epoch 28, Batch: 0, Loss: 0.689211
Train - Epoch 29, Batch: 0, Loss: 0.689109
Train - Epoch 30, Batch: 0, Loss: 0.689079
Train - Epoch 31, Batch: 0, Loss: 0.688721
Train - Epoch 32, Batch: 0, Loss: 0.688811
Train - Epoch 33, Batch: 0, Loss: 0.688538
Train - Epoch 34, Batch: 0, Loss: 0.688362
Train - Epoch 35, Batch: 0, Loss: 0.688203
Train - Epoch 36, Batch: 0, Loss: 0.688299
Train - Epoch 37, Batch: 0, Loss: 0.688026
Train - Epoch 38, Batch: 0, Loss: 0.688041
Train - Epoch 39, Batch: 0, Loss: 0.687760
Train - Epoch 40, Batch: 0, Loss: 0.687645
Train - Epoch 41, Batch: 0, Loss: 0.687539
Train - Epoch 42, Batch: 0, Loss: 0.687321
Train - Epoch 43, Batch: 0, Loss: 0.687401
Train - Epoch 44, Batch: 0, Loss: 0.687106
Train - Epoch 45, Batch: 0, Loss: 0.687123
Train - Epoch 46, Batch: 0, Loss: 0.686768
Train - Epoch 47, Batch: 0, Loss: 0.686869
Train - Epoch 48, Batch: 0, Loss: 0.686719
Train - Epoch 49, Batch: 0, Loss: 0.686721
Train - Epoch 50, Batch: 0, Loss: 0.686671
Train - Epoch 51, Batch: 0, Loss: 0.686004
Train - Epoch 52, Batch: 0, Loss: 0.686490
Train - Epoch 53, Batch: 0, Loss: 0.685782
Train - Epoch 54, Batch: 0, Loss: 0.685867
Train - Epoch 55, Batch: 0, Loss: 0.686106
Train - Epoch 56, Batch: 0, Loss: 0.685693
Train - Epoch 57, Batch: 0, Loss: 0.685604
Train - Epoch 58, Batch: 0, Loss: 0.685378
Train - Epoch 59, Batch: 0, Loss: 0.685397
Train - Epoch 60, Batch: 0, Loss: 0.685430
Train - Epoch 61, Batch: 0, Loss: 0.684914
Train - Epoch 62, Batch: 0, Loss: 0.685099
Train - Epoch 63, Batch: 0, Loss: 0.684840
Train - Epoch 64, Batch: 0, Loss: 0.684746
Train - Epoch 65, Batch: 0, Loss: 0.684650
Train - Epoch 66, Batch: 0, Loss: 0.684497
Train - Epoch 67, Batch: 0, Loss: 0.684384
Train - Epoch 68, Batch: 0, Loss: 0.684575
Train - Epoch 69, Batch: 0, Loss: 0.684294
Train - Epoch 70, Batch: 0, Loss: 0.683520
Train - Epoch 71, Batch: 0, Loss: 0.684106
Train - Epoch 72, Batch: 0, Loss: 0.684158
Train - Epoch 73, Batch: 0, Loss: 0.683973
Train - Epoch 74, Batch: 0, Loss: 0.683793
Train - Epoch 75, Batch: 0, Loss: 0.683397
Train - Epoch 76, Batch: 0, Loss: 0.683555
Train - Epoch 77, Batch: 0, Loss: 0.683275
Train - Epoch 78, Batch: 0, Loss: 0.683338
Train - Epoch 79, Batch: 0, Loss: 0.683387
Train - Epoch 80, Batch: 0, Loss: 0.683112
Train - Epoch 81, Batch: 0, Loss: 0.682914
Train - Epoch 82, Batch: 0, Loss: 0.682341
Train - Epoch 83, Batch: 0, Loss: 0.682848
Train - Epoch 84, Batch: 0, Loss: 0.682666
Train - Epoch 85, Batch: 0, Loss: 0.682669
Train - Epoch 86, Batch: 0, Loss: 0.682736
Train - Epoch 87, Batch: 0, Loss: 0.682328
Train - Epoch 88, Batch: 0, Loss: 0.682444
Train - Epoch 89, Batch: 0, Loss: 0.682072
Train - Epoch 90, Batch: 0, Loss: 0.681768
Train - Epoch 91, Batch: 0, Loss: 0.681930
Train - Epoch 92, Batch: 0, Loss: 0.682018
Train - Epoch 93, Batch: 0, Loss: 0.681935
Train - Epoch 94, Batch: 0, Loss: 0.681439
Train - Epoch 95, Batch: 0, Loss: 0.681595
Train - Epoch 96, Batch: 0, Loss: 0.681106
Train - Epoch 97, Batch: 0, Loss: 0.681212
Train - Epoch 98, Batch: 0, Loss: 0.680713
Train - Epoch 99, Batch: 0, Loss: 0.681465
Train - Epoch 100, Batch: 0, Loss: 0.680947
Train - Epoch 101, Batch: 0, Loss: 0.680749
Train - Epoch 102, Batch: 0, Loss: 0.680600
Train - Epoch 103, Batch: 0, Loss: 0.680556
Train - Epoch 104, Batch: 0, Loss: 0.680722
Train - Epoch 105, Batch: 0, Loss: 0.680066
Train - Epoch 106, Batch: 0, Loss: 0.680291
Train - Epoch 107, Batch: 0, Loss: 0.680257
Train - Epoch 108, Batch: 0, Loss: 0.680005
Train - Epoch 109, Batch: 0, Loss: 0.680375
Train - Epoch 110, Batch: 0, Loss: 0.679754
Train - Epoch 111, Batch: 0, Loss: 0.680016
Train - Epoch 112, Batch: 0, Loss: 0.679605
Train - Epoch 113, Batch: 0, Loss: 0.679510
Train - Epoch 114, Batch: 0, Loss: 0.679654
Train - Epoch 115, Batch: 0, Loss: 0.679298
Train - Epoch 116, Batch: 0, Loss: 0.679775
Train - Epoch 117, Batch: 0, Loss: 0.679352
Train - Epoch 118, Batch: 0, Loss: 0.679063
Train - Epoch 119, Batch: 0, Loss: 0.679776
Train - Epoch 120, Batch: 0, Loss: 0.679195
Train - Epoch 121, Batch: 0, Loss: 0.679160
Train - Epoch 122, Batch: 0, Loss: 0.678719
Train - Epoch 123, Batch: 0, Loss: 0.678430
Train - Epoch 124, Batch: 0, Loss: 0.679245
Train - Epoch 125, Batch: 0, Loss: 0.679111
Train - Epoch 126, Batch: 0, Loss: 0.678909
Train - Epoch 127, Batch: 0, Loss: 0.678528
Train - Epoch 128, Batch: 0, Loss: 0.678484
Train - Epoch 129, Batch: 0, Loss: 0.678125
Train - Epoch 130, Batch: 0, Loss: 0.677557
Train - Epoch 131, Batch: 0, Loss: 0.677974
Train - Epoch 132, Batch: 0, Loss: 0.678162
Train - Epoch 133, Batch: 0, Loss: 0.677713
Train - Epoch 134, Batch: 0, Loss: 0.678151
Train - Epoch 135, Batch: 0, Loss: 0.677809
Train - Epoch 136, Batch: 0, Loss: 0.677484
Train - Epoch 137, Batch: 0, Loss: 0.677448
Train - Epoch 138, Batch: 0, Loss: 0.677240
Train - Epoch 139, Batch: 0, Loss: 0.677323
Train - Epoch 140, Batch: 0, Loss: 0.677019
Train - Epoch 141, Batch: 0, Loss: 0.676875
Train - Epoch 142, Batch: 0, Loss: 0.677020
Train - Epoch 143, Batch: 0, Loss: 0.677317
Train - Epoch 144, Batch: 0, Loss: 0.676862
Train - Epoch 145, Batch: 0, Loss: 0.676675
Train - Epoch 146, Batch: 0, Loss: 0.676596
Train - Epoch 147, Batch: 0, Loss: 0.676171
Train - Epoch 148, Batch: 0, Loss: 0.676085
Train - Epoch 149, Batch: 0, Loss: 0.677146
Train - Epoch 150, Batch: 0, Loss: 0.676068
Train - Epoch 151, Batch: 0, Loss: 0.676191
Train - Epoch 152, Batch: 0, Loss: 0.676293
Train - Epoch 153, Batch: 0, Loss: 0.676482
Train - Epoch 154, Batch: 0, Loss: 0.676115
Train - Epoch 155, Batch: 0, Loss: 0.675806
Train - Epoch 156, Batch: 0, Loss: 0.675519
Train - Epoch 157, Batch: 0, Loss: 0.675565
Train - Epoch 158, Batch: 0, Loss: 0.675497
Train - Epoch 159, Batch: 0, Loss: 0.675659
Train - Epoch 160, Batch: 0, Loss: 0.674654
Train - Epoch 161, Batch: 0, Loss: 0.675934
Train - Epoch 162, Batch: 0, Loss: 0.674971
Train - Epoch 163, Batch: 0, Loss: 0.675291
Train - Epoch 164, Batch: 0, Loss: 0.675030
Train - Epoch 165, Batch: 0, Loss: 0.674532
Train - Epoch 166, Batch: 0, Loss: 0.674924
Train - Epoch 167, Batch: 0, Loss: 0.674865
Train - Epoch 168, Batch: 0, Loss: 0.673966
Train - Epoch 169, Batch: 0, Loss: 0.675036
Train - Epoch 170, Batch: 0, Loss: 0.675169
Train - Epoch 171, Batch: 0, Loss: 0.674574
Train - Epoch 172, Batch: 0, Loss: 0.674305
Train - Epoch 173, Batch: 0, Loss: 0.674217
Train - Epoch 174, Batch: 0, Loss: 0.675086
Train - Epoch 175, Batch: 0, Loss: 0.673518
Train - Epoch 176, Batch: 0, Loss: 0.673896
Train - Epoch 177, Batch: 0, Loss: 0.674674
Train - Epoch 178, Batch: 0, Loss: 0.674514
Train - Epoch 179, Batch: 0, Loss: 0.673589
Train - Epoch 180, Batch: 0, Loss: 0.673870
Train - Epoch 181, Batch: 0, Loss: 0.673574
Train - Epoch 182, Batch: 0, Loss: 0.673742/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 183, Batch: 0, Loss: 0.673795
Train - Epoch 184, Batch: 0, Loss: 0.673988
Train - Epoch 185, Batch: 0, Loss: 0.674270
Train - Epoch 186, Batch: 0, Loss: 0.674345
Train - Epoch 187, Batch: 0, Loss: 0.672444
Train - Epoch 188, Batch: 0, Loss: 0.673025
Train - Epoch 189, Batch: 0, Loss: 0.673078
Train - Epoch 190, Batch: 0, Loss: 0.673721
Train - Epoch 191, Batch: 0, Loss: 0.673160
Train - Epoch 192, Batch: 0, Loss: 0.673516
Train - Epoch 193, Batch: 0, Loss: 0.672781
Train - Epoch 194, Batch: 0, Loss: 0.672322
Train - Epoch 195, Batch: 0, Loss: 0.673027
Train - Epoch 196, Batch: 0, Loss: 0.672176
Train - Epoch 197, Batch: 0, Loss: 0.672738
Train - Epoch 198, Batch: 0, Loss: 0.671889
Train - Epoch 199, Batch: 0, Loss: 0.673368
training_time:: 352.42863368988037
training time full:: 352.42867064476013
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000067, Accuracy: 0.482759
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 2024
max_epoch:: 200
batch_size:: 10200
epoch:: 0
tensor([  0,   1,   5,   8,  10,  12,  17,  19,  22,  23,  25,  32,  37,  40,
         43,  44,  46,  49,  51,  52,  54,  57,  60,  64,  68,  73,  75,  76,
         77,  80,  81,  84,  86,  89,  92,  93,  95,  96,  98,  99, 102, 105,
        106, 107, 109, 115, 116, 119, 123, 125])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
epoch:: 60
epoch:: 61
epoch:: 62
epoch:: 63
epoch:: 64
epoch:: 65
epoch:: 66
epoch:: 67
epoch:: 68
epoch:: 69
epoch:: 70
epoch:: 71
epoch:: 72
epoch:: 73
epoch:: 74
epoch:: 75
epoch:: 76
epoch:: 77
epoch:: 78
epoch:: 79
epoch:: 80
epoch:: 81
epoch:: 82
epoch:: 83
epoch:: 84
epoch:: 85
epoch:: 86
epoch:: 87
epoch:: 88
epoch:: 89
epoch:: 90
epoch:: 91
epoch:: 92
epoch:: 93
epoch:: 94
epoch:: 95
epoch:: 96
epoch:: 97
epoch:: 98
epoch:: 99
epoch:: 100
epoch:: 101
epoch:: 102
epoch:: 103
epoch:: 104
epoch:: 105
epoch:: 106
epoch:: 107
epoch:: 108
epoch:: 109
epoch:: 110
epoch:: 111
epoch:: 112
epoch:: 113
epoch:: 114
epoch:: 115
epoch:: 116
epoch:: 117
epoch:: 118
epoch:: 119
epoch:: 120
epoch:: 121
epoch:: 122
epoch:: 123
epoch:: 124
epoch:: 125
epoch:: 126
epoch:: 127
epoch:: 128
epoch:: 129
epoch:: 130
epoch:: 131
epoch:: 132
epoch:: 133
epoch:: 134
epoch:: 135
epoch:: 136
epoch:: 137
epoch:: 138
epoch:: 139
epoch:: 140
epoch:: 141
epoch:: 142
epoch:: 143
epoch:: 144
epoch:: 145
epoch:: 146
epoch:: 147
epoch:: 148
epoch:: 149
epoch:: 150
epoch:: 151
epoch:: 152
epoch:: 153
epoch:: 154
epoch:: 155
epoch:: 156
epoch:: 157
epoch:: 158
epoch:: 159
epoch:: 160
epoch:: 161
epoch:: 162
epoch:: 163
epoch:: 164
epoch:: 165
epoch:: 166
epoch:: 167
epoch:: 168
epoch:: 169
epoch:: 170
epoch:: 171
epoch:: 172
epoch:: 173
epoch:: 174
epoch:: 175
epoch:: 176
epoch:: 177
epoch:: 178
epoch:: 179
epoch:: 180
epoch:: 181
epoch:: 182
epoch:: 183
epoch:: 184
epoch:: 185
epoch:: 186
epoch:: 187
epoch:: 188
epoch:: 189
epoch:: 190
epoch:: 191
epoch:: 192
epoch:: 193
epoch:: 194
epoch:: 195
epoch:: 196
epoch:: 197
epoch:: 198
epoch:: 199
training time is 249.39203333854675
overhead:: 0
overhead2:: 0.45075201988220215
overhead3:: 0
time_baseline:: 249.39211559295654
curr_diff: 0 tensor(0.5836, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.5836, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8512, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.619504
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 1 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 2024
max_epoch:: 200
overhead:: 0
overhead2:: 0.022296428680419922
overhead3:: 0.1379561424255371
overhead4:: 29.731672048568726
overhead5:: 0
memory usage:: 26604048384
time_provenance:: 79.21394157409668
curr_diff: 0 tensor(0.0059, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0059, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.5779, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.5779, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8549, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.614613
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 1 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 2024
max_epoch:: 200
overhead:: 0
overhead2:: 0.029417037963867188
overhead3:: 0.14872241020202637
overhead4:: 34.983622789382935
overhead5:: 0
memory usage:: 26608111616
time_provenance:: 85.65493893623352
curr_diff: 0 tensor(0.0059, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0059, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.5779, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.5779, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8549, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.614613
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 1 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 2024
max_epoch:: 200
overhead:: 0
overhead2:: 0.030091285705566406
overhead3:: 0.1561875343322754
overhead4:: 40.088956356048584
overhead5:: 0
memory usage:: 26606530560
time_provenance:: 92.0493004322052
curr_diff: 0 tensor(0.0059, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0059, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.5779, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.5779, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8549, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.614613
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 1 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 2024
max_epoch:: 200
overhead:: 0
overhead2:: 0.039156436920166016
overhead3:: 0.17784571647644043
overhead4:: 52.69248080253601
overhead5:: 0
memory usage:: 26622009344
time_provenance:: 108.31442451477051
curr_diff: 0 tensor(0.0050, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0050, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.5788, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.5788, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8543, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.615255
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 1 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 2024
max_epoch:: 200
overhead:: 0
overhead2:: 0.04779219627380371
overhead3:: 0.1865859031677246
overhead4:: 57.70334219932556
overhead5:: 0
memory usage:: 26616487936
time_provenance:: 114.5880560874939
curr_diff: 0 tensor(0.0049, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0049, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.5788, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.5788, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8543, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.615255
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 1 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 2024
max_epoch:: 200
overhead:: 0
overhead2:: 0.04736924171447754
overhead3:: 0.185805082321167
overhead4:: 62.500749349594116
overhead5:: 0
memory usage:: 26602094592
time_provenance:: 120.38286113739014
curr_diff: 0 tensor(0.0049, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0049, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.5788, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.5788, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8543, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.615255
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 1 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 2024
max_epoch:: 200
overhead:: 0
overhead2:: 0.09482622146606445
overhead3:: 0.2956538200378418
overhead4:: 126.10988163948059
overhead5:: 0
memory usage:: 26620477440
time_provenance:: 199.5019280910492
curr_diff: 0 tensor(0.0032, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0032, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.5804, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.5804, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8533, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.616787
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 1 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 2024
max_epoch:: 200
overhead:: 0
overhead2:: 0.09171438217163086
overhead3:: 0.2963235378265381
overhead4:: 128.98320174217224
overhead5:: 0
memory usage:: 26604670976
time_provenance:: 203.02982378005981
curr_diff: 0 tensor(0.0032, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0032, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.5804, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.5804, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8532, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.616836
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 1 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 2024
max_epoch:: 200
overhead:: 0
overhead2:: 0.09520316123962402
overhead3:: 0.30004215240478516
overhead4:: 131.4870467185974
overhead5:: 0
memory usage:: 26605420544
time_provenance:: 206.27829813957214
curr_diff: 0 tensor(0.0032, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0032, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.5805, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.5805, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8532, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.616836
repetition 2
python3 benchmark_exp_lr.py 0.001 10200 200 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression rcv1 2 0.005 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 0.693050
Train - Epoch 1, Batch: 0, Loss: 0.692939
Train - Epoch 2, Batch: 0, Loss: 0.692773
Train - Epoch 3, Batch: 0, Loss: 0.692614
Train - Epoch 4, Batch: 0, Loss: 0.692478
Train - Epoch 5, Batch: 0, Loss: 0.692338
Train - Epoch 6, Batch: 0, Loss: 0.692159
Train - Epoch 7, Batch: 0, Loss: 0.692055
Train - Epoch 8, Batch: 0, Loss: 0.691902
Train - Epoch 9, Batch: 0, Loss: 0.691720
Train - Epoch 10, Batch: 0, Loss: 0.691593
Train - Epoch 11, Batch: 0, Loss: 0.691431
Train - Epoch 12, Batch: 0, Loss: 0.691393
Train - Epoch 13, Batch: 0, Loss: 0.691158
Train - Epoch 14, Batch: 0, Loss: 0.691081
Train - Epoch 15, Batch: 0, Loss: 0.690853
Train - Epoch 16, Batch: 0, Loss: 0.690810
Train - Epoch 17, Batch: 0, Loss: 0.690732
Train - Epoch 18, Batch: 0, Loss: 0.690538
Train - Epoch 19, Batch: 0, Loss: 0.690487
Train - Epoch 20, Batch: 0, Loss: 0.690297
Train - Epoch 21, Batch: 0, Loss: 0.690130
Train - Epoch 22, Batch: 0, Loss: 0.689944
Train - Epoch 23, Batch: 0, Loss: 0.689824
Train - Epoch 24, Batch: 0, Loss: 0.689532
Train - Epoch 25, Batch: 0, Loss: 0.689500
Train - Epoch 26, Batch: 0, Loss: 0.689558
Train - Epoch 27, Batch: 0, Loss: 0.689284
Train - Epoch 28, Batch: 0, Loss: 0.689190
Train - Epoch 29, Batch: 0, Loss: 0.689010
Train - Epoch 30, Batch: 0, Loss: 0.688869
Train - Epoch 31, Batch: 0, Loss: 0.688516
Train - Epoch 32, Batch: 0, Loss: 0.688585
Train - Epoch 33, Batch: 0, Loss: 0.688526
Train - Epoch 34, Batch: 0, Loss: 0.688371
Train - Epoch 35, Batch: 0, Loss: 0.688198
Train - Epoch 36, Batch: 0, Loss: 0.688141
Train - Epoch 37, Batch: 0, Loss: 0.688024
Train - Epoch 38, Batch: 0, Loss: 0.687985
Train - Epoch 39, Batch: 0, Loss: 0.687729
Train - Epoch 40, Batch: 0, Loss: 0.687473
Train - Epoch 41, Batch: 0, Loss: 0.687384
Train - Epoch 42, Batch: 0, Loss: 0.687138
Train - Epoch 43, Batch: 0, Loss: 0.687322
Train - Epoch 44, Batch: 0, Loss: 0.686875
Train - Epoch 45, Batch: 0, Loss: 0.686966
Train - Epoch 46, Batch: 0, Loss: 0.687192
Train - Epoch 47, Batch: 0, Loss: 0.686624
Train - Epoch 48, Batch: 0, Loss: 0.686720
Train - Epoch 49, Batch: 0, Loss: 0.686261
Train - Epoch 50, Batch: 0, Loss: 0.686146
Train - Epoch 51, Batch: 0, Loss: 0.686232
Train - Epoch 52, Batch: 0, Loss: 0.686225
Train - Epoch 53, Batch: 0, Loss: 0.686136
Train - Epoch 54, Batch: 0, Loss: 0.685676
Train - Epoch 55, Batch: 0, Loss: 0.685769
Train - Epoch 56, Batch: 0, Loss: 0.685805
Train - Epoch 57, Batch: 0, Loss: 0.685674
Train - Epoch 58, Batch: 0, Loss: 0.685304
Train - Epoch 59, Batch: 0, Loss: 0.685388
Train - Epoch 60, Batch: 0, Loss: 0.685249
Train - Epoch 61, Batch: 0, Loss: 0.685121
Train - Epoch 62, Batch: 0, Loss: 0.685051
Train - Epoch 63, Batch: 0, Loss: 0.684758
Train - Epoch 64, Batch: 0, Loss: 0.684551
Train - Epoch 65, Batch: 0, Loss: 0.684535
Train - Epoch 66, Batch: 0, Loss: 0.684811
Train - Epoch 67, Batch: 0, Loss: 0.683934
Train - Epoch 68, Batch: 0, Loss: 0.684237
Train - Epoch 69, Batch: 0, Loss: 0.684067
Train - Epoch 70, Batch: 0, Loss: 0.684359
Train - Epoch 71, Batch: 0, Loss: 0.683995
Train - Epoch 72, Batch: 0, Loss: 0.683845
Train - Epoch 73, Batch: 0, Loss: 0.683530
Train - Epoch 74, Batch: 0, Loss: 0.683242
Train - Epoch 75, Batch: 0, Loss: 0.683474
Train - Epoch 76, Batch: 0, Loss: 0.683433
Train - Epoch 77, Batch: 0, Loss: 0.683484
Train - Epoch 78, Batch: 0, Loss: 0.683276
Train - Epoch 79, Batch: 0, Loss: 0.682352
Train - Epoch 80, Batch: 0, Loss: 0.682879
Train - Epoch 81, Batch: 0, Loss: 0.682746
Train - Epoch 82, Batch: 0, Loss: 0.682726
Train - Epoch 83, Batch: 0, Loss: 0.682737
Train - Epoch 84, Batch: 0, Loss: 0.682886
Train - Epoch 85, Batch: 0, Loss: 0.683003
Train - Epoch 86, Batch: 0, Loss: 0.682412
Train - Epoch 87, Batch: 0, Loss: 0.682168
Train - Epoch 88, Batch: 0, Loss: 0.682343
Train - Epoch 89, Batch: 0, Loss: 0.682131
Train - Epoch 90, Batch: 0, Loss: 0.681967
Train - Epoch 91, Batch: 0, Loss: 0.681783
Train - Epoch 92, Batch: 0, Loss: 0.682378
Train - Epoch 93, Batch: 0, Loss: 0.681648
Train - Epoch 94, Batch: 0, Loss: 0.681546
Train - Epoch 95, Batch: 0, Loss: 0.681620
Train - Epoch 96, Batch: 0, Loss: 0.681324
Train - Epoch 97, Batch: 0, Loss: 0.681278
Train - Epoch 98, Batch: 0, Loss: 0.681109
Train - Epoch 99, Batch: 0, Loss: 0.681003
Train - Epoch 100, Batch: 0, Loss: 0.680779
Train - Epoch 101, Batch: 0, Loss: 0.680912
Train - Epoch 102, Batch: 0, Loss: 0.680311
Train - Epoch 103, Batch: 0, Loss: 0.680624
Train - Epoch 104, Batch: 0, Loss: 0.680674
Train - Epoch 105, Batch: 0, Loss: 0.680431
Train - Epoch 106, Batch: 0, Loss: 0.680331
Train - Epoch 107, Batch: 0, Loss: 0.680538
Train - Epoch 108, Batch: 0, Loss: 0.680012
Train - Epoch 109, Batch: 0, Loss: 0.679833
Train - Epoch 110, Batch: 0, Loss: 0.679525
Train - Epoch 111, Batch: 0, Loss: 0.680348
Train - Epoch 112, Batch: 0, Loss: 0.679789
Train - Epoch 113, Batch: 0, Loss: 0.679510
Train - Epoch 114, Batch: 0, Loss: 0.679367
Train - Epoch 115, Batch: 0, Loss: 0.679160
Train - Epoch 116, Batch: 0, Loss: 0.678598
Train - Epoch 117, Batch: 0, Loss: 0.678950
Train - Epoch 118, Batch: 0, Loss: 0.679101
Train - Epoch 119, Batch: 0, Loss: 0.678901
Train - Epoch 120, Batch: 0, Loss: 0.679252
Train - Epoch 121, Batch: 0, Loss: 0.678865
Train - Epoch 122, Batch: 0, Loss: 0.678803
Train - Epoch 123, Batch: 0, Loss: 0.678946
Train - Epoch 124, Batch: 0, Loss: 0.678300
Train - Epoch 125, Batch: 0, Loss: 0.678135
Train - Epoch 126, Batch: 0, Loss: 0.678565
Train - Epoch 127, Batch: 0, Loss: 0.678421
Train - Epoch 128, Batch: 0, Loss: 0.678144
Train - Epoch 129, Batch: 0, Loss: 0.678249
Train - Epoch 130, Batch: 0, Loss: 0.678036
Train - Epoch 131, Batch: 0, Loss: 0.677057
Train - Epoch 132, Batch: 0, Loss: 0.677940
Train - Epoch 133, Batch: 0, Loss: 0.677894
Train - Epoch 134, Batch: 0, Loss: 0.677309
Train - Epoch 135, Batch: 0, Loss: 0.678066
Train - Epoch 136, Batch: 0, Loss: 0.677195
Train - Epoch 137, Batch: 0, Loss: 0.676933
Train - Epoch 138, Batch: 0, Loss: 0.677748
Train - Epoch 139, Batch: 0, Loss: 0.677123
Train - Epoch 140, Batch: 0, Loss: 0.676836
Train - Epoch 141, Batch: 0, Loss: 0.676760
Train - Epoch 142, Batch: 0, Loss: 0.676392
Train - Epoch 143, Batch: 0, Loss: 0.676795
Train - Epoch 144, Batch: 0, Loss: 0.675894
Train - Epoch 145, Batch: 0, Loss: 0.676602
Train - Epoch 146, Batch: 0, Loss: 0.676822
Train - Epoch 147, Batch: 0, Loss: 0.677088
Train - Epoch 148, Batch: 0, Loss: 0.676028
Train - Epoch 149, Batch: 0, Loss: 0.676125
Train - Epoch 150, Batch: 0, Loss: 0.676050
Train - Epoch 151, Batch: 0, Loss: 0.675845
Train - Epoch 152, Batch: 0, Loss: 0.675889
Train - Epoch 153, Batch: 0, Loss: 0.676423
Train - Epoch 154, Batch: 0, Loss: 0.675346
Train - Epoch 155, Batch: 0, Loss: 0.675918
Train - Epoch 156, Batch: 0, Loss: 0.675667
Train - Epoch 157, Batch: 0, Loss: 0.675782
Train - Epoch 158, Batch: 0, Loss: 0.675606
Train - Epoch 159, Batch: 0, Loss: 0.675825
Train - Epoch 160, Batch: 0, Loss: 0.675763
Train - Epoch 161, Batch: 0, Loss: 0.675993
Train - Epoch 162, Batch: 0, Loss: 0.675796
Train - Epoch 163, Batch: 0, Loss: 0.674874
Train - Epoch 164, Batch: 0, Loss: 0.674921
Train - Epoch 165, Batch: 0, Loss: 0.675239
Train - Epoch 166, Batch: 0, Loss: 0.674192
Train - Epoch 167, Batch: 0, Loss: 0.675068
Train - Epoch 168, Batch: 0, Loss: 0.674651
Train - Epoch 169, Batch: 0, Loss: 0.674835
Train - Epoch 170, Batch: 0, Loss: 0.674113
Train - Epoch 171, Batch: 0, Loss: 0.674914
Train - Epoch 172, Batch: 0, Loss: 0.674790
Train - Epoch 173, Batch: 0, Loss: 0.674751
Train - Epoch 174, Batch: 0, Loss: 0.674577
Train - Epoch 175, Batch: 0, Loss: 0.674404
Train - Epoch 176, Batch: 0, Loss: 0.674054
Train - Epoch 177, Batch: 0, Loss: 0.673710
Train - Epoch 178, Batch: 0, Loss: 0.673529
Train - Epoch 179, Batch: 0, Loss: 0.674351
Train - Epoch 180, Batch: 0, Loss: 0.674511
Train - Epoch 181, Batch: 0, Loss: 0.673629
Train - Epoch 182, Batch: 0, Loss: 0.674387
Train - Epoch 183, Batch: 0, Loss: 0.673654
Train - Epoch 184, Batch: 0, Loss: 0.674264
Train - Epoch 185, Batch: 0, Loss: 0.673642
Train - Epoch 186, Batch: 0, Loss: 0.673950/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 187, Batch: 0, Loss: 0.673893
Train - Epoch 188, Batch: 0, Loss: 0.673128
Train - Epoch 189, Batch: 0, Loss: 0.673539
Train - Epoch 190, Batch: 0, Loss: 0.673034
Train - Epoch 191, Batch: 0, Loss: 0.673940
Train - Epoch 192, Batch: 0, Loss: 0.673570
Train - Epoch 193, Batch: 0, Loss: 0.672423
Train - Epoch 194, Batch: 0, Loss: 0.672599
Train - Epoch 195, Batch: 0, Loss: 0.673388
Train - Epoch 196, Batch: 0, Loss: 0.673231
Train - Epoch 197, Batch: 0, Loss: 0.673654
Train - Epoch 198, Batch: 0, Loss: 0.672908
Train - Epoch 199, Batch: 0, Loss: 0.672746
training_time:: 352.3908977508545
training time full:: 352.39096212387085
provenance prepare time:: 4.76837158203125e-07
here
start test::
Test Avg. Loss: 0.000067, Accuracy: 0.482660
adding noise deletion rate:: 0.01
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
tensor([0.5324, 0.5414, 0.5131,  ..., 0.5350, 0.5205, 0.5204],
       dtype=torch.float64, grad_fn=<MaxBackward0>)
tensor(0.8685, dtype=torch.float64) tensor(0., dtype=torch.float64)
Train - Epoch 0, Batch: 0, Loss: 0.693042
Train - Epoch 1, Batch: 0, Loss: 0.692948
Train - Epoch 2, Batch: 0, Loss: 0.692869
Train - Epoch 3, Batch: 0, Loss: 0.692775
Train - Epoch 4, Batch: 0, Loss: 0.692660
Train - Epoch 5, Batch: 0, Loss: 0.692540
Train - Epoch 6, Batch: 0, Loss: 0.692475
Train - Epoch 7, Batch: 0, Loss: 0.692411
Train - Epoch 8, Batch: 0, Loss: 0.692272
Train - Epoch 9, Batch: 0, Loss: 0.692166
Train - Epoch 10, Batch: 0, Loss: 0.692041
Train - Epoch 11, Batch: 0, Loss: 0.691948
Train - Epoch 12, Batch: 0, Loss: 0.691951
Train - Epoch 13, Batch: 0, Loss: 0.691825
Train - Epoch 14, Batch: 0, Loss: 0.691738
Train - Epoch 15, Batch: 0, Loss: 0.691601
Train - Epoch 16, Batch: 0, Loss: 0.691562
Train - Epoch 17, Batch: 0, Loss: 0.691495
Train - Epoch 18, Batch: 0, Loss: 0.691376
Train - Epoch 19, Batch: 0, Loss: 0.691356
Train - Epoch 20, Batch: 0, Loss: 0.691245
Train - Epoch 21, Batch: 0, Loss: 0.691075
Train - Epoch 22, Batch: 0, Loss: 0.690993
Train - Epoch 23, Batch: 0, Loss: 0.690904
Train - Epoch 24, Batch: 0, Loss: 0.690769
Train - Epoch 25, Batch: 0, Loss: 0.690654
Train - Epoch 26, Batch: 0, Loss: 0.690731
Train - Epoch 27, Batch: 0, Loss: 0.690552
Train - Epoch 28, Batch: 0, Loss: 0.690502
Train - Epoch 29, Batch: 0, Loss: 0.690423
Train - Epoch 30, Batch: 0, Loss: 0.690282
Train - Epoch 31, Batch: 0, Loss: 0.690003
Train - Epoch 32, Batch: 0, Loss: 0.690053
Train - Epoch 33, Batch: 0, Loss: 0.689998
Train - Epoch 34, Batch: 0, Loss: 0.689945
Train - Epoch 35, Batch: 0, Loss: 0.689853
Train - Epoch 36, Batch: 0, Loss: 0.689666
Train - Epoch 37, Batch: 0, Loss: 0.689799
Train - Epoch 38, Batch: 0, Loss: 0.689768
Train - Epoch 39, Batch: 0, Loss: 0.689597
Train - Epoch 40, Batch: 0, Loss: 0.689292
Train - Epoch 41, Batch: 0, Loss: 0.689177
Train - Epoch 42, Batch: 0, Loss: 0.689125
Train - Epoch 43, Batch: 0, Loss: 0.689255
Train - Epoch 44, Batch: 0, Loss: 0.688992
Train - Epoch 45, Batch: 0, Loss: 0.689015
Train - Epoch 46, Batch: 0, Loss: 0.689175
Train - Epoch 47, Batch: 0, Loss: 0.688829
Train - Epoch 48, Batch: 0, Loss: 0.688820
Train - Epoch 49, Batch: 0, Loss: 0.688580
Train - Epoch 50, Batch: 0, Loss: 0.688375
Train - Epoch 51, Batch: 0, Loss: 0.688500
Train - Epoch 52, Batch: 0, Loss: 0.688476
Train - Epoch 53, Batch: 0, Loss: 0.688451
Train - Epoch 54, Batch: 0, Loss: 0.688133
Train - Epoch 55, Batch: 0, Loss: 0.688334
Train - Epoch 56, Batch: 0, Loss: 0.688282
Train - Epoch 57, Batch: 0, Loss: 0.688078
Train - Epoch 58, Batch: 0, Loss: 0.687893
Train - Epoch 59, Batch: 0, Loss: 0.687811
Train - Epoch 60, Batch: 0, Loss: 0.687776
Train - Epoch 61, Batch: 0, Loss: 0.687842
Train - Epoch 62, Batch: 0, Loss: 0.687749
Train - Epoch 63, Batch: 0, Loss: 0.687581
Train - Epoch 64, Batch: 0, Loss: 0.687324
Train - Epoch 65, Batch: 0, Loss: 0.687485
Train - Epoch 66, Batch: 0, Loss: 0.687549
Train - Epoch 67, Batch: 0, Loss: 0.686927
Train - Epoch 68, Batch: 0, Loss: 0.687330
Train - Epoch 69, Batch: 0, Loss: 0.687002
Train - Epoch 70, Batch: 0, Loss: 0.687089
Train - Epoch 71, Batch: 0, Loss: 0.686864
Train - Epoch 72, Batch: 0, Loss: 0.686873
Train - Epoch 73, Batch: 0, Loss: 0.686726
Train - Epoch 74, Batch: 0, Loss: 0.686572
Train - Epoch 75, Batch: 0, Loss: 0.686520
Train - Epoch 76, Batch: 0, Loss: 0.686605
Train - Epoch 77, Batch: 0, Loss: 0.686643
Train - Epoch 78, Batch: 0, Loss: 0.686384
Train - Epoch 79, Batch: 0, Loss: 0.685794
Train - Epoch 80, Batch: 0, Loss: 0.686177
Train - Epoch 81, Batch: 0, Loss: 0.686399
Train - Epoch 82, Batch: 0, Loss: 0.686291
Train - Epoch 83, Batch: 0, Loss: 0.686183
Train - Epoch 84, Batch: 0, Loss: 0.686332
Train - Epoch 85, Batch: 0, Loss: 0.686472
Train - Epoch 86, Batch: 0, Loss: 0.686140
Train - Epoch 87, Batch: 0, Loss: 0.685689
Train - Epoch 88, Batch: 0, Loss: 0.685682
Train - Epoch 89, Batch: 0, Loss: 0.685579
Train - Epoch 90, Batch: 0, Loss: 0.685729
Train - Epoch 91, Batch: 0, Loss: 0.685635
Train - Epoch 92, Batch: 0, Loss: 0.685954
Train - Epoch 93, Batch: 0, Loss: 0.685525
Train - Epoch 94, Batch: 0, Loss: 0.685450
Train - Epoch 95, Batch: 0, Loss: 0.685471
Train - Epoch 96, Batch: 0, Loss: 0.685248
Train - Epoch 97, Batch: 0, Loss: 0.685113
Train - Epoch 98, Batch: 0, Loss: 0.685269
Train - Epoch 99, Batch: 0, Loss: 0.684948
Train - Epoch 100, Batch: 0, Loss: 0.685036
Train - Epoch 101, Batch: 0, Loss: 0.684900
Train - Epoch 102, Batch: 0, Loss: 0.684694
Train - Epoch 103, Batch: 0, Loss: 0.684797
Train - Epoch 104, Batch: 0, Loss: 0.684926
Train - Epoch 105, Batch: 0, Loss: 0.684648
Train - Epoch 106, Batch: 0, Loss: 0.684458
Train - Epoch 107, Batch: 0, Loss: 0.684794
Train - Epoch 108, Batch: 0, Loss: 0.684305
Train - Epoch 109, Batch: 0, Loss: 0.684006
Train - Epoch 110, Batch: 0, Loss: 0.683988
Train - Epoch 111, Batch: 0, Loss: 0.684518
Train - Epoch 112, Batch: 0, Loss: 0.684321
Train - Epoch 113, Batch: 0, Loss: 0.683805
Train - Epoch 114, Batch: 0, Loss: 0.684041
Train - Epoch 115, Batch: 0, Loss: 0.683897
Train - Epoch 116, Batch: 0, Loss: 0.683231
Train - Epoch 117, Batch: 0, Loss: 0.683769
Train - Epoch 118, Batch: 0, Loss: 0.683886
Train - Epoch 119, Batch: 0, Loss: 0.683535
Train - Epoch 120, Batch: 0, Loss: 0.683842
Train - Epoch 121, Batch: 0, Loss: 0.683714
Train - Epoch 122, Batch: 0, Loss: 0.683681
Train - Epoch 123, Batch: 0, Loss: 0.683387
Train - Epoch 124, Batch: 0, Loss: 0.683397
Train - Epoch 125, Batch: 0, Loss: 0.683295
Train - Epoch 126, Batch: 0, Loss: 0.683142
Train - Epoch 127, Batch: 0, Loss: 0.683145
Train - Epoch 128, Batch: 0, Loss: 0.683056
Train - Epoch 129, Batch: 0, Loss: 0.683071
Train - Epoch 130, Batch: 0, Loss: 0.682977
Train - Epoch 131, Batch: 0, Loss: 0.682366
Train - Epoch 132, Batch: 0, Loss: 0.682712
Train - Epoch 133, Batch: 0, Loss: 0.683110
Train - Epoch 134, Batch: 0, Loss: 0.682641
Train - Epoch 135, Batch: 0, Loss: 0.683001
Train - Epoch 136, Batch: 0, Loss: 0.682533
Train - Epoch 137, Batch: 0, Loss: 0.682154
Train - Epoch 138, Batch: 0, Loss: 0.682926
Train - Epoch 139, Batch: 0, Loss: 0.682168
Train - Epoch 140, Batch: 0, Loss: 0.682216
Train - Epoch 141, Batch: 0, Loss: 0.681847
Train - Epoch 142, Batch: 0, Loss: 0.681769
Train - Epoch 143, Batch: 0, Loss: 0.682005
Train - Epoch 144, Batch: 0, Loss: 0.681437
Train - Epoch 145, Batch: 0, Loss: 0.682005
Train - Epoch 146, Batch: 0, Loss: 0.682093
Train - Epoch 147, Batch: 0, Loss: 0.682069
Train - Epoch 148, Batch: 0, Loss: 0.681797
Train - Epoch 149, Batch: 0, Loss: 0.681764
Train - Epoch 150, Batch: 0, Loss: 0.681494
Train - Epoch 151, Batch: 0, Loss: 0.681250
Train - Epoch 152, Batch: 0, Loss: 0.681267
Train - Epoch 153, Batch: 0, Loss: 0.681931
Train - Epoch 154, Batch: 0, Loss: 0.681278
Train - Epoch 155, Batch: 0, Loss: 0.681463
Train - Epoch 156, Batch: 0, Loss: 0.681263
Train - Epoch 157, Batch: 0, Loss: 0.681495
Train - Epoch 158, Batch: 0, Loss: 0.681639
Train - Epoch 159, Batch: 0, Loss: 0.681656
Train - Epoch 160, Batch: 0, Loss: 0.681282
Train - Epoch 161, Batch: 0, Loss: 0.681574
Train - Epoch 162, Batch: 0, Loss: 0.681640
Train - Epoch 163, Batch: 0, Loss: 0.680903
Train - Epoch 164, Batch: 0, Loss: 0.681094
Train - Epoch 165, Batch: 0, Loss: 0.681193
Train - Epoch 166, Batch: 0, Loss: 0.680483
Train - Epoch 167, Batch: 0, Loss: 0.680653
Train - Epoch 168, Batch: 0, Loss: 0.680503
Train - Epoch 169, Batch: 0, Loss: 0.680836
Train - Epoch 170, Batch: 0, Loss: 0.679976
Train - Epoch 171, Batch: 0, Loss: 0.681062
Train - Epoch 172, Batch: 0, Loss: 0.680293
Train - Epoch 173, Batch: 0, Loss: 0.680993
Train - Epoch 174, Batch: 0, Loss: 0.680359
Train - Epoch 175, Batch: 0, Loss: 0.679928
Train - Epoch 176, Batch: 0, Loss: 0.680087
Train - Epoch 177, Batch: 0, Loss: 0.680337
Train - Epoch 178, Batch: 0, Loss: 0.679932
Train - Epoch 179, Batch: 0, Loss: 0.680449
Train - Epoch 180, Batch: 0, Loss: 0.680199
Train - Epoch 181, Batch: 0, Loss: 0.679756
Train - Epoch 182, Batch: 0, Loss: 0.680429/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 183, Batch: 0, Loss: 0.679767
Train - Epoch 184, Batch: 0, Loss: 0.680323
Train - Epoch 185, Batch: 0, Loss: 0.680111
Train - Epoch 186, Batch: 0, Loss: 0.679716
Train - Epoch 187, Batch: 0, Loss: 0.680018
Train - Epoch 188, Batch: 0, Loss: 0.679476
Train - Epoch 189, Batch: 0, Loss: 0.679999
Train - Epoch 190, Batch: 0, Loss: 0.679956
Train - Epoch 191, Batch: 0, Loss: 0.679995
Train - Epoch 192, Batch: 0, Loss: 0.679792
Train - Epoch 193, Batch: 0, Loss: 0.679774
Train - Epoch 194, Batch: 0, Loss: 0.678893
Train - Epoch 195, Batch: 0, Loss: 0.679689
Train - Epoch 196, Batch: 0, Loss: 0.679913
Train - Epoch 197, Batch: 0, Loss: 0.680303
Train - Epoch 198, Batch: 0, Loss: 0.679510
Train - Epoch 199, Batch: 0, Loss: 0.679694
training_time:: 350.154976606369
training time full:: 350.1550166606903
provenance prepare time:: 4.76837158203125e-07
here
start test::
Test Avg. Loss: 0.000067, Accuracy: 0.486612
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 202
max_epoch:: 200
batch_size:: 10200
epoch:: 0
tensor([  0,   3,   5,   6,  18,  24,  25,  26,  27,  29,  31,  35,  38,  39,
         41,  42,  44,  46,  52,  56,  57,  58,  59,  63,  64,  66,  67,  69,
         71,  75,  79,  80,  81,  84,  85,  87,  88,  90,  91,  94,  95,  97,
         98,  99, 104, 109, 110, 111, 115, 117])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
epoch:: 60
epoch:: 61
epoch:: 62
epoch:: 63
epoch:: 64
epoch:: 65
epoch:: 66
epoch:: 67
epoch:: 68
epoch:: 69
epoch:: 70
epoch:: 71
epoch:: 72
epoch:: 73
epoch:: 74
epoch:: 75
epoch:: 76
epoch:: 77
epoch:: 78
epoch:: 79
epoch:: 80
epoch:: 81
epoch:: 82
epoch:: 83
epoch:: 84
epoch:: 85
epoch:: 86
epoch:: 87
epoch:: 88
epoch:: 89
epoch:: 90
epoch:: 91
epoch:: 92
epoch:: 93
epoch:: 94
epoch:: 95
epoch:: 96
epoch:: 97
epoch:: 98
epoch:: 99
epoch:: 100
epoch:: 101
epoch:: 102
epoch:: 103
epoch:: 104
epoch:: 105
epoch:: 106
epoch:: 107
epoch:: 108
epoch:: 109
epoch:: 110
epoch:: 111
epoch:: 112
epoch:: 113
epoch:: 114
epoch:: 115
epoch:: 116
epoch:: 117
epoch:: 118
epoch:: 119
epoch:: 120
epoch:: 121
epoch:: 122
epoch:: 123
epoch:: 124
epoch:: 125
epoch:: 126
epoch:: 127
epoch:: 128
epoch:: 129
epoch:: 130
epoch:: 131
epoch:: 132
epoch:: 133
epoch:: 134
epoch:: 135
epoch:: 136
epoch:: 137
epoch:: 138
epoch:: 139
epoch:: 140
epoch:: 141
epoch:: 142
epoch:: 143
epoch:: 144
epoch:: 145
epoch:: 146
epoch:: 147
epoch:: 148
epoch:: 149
epoch:: 150
epoch:: 151
epoch:: 152
epoch:: 153
epoch:: 154
epoch:: 155
epoch:: 156
epoch:: 157
epoch:: 158
epoch:: 159
epoch:: 160
epoch:: 161
epoch:: 162
epoch:: 163
epoch:: 164
epoch:: 165
epoch:: 166
epoch:: 167
epoch:: 168
epoch:: 169
epoch:: 170
epoch:: 171
epoch:: 172
epoch:: 173
epoch:: 174
epoch:: 175
epoch:: 176
epoch:: 177
epoch:: 178
epoch:: 179
epoch:: 180
epoch:: 181
epoch:: 182
epoch:: 183
epoch:: 184
epoch:: 185
epoch:: 186
epoch:: 187
epoch:: 188
epoch:: 189
epoch:: 190
epoch:: 191
epoch:: 192
epoch:: 193
epoch:: 194
epoch:: 195
epoch:: 196
epoch:: 197
epoch:: 198
epoch:: 199
training time is 272.87366819381714
overhead:: 0
overhead2:: 0.4169442653656006
overhead3:: 0
time_baseline:: 272.8737061023712
curr_diff: 0 tensor(0.1176, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1176, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9954, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000067, Accuracy: 0.484043
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 202
max_epoch:: 200
overhead:: 0
overhead2:: 0.022492170333862305
overhead3:: 0.12464523315429688
overhead4:: 32.369935274124146
overhead5:: 0
memory usage:: 26629976064
time_provenance:: 47.64384388923645
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1175, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1175, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9954, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000067, Accuracy: 0.484043
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 202
max_epoch:: 200
overhead:: 0
overhead2:: 0.026647329330444336
overhead3:: 0.1300678253173828
overhead4:: 38.10884189605713
overhead5:: 0
memory usage:: 26634645504
time_provenance:: 55.15496873855591
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1175, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1175, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9954, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000067, Accuracy: 0.484043
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 202
max_epoch:: 200
overhead:: 0
overhead2:: 0.02845478057861328
overhead3:: 0.13672995567321777
overhead4:: 44.13364839553833
overhead5:: 0
memory usage:: 26641870848
time_provenance:: 62.84822750091553
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1175, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1175, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9954, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000067, Accuracy: 0.484043
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 202
max_epoch:: 200
overhead:: 0
overhead2:: 0.04066038131713867
overhead3:: 0.15819239616394043
overhead4:: 57.89171051979065
overhead5:: 0
memory usage:: 26651496448
time_provenance:: 80.80105137825012
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1175, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1175, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9954, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000067, Accuracy: 0.484043
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 202
max_epoch:: 200
overhead:: 0
overhead2:: 0.0409395694732666
overhead3:: 0.16756415367126465
overhead4:: 63.29879593849182
overhead5:: 0
memory usage:: 26636017664
time_provenance:: 87.69851875305176
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1175, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1175, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9954, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000067, Accuracy: 0.484043
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 202
max_epoch:: 200
overhead:: 0
overhead2:: 0.04481148719787598
overhead3:: 0.16886591911315918
overhead4:: 68.22616839408875
overhead5:: 0
memory usage:: 26693607424
time_provenance:: 94.22059297561646
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1175, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1175, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9954, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000067, Accuracy: 0.484043
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 202
max_epoch:: 200
overhead:: 0
overhead2:: 0.09789705276489258
overhead3:: 0.2832646369934082
overhead4:: 139.13658213615417
overhead5:: 0
memory usage:: 26648129536
time_provenance:: 185.1343538761139
curr_diff: 0 tensor(8.6952e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.6952e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1175, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1175, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9954, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000067, Accuracy: 0.484043
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 202
max_epoch:: 200
overhead:: 0
overhead2:: 0.09454679489135742
overhead3:: 0.2762024402618408
overhead4:: 140.0866289138794
overhead5:: 0
memory usage:: 26652131328
time_provenance:: 187.27882266044617
curr_diff: 0 tensor(8.6829e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.6829e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1175, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1175, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9954, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000067, Accuracy: 0.484043
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 202
max_epoch:: 200
overhead:: 0
overhead2:: 0.09249329566955566
overhead3:: 0.27696776390075684
overhead4:: 144.46318697929382
overhead5:: 0
memory usage:: 26632245248
time_provenance:: 192.40951418876648
curr_diff: 0 tensor(8.6722e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.6722e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1175, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1175, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9954, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000067, Accuracy: 0.484043
adding noise deletion rate:: 0.02
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
tensor([0.5258, 0.5331, 0.5104,  ..., 0.5301, 0.5168, 0.5172],
       dtype=torch.float64, grad_fn=<MaxBackward0>)
tensor(0.8685, dtype=torch.float64) tensor(0., dtype=torch.float64)
Train - Epoch 0, Batch: 0, Loss: 0.693187
Train - Epoch 1, Batch: 0, Loss: 0.693165
Train - Epoch 2, Batch: 0, Loss: 0.693114
Train - Epoch 3, Batch: 0, Loss: 0.693087
Train - Epoch 4, Batch: 0, Loss: 0.693033
Train - Epoch 5, Batch: 0, Loss: 0.692990
Train - Epoch 6, Batch: 0, Loss: 0.692930
Train - Epoch 7, Batch: 0, Loss: 0.692926
Train - Epoch 8, Batch: 0, Loss: 0.692858
Train - Epoch 9, Batch: 0, Loss: 0.692814
Train - Epoch 10, Batch: 0, Loss: 0.692753
Train - Epoch 11, Batch: 0, Loss: 0.692695
Train - Epoch 12, Batch: 0, Loss: 0.692704
Train - Epoch 13, Batch: 0, Loss: 0.692664
Train - Epoch 14, Batch: 0, Loss: 0.692580
Train - Epoch 15, Batch: 0, Loss: 0.692556
Train - Epoch 16, Batch: 0, Loss: 0.692558
Train - Epoch 17, Batch: 0, Loss: 0.692474
Train - Epoch 18, Batch: 0, Loss: 0.692419
Train - Epoch 19, Batch: 0, Loss: 0.692416
Train - Epoch 20, Batch: 0, Loss: 0.692390
Train - Epoch 21, Batch: 0, Loss: 0.692303
Train - Epoch 22, Batch: 0, Loss: 0.692297
Train - Epoch 23, Batch: 0, Loss: 0.692245
Train - Epoch 24, Batch: 0, Loss: 0.692140
Train - Epoch 25, Batch: 0, Loss: 0.692075
Train - Epoch 26, Batch: 0, Loss: 0.692163
Train - Epoch 27, Batch: 0, Loss: 0.692040
Train - Epoch 28, Batch: 0, Loss: 0.692060
Train - Epoch 29, Batch: 0, Loss: 0.691982
Train - Epoch 30, Batch: 0, Loss: 0.691909
Train - Epoch 31, Batch: 0, Loss: 0.691785
Train - Epoch 32, Batch: 0, Loss: 0.691842
Train - Epoch 33, Batch: 0, Loss: 0.691805
Train - Epoch 34, Batch: 0, Loss: 0.691854
Train - Epoch 35, Batch: 0, Loss: 0.691712
Train - Epoch 36, Batch: 0, Loss: 0.691590
Train - Epoch 37, Batch: 0, Loss: 0.691712
Train - Epoch 38, Batch: 0, Loss: 0.691696
Train - Epoch 39, Batch: 0, Loss: 0.691671
Train - Epoch 40, Batch: 0, Loss: 0.691467
Train - Epoch 41, Batch: 0, Loss: 0.691443
Train - Epoch 42, Batch: 0, Loss: 0.691403
Train - Epoch 43, Batch: 0, Loss: 0.691506
Train - Epoch 44, Batch: 0, Loss: 0.691364
Train - Epoch 45, Batch: 0, Loss: 0.691278
Train - Epoch 46, Batch: 0, Loss: 0.691388
Train - Epoch 47, Batch: 0, Loss: 0.691247
Train - Epoch 48, Batch: 0, Loss: 0.691216
Train - Epoch 49, Batch: 0, Loss: 0.691196
Train - Epoch 50, Batch: 0, Loss: 0.691105
Train - Epoch 51, Batch: 0, Loss: 0.691135
Train - Epoch 52, Batch: 0, Loss: 0.691132
Train - Epoch 53, Batch: 0, Loss: 0.691084
Train - Epoch 54, Batch: 0, Loss: 0.690913
Train - Epoch 55, Batch: 0, Loss: 0.691076
Train - Epoch 56, Batch: 0, Loss: 0.691018
Train - Epoch 57, Batch: 0, Loss: 0.690913
Train - Epoch 58, Batch: 0, Loss: 0.690816
Train - Epoch 59, Batch: 0, Loss: 0.690875
Train - Epoch 60, Batch: 0, Loss: 0.690786
Train - Epoch 61, Batch: 0, Loss: 0.690835
Train - Epoch 62, Batch: 0, Loss: 0.690836
Train - Epoch 63, Batch: 0, Loss: 0.690687
Train - Epoch 64, Batch: 0, Loss: 0.690462
Train - Epoch 65, Batch: 0, Loss: 0.690581
Train - Epoch 66, Batch: 0, Loss: 0.690730
Train - Epoch 67, Batch: 0, Loss: 0.690426
Train - Epoch 68, Batch: 0, Loss: 0.690596
Train - Epoch 69, Batch: 0, Loss: 0.690369
Train - Epoch 70, Batch: 0, Loss: 0.690394
Train - Epoch 71, Batch: 0, Loss: 0.690342
Train - Epoch 72, Batch: 0, Loss: 0.690248
Train - Epoch 73, Batch: 0, Loss: 0.690328
Train - Epoch 74, Batch: 0, Loss: 0.690233
Train - Epoch 75, Batch: 0, Loss: 0.690164
Train - Epoch 76, Batch: 0, Loss: 0.690282
Train - Epoch 77, Batch: 0, Loss: 0.690140
Train - Epoch 78, Batch: 0, Loss: 0.690178
Train - Epoch 79, Batch: 0, Loss: 0.689861
Train - Epoch 80, Batch: 0, Loss: 0.689993
Train - Epoch 81, Batch: 0, Loss: 0.690057
Train - Epoch 82, Batch: 0, Loss: 0.690026
Train - Epoch 83, Batch: 0, Loss: 0.690033
Train - Epoch 84, Batch: 0, Loss: 0.690202
Train - Epoch 85, Batch: 0, Loss: 0.690160
Train - Epoch 86, Batch: 0, Loss: 0.689968
Train - Epoch 87, Batch: 0, Loss: 0.689825
Train - Epoch 88, Batch: 0, Loss: 0.689788
Train - Epoch 89, Batch: 0, Loss: 0.689604
Train - Epoch 90, Batch: 0, Loss: 0.689738
Train - Epoch 91, Batch: 0, Loss: 0.689885
Train - Epoch 92, Batch: 0, Loss: 0.689848
Train - Epoch 93, Batch: 0, Loss: 0.689732
Train - Epoch 94, Batch: 0, Loss: 0.689716
Train - Epoch 95, Batch: 0, Loss: 0.689707
Train - Epoch 96, Batch: 0, Loss: 0.689472
Train - Epoch 97, Batch: 0, Loss: 0.689508
Train - Epoch 98, Batch: 0, Loss: 0.689680
Train - Epoch 99, Batch: 0, Loss: 0.689481
Train - Epoch 100, Batch: 0, Loss: 0.689436
Train - Epoch 101, Batch: 0, Loss: 0.689499
Train - Epoch 102, Batch: 0, Loss: 0.689349
Train - Epoch 103, Batch: 0, Loss: 0.689406
Train - Epoch 104, Batch: 0, Loss: 0.689411
Train - Epoch 105, Batch: 0, Loss: 0.689339
Train - Epoch 106, Batch: 0, Loss: 0.689237
Train - Epoch 107, Batch: 0, Loss: 0.689418
Train - Epoch 108, Batch: 0, Loss: 0.688949
Train - Epoch 109, Batch: 0, Loss: 0.689012
Train - Epoch 110, Batch: 0, Loss: 0.689004
Train - Epoch 111, Batch: 0, Loss: 0.689268
Train - Epoch 112, Batch: 0, Loss: 0.689137
Train - Epoch 113, Batch: 0, Loss: 0.688778
Train - Epoch 114, Batch: 0, Loss: 0.689088
Train - Epoch 115, Batch: 0, Loss: 0.689070
Train - Epoch 116, Batch: 0, Loss: 0.688533
Train - Epoch 117, Batch: 0, Loss: 0.688826
Train - Epoch 118, Batch: 0, Loss: 0.688849
Train - Epoch 119, Batch: 0, Loss: 0.688630
Train - Epoch 120, Batch: 0, Loss: 0.688924
Train - Epoch 121, Batch: 0, Loss: 0.688850
Train - Epoch 122, Batch: 0, Loss: 0.688701
Train - Epoch 123, Batch: 0, Loss: 0.688622
Train - Epoch 124, Batch: 0, Loss: 0.688683
Train - Epoch 125, Batch: 0, Loss: 0.688542
Train - Epoch 126, Batch: 0, Loss: 0.688673
Train - Epoch 127, Batch: 0, Loss: 0.688551
Train - Epoch 128, Batch: 0, Loss: 0.688319
Train - Epoch 129, Batch: 0, Loss: 0.688536
Train - Epoch 130, Batch: 0, Loss: 0.688376
Train - Epoch 131, Batch: 0, Loss: 0.688209
Train - Epoch 132, Batch: 0, Loss: 0.688390
Train - Epoch 133, Batch: 0, Loss: 0.688574
Train - Epoch 134, Batch: 0, Loss: 0.688216
Train - Epoch 135, Batch: 0, Loss: 0.688458
Train - Epoch 136, Batch: 0, Loss: 0.688262
Train - Epoch 137, Batch: 0, Loss: 0.687990
Train - Epoch 138, Batch: 0, Loss: 0.688505
Train - Epoch 139, Batch: 0, Loss: 0.688107
Train - Epoch 140, Batch: 0, Loss: 0.688112
Train - Epoch 141, Batch: 0, Loss: 0.687955
Train - Epoch 142, Batch: 0, Loss: 0.687844
Train - Epoch 143, Batch: 0, Loss: 0.688089
Train - Epoch 144, Batch: 0, Loss: 0.687828
Train - Epoch 145, Batch: 0, Loss: 0.687948
Train - Epoch 146, Batch: 0, Loss: 0.688132
Train - Epoch 147, Batch: 0, Loss: 0.687789
Train - Epoch 148, Batch: 0, Loss: 0.687694
Train - Epoch 149, Batch: 0, Loss: 0.687975
Train - Epoch 150, Batch: 0, Loss: 0.687759
Train - Epoch 151, Batch: 0, Loss: 0.687533
Train - Epoch 152, Batch: 0, Loss: 0.687536
Train - Epoch 153, Batch: 0, Loss: 0.687838
Train - Epoch 154, Batch: 0, Loss: 0.687738
Train - Epoch 155, Batch: 0, Loss: 0.687934
Train - Epoch 156, Batch: 0, Loss: 0.687669
Train - Epoch 157, Batch: 0, Loss: 0.687826
Train - Epoch 158, Batch: 0, Loss: 0.687814
Train - Epoch 159, Batch: 0, Loss: 0.687787
Train - Epoch 160, Batch: 0, Loss: 0.687481
Train - Epoch 161, Batch: 0, Loss: 0.687770
Train - Epoch 162, Batch: 0, Loss: 0.687777
Train - Epoch 163, Batch: 0, Loss: 0.687358
Train - Epoch 164, Batch: 0, Loss: 0.687699
Train - Epoch 165, Batch: 0, Loss: 0.687527
Train - Epoch 166, Batch: 0, Loss: 0.687238
Train - Epoch 167, Batch: 0, Loss: 0.687214
Train - Epoch 168, Batch: 0, Loss: 0.687210
Train - Epoch 169, Batch: 0, Loss: 0.687429
Train - Epoch 170, Batch: 0, Loss: 0.687077
Train - Epoch 171, Batch: 0, Loss: 0.687499
Train - Epoch 172, Batch: 0, Loss: 0.686936
Train - Epoch 173, Batch: 0, Loss: 0.687153
Train - Epoch 174, Batch: 0, Loss: 0.687074
Train - Epoch 175, Batch: 0, Loss: 0.687126
Train - Epoch 176, Batch: 0, Loss: 0.686768
Train - Epoch 177, Batch: 0, Loss: 0.687196
Train - Epoch 178, Batch: 0, Loss: 0.686736
Train - Epoch 179, Batch: 0, Loss: 0.687306
Train - Epoch 180, Batch: 0, Loss: 0.686930
Train - Epoch 181, Batch: 0, Loss: 0.686946
Train - Epoch 182, Batch: 0, Loss: 0.687238/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 183, Batch: 0, Loss: 0.686855
Train - Epoch 184, Batch: 0, Loss: 0.687085
Train - Epoch 185, Batch: 0, Loss: 0.686917
Train - Epoch 186, Batch: 0, Loss: 0.686704
Train - Epoch 187, Batch: 0, Loss: 0.686825
Train - Epoch 188, Batch: 0, Loss: 0.686557
Train - Epoch 189, Batch: 0, Loss: 0.686961
Train - Epoch 190, Batch: 0, Loss: 0.687028
Train - Epoch 191, Batch: 0, Loss: 0.686917
Train - Epoch 192, Batch: 0, Loss: 0.686771
Train - Epoch 193, Batch: 0, Loss: 0.686826
Train - Epoch 194, Batch: 0, Loss: 0.686299
Train - Epoch 195, Batch: 0, Loss: 0.686694
Train - Epoch 196, Batch: 0, Loss: 0.686975
Train - Epoch 197, Batch: 0, Loss: 0.686911
Train - Epoch 198, Batch: 0, Loss: 0.686882
Train - Epoch 199, Batch: 0, Loss: 0.686654
training_time:: 350.64839911460876
training time full:: 350.64844012260437
provenance prepare time:: 0.0
here
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.519415
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 404
max_epoch:: 200
batch_size:: 10200
epoch:: 0
tensor([  0,   5,   6,  18,  24,  25,  26,  27,  29,  31,  38,  39,  41,  42,
         44,  46,  52,  56,  57,  58,  59,  63,  64,  66,  69,  71,  72,  75,
         78,  79,  80,  81,  84,  85,  87,  88,  90,  91,  94,  95,  97,  98,
         99, 104, 109, 110, 115, 117, 118, 125])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
epoch:: 60
epoch:: 61
epoch:: 62
epoch:: 63
epoch:: 64
epoch:: 65
epoch:: 66
epoch:: 67
epoch:: 68
epoch:: 69
epoch:: 70
epoch:: 71
epoch:: 72
epoch:: 73
epoch:: 74
epoch:: 75
epoch:: 76
epoch:: 77
epoch:: 78
epoch:: 79
epoch:: 80
epoch:: 81
epoch:: 82
epoch:: 83
epoch:: 84
epoch:: 85
epoch:: 86
epoch:: 87
epoch:: 88
epoch:: 89
epoch:: 90
epoch:: 91
epoch:: 92
epoch:: 93
epoch:: 94
epoch:: 95
epoch:: 96
epoch:: 97
epoch:: 98
epoch:: 99
epoch:: 100
epoch:: 101
epoch:: 102
epoch:: 103
epoch:: 104
epoch:: 105
epoch:: 106
epoch:: 107
epoch:: 108
epoch:: 109
epoch:: 110
epoch:: 111
epoch:: 112
epoch:: 113
epoch:: 114
epoch:: 115
epoch:: 116
epoch:: 117
epoch:: 118
epoch:: 119
epoch:: 120
epoch:: 121
epoch:: 122
epoch:: 123
epoch:: 124
epoch:: 125
epoch:: 126
epoch:: 127
epoch:: 128
epoch:: 129
epoch:: 130
epoch:: 131
epoch:: 132
epoch:: 133
epoch:: 134
epoch:: 135
epoch:: 136
epoch:: 137
epoch:: 138
epoch:: 139
epoch:: 140
epoch:: 141
epoch:: 142
epoch:: 143
epoch:: 144
epoch:: 145
epoch:: 146
epoch:: 147
epoch:: 148
epoch:: 149
epoch:: 150
epoch:: 151
epoch:: 152
epoch:: 153
epoch:: 154
epoch:: 155
epoch:: 156
epoch:: 157
epoch:: 158
epoch:: 159
epoch:: 160
epoch:: 161
epoch:: 162
epoch:: 163
epoch:: 164
epoch:: 165
epoch:: 166
epoch:: 167
epoch:: 168
epoch:: 169
epoch:: 170
epoch:: 171
epoch:: 172
epoch:: 173
epoch:: 174
epoch:: 175
epoch:: 176
epoch:: 177
epoch:: 178
epoch:: 179
epoch:: 180
epoch:: 181
epoch:: 182
epoch:: 183
epoch:: 184
epoch:: 185
epoch:: 186
epoch:: 187
epoch:: 188
epoch:: 189
epoch:: 190
epoch:: 191
epoch:: 192
epoch:: 193
epoch:: 194
epoch:: 195
epoch:: 196
epoch:: 197
epoch:: 198
epoch:: 199
training time is 270.1483497619629
overhead:: 0
overhead2:: 0.4091198444366455
overhead3:: 0
time_baseline:: 270.14839577674866
curr_diff: 0 tensor(0.1616, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1616, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9862, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000067, Accuracy: 0.494418
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 2 0.02 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 404
max_epoch:: 200
overhead:: 0
overhead2:: 0.022053241729736328
overhead3:: 0.1246638298034668
overhead4:: 31.53985023498535
overhead5:: 0
memory usage:: 26614198272
time_provenance:: 49.94321012496948
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1613, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1613, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9862, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000067, Accuracy: 0.494467
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 2 0.02 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 404
max_epoch:: 200
overhead:: 0
overhead2:: 0.02494335174560547
overhead3:: 0.1305534839630127
overhead4:: 37.55122447013855
overhead5:: 0
memory usage:: 26621034496
time_provenance:: 57.54880237579346
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1613, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1613, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9862, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000067, Accuracy: 0.494467
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 2 0.02 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 404
max_epoch:: 200
overhead:: 0
overhead2:: 0.029668807983398438
overhead3:: 0.14263606071472168
overhead4:: 43.46794295310974
overhead5:: 0
memory usage:: 26663043072
time_provenance:: 65.20131206512451
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1613, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1613, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9862, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000067, Accuracy: 0.494467
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 2 0.02 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 404
max_epoch:: 200
overhead:: 0
overhead2:: 0.034928321838378906
overhead3:: 0.14610791206359863
overhead4:: 57.010231494903564
overhead5:: 0
memory usage:: 26602987520
time_provenance:: 82.91772031784058
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1613, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1613, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9862, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000067, Accuracy: 0.494467
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 2 0.02 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 404
max_epoch:: 200
overhead:: 0
overhead2:: 0.04108691215515137
overhead3:: 0.15960931777954102
overhead4:: 62.83287835121155
overhead5:: 0
memory usage:: 26612109312
time_provenance:: 90.19924712181091
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1613, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1613, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9862, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000067, Accuracy: 0.494467
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 2 0.02 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 404
max_epoch:: 200
overhead:: 0
overhead2:: 0.04603099822998047
overhead3:: 0.17326879501342773
overhead4:: 67.72191643714905
overhead5:: 0
memory usage:: 26614378496
time_provenance:: 96.55826807022095
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1613, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1613, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9862, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000067, Accuracy: 0.494467
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 2 0.02 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 404
max_epoch:: 200
overhead:: 0
overhead2:: 0.0919194221496582
overhead3:: 0.27577948570251465
overhead4:: 135.27630710601807
overhead5:: 0
memory usage:: 26611597312
time_provenance:: 184.07020688056946
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1614, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1614, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9862, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000067, Accuracy: 0.494418
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 2 0.02 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 404
max_epoch:: 200
overhead:: 0
overhead2:: 0.09400701522827148
overhead3:: 0.28038477897644043
overhead4:: 139.25053143501282
overhead5:: 0
memory usage:: 26614325248
time_provenance:: 189.04689192771912
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1614, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1614, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9862, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000067, Accuracy: 0.494418
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 2 0.02 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 404
max_epoch:: 200
overhead:: 0
overhead2:: 0.0982980728149414
overhead3:: 0.2898290157318115
overhead4:: 141.907479763031
overhead5:: 0
memory usage:: 26605236224
time_provenance:: 192.49490404129028
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1614, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1614, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9862, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000067, Accuracy: 0.494418
adding noise deletion rate:: 0.04
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
tensor([0.5151, 0.5181, 0.5048,  ..., 0.5209, 0.5097, 0.5122],
       dtype=torch.float64, grad_fn=<MaxBackward0>)
tensor(0.8685, dtype=torch.float64) tensor(0., dtype=torch.float64)
Train - Epoch 0, Batch: 0, Loss: 0.693138
Train - Epoch 1, Batch: 0, Loss: 0.693162
Train - Epoch 2, Batch: 0, Loss: 0.693100
Train - Epoch 3, Batch: 0, Loss: 0.693094
Train - Epoch 4, Batch: 0, Loss: 0.693083
Train - Epoch 5, Batch: 0, Loss: 0.693056
Train - Epoch 6, Batch: 0, Loss: 0.693021
Train - Epoch 7, Batch: 0, Loss: 0.692995
Train - Epoch 8, Batch: 0, Loss: 0.692992
Train - Epoch 9, Batch: 0, Loss: 0.692940
Train - Epoch 10, Batch: 0, Loss: 0.692957
Train - Epoch 11, Batch: 0, Loss: 0.692915
Train - Epoch 12, Batch: 0, Loss: 0.692882
Train - Epoch 13, Batch: 0, Loss: 0.692837
Train - Epoch 14, Batch: 0, Loss: 0.692815
Train - Epoch 15, Batch: 0, Loss: 0.692818
Train - Epoch 16, Batch: 0, Loss: 0.692790
Train - Epoch 17, Batch: 0, Loss: 0.692770
Train - Epoch 18, Batch: 0, Loss: 0.692721
Train - Epoch 19, Batch: 0, Loss: 0.692717
Train - Epoch 20, Batch: 0, Loss: 0.692706
Train - Epoch 21, Batch: 0, Loss: 0.692678
Train - Epoch 22, Batch: 0, Loss: 0.692664
Train - Epoch 23, Batch: 0, Loss: 0.692618
Train - Epoch 24, Batch: 0, Loss: 0.692598
Train - Epoch 25, Batch: 0, Loss: 0.692629
Train - Epoch 26, Batch: 0, Loss: 0.692549
Train - Epoch 27, Batch: 0, Loss: 0.692564
Train - Epoch 28, Batch: 0, Loss: 0.692523
Train - Epoch 29, Batch: 0, Loss: 0.692449
Train - Epoch 30, Batch: 0, Loss: 0.692515
Train - Epoch 31, Batch: 0, Loss: 0.692475
Train - Epoch 32, Batch: 0, Loss: 0.692424
Train - Epoch 33, Batch: 0, Loss: 0.692429
Train - Epoch 34, Batch: 0, Loss: 0.692398
Train - Epoch 35, Batch: 0, Loss: 0.692354
Train - Epoch 36, Batch: 0, Loss: 0.692360
Train - Epoch 37, Batch: 0, Loss: 0.692359
Train - Epoch 38, Batch: 0, Loss: 0.692282
Train - Epoch 39, Batch: 0, Loss: 0.692289
Train - Epoch 40, Batch: 0, Loss: 0.692313
Train - Epoch 41, Batch: 0, Loss: 0.692246
Train - Epoch 42, Batch: 0, Loss: 0.692232
Train - Epoch 43, Batch: 0, Loss: 0.692205
Train - Epoch 44, Batch: 0, Loss: 0.692212
Train - Epoch 45, Batch: 0, Loss: 0.692190
Train - Epoch 46, Batch: 0, Loss: 0.692087
Train - Epoch 47, Batch: 0, Loss: 0.692086
Train - Epoch 48, Batch: 0, Loss: 0.692085
Train - Epoch 49, Batch: 0, Loss: 0.692041
Train - Epoch 50, Batch: 0, Loss: 0.692081
Train - Epoch 51, Batch: 0, Loss: 0.691977
Train - Epoch 52, Batch: 0, Loss: 0.692068
Train - Epoch 53, Batch: 0, Loss: 0.692041
Train - Epoch 54, Batch: 0, Loss: 0.692016
Train - Epoch 55, Batch: 0, Loss: 0.691989
Train - Epoch 56, Batch: 0, Loss: 0.691869
Train - Epoch 57, Batch: 0, Loss: 0.691995
Train - Epoch 58, Batch: 0, Loss: 0.691864
Train - Epoch 59, Batch: 0, Loss: 0.691872
Train - Epoch 60, Batch: 0, Loss: 0.691886
Train - Epoch 61, Batch: 0, Loss: 0.691771
Train - Epoch 62, Batch: 0, Loss: 0.691798
Train - Epoch 63, Batch: 0, Loss: 0.691725
Train - Epoch 64, Batch: 0, Loss: 0.691799
Train - Epoch 65, Batch: 0, Loss: 0.691734
Train - Epoch 66, Batch: 0, Loss: 0.691751
Train - Epoch 67, Batch: 0, Loss: 0.691699
Train - Epoch 68, Batch: 0, Loss: 0.691649
Train - Epoch 69, Batch: 0, Loss: 0.691716
Train - Epoch 70, Batch: 0, Loss: 0.691662
Train - Epoch 71, Batch: 0, Loss: 0.691605
Train - Epoch 72, Batch: 0, Loss: 0.691643
Train - Epoch 73, Batch: 0, Loss: 0.691642
Train - Epoch 74, Batch: 0, Loss: 0.691529
Train - Epoch 75, Batch: 0, Loss: 0.691471
Train - Epoch 76, Batch: 0, Loss: 0.691516
Train - Epoch 77, Batch: 0, Loss: 0.691484
Train - Epoch 78, Batch: 0, Loss: 0.691422
Train - Epoch 79, Batch: 0, Loss: 0.691470
Train - Epoch 80, Batch: 0, Loss: 0.691475
Train - Epoch 81, Batch: 0, Loss: 0.691346
Train - Epoch 82, Batch: 0, Loss: 0.691379
Train - Epoch 83, Batch: 0, Loss: 0.691339
Train - Epoch 84, Batch: 0, Loss: 0.691372
Train - Epoch 85, Batch: 0, Loss: 0.691228
Train - Epoch 86, Batch: 0, Loss: 0.691310
Train - Epoch 87, Batch: 0, Loss: 0.691314
Train - Epoch 88, Batch: 0, Loss: 0.691330
Train - Epoch 89, Batch: 0, Loss: 0.691260
Train - Epoch 90, Batch: 0, Loss: 0.691354
Train - Epoch 91, Batch: 0, Loss: 0.691184
Train - Epoch 92, Batch: 0, Loss: 0.691125
Train - Epoch 93, Batch: 0, Loss: 0.691128
Train - Epoch 94, Batch: 0, Loss: 0.691159
Train - Epoch 95, Batch: 0, Loss: 0.691141
Train - Epoch 96, Batch: 0, Loss: 0.691167
Train - Epoch 97, Batch: 0, Loss: 0.691052
Train - Epoch 98, Batch: 0, Loss: 0.691076
Train - Epoch 99, Batch: 0, Loss: 0.691055
Train - Epoch 100, Batch: 0, Loss: 0.691151
Train - Epoch 101, Batch: 0, Loss: 0.691000
Train - Epoch 102, Batch: 0, Loss: 0.690984
Train - Epoch 103, Batch: 0, Loss: 0.690874
Train - Epoch 104, Batch: 0, Loss: 0.690971
Train - Epoch 105, Batch: 0, Loss: 0.690832
Train - Epoch 106, Batch: 0, Loss: 0.690924
Train - Epoch 107, Batch: 0, Loss: 0.690887
Train - Epoch 108, Batch: 0, Loss: 0.690922
Train - Epoch 109, Batch: 0, Loss: 0.690876
Train - Epoch 110, Batch: 0, Loss: 0.690936
Train - Epoch 111, Batch: 0, Loss: 0.690757
Train - Epoch 112, Batch: 0, Loss: 0.690893
Train - Epoch 113, Batch: 0, Loss: 0.690866
Train - Epoch 114, Batch: 0, Loss: 0.690650
Train - Epoch 115, Batch: 0, Loss: 0.690801
Train - Epoch 116, Batch: 0, Loss: 0.690684
Train - Epoch 117, Batch: 0, Loss: 0.690766
Train - Epoch 118, Batch: 0, Loss: 0.690704
Train - Epoch 119, Batch: 0, Loss: 0.690608
Train - Epoch 120, Batch: 0, Loss: 0.690640
Train - Epoch 121, Batch: 0, Loss: 0.690622
Train - Epoch 122, Batch: 0, Loss: 0.690699
Train - Epoch 123, Batch: 0, Loss: 0.690677
Train - Epoch 124, Batch: 0, Loss: 0.690583
Train - Epoch 125, Batch: 0, Loss: 0.690534
Train - Epoch 126, Batch: 0, Loss: 0.690658
Train - Epoch 127, Batch: 0, Loss: 0.690529
Train - Epoch 128, Batch: 0, Loss: 0.690552
Train - Epoch 129, Batch: 0, Loss: 0.690533
Train - Epoch 130, Batch: 0, Loss: 0.690508
Train - Epoch 131, Batch: 0, Loss: 0.690611
Train - Epoch 132, Batch: 0, Loss: 0.690599
Train - Epoch 133, Batch: 0, Loss: 0.690429
Train - Epoch 134, Batch: 0, Loss: 0.690337
Train - Epoch 135, Batch: 0, Loss: 0.690433
Train - Epoch 136, Batch: 0, Loss: 0.690386
Train - Epoch 137, Batch: 0, Loss: 0.690499
Train - Epoch 138, Batch: 0, Loss: 0.690370
Train - Epoch 139, Batch: 0, Loss: 0.690305
Train - Epoch 140, Batch: 0, Loss: 0.690329
Train - Epoch 141, Batch: 0, Loss: 0.690340
Train - Epoch 142, Batch: 0, Loss: 0.690323
Train - Epoch 143, Batch: 0, Loss: 0.690278
Train - Epoch 144, Batch: 0, Loss: 0.690245
Train - Epoch 145, Batch: 0, Loss: 0.690343
Train - Epoch 146, Batch: 0, Loss: 0.690104
Train - Epoch 147, Batch: 0, Loss: 0.690222
Train - Epoch 148, Batch: 0, Loss: 0.690307
Train - Epoch 149, Batch: 0, Loss: 0.690270
Train - Epoch 150, Batch: 0, Loss: 0.690031
Train - Epoch 151, Batch: 0, Loss: 0.690115
Train - Epoch 152, Batch: 0, Loss: 0.690022
Train - Epoch 153, Batch: 0, Loss: 0.689974
Train - Epoch 154, Batch: 0, Loss: 0.690126
Train - Epoch 155, Batch: 0, Loss: 0.689885
Train - Epoch 156, Batch: 0, Loss: 0.690001
Train - Epoch 157, Batch: 0, Loss: 0.690049
Train - Epoch 158, Batch: 0, Loss: 0.689848
Train - Epoch 159, Batch: 0, Loss: 0.689960
Train - Epoch 160, Batch: 0, Loss: 0.690111
Train - Epoch 161, Batch: 0, Loss: 0.689992
Train - Epoch 162, Batch: 0, Loss: 0.689810
Train - Epoch 163, Batch: 0, Loss: 0.689840
Train - Epoch 164, Batch: 0, Loss: 0.689824
Train - Epoch 165, Batch: 0, Loss: 0.689837
Train - Epoch 166, Batch: 0, Loss: 0.689828
Train - Epoch 167, Batch: 0, Loss: 0.689715
Train - Epoch 168, Batch: 0, Loss: 0.689922
Train - Epoch 169, Batch: 0, Loss: 0.689823
Train - Epoch 170, Batch: 0, Loss: 0.689815
Train - Epoch 171, Batch: 0, Loss: 0.689755
Train - Epoch 172, Batch: 0, Loss: 0.689774
Train - Epoch 173, Batch: 0, Loss: 0.689711
Train - Epoch 174, Batch: 0, Loss: 0.689819
Train - Epoch 175, Batch: 0, Loss: 0.689625
Train - Epoch 176, Batch: 0, Loss: 0.689841
Train - Epoch 177, Batch: 0, Loss: 0.689605
Train - Epoch 178, Batch: 0, Loss: 0.689679
Train - Epoch 179, Batch: 0, Loss: 0.689558
Train - Epoch 180, Batch: 0, Loss: 0.689665
Train - Epoch 181, Batch: 0, Loss: 0.689520
Train - Epoch 182, Batch: 0, Loss: 0.689349/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 183, Batch: 0, Loss: 0.689623
Train - Epoch 184, Batch: 0, Loss: 0.689500
Train - Epoch 185, Batch: 0, Loss: 0.689345
Train - Epoch 186, Batch: 0, Loss: 0.689469
Train - Epoch 187, Batch: 0, Loss: 0.689289
Train - Epoch 188, Batch: 0, Loss: 0.689446
Train - Epoch 189, Batch: 0, Loss: 0.689384
Train - Epoch 190, Batch: 0, Loss: 0.689472
Train - Epoch 191, Batch: 0, Loss: 0.689517
Train - Epoch 192, Batch: 0, Loss: 0.689353
Train - Epoch 193, Batch: 0, Loss: 0.689326
Train - Epoch 194, Batch: 0, Loss: 0.689500
Train - Epoch 195, Batch: 0, Loss: 0.689385
Train - Epoch 196, Batch: 0, Loss: 0.689273
Train - Epoch 197, Batch: 0, Loss: 0.689317
Train - Epoch 198, Batch: 0, Loss: 0.689262
Train - Epoch 199, Batch: 0, Loss: 0.689276
training_time:: 351.6298747062683
training time full:: 351.629914522171
provenance prepare time:: 0.0
here
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.761980
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 809
max_epoch:: 200
batch_size:: 10200
epoch:: 0
tensor([  0,   3,   5,   6,  18,  24,  25,  26,  27,  29,  31,  35,  41,  42,
         44,  46,  52,  56,  57,  58,  59,  64,  66,  67,  69,  71,  72,  75,
         78,  79,  80,  81,  84,  85,  87,  88,  90,  91,  94,  95,  97,  98,
         99, 104, 109, 110, 111, 115, 117, 118])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
epoch:: 60
epoch:: 61
epoch:: 62
epoch:: 63
epoch:: 64
epoch:: 65
epoch:: 66
epoch:: 67
epoch:: 68
epoch:: 69
epoch:: 70
epoch:: 71
epoch:: 72
epoch:: 73
epoch:: 74
epoch:: 75
epoch:: 76
epoch:: 77
epoch:: 78
epoch:: 79
epoch:: 80
epoch:: 81
epoch:: 82
epoch:: 83
epoch:: 84
epoch:: 85
epoch:: 86
epoch:: 87
epoch:: 88
epoch:: 89
epoch:: 90
epoch:: 91
epoch:: 92
epoch:: 93
epoch:: 94
epoch:: 95
epoch:: 96
epoch:: 97
epoch:: 98
epoch:: 99
epoch:: 100
epoch:: 101
epoch:: 102
epoch:: 103
epoch:: 104
epoch:: 105
epoch:: 106
epoch:: 107
epoch:: 108
epoch:: 109
epoch:: 110
epoch:: 111
epoch:: 112
epoch:: 113
epoch:: 114
epoch:: 115
epoch:: 116
epoch:: 117
epoch:: 118
epoch:: 119
epoch:: 120
epoch:: 121
epoch:: 122
epoch:: 123
epoch:: 124
epoch:: 125
epoch:: 126
epoch:: 127
epoch:: 128
epoch:: 129
epoch:: 130
epoch:: 131
epoch:: 132
epoch:: 133
epoch:: 134
epoch:: 135
epoch:: 136
epoch:: 137
epoch:: 138
epoch:: 139
epoch:: 140
epoch:: 141
epoch:: 142
epoch:: 143
epoch:: 144
epoch:: 145
epoch:: 146
epoch:: 147
epoch:: 148
epoch:: 149
epoch:: 150
epoch:: 151
epoch:: 152
epoch:: 153
epoch:: 154
epoch:: 155
epoch:: 156
epoch:: 157
epoch:: 158
epoch:: 159
epoch:: 160
epoch:: 161
epoch:: 162
epoch:: 163
epoch:: 164
epoch:: 165
epoch:: 166
epoch:: 167
epoch:: 168
epoch:: 169
epoch:: 170
epoch:: 171
epoch:: 172
epoch:: 173
epoch:: 174
epoch:: 175
epoch:: 176
epoch:: 177
epoch:: 178
epoch:: 179
epoch:: 180
epoch:: 181
epoch:: 182
epoch:: 183
epoch:: 184
epoch:: 185
epoch:: 186
epoch:: 187
epoch:: 188
epoch:: 189
epoch:: 190
epoch:: 191
epoch:: 192
epoch:: 193
epoch:: 194
epoch:: 195
epoch:: 196
epoch:: 197
epoch:: 198
epoch:: 199
training time is 264.86364483833313
overhead:: 0
overhead2:: 0.417630672454834
overhead3:: 0
time_baseline:: 264.86369824409485
curr_diff: 0 tensor(0.2463, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.2463, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9486, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.662929
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 2 0.04 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 809
max_epoch:: 200
overhead:: 0
overhead2:: 0.02353525161743164
overhead3:: 0.1308155059814453
overhead4:: 31.586710929870605
overhead5:: 0
memory usage:: 26607964160
time_provenance:: 56.372095584869385
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.2454, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.2454, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9490, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.663719
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 2 0.04 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 809
max_epoch:: 200
overhead:: 0
overhead2:: 0.026815414428710938
overhead3:: 0.1387488842010498
overhead4:: 37.39430546760559
overhead5:: 0
memory usage:: 26609954816
time_provenance:: 63.63246536254883
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.2454, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.2454, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9490, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.663719
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 2 0.04 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 809
max_epoch:: 200
overhead:: 0
overhead2:: 0.031029462814331055
overhead3:: 0.14715313911437988
overhead4:: 43.35694408416748
overhead5:: 0
memory usage:: 26623356928
time_provenance:: 71.40848755836487
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.2454, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.2454, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9490, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.663719
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 2 0.04 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 809
max_epoch:: 200
overhead:: 0
overhead2:: 0.04287457466125488
overhead3:: 0.1705493927001953
overhead4:: 56.703863859176636
overhead5:: 0
memory usage:: 26603597824
time_provenance:: 88.6778154373169
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.2455, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.2455, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9490, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.663670
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 2 0.04 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 809
max_epoch:: 200
overhead:: 0
overhead2:: 0.04157876968383789
overhead3:: 0.17309880256652832
overhead4:: 61.60400176048279
overhead5:: 0
memory usage:: 26605580288
time_provenance:: 94.99476909637451
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.2455, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.2455, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9490, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.663670
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 2 0.04 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 809
max_epoch:: 200
overhead:: 0
overhead2:: 0.04541969299316406
overhead3:: 0.18153786659240723
overhead4:: 66.64245676994324
overhead5:: 0
memory usage:: 26602397696
time_provenance:: 101.41085243225098
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.2455, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.2455, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9490, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.663670
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 2 0.04 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 809
max_epoch:: 200
overhead:: 0
overhead2:: 0.091400146484375
overhead3:: 0.27973437309265137
overhead4:: 131.90600395202637
overhead5:: 0
memory usage:: 26598760448
time_provenance:: 186.0378773212433
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.2458, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.2458, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9488, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.663423
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 2 0.04 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 809
max_epoch:: 200
overhead:: 0
overhead2:: 0.09299945831298828
overhead3:: 0.28458142280578613
overhead4:: 134.92265009880066
overhead5:: 0
memory usage:: 26614501376
time_provenance:: 189.92371940612793
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.2458, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.2458, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9488, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.663423
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 2 0.04 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 809
max_epoch:: 200
overhead:: 0
overhead2:: 0.10667538642883301
overhead3:: 0.29315876960754395
overhead4:: 139.3806700706482
overhead5:: 0
memory usage:: 26619465728
time_provenance:: 195.2479693889618
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.2458, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.2458, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9488, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.663423
adding noise deletion rate:: 0.05
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
tensor([0.5067, 0.5060, 0.5027,  ..., 0.5024, 0.5034, 0.5022],
       dtype=torch.float64, grad_fn=<MaxBackward0>)
tensor(0.8685, dtype=torch.float64) tensor(0., dtype=torch.float64)
Train - Epoch 0, Batch: 0, Loss: 0.693122
Train - Epoch 1, Batch: 0, Loss: 0.693053
Train - Epoch 2, Batch: 0, Loss: 0.692997
Train - Epoch 3, Batch: 0, Loss: 0.692904
Train - Epoch 4, Batch: 0, Loss: 0.692830
Train - Epoch 5, Batch: 0, Loss: 0.692755
Train - Epoch 6, Batch: 0, Loss: 0.692683
Train - Epoch 7, Batch: 0, Loss: 0.692676
Train - Epoch 8, Batch: 0, Loss: 0.692577
Train - Epoch 9, Batch: 0, Loss: 0.692490
Train - Epoch 10, Batch: 0, Loss: 0.692378
Train - Epoch 11, Batch: 0, Loss: 0.692362
Train - Epoch 12, Batch: 0, Loss: 0.692323
Train - Epoch 13, Batch: 0, Loss: 0.692255
Train - Epoch 14, Batch: 0, Loss: 0.692217
Train - Epoch 15, Batch: 0, Loss: 0.692076
Train - Epoch 16, Batch: 0, Loss: 0.692084
Train - Epoch 17, Batch: 0, Loss: 0.692042
Train - Epoch 18, Batch: 0, Loss: 0.691923
Train - Epoch 19, Batch: 0, Loss: 0.691938
Train - Epoch 20, Batch: 0, Loss: 0.691883
Train - Epoch 21, Batch: 0, Loss: 0.691773
Train - Epoch 22, Batch: 0, Loss: 0.691665
Train - Epoch 23, Batch: 0, Loss: 0.691584
Train - Epoch 24, Batch: 0, Loss: 0.691412
Train - Epoch 25, Batch: 0, Loss: 0.691449
Train - Epoch 26, Batch: 0, Loss: 0.691457
Train - Epoch 27, Batch: 0, Loss: 0.691333
Train - Epoch 28, Batch: 0, Loss: 0.691277
Train - Epoch 29, Batch: 0, Loss: 0.691187
Train - Epoch 30, Batch: 0, Loss: 0.691122
Train - Epoch 31, Batch: 0, Loss: 0.691003
Train - Epoch 32, Batch: 0, Loss: 0.691099
Train - Epoch 33, Batch: 0, Loss: 0.691023
Train - Epoch 34, Batch: 0, Loss: 0.690940
Train - Epoch 35, Batch: 0, Loss: 0.690842
Train - Epoch 36, Batch: 0, Loss: 0.690781
Train - Epoch 37, Batch: 0, Loss: 0.690730
Train - Epoch 38, Batch: 0, Loss: 0.690665
Train - Epoch 39, Batch: 0, Loss: 0.690626
Train - Epoch 40, Batch: 0, Loss: 0.690507
Train - Epoch 41, Batch: 0, Loss: 0.690443
Train - Epoch 42, Batch: 0, Loss: 0.690280
Train - Epoch 43, Batch: 0, Loss: 0.690345
Train - Epoch 44, Batch: 0, Loss: 0.690155
Train - Epoch 45, Batch: 0, Loss: 0.690226
Train - Epoch 46, Batch: 0, Loss: 0.690426
Train - Epoch 47, Batch: 0, Loss: 0.690004
Train - Epoch 48, Batch: 0, Loss: 0.690179
Train - Epoch 49, Batch: 0, Loss: 0.690002
Train - Epoch 50, Batch: 0, Loss: 0.689869
Train - Epoch 51, Batch: 0, Loss: 0.690033
Train - Epoch 52, Batch: 0, Loss: 0.689830
Train - Epoch 53, Batch: 0, Loss: 0.689831
Train - Epoch 54, Batch: 0, Loss: 0.689666
Train - Epoch 55, Batch: 0, Loss: 0.689702
Train - Epoch 56, Batch: 0, Loss: 0.689765
Train - Epoch 57, Batch: 0, Loss: 0.689648
Train - Epoch 58, Batch: 0, Loss: 0.689412
Train - Epoch 59, Batch: 0, Loss: 0.689447
Train - Epoch 60, Batch: 0, Loss: 0.689441
Train - Epoch 61, Batch: 0, Loss: 0.689370
Train - Epoch 62, Batch: 0, Loss: 0.689351
Train - Epoch 63, Batch: 0, Loss: 0.689198
Train - Epoch 64, Batch: 0, Loss: 0.688970
Train - Epoch 65, Batch: 0, Loss: 0.689006
Train - Epoch 66, Batch: 0, Loss: 0.689347
Train - Epoch 67, Batch: 0, Loss: 0.688825
Train - Epoch 68, Batch: 0, Loss: 0.688946
Train - Epoch 69, Batch: 0, Loss: 0.688687
Train - Epoch 70, Batch: 0, Loss: 0.689165
Train - Epoch 71, Batch: 0, Loss: 0.688793
Train - Epoch 72, Batch: 0, Loss: 0.688920
Train - Epoch 73, Batch: 0, Loss: 0.688647
Train - Epoch 74, Batch: 0, Loss: 0.688510
Train - Epoch 75, Batch: 0, Loss: 0.688489
Train - Epoch 76, Batch: 0, Loss: 0.688603
Train - Epoch 77, Batch: 0, Loss: 0.688565
Train - Epoch 78, Batch: 0, Loss: 0.688651
Train - Epoch 79, Batch: 0, Loss: 0.687991
Train - Epoch 80, Batch: 0, Loss: 0.688257
Train - Epoch 81, Batch: 0, Loss: 0.688165
Train - Epoch 82, Batch: 0, Loss: 0.688214
Train - Epoch 83, Batch: 0, Loss: 0.688051
Train - Epoch 84, Batch: 0, Loss: 0.688386
Train - Epoch 85, Batch: 0, Loss: 0.688385
Train - Epoch 86, Batch: 0, Loss: 0.688120
Train - Epoch 87, Batch: 0, Loss: 0.688101
Train - Epoch 88, Batch: 0, Loss: 0.688014
Train - Epoch 89, Batch: 0, Loss: 0.687981
Train - Epoch 90, Batch: 0, Loss: 0.687811
Train - Epoch 91, Batch: 0, Loss: 0.687816
Train - Epoch 92, Batch: 0, Loss: 0.688082
Train - Epoch 93, Batch: 0, Loss: 0.687715
Train - Epoch 94, Batch: 0, Loss: 0.687583
Train - Epoch 95, Batch: 0, Loss: 0.687675
Train - Epoch 96, Batch: 0, Loss: 0.687426
Train - Epoch 97, Batch: 0, Loss: 0.687732
Train - Epoch 98, Batch: 0, Loss: 0.687435
Train - Epoch 99, Batch: 0, Loss: 0.687410
Train - Epoch 100, Batch: 0, Loss: 0.687323
Train - Epoch 101, Batch: 0, Loss: 0.687392
Train - Epoch 102, Batch: 0, Loss: 0.687034
Train - Epoch 103, Batch: 0, Loss: 0.686935
Train - Epoch 104, Batch: 0, Loss: 0.687217
Train - Epoch 105, Batch: 0, Loss: 0.686959
Train - Epoch 106, Batch: 0, Loss: 0.686939
Train - Epoch 107, Batch: 0, Loss: 0.687380
Train - Epoch 108, Batch: 0, Loss: 0.686744
Train - Epoch 109, Batch: 0, Loss: 0.686821
Train - Epoch 110, Batch: 0, Loss: 0.686809
Train - Epoch 111, Batch: 0, Loss: 0.687137
Train - Epoch 112, Batch: 0, Loss: 0.686700
Train - Epoch 113, Batch: 0, Loss: 0.686607
Train - Epoch 114, Batch: 0, Loss: 0.686573
Train - Epoch 115, Batch: 0, Loss: 0.686161
Train - Epoch 116, Batch: 0, Loss: 0.686338
Train - Epoch 117, Batch: 0, Loss: 0.686392
Train - Epoch 118, Batch: 0, Loss: 0.686508
Train - Epoch 119, Batch: 0, Loss: 0.686247
Train - Epoch 120, Batch: 0, Loss: 0.686531
Train - Epoch 121, Batch: 0, Loss: 0.686297
Train - Epoch 122, Batch: 0, Loss: 0.686166
Train - Epoch 123, Batch: 0, Loss: 0.686291
Train - Epoch 124, Batch: 0, Loss: 0.686015
Train - Epoch 125, Batch: 0, Loss: 0.686181
Train - Epoch 126, Batch: 0, Loss: 0.686127
Train - Epoch 127, Batch: 0, Loss: 0.686010
Train - Epoch 128, Batch: 0, Loss: 0.685903
Train - Epoch 129, Batch: 0, Loss: 0.685860
Train - Epoch 130, Batch: 0, Loss: 0.685904
Train - Epoch 131, Batch: 0, Loss: 0.685246
Train - Epoch 132, Batch: 0, Loss: 0.685639
Train - Epoch 133, Batch: 0, Loss: 0.685859
Train - Epoch 134, Batch: 0, Loss: 0.685558
Train - Epoch 135, Batch: 0, Loss: 0.685843
Train - Epoch 136, Batch: 0, Loss: 0.685353
Train - Epoch 137, Batch: 0, Loss: 0.685391
Train - Epoch 138, Batch: 0, Loss: 0.685942
Train - Epoch 139, Batch: 0, Loss: 0.685800
Train - Epoch 140, Batch: 0, Loss: 0.685202
Train - Epoch 141, Batch: 0, Loss: 0.685334
Train - Epoch 142, Batch: 0, Loss: 0.684876
Train - Epoch 143, Batch: 0, Loss: 0.685573
Train - Epoch 144, Batch: 0, Loss: 0.684531
Train - Epoch 145, Batch: 0, Loss: 0.684960
Train - Epoch 146, Batch: 0, Loss: 0.685029
Train - Epoch 147, Batch: 0, Loss: 0.685405
Train - Epoch 148, Batch: 0, Loss: 0.684780
Train - Epoch 149, Batch: 0, Loss: 0.684904
Train - Epoch 150, Batch: 0, Loss: 0.685032
Train - Epoch 151, Batch: 0, Loss: 0.684638
Train - Epoch 152, Batch: 0, Loss: 0.684518
Train - Epoch 153, Batch: 0, Loss: 0.685365
Train - Epoch 154, Batch: 0, Loss: 0.684567
Train - Epoch 155, Batch: 0, Loss: 0.685299
Train - Epoch 156, Batch: 0, Loss: 0.684737
Train - Epoch 157, Batch: 0, Loss: 0.684963
Train - Epoch 158, Batch: 0, Loss: 0.684731
Train - Epoch 159, Batch: 0, Loss: 0.684703
Train - Epoch 160, Batch: 0, Loss: 0.684967
Train - Epoch 161, Batch: 0, Loss: 0.684630
Train - Epoch 162, Batch: 0, Loss: 0.684854
Train - Epoch 163, Batch: 0, Loss: 0.684140
Train - Epoch 164, Batch: 0, Loss: 0.684815
Train - Epoch 165, Batch: 0, Loss: 0.684511
Train - Epoch 166, Batch: 0, Loss: 0.684110
Train - Epoch 167, Batch: 0, Loss: 0.684401
Train - Epoch 168, Batch: 0, Loss: 0.684301
Train - Epoch 169, Batch: 0, Loss: 0.684041
Train - Epoch 170, Batch: 0, Loss: 0.684040
Train - Epoch 171, Batch: 0, Loss: 0.684659
Train - Epoch 172, Batch: 0, Loss: 0.684418
Train - Epoch 173, Batch: 0, Loss: 0.684478
Train - Epoch 174, Batch: 0, Loss: 0.684421
Train - Epoch 175, Batch: 0, Loss: 0.683936
Train - Epoch 176, Batch: 0, Loss: 0.683814
Train - Epoch 177, Batch: 0, Loss: 0.684053
Train - Epoch 178, Batch: 0, Loss: 0.683436
Train - Epoch 179, Batch: 0, Loss: 0.684201
Train - Epoch 180, Batch: 0, Loss: 0.684256
Train - Epoch 181, Batch: 0, Loss: 0.683749
Train - Epoch 182, Batch: 0, Loss: 0.684546/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 183, Batch: 0, Loss: 0.683420
Train - Epoch 184, Batch: 0, Loss: 0.684253
Train - Epoch 185, Batch: 0, Loss: 0.683682
Train - Epoch 186, Batch: 0, Loss: 0.683948
Train - Epoch 187, Batch: 0, Loss: 0.684004
Train - Epoch 188, Batch: 0, Loss: 0.683411
Train - Epoch 189, Batch: 0, Loss: 0.683653
Train - Epoch 190, Batch: 0, Loss: 0.683366
Train - Epoch 191, Batch: 0, Loss: 0.683877
Train - Epoch 192, Batch: 0, Loss: 0.683598
Train - Epoch 193, Batch: 0, Loss: 0.682881
Train - Epoch 194, Batch: 0, Loss: 0.683045
Train - Epoch 195, Batch: 0, Loss: 0.683735
Train - Epoch 196, Batch: 0, Loss: 0.683438
Train - Epoch 197, Batch: 0, Loss: 0.683998
Train - Epoch 198, Batch: 0, Loss: 0.683351
Train - Epoch 199, Batch: 0, Loss: 0.682968
training_time:: 352.3742480278015
training time full:: 352.3742880821228
provenance prepare time:: 4.76837158203125e-07
here
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.487946
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1012
max_epoch:: 200
batch_size:: 10200
epoch:: 0
tensor([  0,   5,   6,  18,  24,  25,  26,  27,  29,  31,  35,  38,  39,  41,
         42,  44,  46,  52,  56,  57,  58,  59,  63,  64,  66,  69,  71,  75,
         79,  80,  81,  84,  85,  87,  88,  90,  91,  94,  95,  97,  98,  99,
        104, 109, 110, 115, 117, 118, 125, 127])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
epoch:: 60
epoch:: 61
epoch:: 62
epoch:: 63
epoch:: 64
epoch:: 65
epoch:: 66
epoch:: 67
epoch:: 68
epoch:: 69
epoch:: 70
epoch:: 71
epoch:: 72
epoch:: 73
epoch:: 74
epoch:: 75
epoch:: 76
epoch:: 77
epoch:: 78
epoch:: 79
epoch:: 80
epoch:: 81
epoch:: 82
epoch:: 83
epoch:: 84
epoch:: 85
epoch:: 86
epoch:: 87
epoch:: 88
epoch:: 89
epoch:: 90
epoch:: 91
epoch:: 92
epoch:: 93
epoch:: 94
epoch:: 95
epoch:: 96
epoch:: 97
epoch:: 98
epoch:: 99
epoch:: 100
epoch:: 101
epoch:: 102
epoch:: 103
epoch:: 104
epoch:: 105
epoch:: 106
epoch:: 107
epoch:: 108
epoch:: 109
epoch:: 110
epoch:: 111
epoch:: 112
epoch:: 113
epoch:: 114
epoch:: 115
epoch:: 116
epoch:: 117
epoch:: 118
epoch:: 119
epoch:: 120
epoch:: 121
epoch:: 122
epoch:: 123
epoch:: 124
epoch:: 125
epoch:: 126
epoch:: 127
epoch:: 128
epoch:: 129
epoch:: 130
epoch:: 131
epoch:: 132
epoch:: 133
epoch:: 134
epoch:: 135
epoch:: 136
epoch:: 137
epoch:: 138
epoch:: 139
epoch:: 140
epoch:: 141
epoch:: 142
epoch:: 143
epoch:: 144
epoch:: 145
epoch:: 146
epoch:: 147
epoch:: 148
epoch:: 149
epoch:: 150
epoch:: 151
epoch:: 152
epoch:: 153
epoch:: 154
epoch:: 155
epoch:: 156
epoch:: 157
epoch:: 158
epoch:: 159
epoch:: 160
epoch:: 161
epoch:: 162
epoch:: 163
epoch:: 164
epoch:: 165
epoch:: 166
epoch:: 167
epoch:: 168
epoch:: 169
epoch:: 170
epoch:: 171
epoch:: 172
epoch:: 173
epoch:: 174
epoch:: 175
epoch:: 176
epoch:: 177
epoch:: 178
epoch:: 179
epoch:: 180
epoch:: 181
epoch:: 182
epoch:: 183
epoch:: 184
epoch:: 185
epoch:: 186
epoch:: 187
epoch:: 188
epoch:: 189
epoch:: 190
epoch:: 191
epoch:: 192
epoch:: 193
epoch:: 194
epoch:: 195
epoch:: 196
epoch:: 197
epoch:: 198
epoch:: 199
training time is 262.58033752441406
overhead:: 0
overhead2:: 0.43834662437438965
overhead3:: 0
time_baseline:: 262.580393075943
curr_diff: 0 tensor(0.3127, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.3127, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9400, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.581217
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 2 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1012
max_epoch:: 200
overhead:: 0
overhead2:: 0.0216519832611084
overhead3:: 0.13038086891174316
overhead4:: 31.01268982887268
overhead5:: 0
memory usage:: 26600574976
time_provenance:: 59.92229461669922
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.3113, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.3113, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9406, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.580427
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 2 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1012
max_epoch:: 200
overhead:: 0
overhead2:: 0.02737569808959961
overhead3:: 0.14127421379089355
overhead4:: 36.64770817756653
overhead5:: 0
memory usage:: 26603327488
time_provenance:: 67.1444263458252
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.3113, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.3113, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9406, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.580427
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 2 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1012
max_epoch:: 200
overhead:: 0
overhead2:: 0.0306398868560791
overhead3:: 0.14982867240905762
overhead4:: 42.274736166000366
overhead5:: 0
memory usage:: 26617253888
time_provenance:: 74.1805739402771
curr_diff: 0 tensor(0.0015, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0015, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.3113, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.3113, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9406, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.580427
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 2 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1012
max_epoch:: 200
overhead:: 0
overhead2:: 0.03845620155334473
overhead3:: 0.1665956974029541
overhead4:: 55.804255962371826
overhead5:: 0
memory usage:: 26609553408
time_provenance:: 91.99827551841736
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.3114, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.3114, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9406, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.580476
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 2 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1012
max_epoch:: 200
overhead:: 0
overhead2:: 0.044695377349853516
overhead3:: 0.17784571647644043
overhead4:: 60.735979318618774
overhead5:: 0
memory usage:: 26619219968
time_provenance:: 98.14687490463257
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.3114, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.3114, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9406, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.580476
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 2 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1012
max_epoch:: 200
overhead:: 0
overhead2:: 0.04558968544006348
overhead3:: 0.18221831321716309
overhead4:: 65.41061305999756
overhead5:: 0
memory usage:: 26640109568
time_provenance:: 103.88102078437805
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.3114, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.3114, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9406, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.580476
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 2 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1012
max_epoch:: 200
overhead:: 0
overhead2:: 0.09208559989929199
overhead3:: 0.2826724052429199
overhead4:: 131.38000917434692
overhead5:: 0
memory usage:: 26600820736
time_provenance:: 188.88372802734375
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.3120, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.3120, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9404, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.580871
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 2 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1012
max_epoch:: 200
overhead:: 0
overhead2:: 0.09720563888549805
overhead3:: 0.2922203540802002
overhead4:: 133.56957697868347
overhead5:: 0
memory usage:: 26606743552
time_provenance:: 191.81810569763184
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.3120, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.3120, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9404, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.580871
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 2 0.05 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1012
max_epoch:: 200
overhead:: 0
overhead2:: 0.12233757972717285
overhead3:: 0.2975881099700928
overhead4:: 136.27437949180603
overhead5:: 0
memory usage:: 26622246912
time_provenance:: 195.3055341243744
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.3120, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.3120, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9404, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.580921
adding noise deletion rate:: 0.06
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
tensor([0.5176, 0.5273, 0.5065,  ..., 0.5236, 0.5121, 0.5139],
       dtype=torch.float64, grad_fn=<MaxBackward0>)
tensor(0.8685, dtype=torch.float64) tensor(0., dtype=torch.float64)
Train - Epoch 0, Batch: 0, Loss: 0.693149
Train - Epoch 1, Batch: 0, Loss: 0.693145
Train - Epoch 2, Batch: 0, Loss: 0.693079
Train - Epoch 3, Batch: 0, Loss: 0.693052
Train - Epoch 4, Batch: 0, Loss: 0.693012
Train - Epoch 5, Batch: 0, Loss: 0.692973
Train - Epoch 6, Batch: 0, Loss: 0.692923
Train - Epoch 7, Batch: 0, Loss: 0.692897
Train - Epoch 8, Batch: 0, Loss: 0.692869
Train - Epoch 9, Batch: 0, Loss: 0.692826
Train - Epoch 10, Batch: 0, Loss: 0.692795
Train - Epoch 11, Batch: 0, Loss: 0.692763
Train - Epoch 12, Batch: 0, Loss: 0.692702
Train - Epoch 13, Batch: 0, Loss: 0.692693
Train - Epoch 14, Batch: 0, Loss: 0.692643
Train - Epoch 15, Batch: 0, Loss: 0.692597
Train - Epoch 16, Batch: 0, Loss: 0.692590
Train - Epoch 17, Batch: 0, Loss: 0.692549
Train - Epoch 18, Batch: 0, Loss: 0.692474
Train - Epoch 19, Batch: 0, Loss: 0.692477
Train - Epoch 20, Batch: 0, Loss: 0.692382
Train - Epoch 21, Batch: 0, Loss: 0.692420
Train - Epoch 22, Batch: 0, Loss: 0.692372
Train - Epoch 23, Batch: 0, Loss: 0.692293
Train - Epoch 24, Batch: 0, Loss: 0.692320
Train - Epoch 25, Batch: 0, Loss: 0.692304
Train - Epoch 26, Batch: 0, Loss: 0.692192
Train - Epoch 27, Batch: 0, Loss: 0.692179
Train - Epoch 28, Batch: 0, Loss: 0.692169
Train - Epoch 29, Batch: 0, Loss: 0.692018
Train - Epoch 30, Batch: 0, Loss: 0.692082
Train - Epoch 31, Batch: 0, Loss: 0.692098
Train - Epoch 32, Batch: 0, Loss: 0.691992
Train - Epoch 33, Batch: 0, Loss: 0.691958
Train - Epoch 34, Batch: 0, Loss: 0.691960
Train - Epoch 35, Batch: 0, Loss: 0.691884
Train - Epoch 36, Batch: 0, Loss: 0.691914
Train - Epoch 37, Batch: 0, Loss: 0.691862
Train - Epoch 38, Batch: 0, Loss: 0.691773
Train - Epoch 39, Batch: 0, Loss: 0.691732
Train - Epoch 40, Batch: 0, Loss: 0.691801
Train - Epoch 41, Batch: 0, Loss: 0.691672
Train - Epoch 42, Batch: 0, Loss: 0.691756
Train - Epoch 43, Batch: 0, Loss: 0.691614
Train - Epoch 44, Batch: 0, Loss: 0.691640
Train - Epoch 45, Batch: 0, Loss: 0.691591
Train - Epoch 46, Batch: 0, Loss: 0.691493
Train - Epoch 47, Batch: 0, Loss: 0.691466
Train - Epoch 48, Batch: 0, Loss: 0.691422
Train - Epoch 49, Batch: 0, Loss: 0.691405
Train - Epoch 50, Batch: 0, Loss: 0.691435
Train - Epoch 51, Batch: 0, Loss: 0.691344
Train - Epoch 52, Batch: 0, Loss: 0.691426
Train - Epoch 53, Batch: 0, Loss: 0.691368
Train - Epoch 54, Batch: 0, Loss: 0.691388
Train - Epoch 55, Batch: 0, Loss: 0.691175
Train - Epoch 56, Batch: 0, Loss: 0.691119
Train - Epoch 57, Batch: 0, Loss: 0.691277
Train - Epoch 58, Batch: 0, Loss: 0.691114
Train - Epoch 59, Batch: 0, Loss: 0.691145
Train - Epoch 60, Batch: 0, Loss: 0.691126
Train - Epoch 61, Batch: 0, Loss: 0.690976
Train - Epoch 62, Batch: 0, Loss: 0.690953
Train - Epoch 63, Batch: 0, Loss: 0.690897
Train - Epoch 64, Batch: 0, Loss: 0.691016
Train - Epoch 65, Batch: 0, Loss: 0.690898
Train - Epoch 66, Batch: 0, Loss: 0.690901
Train - Epoch 67, Batch: 0, Loss: 0.690843
Train - Epoch 68, Batch: 0, Loss: 0.690766
Train - Epoch 69, Batch: 0, Loss: 0.690905
Train - Epoch 70, Batch: 0, Loss: 0.690820
Train - Epoch 71, Batch: 0, Loss: 0.690755
Train - Epoch 72, Batch: 0, Loss: 0.690794
Train - Epoch 73, Batch: 0, Loss: 0.690749
Train - Epoch 74, Batch: 0, Loss: 0.690616
Train - Epoch 75, Batch: 0, Loss: 0.690520
Train - Epoch 76, Batch: 0, Loss: 0.690569
Train - Epoch 77, Batch: 0, Loss: 0.690587
Train - Epoch 78, Batch: 0, Loss: 0.690438
Train - Epoch 79, Batch: 0, Loss: 0.690612
Train - Epoch 80, Batch: 0, Loss: 0.690525
Train - Epoch 81, Batch: 0, Loss: 0.690376
Train - Epoch 82, Batch: 0, Loss: 0.690384
Train - Epoch 83, Batch: 0, Loss: 0.690337
Train - Epoch 84, Batch: 0, Loss: 0.690362
Train - Epoch 85, Batch: 0, Loss: 0.690133
Train - Epoch 86, Batch: 0, Loss: 0.690303
Train - Epoch 87, Batch: 0, Loss: 0.690214
Train - Epoch 88, Batch: 0, Loss: 0.690296
Train - Epoch 89, Batch: 0, Loss: 0.690247
Train - Epoch 90, Batch: 0, Loss: 0.690285
Train - Epoch 91, Batch: 0, Loss: 0.690175
Train - Epoch 92, Batch: 0, Loss: 0.690058
Train - Epoch 93, Batch: 0, Loss: 0.690034
Train - Epoch 94, Batch: 0, Loss: 0.690082
Train - Epoch 95, Batch: 0, Loss: 0.690031
Train - Epoch 96, Batch: 0, Loss: 0.690113
Train - Epoch 97, Batch: 0, Loss: 0.690033
Train - Epoch 98, Batch: 0, Loss: 0.689962
Train - Epoch 99, Batch: 0, Loss: 0.689998
Train - Epoch 100, Batch: 0, Loss: 0.689988
Train - Epoch 101, Batch: 0, Loss: 0.689871
Train - Epoch 102, Batch: 0, Loss: 0.689909
Train - Epoch 103, Batch: 0, Loss: 0.689706
Train - Epoch 104, Batch: 0, Loss: 0.689810
Train - Epoch 105, Batch: 0, Loss: 0.689596
Train - Epoch 106, Batch: 0, Loss: 0.689683
Train - Epoch 107, Batch: 0, Loss: 0.689500
Train - Epoch 108, Batch: 0, Loss: 0.689621
Train - Epoch 109, Batch: 0, Loss: 0.689697
Train - Epoch 110, Batch: 0, Loss: 0.689647
Train - Epoch 111, Batch: 0, Loss: 0.689442
Train - Epoch 112, Batch: 0, Loss: 0.689608
Train - Epoch 113, Batch: 0, Loss: 0.689791
Train - Epoch 114, Batch: 0, Loss: 0.689233
Train - Epoch 115, Batch: 0, Loss: 0.689552
Train - Epoch 116, Batch: 0, Loss: 0.689441
Train - Epoch 117, Batch: 0, Loss: 0.689496
Train - Epoch 118, Batch: 0, Loss: 0.689374
Train - Epoch 119, Batch: 0, Loss: 0.689296
Train - Epoch 120, Batch: 0, Loss: 0.689288
Train - Epoch 121, Batch: 0, Loss: 0.689301
Train - Epoch 122, Batch: 0, Loss: 0.689433
Train - Epoch 123, Batch: 0, Loss: 0.689392
Train - Epoch 124, Batch: 0, Loss: 0.689395
Train - Epoch 125, Batch: 0, Loss: 0.689105
Train - Epoch 126, Batch: 0, Loss: 0.689326
Train - Epoch 127, Batch: 0, Loss: 0.689199
Train - Epoch 128, Batch: 0, Loss: 0.689282
Train - Epoch 129, Batch: 0, Loss: 0.688909
Train - Epoch 130, Batch: 0, Loss: 0.689040
Train - Epoch 131, Batch: 0, Loss: 0.689356
Train - Epoch 132, Batch: 0, Loss: 0.689320
Train - Epoch 133, Batch: 0, Loss: 0.688979
Train - Epoch 134, Batch: 0, Loss: 0.688843
Train - Epoch 135, Batch: 0, Loss: 0.689021
Train - Epoch 136, Batch: 0, Loss: 0.688786
Train - Epoch 137, Batch: 0, Loss: 0.689081
Train - Epoch 138, Batch: 0, Loss: 0.688762
Train - Epoch 139, Batch: 0, Loss: 0.688770
Train - Epoch 140, Batch: 0, Loss: 0.688810
Train - Epoch 141, Batch: 0, Loss: 0.688672
Train - Epoch 142, Batch: 0, Loss: 0.688908
Train - Epoch 143, Batch: 0, Loss: 0.688627
Train - Epoch 144, Batch: 0, Loss: 0.688782
Train - Epoch 145, Batch: 0, Loss: 0.689040
Train - Epoch 146, Batch: 0, Loss: 0.688463
Train - Epoch 147, Batch: 0, Loss: 0.688838
Train - Epoch 148, Batch: 0, Loss: 0.688701
Train - Epoch 149, Batch: 0, Loss: 0.688805
Train - Epoch 150, Batch: 0, Loss: 0.688441
Train - Epoch 151, Batch: 0, Loss: 0.688508
Train - Epoch 152, Batch: 0, Loss: 0.688458
Train - Epoch 153, Batch: 0, Loss: 0.688283
Train - Epoch 154, Batch: 0, Loss: 0.688513
Train - Epoch 155, Batch: 0, Loss: 0.688182
Train - Epoch 156, Batch: 0, Loss: 0.688177
Train - Epoch 157, Batch: 0, Loss: 0.688374
Train - Epoch 158, Batch: 0, Loss: 0.688020
Train - Epoch 159, Batch: 0, Loss: 0.688213
Train - Epoch 160, Batch: 0, Loss: 0.688435
Train - Epoch 161, Batch: 0, Loss: 0.688321
Train - Epoch 162, Batch: 0, Loss: 0.688108
Train - Epoch 163, Batch: 0, Loss: 0.688224
Train - Epoch 164, Batch: 0, Loss: 0.688057
Train - Epoch 165, Batch: 0, Loss: 0.688048
Train - Epoch 166, Batch: 0, Loss: 0.688214
Train - Epoch 167, Batch: 0, Loss: 0.688068
Train - Epoch 168, Batch: 0, Loss: 0.688208
Train - Epoch 169, Batch: 0, Loss: 0.688251
Train - Epoch 170, Batch: 0, Loss: 0.688011
Train - Epoch 171, Batch: 0, Loss: 0.688103
Train - Epoch 172, Batch: 0, Loss: 0.688094
Train - Epoch 173, Batch: 0, Loss: 0.687929
Train - Epoch 174, Batch: 0, Loss: 0.688186
Train - Epoch 175, Batch: 0, Loss: 0.687793
Train - Epoch 176, Batch: 0, Loss: 0.688245
Train - Epoch 177, Batch: 0, Loss: 0.687467
Train - Epoch 178, Batch: 0, Loss: 0.687898
Train - Epoch 179, Batch: 0, Loss: 0.687438
Train - Epoch 180, Batch: 0, Loss: 0.687882
Train - Epoch 181, Batch: 0, Loss: 0.687681
Train - Epoch 182, Batch: 0, Loss: 0.687314/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 183, Batch: 0, Loss: 0.687852
Train - Epoch 184, Batch: 0, Loss: 0.687722
Train - Epoch 185, Batch: 0, Loss: 0.687553
Train - Epoch 186, Batch: 0, Loss: 0.687688
Train - Epoch 187, Batch: 0, Loss: 0.687314
Train - Epoch 188, Batch: 0, Loss: 0.687799
Train - Epoch 189, Batch: 0, Loss: 0.687556
Train - Epoch 190, Batch: 0, Loss: 0.687489
Train - Epoch 191, Batch: 0, Loss: 0.687449
Train - Epoch 192, Batch: 0, Loss: 0.687646
Train - Epoch 193, Batch: 0, Loss: 0.687376
Train - Epoch 194, Batch: 0, Loss: 0.687771
Train - Epoch 195, Batch: 0, Loss: 0.687257
Train - Epoch 196, Batch: 0, Loss: 0.687319
Train - Epoch 197, Batch: 0, Loss: 0.687311
Train - Epoch 198, Batch: 0, Loss: 0.687280
Train - Epoch 199, Batch: 0, Loss: 0.687351
training_time:: 350.5194294452667
training time full:: 350.5194687843323
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.659075
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1214
max_epoch:: 200
batch_size:: 10200
epoch:: 0
tensor([  0,   5,   6,  18,  25,  26,  27,  29,  31,  35,  38,  39,  41,  42,
         44,  46,  52,  56,  57,  58,  59,  63,  64,  66,  69,  71,  75,  79,
         80,  81,  84,  85,  87,  88,  90,  91,  94,  95,  97,  98,  99, 104,
        109, 110, 115, 117, 118, 125, 127, 128])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
epoch:: 60
epoch:: 61
epoch:: 62
epoch:: 63
epoch:: 64
epoch:: 65
epoch:: 66
epoch:: 67
epoch:: 68
epoch:: 69
epoch:: 70
epoch:: 71
epoch:: 72
epoch:: 73
epoch:: 74
epoch:: 75
epoch:: 76
epoch:: 77
epoch:: 78
epoch:: 79
epoch:: 80
epoch:: 81
epoch:: 82
epoch:: 83
epoch:: 84
epoch:: 85
epoch:: 86
epoch:: 87
epoch:: 88
epoch:: 89
epoch:: 90
epoch:: 91
epoch:: 92
epoch:: 93
epoch:: 94
epoch:: 95
epoch:: 96
epoch:: 97
epoch:: 98
epoch:: 99
epoch:: 100
epoch:: 101
epoch:: 102
epoch:: 103
epoch:: 104
epoch:: 105
epoch:: 106
epoch:: 107
epoch:: 108
epoch:: 109
epoch:: 110
epoch:: 111
epoch:: 112
epoch:: 113
epoch:: 114
epoch:: 115
epoch:: 116
epoch:: 117
epoch:: 118
epoch:: 119
epoch:: 120
epoch:: 121
epoch:: 122
epoch:: 123
epoch:: 124
epoch:: 125
epoch:: 126
epoch:: 127
epoch:: 128
epoch:: 129
epoch:: 130
epoch:: 131
epoch:: 132
epoch:: 133
epoch:: 134
epoch:: 135
epoch:: 136
epoch:: 137
epoch:: 138
epoch:: 139
epoch:: 140
epoch:: 141
epoch:: 142
epoch:: 143
epoch:: 144
epoch:: 145
epoch:: 146
epoch:: 147
epoch:: 148
epoch:: 149
epoch:: 150
epoch:: 151
epoch:: 152
epoch:: 153
epoch:: 154
epoch:: 155
epoch:: 156
epoch:: 157
epoch:: 158
epoch:: 159
epoch:: 160
epoch:: 161
epoch:: 162
epoch:: 163
epoch:: 164
epoch:: 165
epoch:: 166
epoch:: 167
epoch:: 168
epoch:: 169
epoch:: 170
epoch:: 171
epoch:: 172
epoch:: 173
epoch:: 174
epoch:: 175
epoch:: 176
epoch:: 177
epoch:: 178
epoch:: 179
epoch:: 180
epoch:: 181
epoch:: 182
epoch:: 183
epoch:: 184
epoch:: 185
epoch:: 186
epoch:: 187
epoch:: 188
epoch:: 189
epoch:: 190
epoch:: 191
epoch:: 192
epoch:: 193
epoch:: 194
epoch:: 195
epoch:: 196
epoch:: 197
epoch:: 198
epoch:: 199
training time is 258.91123628616333
overhead:: 0
overhead2:: 0.4284641742706299
overhead3:: 0
time_baseline:: 258.91129064559937
curr_diff: 0 tensor(0.4211, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.4211, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8587, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.639759
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 2 0.06 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1214
max_epoch:: 200
overhead:: 0
overhead2:: 0.021976947784423828
overhead3:: 0.13237738609313965
overhead4:: 30.256377696990967
overhead5:: 0
memory usage:: 26626981888
time_provenance:: 64.6333999633789
curr_diff: 0 tensor(0.0024, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0024, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.4188, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.4188, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8602, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.642624
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 2 0.06 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1214
max_epoch:: 200
overhead:: 0
overhead2:: 0.025318145751953125
overhead3:: 0.14066028594970703
overhead4:: 36.295554876327515
overhead5:: 0
memory usage:: 26614775808
time_provenance:: 72.10250520706177
curr_diff: 0 tensor(0.0024, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0024, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.4188, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.4188, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8602, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.642624
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 2 0.06 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1214
max_epoch:: 200
overhead:: 0
overhead2:: 0.030487060546875
overhead3:: 0.15274643898010254
overhead4:: 41.62919569015503
overhead5:: 0
memory usage:: 26646376448
time_provenance:: 78.90824842453003
curr_diff: 0 tensor(0.0024, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0024, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.4188, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.4188, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8602, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.642624
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 2 0.06 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1214
max_epoch:: 200
overhead:: 0
overhead2:: 0.03693270683288574
overhead3:: 0.1678318977355957
overhead4:: 54.694567918777466
overhead5:: 0
memory usage:: 26605871104
time_provenance:: 96.0692880153656
curr_diff: 0 tensor(0.0022, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0022, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.4190, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.4190, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8601, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.642624
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 2 0.06 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1214
max_epoch:: 200
overhead:: 0
overhead2:: 0.04443860054016113
overhead3:: 0.18094801902770996
overhead4:: 59.45790505409241
overhead5:: 0
memory usage:: 26620444672
time_provenance:: 102.2796630859375
curr_diff: 0 tensor(0.0022, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0022, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.4190, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.4190, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8601, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.642624
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 2 0.06 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1214
max_epoch:: 200
overhead:: 0
overhead2:: 0.04355454444885254
overhead3:: 0.18126296997070312
overhead4:: 64.67214870452881
overhead5:: 0
memory usage:: 26615406592
time_provenance:: 108.89024829864502
curr_diff: 0 tensor(0.0022, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0022, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.4190, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.4190, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8601, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.642624
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 2 0.06 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1214
max_epoch:: 200
overhead:: 0
overhead2:: 0.09688854217529297
overhead3:: 0.28047776222229004
overhead4:: 129.98899459838867
overhead5:: 0
memory usage:: 26599530496
time_provenance:: 191.68193864822388
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.4198, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.4198, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8596, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.641093
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 2 0.06 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1214
max_epoch:: 200
overhead:: 0
overhead2:: 0.09621310234069824
overhead3:: 0.2925117015838623
overhead4:: 132.9500277042389
overhead5:: 0
memory usage:: 26609745920
time_provenance:: 195.53366041183472
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.4198, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.4198, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8595, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.641093
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 2 0.06 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1214
max_epoch:: 200
overhead:: 0
overhead2:: 0.09865045547485352
overhead3:: 0.29366183280944824
overhead4:: 135.48261618614197
overhead5:: 0
memory usage:: 26603786240
time_provenance:: 198.79662156105042
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.4199, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.4199, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8595, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.641093
adding noise deletion rate:: 0.07
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
tensor([0.5123, 0.5152, 0.5051,  ..., 0.5034, 0.5076, 0.5009],
       dtype=torch.float64, grad_fn=<MaxBackward0>)
tensor(0.8685, dtype=torch.float64) tensor(0., dtype=torch.float64)
Train - Epoch 0, Batch: 0, Loss: 0.693072
Train - Epoch 1, Batch: 0, Loss: 0.693013
Train - Epoch 2, Batch: 0, Loss: 0.692891
Train - Epoch 3, Batch: 0, Loss: 0.692785
Train - Epoch 4, Batch: 0, Loss: 0.692715
Train - Epoch 5, Batch: 0, Loss: 0.692595
Train - Epoch 6, Batch: 0, Loss: 0.692485
Train - Epoch 7, Batch: 0, Loss: 0.692450
Train - Epoch 8, Batch: 0, Loss: 0.692317
Train - Epoch 9, Batch: 0, Loss: 0.692195
Train - Epoch 10, Batch: 0, Loss: 0.692076
Train - Epoch 11, Batch: 0, Loss: 0.692053
Train - Epoch 12, Batch: 0, Loss: 0.691978
Train - Epoch 13, Batch: 0, Loss: 0.691832
Train - Epoch 14, Batch: 0, Loss: 0.691779
Train - Epoch 15, Batch: 0, Loss: 0.691674
Train - Epoch 16, Batch: 0, Loss: 0.691634
Train - Epoch 17, Batch: 0, Loss: 0.691534
Train - Epoch 18, Batch: 0, Loss: 0.691421
Train - Epoch 19, Batch: 0, Loss: 0.691392
Train - Epoch 20, Batch: 0, Loss: 0.691258
Train - Epoch 21, Batch: 0, Loss: 0.691175
Train - Epoch 22, Batch: 0, Loss: 0.691018
Train - Epoch 23, Batch: 0, Loss: 0.690945
Train - Epoch 24, Batch: 0, Loss: 0.690707
Train - Epoch 25, Batch: 0, Loss: 0.690742
Train - Epoch 26, Batch: 0, Loss: 0.690767
Train - Epoch 27, Batch: 0, Loss: 0.690531
Train - Epoch 28, Batch: 0, Loss: 0.690492
Train - Epoch 29, Batch: 0, Loss: 0.690415
Train - Epoch 30, Batch: 0, Loss: 0.690265
Train - Epoch 31, Batch: 0, Loss: 0.690024
Train - Epoch 32, Batch: 0, Loss: 0.690137
Train - Epoch 33, Batch: 0, Loss: 0.690102
Train - Epoch 34, Batch: 0, Loss: 0.689949
Train - Epoch 35, Batch: 0, Loss: 0.689894
Train - Epoch 36, Batch: 0, Loss: 0.689751
Train - Epoch 37, Batch: 0, Loss: 0.689715
Train - Epoch 38, Batch: 0, Loss: 0.689683
Train - Epoch 39, Batch: 0, Loss: 0.689541
Train - Epoch 40, Batch: 0, Loss: 0.689395
Train - Epoch 41, Batch: 0, Loss: 0.689327
Train - Epoch 42, Batch: 0, Loss: 0.689067
Train - Epoch 43, Batch: 0, Loss: 0.689192
Train - Epoch 44, Batch: 0, Loss: 0.688973
Train - Epoch 45, Batch: 0, Loss: 0.689009
Train - Epoch 46, Batch: 0, Loss: 0.689216
Train - Epoch 47, Batch: 0, Loss: 0.688686
Train - Epoch 48, Batch: 0, Loss: 0.688898
Train - Epoch 49, Batch: 0, Loss: 0.688602
Train - Epoch 50, Batch: 0, Loss: 0.688456
Train - Epoch 51, Batch: 0, Loss: 0.688609
Train - Epoch 52, Batch: 0, Loss: 0.688454
Train - Epoch 53, Batch: 0, Loss: 0.688468
Train - Epoch 54, Batch: 0, Loss: 0.688089
Train - Epoch 55, Batch: 0, Loss: 0.688239
Train - Epoch 56, Batch: 0, Loss: 0.688240
Train - Epoch 57, Batch: 0, Loss: 0.688173
Train - Epoch 58, Batch: 0, Loss: 0.687794
Train - Epoch 59, Batch: 0, Loss: 0.688017
Train - Epoch 60, Batch: 0, Loss: 0.687847
Train - Epoch 61, Batch: 0, Loss: 0.687757
Train - Epoch 62, Batch: 0, Loss: 0.687787
Train - Epoch 63, Batch: 0, Loss: 0.687502
Train - Epoch 64, Batch: 0, Loss: 0.687303
Train - Epoch 65, Batch: 0, Loss: 0.687380
Train - Epoch 66, Batch: 0, Loss: 0.687604
Train - Epoch 67, Batch: 0, Loss: 0.687039
Train - Epoch 68, Batch: 0, Loss: 0.687159
Train - Epoch 69, Batch: 0, Loss: 0.686953
Train - Epoch 70, Batch: 0, Loss: 0.687444
Train - Epoch 71, Batch: 0, Loss: 0.687018
Train - Epoch 72, Batch: 0, Loss: 0.687074
Train - Epoch 73, Batch: 0, Loss: 0.686688
Train - Epoch 74, Batch: 0, Loss: 0.686430
Train - Epoch 75, Batch: 0, Loss: 0.686656
Train - Epoch 76, Batch: 0, Loss: 0.686650
Train - Epoch 77, Batch: 0, Loss: 0.686621
Train - Epoch 78, Batch: 0, Loss: 0.686559
Train - Epoch 79, Batch: 0, Loss: 0.685830
Train - Epoch 80, Batch: 0, Loss: 0.686197
Train - Epoch 81, Batch: 0, Loss: 0.686062
Train - Epoch 82, Batch: 0, Loss: 0.686255
Train - Epoch 83, Batch: 0, Loss: 0.686104
Train - Epoch 84, Batch: 0, Loss: 0.686269
Train - Epoch 85, Batch: 0, Loss: 0.686356
Train - Epoch 86, Batch: 0, Loss: 0.686029
Train - Epoch 87, Batch: 0, Loss: 0.685963
Train - Epoch 88, Batch: 0, Loss: 0.685878
Train - Epoch 89, Batch: 0, Loss: 0.685860
Train - Epoch 90, Batch: 0, Loss: 0.685594
Train - Epoch 91, Batch: 0, Loss: 0.685515
Train - Epoch 92, Batch: 0, Loss: 0.685900
Train - Epoch 93, Batch: 0, Loss: 0.685420
Train - Epoch 94, Batch: 0, Loss: 0.685288
Train - Epoch 95, Batch: 0, Loss: 0.685254
Train - Epoch 96, Batch: 0, Loss: 0.685052
Train - Epoch 97, Batch: 0, Loss: 0.685278
Train - Epoch 98, Batch: 0, Loss: 0.685133
Train - Epoch 99, Batch: 0, Loss: 0.685024
Train - Epoch 100, Batch: 0, Loss: 0.684976
Train - Epoch 101, Batch: 0, Loss: 0.685043
Train - Epoch 102, Batch: 0, Loss: 0.684452
Train - Epoch 103, Batch: 0, Loss: 0.684506
Train - Epoch 104, Batch: 0, Loss: 0.684693
Train - Epoch 105, Batch: 0, Loss: 0.684449
Train - Epoch 106, Batch: 0, Loss: 0.684482
Train - Epoch 107, Batch: 0, Loss: 0.684818
Train - Epoch 108, Batch: 0, Loss: 0.684233
Train - Epoch 109, Batch: 0, Loss: 0.684108
Train - Epoch 110, Batch: 0, Loss: 0.684010
Train - Epoch 111, Batch: 0, Loss: 0.684793
Train - Epoch 112, Batch: 0, Loss: 0.684107
Train - Epoch 113, Batch: 0, Loss: 0.683941
Train - Epoch 114, Batch: 0, Loss: 0.683696
Train - Epoch 115, Batch: 0, Loss: 0.683591
Train - Epoch 116, Batch: 0, Loss: 0.683397
Train - Epoch 117, Batch: 0, Loss: 0.683639
Train - Epoch 118, Batch: 0, Loss: 0.683894
Train - Epoch 119, Batch: 0, Loss: 0.683461
Train - Epoch 120, Batch: 0, Loss: 0.683931
Train - Epoch 121, Batch: 0, Loss: 0.683512
Train - Epoch 122, Batch: 0, Loss: 0.683302
Train - Epoch 123, Batch: 0, Loss: 0.683569
Train - Epoch 124, Batch: 0, Loss: 0.683113
Train - Epoch 125, Batch: 0, Loss: 0.683032
Train - Epoch 126, Batch: 0, Loss: 0.683458
Train - Epoch 127, Batch: 0, Loss: 0.683101
Train - Epoch 128, Batch: 0, Loss: 0.683027
Train - Epoch 129, Batch: 0, Loss: 0.683103
Train - Epoch 130, Batch: 0, Loss: 0.682887
Train - Epoch 131, Batch: 0, Loss: 0.682317
Train - Epoch 132, Batch: 0, Loss: 0.682769
Train - Epoch 133, Batch: 0, Loss: 0.682927
Train - Epoch 134, Batch: 0, Loss: 0.682301
Train - Epoch 135, Batch: 0, Loss: 0.683088
Train - Epoch 136, Batch: 0, Loss: 0.682512
Train - Epoch 137, Batch: 0, Loss: 0.682196
Train - Epoch 138, Batch: 0, Loss: 0.682773
Train - Epoch 139, Batch: 0, Loss: 0.682599
Train - Epoch 140, Batch: 0, Loss: 0.681962
Train - Epoch 141, Batch: 0, Loss: 0.682096
Train - Epoch 142, Batch: 0, Loss: 0.681766
Train - Epoch 143, Batch: 0, Loss: 0.682402
Train - Epoch 144, Batch: 0, Loss: 0.681353
Train - Epoch 145, Batch: 0, Loss: 0.681865
Train - Epoch 146, Batch: 0, Loss: 0.681972
Train - Epoch 147, Batch: 0, Loss: 0.682164
Train - Epoch 148, Batch: 0, Loss: 0.681286
Train - Epoch 149, Batch: 0, Loss: 0.681729
Train - Epoch 150, Batch: 0, Loss: 0.681802
Train - Epoch 151, Batch: 0, Loss: 0.681441
Train - Epoch 152, Batch: 0, Loss: 0.681380
Train - Epoch 153, Batch: 0, Loss: 0.681925
Train - Epoch 154, Batch: 0, Loss: 0.680919
Train - Epoch 155, Batch: 0, Loss: 0.681489
Train - Epoch 156, Batch: 0, Loss: 0.681304
Train - Epoch 157, Batch: 0, Loss: 0.681603
Train - Epoch 158, Batch: 0, Loss: 0.681396
Train - Epoch 159, Batch: 0, Loss: 0.681433
Train - Epoch 160, Batch: 0, Loss: 0.681394
Train - Epoch 161, Batch: 0, Loss: 0.681485
Train - Epoch 162, Batch: 0, Loss: 0.681350
Train - Epoch 163, Batch: 0, Loss: 0.680822
Train - Epoch 164, Batch: 0, Loss: 0.680992
Train - Epoch 165, Batch: 0, Loss: 0.681141
Train - Epoch 166, Batch: 0, Loss: 0.680373
Train - Epoch 167, Batch: 0, Loss: 0.680961
Train - Epoch 168, Batch: 0, Loss: 0.680700
Train - Epoch 169, Batch: 0, Loss: 0.680441
Train - Epoch 170, Batch: 0, Loss: 0.680394
Train - Epoch 171, Batch: 0, Loss: 0.681176
Train - Epoch 172, Batch: 0, Loss: 0.680737
Train - Epoch 173, Batch: 0, Loss: 0.681007
Train - Epoch 174, Batch: 0, Loss: 0.680931
Train - Epoch 175, Batch: 0, Loss: 0.680484
Train - Epoch 176, Batch: 0, Loss: 0.680160
Train - Epoch 177, Batch: 0, Loss: 0.679996
Train - Epoch 178, Batch: 0, Loss: 0.679618
Train - Epoch 179, Batch: 0, Loss: 0.680384
Train - Epoch 180, Batch: 0, Loss: 0.680634
Train - Epoch 181, Batch: 0, Loss: 0.679896
Train - Epoch 182, Batch: 0, Loss: 0.680801/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 183, Batch: 0, Loss: 0.679505
Train - Epoch 184, Batch: 0, Loss: 0.680583
Train - Epoch 185, Batch: 0, Loss: 0.679874
Train - Epoch 186, Batch: 0, Loss: 0.680175
Train - Epoch 187, Batch: 0, Loss: 0.680309
Train - Epoch 188, Batch: 0, Loss: 0.679333
Train - Epoch 189, Batch: 0, Loss: 0.679899
Train - Epoch 190, Batch: 0, Loss: 0.679457
Train - Epoch 191, Batch: 0, Loss: 0.680221
Train - Epoch 192, Batch: 0, Loss: 0.679629
Train - Epoch 193, Batch: 0, Loss: 0.678960
Train - Epoch 194, Batch: 0, Loss: 0.678866
Train - Epoch 195, Batch: 0, Loss: 0.679860
Train - Epoch 196, Batch: 0, Loss: 0.679820
Train - Epoch 197, Batch: 0, Loss: 0.680178
Train - Epoch 198, Batch: 0, Loss: 0.679287
Train - Epoch 199, Batch: 0, Loss: 0.679100
training_time:: 351.2982187271118
training time full:: 351.2982556819916
provenance prepare time:: 0.0
here
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.483796
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1416
max_epoch:: 200
batch_size:: 10200
epoch:: 0
tensor([  0,   5,   6,  18,  25,  26,  27,  29,  31,  39,  41,  42,  44,  46,
         52,  56,  57,  58,  59,  64,  66,  69,  71,  75,  79,  80,  81,  84,
         85,  87,  88,  90,  91,  94,  95,  97,  98,  99, 104, 109, 110, 115,
        117, 118, 125, 128, 129, 131, 138, 139])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
epoch:: 60
epoch:: 61
epoch:: 62
epoch:: 63
epoch:: 64
epoch:: 65
epoch:: 66
epoch:: 67
epoch:: 68
epoch:: 69
epoch:: 70
epoch:: 71
epoch:: 72
epoch:: 73
epoch:: 74
epoch:: 75
epoch:: 76
epoch:: 77
epoch:: 78
epoch:: 79
epoch:: 80
epoch:: 81
epoch:: 82
epoch:: 83
epoch:: 84
epoch:: 85
epoch:: 86
epoch:: 87
epoch:: 88
epoch:: 89
epoch:: 90
epoch:: 91
epoch:: 92
epoch:: 93
epoch:: 94
epoch:: 95
epoch:: 96
epoch:: 97
epoch:: 98
epoch:: 99
epoch:: 100
epoch:: 101
epoch:: 102
epoch:: 103
epoch:: 104
epoch:: 105
epoch:: 106
epoch:: 107
epoch:: 108
epoch:: 109
epoch:: 110
epoch:: 111
epoch:: 112
epoch:: 113
epoch:: 114
epoch:: 115
epoch:: 116
epoch:: 117
epoch:: 118
epoch:: 119
epoch:: 120
epoch:: 121
epoch:: 122
epoch:: 123
epoch:: 124
epoch:: 125
epoch:: 126
epoch:: 127
epoch:: 128
epoch:: 129
epoch:: 130
epoch:: 131
epoch:: 132
epoch:: 133
epoch:: 134
epoch:: 135
epoch:: 136
epoch:: 137
epoch:: 138
epoch:: 139
epoch:: 140
epoch:: 141
epoch:: 142
epoch:: 143
epoch:: 144
epoch:: 145
epoch:: 146
epoch:: 147
epoch:: 148
epoch:: 149
epoch:: 150
epoch:: 151
epoch:: 152
epoch:: 153
epoch:: 154
epoch:: 155
epoch:: 156
epoch:: 157
epoch:: 158
epoch:: 159
epoch:: 160
epoch:: 161
epoch:: 162
epoch:: 163
epoch:: 164
epoch:: 165
epoch:: 166
epoch:: 167
epoch:: 168
epoch:: 169
epoch:: 170
epoch:: 171
epoch:: 172
epoch:: 173
epoch:: 174
epoch:: 175
epoch:: 176
epoch:: 177
epoch:: 178
epoch:: 179
epoch:: 180
epoch:: 181
epoch:: 182
epoch:: 183
epoch:: 184
epoch:: 185
epoch:: 186
epoch:: 187
epoch:: 188
epoch:: 189
epoch:: 190
epoch:: 191
epoch:: 192
epoch:: 193
epoch:: 194
epoch:: 195
epoch:: 196
epoch:: 197
epoch:: 198
epoch:: 199
training time is 256.9178307056427
overhead:: 0
overhead2:: 0.43422722816467285
overhead3:: 0
time_baseline:: 256.9179015159607
curr_diff: 0 tensor(0.4495, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.4495, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8918, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.587738
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 2 0.07 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1416
max_epoch:: 200
overhead:: 0
overhead2:: 0.02091193199157715
overhead3:: 0.13284945487976074
overhead4:: 29.908522844314575
overhead5:: 0
memory usage:: 26601705472
time_provenance:: 68.08132910728455
curr_diff: 0 tensor(0.0030, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0030, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.4466, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.4466, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8935, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.585565
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 2 0.07 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1416
max_epoch:: 200
overhead:: 0
overhead2:: 0.02448582649230957
overhead3:: 0.14159655570983887
overhead4:: 35.77279448509216
overhead5:: 0
memory usage:: 26607730688
time_provenance:: 75.74103236198425
curr_diff: 0 tensor(0.0030, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0030, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.4466, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.4466, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8934, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.585565
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 2 0.07 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1416
max_epoch:: 200
overhead:: 0
overhead2:: 0.02883124351501465
overhead3:: 0.1501004695892334
overhead4:: 41.14872217178345
overhead5:: 0
memory usage:: 26614956032
time_provenance:: 82.49390721321106
curr_diff: 0 tensor(0.0030, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0030, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.4466, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.4466, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8934, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.585565
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 2 0.07 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1416
max_epoch:: 200
overhead:: 0
overhead2:: 0.039374589920043945
overhead3:: 0.1719188690185547
overhead4:: 54.08600473403931
overhead5:: 0
memory usage:: 26615971840
time_provenance:: 99.12940263748169
curr_diff: 0 tensor(0.0028, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0028, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.4468, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.4468, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8933, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.585762
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 2 0.07 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1416
max_epoch:: 200
overhead:: 0
overhead2:: 0.043074846267700195
overhead3:: 0.17615962028503418
overhead4:: 59.256521463394165
overhead5:: 0
memory usage:: 26603855872
time_provenance:: 105.95118761062622
curr_diff: 0 tensor(0.0028, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0028, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.4468, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.4468, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8933, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.585762
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 2 0.07 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1416
max_epoch:: 200
overhead:: 0
overhead2:: 0.044887542724609375
overhead3:: 0.18597984313964844
overhead4:: 63.916808128356934
overhead5:: 0
memory usage:: 26600951808
time_provenance:: 111.51382446289062
curr_diff: 0 tensor(0.0028, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0028, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.4468, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.4468, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8933, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.585812
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 2 0.07 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1416
max_epoch:: 200
overhead:: 0
overhead2:: 0.09611320495605469
overhead3:: 0.28572988510131836
overhead4:: 129.14380812644958
overhead5:: 0
memory usage:: 26616287232
time_provenance:: 193.83076858520508
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.4479, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.4479, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8927, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.586750
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 2 0.07 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1416
max_epoch:: 200
overhead:: 0
overhead2:: 0.09756135940551758
overhead3:: 0.28910326957702637
overhead4:: 131.28898072242737
overhead5:: 0
memory usage:: 26601963520
time_provenance:: 196.69046139717102
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.4479, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.4479, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8927, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.586750
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 2 0.07 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1416
max_epoch:: 200
overhead:: 0
overhead2:: 0.13851332664489746
overhead3:: 0.29911041259765625
overhead4:: 134.00058674812317
overhead5:: 0
memory usage:: 26600435712
time_provenance:: 199.9531319141388
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.4479, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.4479, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8927, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.586750
adding noise deletion rate:: 0.08
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
tensor([0.5254, 0.5348, 0.5085,  ..., 0.5266, 0.5140, 0.5152],
       dtype=torch.float64, grad_fn=<MaxBackward0>)
tensor(0.8685, dtype=torch.float64) tensor(0., dtype=torch.float64)
Train - Epoch 0, Batch: 0, Loss: 0.693155
Train - Epoch 1, Batch: 0, Loss: 0.693116
Train - Epoch 2, Batch: 0, Loss: 0.693062
Train - Epoch 3, Batch: 0, Loss: 0.693028
Train - Epoch 4, Batch: 0, Loss: 0.692970
Train - Epoch 5, Batch: 0, Loss: 0.692918
Train - Epoch 6, Batch: 0, Loss: 0.692837
Train - Epoch 7, Batch: 0, Loss: 0.692796
Train - Epoch 8, Batch: 0, Loss: 0.692770
Train - Epoch 9, Batch: 0, Loss: 0.692705
Train - Epoch 10, Batch: 0, Loss: 0.692671
Train - Epoch 11, Batch: 0, Loss: 0.692627
Train - Epoch 12, Batch: 0, Loss: 0.692549
Train - Epoch 13, Batch: 0, Loss: 0.692507
Train - Epoch 14, Batch: 0, Loss: 0.692444
Train - Epoch 15, Batch: 0, Loss: 0.692375
Train - Epoch 16, Batch: 0, Loss: 0.692397
Train - Epoch 17, Batch: 0, Loss: 0.692343
Train - Epoch 18, Batch: 0, Loss: 0.692250
Train - Epoch 19, Batch: 0, Loss: 0.692204
Train - Epoch 20, Batch: 0, Loss: 0.692140
Train - Epoch 21, Batch: 0, Loss: 0.692143
Train - Epoch 22, Batch: 0, Loss: 0.692113
Train - Epoch 23, Batch: 0, Loss: 0.692040
Train - Epoch 24, Batch: 0, Loss: 0.692016
Train - Epoch 25, Batch: 0, Loss: 0.691966
Train - Epoch 26, Batch: 0, Loss: 0.691847
Train - Epoch 27, Batch: 0, Loss: 0.691844
Train - Epoch 28, Batch: 0, Loss: 0.691787
Train - Epoch 29, Batch: 0, Loss: 0.691622
Train - Epoch 30, Batch: 0, Loss: 0.691730
Train - Epoch 31, Batch: 0, Loss: 0.691704
Train - Epoch 32, Batch: 0, Loss: 0.691562
Train - Epoch 33, Batch: 0, Loss: 0.691458
Train - Epoch 34, Batch: 0, Loss: 0.691487
Train - Epoch 35, Batch: 0, Loss: 0.691393
Train - Epoch 36, Batch: 0, Loss: 0.691517
Train - Epoch 37, Batch: 0, Loss: 0.691371
Train - Epoch 38, Batch: 0, Loss: 0.691225
Train - Epoch 39, Batch: 0, Loss: 0.691162
Train - Epoch 40, Batch: 0, Loss: 0.691295
Train - Epoch 41, Batch: 0, Loss: 0.691187
Train - Epoch 42, Batch: 0, Loss: 0.691255
Train - Epoch 43, Batch: 0, Loss: 0.691009
Train - Epoch 44, Batch: 0, Loss: 0.691120
Train - Epoch 45, Batch: 0, Loss: 0.691036
Train - Epoch 46, Batch: 0, Loss: 0.690882
Train - Epoch 47, Batch: 0, Loss: 0.690829
Train - Epoch 48, Batch: 0, Loss: 0.690712
Train - Epoch 49, Batch: 0, Loss: 0.690715
Train - Epoch 50, Batch: 0, Loss: 0.690771
Train - Epoch 51, Batch: 0, Loss: 0.690653
Train - Epoch 52, Batch: 0, Loss: 0.690779
Train - Epoch 53, Batch: 0, Loss: 0.690696
Train - Epoch 54, Batch: 0, Loss: 0.690731
Train - Epoch 55, Batch: 0, Loss: 0.690486
Train - Epoch 56, Batch: 0, Loss: 0.690408
Train - Epoch 57, Batch: 0, Loss: 0.690596
Train - Epoch 58, Batch: 0, Loss: 0.690437
Train - Epoch 59, Batch: 0, Loss: 0.690428
Train - Epoch 60, Batch: 0, Loss: 0.690365
Train - Epoch 61, Batch: 0, Loss: 0.690154
Train - Epoch 62, Batch: 0, Loss: 0.690166
Train - Epoch 63, Batch: 0, Loss: 0.690157
Train - Epoch 64, Batch: 0, Loss: 0.690217
Train - Epoch 65, Batch: 0, Loss: 0.690196
Train - Epoch 66, Batch: 0, Loss: 0.690039
Train - Epoch 67, Batch: 0, Loss: 0.689939
Train - Epoch 68, Batch: 0, Loss: 0.689936
Train - Epoch 69, Batch: 0, Loss: 0.690126
Train - Epoch 70, Batch: 0, Loss: 0.690014
Train - Epoch 71, Batch: 0, Loss: 0.689926
Train - Epoch 72, Batch: 0, Loss: 0.689970
Train - Epoch 73, Batch: 0, Loss: 0.689886
Train - Epoch 74, Batch: 0, Loss: 0.689700
Train - Epoch 75, Batch: 0, Loss: 0.689665
Train - Epoch 76, Batch: 0, Loss: 0.689609
Train - Epoch 77, Batch: 0, Loss: 0.689597
Train - Epoch 78, Batch: 0, Loss: 0.689474
Train - Epoch 79, Batch: 0, Loss: 0.689725
Train - Epoch 80, Batch: 0, Loss: 0.689558
Train - Epoch 81, Batch: 0, Loss: 0.689380
Train - Epoch 82, Batch: 0, Loss: 0.689411
Train - Epoch 83, Batch: 0, Loss: 0.689352
Train - Epoch 84, Batch: 0, Loss: 0.689424
Train - Epoch 85, Batch: 0, Loss: 0.689127
Train - Epoch 86, Batch: 0, Loss: 0.689320
Train - Epoch 87, Batch: 0, Loss: 0.689226
Train - Epoch 88, Batch: 0, Loss: 0.689308
Train - Epoch 89, Batch: 0, Loss: 0.689260
Train - Epoch 90, Batch: 0, Loss: 0.689328
Train - Epoch 91, Batch: 0, Loss: 0.689154
Train - Epoch 92, Batch: 0, Loss: 0.688946
Train - Epoch 93, Batch: 0, Loss: 0.688901
Train - Epoch 94, Batch: 0, Loss: 0.688978
Train - Epoch 95, Batch: 0, Loss: 0.688924
Train - Epoch 96, Batch: 0, Loss: 0.689105
Train - Epoch 97, Batch: 0, Loss: 0.688901
Train - Epoch 98, Batch: 0, Loss: 0.688792
Train - Epoch 99, Batch: 0, Loss: 0.688856
Train - Epoch 100, Batch: 0, Loss: 0.688775
Train - Epoch 101, Batch: 0, Loss: 0.688584
Train - Epoch 102, Batch: 0, Loss: 0.688755
Train - Epoch 103, Batch: 0, Loss: 0.688587
Train - Epoch 104, Batch: 0, Loss: 0.688668
Train - Epoch 105, Batch: 0, Loss: 0.688325
Train - Epoch 106, Batch: 0, Loss: 0.688400
Train - Epoch 107, Batch: 0, Loss: 0.688250
Train - Epoch 108, Batch: 0, Loss: 0.688520
Train - Epoch 109, Batch: 0, Loss: 0.688400
Train - Epoch 110, Batch: 0, Loss: 0.688436
Train - Epoch 111, Batch: 0, Loss: 0.688165
Train - Epoch 112, Batch: 0, Loss: 0.688350
Train - Epoch 113, Batch: 0, Loss: 0.688585
Train - Epoch 114, Batch: 0, Loss: 0.687836
Train - Epoch 115, Batch: 0, Loss: 0.688350
Train - Epoch 116, Batch: 0, Loss: 0.688209
Train - Epoch 117, Batch: 0, Loss: 0.688146
Train - Epoch 118, Batch: 0, Loss: 0.688152
Train - Epoch 119, Batch: 0, Loss: 0.688017
Train - Epoch 120, Batch: 0, Loss: 0.687928
Train - Epoch 121, Batch: 0, Loss: 0.687979
Train - Epoch 122, Batch: 0, Loss: 0.688057
Train - Epoch 123, Batch: 0, Loss: 0.688123
Train - Epoch 124, Batch: 0, Loss: 0.687953
Train - Epoch 125, Batch: 0, Loss: 0.687797
Train - Epoch 126, Batch: 0, Loss: 0.688057
Train - Epoch 127, Batch: 0, Loss: 0.687884
Train - Epoch 128, Batch: 0, Loss: 0.688055
Train - Epoch 129, Batch: 0, Loss: 0.687550
Train - Epoch 130, Batch: 0, Loss: 0.687498
Train - Epoch 131, Batch: 0, Loss: 0.688075
Train - Epoch 132, Batch: 0, Loss: 0.687977
Train - Epoch 133, Batch: 0, Loss: 0.687554
Train - Epoch 134, Batch: 0, Loss: 0.687422
Train - Epoch 135, Batch: 0, Loss: 0.687486
Train - Epoch 136, Batch: 0, Loss: 0.687207
Train - Epoch 137, Batch: 0, Loss: 0.687809
Train - Epoch 138, Batch: 0, Loss: 0.687268
Train - Epoch 139, Batch: 0, Loss: 0.687350
Train - Epoch 140, Batch: 0, Loss: 0.687300
Train - Epoch 141, Batch: 0, Loss: 0.687297
Train - Epoch 142, Batch: 0, Loss: 0.687439
Train - Epoch 143, Batch: 0, Loss: 0.687142
Train - Epoch 144, Batch: 0, Loss: 0.687287
Train - Epoch 145, Batch: 0, Loss: 0.687738
Train - Epoch 146, Batch: 0, Loss: 0.686997
Train - Epoch 147, Batch: 0, Loss: 0.687421
Train - Epoch 148, Batch: 0, Loss: 0.687122
Train - Epoch 149, Batch: 0, Loss: 0.687401
Train - Epoch 150, Batch: 0, Loss: 0.686827
Train - Epoch 151, Batch: 0, Loss: 0.687004
Train - Epoch 152, Batch: 0, Loss: 0.686991
Train - Epoch 153, Batch: 0, Loss: 0.686819
Train - Epoch 154, Batch: 0, Loss: 0.687056
Train - Epoch 155, Batch: 0, Loss: 0.686509
Train - Epoch 156, Batch: 0, Loss: 0.686565
Train - Epoch 157, Batch: 0, Loss: 0.686646
Train - Epoch 158, Batch: 0, Loss: 0.686328
Train - Epoch 159, Batch: 0, Loss: 0.686538
Train - Epoch 160, Batch: 0, Loss: 0.686840
Train - Epoch 161, Batch: 0, Loss: 0.686654
Train - Epoch 162, Batch: 0, Loss: 0.686476
Train - Epoch 163, Batch: 0, Loss: 0.686567
Train - Epoch 164, Batch: 0, Loss: 0.686355
Train - Epoch 165, Batch: 0, Loss: 0.686389
Train - Epoch 166, Batch: 0, Loss: 0.686621
Train - Epoch 167, Batch: 0, Loss: 0.686410
Train - Epoch 168, Batch: 0, Loss: 0.686602
Train - Epoch 169, Batch: 0, Loss: 0.686588
Train - Epoch 170, Batch: 0, Loss: 0.686349
Train - Epoch 171, Batch: 0, Loss: 0.686412
Train - Epoch 172, Batch: 0, Loss: 0.686407
Train - Epoch 173, Batch: 0, Loss: 0.686310
Train - Epoch 174, Batch: 0, Loss: 0.686570
Train - Epoch 175, Batch: 0, Loss: 0.685961
Train - Epoch 176, Batch: 0, Loss: 0.686528
Train - Epoch 177, Batch: 0, Loss: 0.685869
Train - Epoch 178, Batch: 0, Loss: 0.686159
Train - Epoch 179, Batch: 0, Loss: 0.685720
Train - Epoch 180, Batch: 0, Loss: 0.686130
Train - Epoch 181, Batch: 0, Loss: 0.685959
Train - Epoch 182, Batch: 0, Loss: 0.685363/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 183, Batch: 0, Loss: 0.686158
Train - Epoch 184, Batch: 0, Loss: 0.685922
Train - Epoch 185, Batch: 0, Loss: 0.685880
Train - Epoch 186, Batch: 0, Loss: 0.685839
Train - Epoch 187, Batch: 0, Loss: 0.685456
Train - Epoch 188, Batch: 0, Loss: 0.686186
Train - Epoch 189, Batch: 0, Loss: 0.685575
Train - Epoch 190, Batch: 0, Loss: 0.685610
Train - Epoch 191, Batch: 0, Loss: 0.685706
Train - Epoch 192, Batch: 0, Loss: 0.685734
Train - Epoch 193, Batch: 0, Loss: 0.685443
Train - Epoch 194, Batch: 0, Loss: 0.685996
Train - Epoch 195, Batch: 0, Loss: 0.685398
Train - Epoch 196, Batch: 0, Loss: 0.685450
Train - Epoch 197, Batch: 0, Loss: 0.685429
Train - Epoch 198, Batch: 0, Loss: 0.685419
Train - Epoch 199, Batch: 0, Loss: 0.685490
training_time:: 351.59906816482544
training time full:: 351.59910917282104
provenance prepare time:: 0.0
here
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.607055
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1619
max_epoch:: 200
batch_size:: 10200
epoch:: 0
tensor([  0,   5,   6,  18,  25,  27,  29,  31,  39,  41,  42,  44,  46,  52,
         56,  57,  58,  59,  64,  66,  69,  71,  75,  79,  80,  81,  84,  85,
         87,  88,  90,  91,  94,  95,  97,  98,  99, 104, 109, 110, 115, 117,
        118, 125, 128, 129, 131, 138, 139, 140])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
epoch:: 60
epoch:: 61
epoch:: 62
epoch:: 63
epoch:: 64
epoch:: 65
epoch:: 66
epoch:: 67
epoch:: 68
epoch:: 69
epoch:: 70
epoch:: 71
epoch:: 72
epoch:: 73
epoch:: 74
epoch:: 75
epoch:: 76
epoch:: 77
epoch:: 78
epoch:: 79
epoch:: 80
epoch:: 81
epoch:: 82
epoch:: 83
epoch:: 84
epoch:: 85
epoch:: 86
epoch:: 87
epoch:: 88
epoch:: 89
epoch:: 90
epoch:: 91
epoch:: 92
epoch:: 93
epoch:: 94
epoch:: 95
epoch:: 96
epoch:: 97
epoch:: 98
epoch:: 99
epoch:: 100
epoch:: 101
epoch:: 102
epoch:: 103
epoch:: 104
epoch:: 105
epoch:: 106
epoch:: 107
epoch:: 108
epoch:: 109
epoch:: 110
epoch:: 111
epoch:: 112
epoch:: 113
epoch:: 114
epoch:: 115
epoch:: 116
epoch:: 117
epoch:: 118
epoch:: 119
epoch:: 120
epoch:: 121
epoch:: 122
epoch:: 123
epoch:: 124
epoch:: 125
epoch:: 126
epoch:: 127
epoch:: 128
epoch:: 129
epoch:: 130
epoch:: 131
epoch:: 132
epoch:: 133
epoch:: 134
epoch:: 135
epoch:: 136
epoch:: 137
epoch:: 138
epoch:: 139
epoch:: 140
epoch:: 141
epoch:: 142
epoch:: 143
epoch:: 144
epoch:: 145
epoch:: 146
epoch:: 147
epoch:: 148
epoch:: 149
epoch:: 150
epoch:: 151
epoch:: 152
epoch:: 153
epoch:: 154
epoch:: 155
epoch:: 156
epoch:: 157
epoch:: 158
epoch:: 159
epoch:: 160
epoch:: 161
epoch:: 162
epoch:: 163
epoch:: 164
epoch:: 165
epoch:: 166
epoch:: 167
epoch:: 168
epoch:: 169
epoch:: 170
epoch:: 171
epoch:: 172
epoch:: 173
epoch:: 174
epoch:: 175
epoch:: 176
epoch:: 177
epoch:: 178
epoch:: 179
epoch:: 180
epoch:: 181
epoch:: 182
epoch:: 183
epoch:: 184
epoch:: 185
epoch:: 186
epoch:: 187
epoch:: 188
epoch:: 189
epoch:: 190
epoch:: 191
epoch:: 192
epoch:: 193
epoch:: 194
epoch:: 195
epoch:: 196
epoch:: 197
epoch:: 198
epoch:: 199
training time is 254.23524355888367
overhead:: 0
overhead2:: 0.4408590793609619
overhead3:: 0
time_baseline:: 254.23531913757324
curr_diff: 0 tensor(0.5361, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.5361, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.7850, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.642674
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 2 0.08 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1619
max_epoch:: 200
overhead:: 0
overhead2:: 0.021239757537841797
overhead3:: 0.1344902515411377
overhead4:: 29.830255031585693
overhead5:: 0
memory usage:: 26601234432
time_provenance:: 71.55390644073486
curr_diff: 0 tensor(0.0041, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0041, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.5321, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.5321, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.7881, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.646478
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 2 0.08 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1619
max_epoch:: 200
overhead:: 0
overhead2:: 0.02634119987487793
overhead3:: 0.14271092414855957
overhead4:: 35.39876055717468
overhead5:: 0
memory usage:: 26601566208
time_provenance:: 79.40981888771057
curr_diff: 0 tensor(0.0041, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0041, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.5321, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.5321, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.7881, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.646478
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 2 0.08 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1619
max_epoch:: 200
overhead:: 0
overhead2:: 0.030427932739257812
overhead3:: 0.15312957763671875
overhead4:: 40.94198799133301
overhead5:: 0
memory usage:: 26600984576
time_provenance:: 85.88981080055237
curr_diff: 0 tensor(0.0041, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0041, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.5321, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.5321, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.7881, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.646478
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 2 0.08 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1619
max_epoch:: 200
overhead:: 0
overhead2:: 0.04179859161376953
overhead3:: 0.17499852180480957
overhead4:: 53.80017018318176
overhead5:: 0
memory usage:: 26612531200
time_provenance:: 102.56619882583618
curr_diff: 0 tensor(0.0038, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0038, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.5324, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.5324, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.7879, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.646379
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 2 0.08 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1619
max_epoch:: 200
overhead:: 0
overhead2:: 0.04324030876159668
overhead3:: 0.1807260513305664
overhead4:: 58.28945469856262
overhead5:: 0
memory usage:: 26601381888
time_provenance:: 108.0229287147522
curr_diff: 0 tensor(0.0037, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0037, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.5324, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.5324, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.7879, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.646379
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 2 0.08 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1619
max_epoch:: 200
overhead:: 0
overhead2:: 0.04593992233276367
overhead3:: 0.18860793113708496
overhead4:: 62.80964708328247
overhead5:: 0
memory usage:: 26622742528
time_provenance:: 113.75203704833984
curr_diff: 0 tensor(0.0037, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0037, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.5324, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.5324, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.7879, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.646379
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 2 0.08 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1619
max_epoch:: 200
overhead:: 0
overhead2:: 0.09205770492553711
overhead3:: 0.28725743293762207
overhead4:: 127.59797382354736
overhead5:: 0
memory usage:: 26602844160
time_provenance:: 195.25288248062134
curr_diff: 0 tensor(0.0023, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0023, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.5339, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.5339, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.7867, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.644847
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 2 0.08 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1619
max_epoch:: 200
overhead:: 0
overhead2:: 0.09741497039794922
overhead3:: 0.2965242862701416
overhead4:: 129.76977038383484
overhead5:: 0
memory usage:: 26611752960
time_provenance:: 198.0782060623169
curr_diff: 0 tensor(0.0022, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0022, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.5339, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.5339, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.7867, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.644847
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 2 0.08 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 1619
max_epoch:: 200
overhead:: 0
overhead2:: 0.09486937522888184
overhead3:: 0.29506802558898926
overhead4:: 132.32848381996155
overhead5:: 0
memory usage:: 26621067264
time_provenance:: 201.0937647819519
curr_diff: 0 tensor(0.0022, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0022, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.5339, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.5339, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.7867, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.644847
adding noise deletion rate:: 0.1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
tensor([0.5185, 0.5207, 0.5069,  ..., 0.5082, 0.5107, 0.5027],
       dtype=torch.float64, grad_fn=<MaxBackward0>)
tensor(0.8685, dtype=torch.float64) tensor(0., dtype=torch.float64)
Train - Epoch 0, Batch: 0, Loss: 0.693217
Train - Epoch 1, Batch: 0, Loss: 0.693048
Train - Epoch 2, Batch: 0, Loss: 0.692925
Train - Epoch 3, Batch: 0, Loss: 0.692753
Train - Epoch 4, Batch: 0, Loss: 0.692570
Train - Epoch 5, Batch: 0, Loss: 0.692407
Train - Epoch 6, Batch: 0, Loss: 0.692270
Train - Epoch 7, Batch: 0, Loss: 0.692140
Train - Epoch 8, Batch: 0, Loss: 0.691937
Train - Epoch 9, Batch: 0, Loss: 0.691746
Train - Epoch 10, Batch: 0, Loss: 0.691568
Train - Epoch 11, Batch: 0, Loss: 0.691463
Train - Epoch 12, Batch: 0, Loss: 0.691357
Train - Epoch 13, Batch: 0, Loss: 0.691153
Train - Epoch 14, Batch: 0, Loss: 0.691058
Train - Epoch 15, Batch: 0, Loss: 0.690798
Train - Epoch 16, Batch: 0, Loss: 0.690766
Train - Epoch 17, Batch: 0, Loss: 0.690667
Train - Epoch 18, Batch: 0, Loss: 0.690440
Train - Epoch 19, Batch: 0, Loss: 0.690388
Train - Epoch 20, Batch: 0, Loss: 0.690227
Train - Epoch 21, Batch: 0, Loss: 0.690037
Train - Epoch 22, Batch: 0, Loss: 0.689841
Train - Epoch 23, Batch: 0, Loss: 0.689656
Train - Epoch 24, Batch: 0, Loss: 0.689400
Train - Epoch 25, Batch: 0, Loss: 0.689320
Train - Epoch 26, Batch: 0, Loss: 0.689409
Train - Epoch 27, Batch: 0, Loss: 0.689139
Train - Epoch 28, Batch: 0, Loss: 0.688952
Train - Epoch 29, Batch: 0, Loss: 0.688810
Train - Epoch 30, Batch: 0, Loss: 0.688623
Train - Epoch 31, Batch: 0, Loss: 0.688278
Train - Epoch 32, Batch: 0, Loss: 0.688291
Train - Epoch 33, Batch: 0, Loss: 0.688304
Train - Epoch 34, Batch: 0, Loss: 0.688151
Train - Epoch 35, Batch: 0, Loss: 0.687875
Train - Epoch 36, Batch: 0, Loss: 0.687821
Train - Epoch 37, Batch: 0, Loss: 0.687753
Train - Epoch 38, Batch: 0, Loss: 0.687640
Train - Epoch 39, Batch: 0, Loss: 0.687328
Train - Epoch 40, Batch: 0, Loss: 0.687183
Train - Epoch 41, Batch: 0, Loss: 0.687010
Train - Epoch 42, Batch: 0, Loss: 0.686808
Train - Epoch 43, Batch: 0, Loss: 0.686984
Train - Epoch 44, Batch: 0, Loss: 0.686489
Train - Epoch 45, Batch: 0, Loss: 0.686627
Train - Epoch 46, Batch: 0, Loss: 0.686845
Train - Epoch 47, Batch: 0, Loss: 0.686154
Train - Epoch 48, Batch: 0, Loss: 0.686307
Train - Epoch 49, Batch: 0, Loss: 0.685769
Train - Epoch 50, Batch: 0, Loss: 0.685714
Train - Epoch 51, Batch: 0, Loss: 0.685781
Train - Epoch 52, Batch: 0, Loss: 0.685797
Train - Epoch 53, Batch: 0, Loss: 0.685676
Train - Epoch 54, Batch: 0, Loss: 0.685188
Train - Epoch 55, Batch: 0, Loss: 0.685252
Train - Epoch 56, Batch: 0, Loss: 0.685284
Train - Epoch 57, Batch: 0, Loss: 0.685179
Train - Epoch 58, Batch: 0, Loss: 0.684772
Train - Epoch 59, Batch: 0, Loss: 0.684810
Train - Epoch 60, Batch: 0, Loss: 0.684658
Train - Epoch 61, Batch: 0, Loss: 0.684610
Train - Epoch 62, Batch: 0, Loss: 0.684437
Train - Epoch 63, Batch: 0, Loss: 0.684167
Train - Epoch 64, Batch: 0, Loss: 0.683951
Train - Epoch 65, Batch: 0, Loss: 0.683943
Train - Epoch 66, Batch: 0, Loss: 0.684179
Train - Epoch 67, Batch: 0, Loss: 0.683310
Train - Epoch 68, Batch: 0, Loss: 0.683543
Train - Epoch 69, Batch: 0, Loss: 0.683444
Train - Epoch 70, Batch: 0, Loss: 0.683645
Train - Epoch 71, Batch: 0, Loss: 0.683323
Train - Epoch 72, Batch: 0, Loss: 0.683125
Train - Epoch 73, Batch: 0, Loss: 0.682904
Train - Epoch 74, Batch: 0, Loss: 0.682521
Train - Epoch 75, Batch: 0, Loss: 0.682719
Train - Epoch 76, Batch: 0, Loss: 0.682640
Train - Epoch 77, Batch: 0, Loss: 0.682752
Train - Epoch 78, Batch: 0, Loss: 0.682534
Train - Epoch 79, Batch: 0, Loss: 0.681604
Train - Epoch 80, Batch: 0, Loss: 0.682105
Train - Epoch 81, Batch: 0, Loss: 0.682033
Train - Epoch 82, Batch: 0, Loss: 0.681946
Train - Epoch 83, Batch: 0, Loss: 0.681996
Train - Epoch 84, Batch: 0, Loss: 0.682253
Train - Epoch 85, Batch: 0, Loss: 0.682291
Train - Epoch 86, Batch: 0, Loss: 0.681529
Train - Epoch 87, Batch: 0, Loss: 0.681371
Train - Epoch 88, Batch: 0, Loss: 0.681382
Train - Epoch 89, Batch: 0, Loss: 0.681316
Train - Epoch 90, Batch: 0, Loss: 0.681145
Train - Epoch 91, Batch: 0, Loss: 0.680880
Train - Epoch 92, Batch: 0, Loss: 0.681522
Train - Epoch 93, Batch: 0, Loss: 0.680841
Train - Epoch 94, Batch: 0, Loss: 0.680742
Train - Epoch 95, Batch: 0, Loss: 0.680738
Train - Epoch 96, Batch: 0, Loss: 0.680432
Train - Epoch 97, Batch: 0, Loss: 0.680372
Train - Epoch 98, Batch: 0, Loss: 0.680204
Train - Epoch 99, Batch: 0, Loss: 0.680082
Train - Epoch 100, Batch: 0, Loss: 0.679894
Train - Epoch 101, Batch: 0, Loss: 0.679956
Train - Epoch 102, Batch: 0, Loss: 0.679369
Train - Epoch 103, Batch: 0, Loss: 0.679697
Train - Epoch 104, Batch: 0, Loss: 0.679878
Train - Epoch 105, Batch: 0, Loss: 0.679325
Train - Epoch 106, Batch: 0, Loss: 0.679204
Train - Epoch 107, Batch: 0, Loss: 0.679542
Train - Epoch 108, Batch: 0, Loss: 0.679017
Train - Epoch 109, Batch: 0, Loss: 0.678721
Train - Epoch 110, Batch: 0, Loss: 0.678574
Train - Epoch 111, Batch: 0, Loss: 0.679489
Train - Epoch 112, Batch: 0, Loss: 0.678824
Train - Epoch 113, Batch: 0, Loss: 0.678555
Train - Epoch 114, Batch: 0, Loss: 0.678432
Train - Epoch 115, Batch: 0, Loss: 0.678174
Train - Epoch 116, Batch: 0, Loss: 0.677547
Train - Epoch 117, Batch: 0, Loss: 0.677867
Train - Epoch 118, Batch: 0, Loss: 0.678023
Train - Epoch 119, Batch: 0, Loss: 0.677814
Train - Epoch 120, Batch: 0, Loss: 0.678257
Train - Epoch 121, Batch: 0, Loss: 0.677816
Train - Epoch 122, Batch: 0, Loss: 0.677730
Train - Epoch 123, Batch: 0, Loss: 0.677870
Train - Epoch 124, Batch: 0, Loss: 0.677292
Train - Epoch 125, Batch: 0, Loss: 0.677035
Train - Epoch 126, Batch: 0, Loss: 0.677473
Train - Epoch 127, Batch: 0, Loss: 0.677324
Train - Epoch 128, Batch: 0, Loss: 0.677098
Train - Epoch 129, Batch: 0, Loss: 0.677147
Train - Epoch 130, Batch: 0, Loss: 0.676919
Train - Epoch 131, Batch: 0, Loss: 0.676035
Train - Epoch 132, Batch: 0, Loss: 0.676739
Train - Epoch 133, Batch: 0, Loss: 0.676723
Train - Epoch 134, Batch: 0, Loss: 0.676138
Train - Epoch 135, Batch: 0, Loss: 0.676965
Train - Epoch 136, Batch: 0, Loss: 0.675858
Train - Epoch 137, Batch: 0, Loss: 0.675692
Train - Epoch 138, Batch: 0, Loss: 0.676545
Train - Epoch 139, Batch: 0, Loss: 0.676094
Train - Epoch 140, Batch: 0, Loss: 0.675708
Train - Epoch 141, Batch: 0, Loss: 0.675347
Train - Epoch 142, Batch: 0, Loss: 0.675096
Train - Epoch 143, Batch: 0, Loss: 0.675608
Train - Epoch 144, Batch: 0, Loss: 0.674490
Train - Epoch 145, Batch: 0, Loss: 0.675362
Train - Epoch 146, Batch: 0, Loss: 0.675744
Train - Epoch 147, Batch: 0, Loss: 0.675970
Train - Epoch 148, Batch: 0, Loss: 0.674809
Train - Epoch 149, Batch: 0, Loss: 0.674870
Train - Epoch 150, Batch: 0, Loss: 0.674686
Train - Epoch 151, Batch: 0, Loss: 0.674627
Train - Epoch 152, Batch: 0, Loss: 0.674579
Train - Epoch 153, Batch: 0, Loss: 0.675119
Train - Epoch 154, Batch: 0, Loss: 0.673962
Train - Epoch 155, Batch: 0, Loss: 0.674648
Train - Epoch 156, Batch: 0, Loss: 0.674427
Train - Epoch 157, Batch: 0, Loss: 0.674456
Train - Epoch 158, Batch: 0, Loss: 0.674229
Train - Epoch 159, Batch: 0, Loss: 0.674582
Train - Epoch 160, Batch: 0, Loss: 0.674458
Train - Epoch 161, Batch: 0, Loss: 0.674569
Train - Epoch 162, Batch: 0, Loss: 0.674510
Train - Epoch 163, Batch: 0, Loss: 0.673431
Train - Epoch 164, Batch: 0, Loss: 0.673534
Train - Epoch 165, Batch: 0, Loss: 0.673848
Train - Epoch 166, Batch: 0, Loss: 0.672794
Train - Epoch 167, Batch: 0, Loss: 0.673806
Train - Epoch 168, Batch: 0, Loss: 0.673420
Train - Epoch 169, Batch: 0, Loss: 0.673488
Train - Epoch 170, Batch: 0, Loss: 0.672790
Train - Epoch 171, Batch: 0, Loss: 0.673574
Train - Epoch 172, Batch: 0, Loss: 0.673395
Train - Epoch 173, Batch: 0, Loss: 0.673405
Train - Epoch 174, Batch: 0, Loss: 0.673197
Train - Epoch 175, Batch: 0, Loss: 0.673015
Train - Epoch 176, Batch: 0, Loss: 0.672536
Train - Epoch 177, Batch: 0, Loss: 0.672287
Train - Epoch 178, Batch: 0, Loss: 0.672131
Train - Epoch 179, Batch: 0, Loss: 0.673021
Train - Epoch 180, Batch: 0, Loss: 0.673210
Train - Epoch 181, Batch: 0, Loss: 0.672174
Train - Epoch 182, Batch: 0, Loss: 0.673129/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 183, Batch: 0, Loss: 0.672108
Train - Epoch 184, Batch: 0, Loss: 0.672856
Train - Epoch 185, Batch: 0, Loss: 0.672162
Train - Epoch 186, Batch: 0, Loss: 0.672360
Train - Epoch 187, Batch: 0, Loss: 0.672494
Train - Epoch 188, Batch: 0, Loss: 0.671639
Train - Epoch 189, Batch: 0, Loss: 0.672091
Train - Epoch 190, Batch: 0, Loss: 0.671608
Train - Epoch 191, Batch: 0, Loss: 0.672498
Train - Epoch 192, Batch: 0, Loss: 0.671927
Train - Epoch 193, Batch: 0, Loss: 0.670784
Train - Epoch 194, Batch: 0, Loss: 0.671115
Train - Epoch 195, Batch: 0, Loss: 0.671796
Train - Epoch 196, Batch: 0, Loss: 0.671646
Train - Epoch 197, Batch: 0, Loss: 0.672265
Train - Epoch 198, Batch: 0, Loss: 0.671297
Train - Epoch 199, Batch: 0, Loss: 0.671018
training_time:: 350.5715832710266
training time full:: 350.5716209411621
provenance prepare time:: 4.76837158203125e-07
here
start test::
Test Avg. Loss: 0.000067, Accuracy: 0.481968
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 2024
max_epoch:: 200
batch_size:: 10200
epoch:: 0
tensor([  0,   5,   6,  18,  25,  27,  29,  31,  41,  42,  44,  46,  52,  56,
         57,  58,  59,  64,  66,  69,  71,  75,  79,  80,  81,  84,  85,  87,
         88,  90,  91,  94,  95,  97,  98,  99, 104, 109, 110, 115, 117, 118,
        125, 128, 129, 131, 138, 139, 140, 146])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
epoch:: 60
epoch:: 61
epoch:: 62
epoch:: 63
epoch:: 64
epoch:: 65
epoch:: 66
epoch:: 67
epoch:: 68
epoch:: 69
epoch:: 70
epoch:: 71
epoch:: 72
epoch:: 73
epoch:: 74
epoch:: 75
epoch:: 76
epoch:: 77
epoch:: 78
epoch:: 79
epoch:: 80
epoch:: 81
epoch:: 82
epoch:: 83
epoch:: 84
epoch:: 85
epoch:: 86
epoch:: 87
epoch:: 88
epoch:: 89
epoch:: 90
epoch:: 91
epoch:: 92
epoch:: 93
epoch:: 94
epoch:: 95
epoch:: 96
epoch:: 97
epoch:: 98
epoch:: 99
epoch:: 100
epoch:: 101
epoch:: 102
epoch:: 103
epoch:: 104
epoch:: 105
epoch:: 106
epoch:: 107
epoch:: 108
epoch:: 109
epoch:: 110
epoch:: 111
epoch:: 112
epoch:: 113
epoch:: 114
epoch:: 115
epoch:: 116
epoch:: 117
epoch:: 118
epoch:: 119
epoch:: 120
epoch:: 121
epoch:: 122
epoch:: 123
epoch:: 124
epoch:: 125
epoch:: 126
epoch:: 127
epoch:: 128
epoch:: 129
epoch:: 130
epoch:: 131
epoch:: 132
epoch:: 133
epoch:: 134
epoch:: 135
epoch:: 136
epoch:: 137
epoch:: 138
epoch:: 139
epoch:: 140
epoch:: 141
epoch:: 142
epoch:: 143
epoch:: 144
epoch:: 145
epoch:: 146
epoch:: 147
epoch:: 148
epoch:: 149
epoch:: 150
epoch:: 151
epoch:: 152
epoch:: 153
epoch:: 154
epoch:: 155
epoch:: 156
epoch:: 157
epoch:: 158
epoch:: 159
epoch:: 160
epoch:: 161
epoch:: 162
epoch:: 163
epoch:: 164
epoch:: 165
epoch:: 166
epoch:: 167
epoch:: 168
epoch:: 169
epoch:: 170
epoch:: 171
epoch:: 172
epoch:: 173
epoch:: 174
epoch:: 175
epoch:: 176
epoch:: 177
epoch:: 178
epoch:: 179
epoch:: 180
epoch:: 181
epoch:: 182
epoch:: 183
epoch:: 184
epoch:: 185
epoch:: 186
epoch:: 187
epoch:: 188
epoch:: 189
epoch:: 190
epoch:: 191
epoch:: 192
epoch:: 193
epoch:: 194
epoch:: 195
epoch:: 196
epoch:: 197
epoch:: 198
epoch:: 199
training time is 248.2074613571167
overhead:: 0
overhead2:: 0.44965410232543945
overhead3:: 0
time_baseline:: 248.20754313468933
curr_diff: 0 tensor(0.5773, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.5773, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8662, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.551774
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 2 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 2024
max_epoch:: 200
overhead:: 0
overhead2:: 0.023215055465698242
overhead3:: 0.1396806240081787
overhead4:: 29.177295207977295
overhead5:: 0
memory usage:: 26603954176
time_provenance:: 77.9393424987793
curr_diff: 0 tensor(0.0057, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0057, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.5718, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.5718, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8695, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.549303
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 2 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 2024
max_epoch:: 200
overhead:: 0
overhead2:: 0.028891801834106445
overhead3:: 0.14829158782958984
overhead4:: 34.670727014541626
overhead5:: 0
memory usage:: 26603646976
time_provenance:: 84.90076518058777
curr_diff: 0 tensor(0.0057, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0057, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.5718, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.5718, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8695, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.549303
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 2 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 2024
max_epoch:: 200
overhead:: 0
overhead2:: 0.03296852111816406
overhead3:: 0.1573333740234375
overhead4:: 39.76793313026428
overhead5:: 0
memory usage:: 26611609600
time_provenance:: 91.47429919242859
curr_diff: 0 tensor(0.0056, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0056, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.5718, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.5718, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8695, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.549353
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 2 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 2024
max_epoch:: 200
overhead:: 0
overhead2:: 0.042400360107421875
overhead3:: 0.17549705505371094
overhead4:: 52.610379219055176
overhead5:: 0
memory usage:: 26630819840
time_provenance:: 108.19273471832275
curr_diff: 0 tensor(0.0052, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0052, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.5722, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.5722, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8692, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.549303
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 2 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 2024
max_epoch:: 200
overhead:: 0
overhead2:: 0.043091773986816406
overhead3:: 0.17929458618164062
overhead4:: 56.98047709465027
overhead5:: 0
memory usage:: 26602242048
time_provenance:: 113.22290802001953
curr_diff: 0 tensor(0.0051, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0051, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.5722, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.5722, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8692, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.549353
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 2 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 2024
max_epoch:: 200
overhead:: 0
overhead2:: 0.052141427993774414
overhead3:: 0.19229936599731445
overhead4:: 61.77176213264465
overhead5:: 0
memory usage:: 26608091136
time_provenance:: 119.02152633666992
curr_diff: 0 tensor(0.0051, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0051, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.5723, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.5723, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8692, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.549353
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 2 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 2024
max_epoch:: 200
overhead:: 0
overhead2:: 0.10128283500671387
overhead3:: 0.2915828227996826
overhead4:: 124.95306730270386
overhead5:: 0
memory usage:: 26611843072
time_provenance:: 197.7754955291748
curr_diff: 0 tensor(0.0031, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0031, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.5742, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.5742, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8680, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.550341
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 2 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 2024
max_epoch:: 200
overhead:: 0
overhead2:: 0.12561464309692383
overhead3:: 0.3003089427947998
overhead4:: 128.2340703010559
overhead5:: 0
memory usage:: 26615988224
time_provenance:: 201.85211515426636
curr_diff: 0 tensor(0.0031, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0031, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.5743, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.5743, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8680, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.550341
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 2 0.1 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 2024
max_epoch:: 200
overhead:: 0
overhead2:: 0.09741783142089844
overhead3:: 0.3016045093536377
overhead4:: 131.15372157096863
overhead5:: 0
memory usage:: 26603851776
time_provenance:: 205.45802187919617
curr_diff: 0 tensor(0.0031, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0031, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.5743, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.5743, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.8680, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.550341
repetition 3
python3 benchmark_exp_lr.py 0.001 10200 200 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression rcv1 3 0.005 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 0.693248
Train - Epoch 1, Batch: 0, Loss: 0.693079
Train - Epoch 2, Batch: 0, Loss: 0.692919
Train - Epoch 3, Batch: 0, Loss: 0.692745
Train - Epoch 4, Batch: 0, Loss: 0.692621
Train - Epoch 5, Batch: 0, Loss: 0.692457
Train - Epoch 6, Batch: 0, Loss: 0.692264
Train - Epoch 7, Batch: 0, Loss: 0.692156
Train - Epoch 8, Batch: 0, Loss: 0.691940
Train - Epoch 9, Batch: 0, Loss: 0.691837
Train - Epoch 10, Batch: 0, Loss: 0.691675
Train - Epoch 11, Batch: 0, Loss: 0.691517
Train - Epoch 12, Batch: 0, Loss: 0.691409
Train - Epoch 13, Batch: 0, Loss: 0.691078
Train - Epoch 14, Batch: 0, Loss: 0.690959
Train - Epoch 15, Batch: 0, Loss: 0.690833
Train - Epoch 16, Batch: 0, Loss: 0.690608
Train - Epoch 17, Batch: 0, Loss: 0.690671
Train - Epoch 18, Batch: 0, Loss: 0.690484
Train - Epoch 19, Batch: 0, Loss: 0.690167
Train - Epoch 20, Batch: 0, Loss: 0.690100
Train - Epoch 21, Batch: 0, Loss: 0.689994
Train - Epoch 22, Batch: 0, Loss: 0.689791
Train - Epoch 23, Batch: 0, Loss: 0.689544
Train - Epoch 24, Batch: 0, Loss: 0.689511
Train - Epoch 25, Batch: 0, Loss: 0.689501
Train - Epoch 26, Batch: 0, Loss: 0.689420
Train - Epoch 27, Batch: 0, Loss: 0.689071
Train - Epoch 28, Batch: 0, Loss: 0.689088
Train - Epoch 29, Batch: 0, Loss: 0.688736
Train - Epoch 30, Batch: 0, Loss: 0.688675
Train - Epoch 31, Batch: 0, Loss: 0.688547
Train - Epoch 32, Batch: 0, Loss: 0.688419
Train - Epoch 33, Batch: 0, Loss: 0.688179
Train - Epoch 34, Batch: 0, Loss: 0.688312
Train - Epoch 35, Batch: 0, Loss: 0.688101
Train - Epoch 36, Batch: 0, Loss: 0.687759
Train - Epoch 37, Batch: 0, Loss: 0.687721
Train - Epoch 38, Batch: 0, Loss: 0.687621
Train - Epoch 39, Batch: 0, Loss: 0.687418
Train - Epoch 40, Batch: 0, Loss: 0.687281
Train - Epoch 41, Batch: 0, Loss: 0.687219
Train - Epoch 42, Batch: 0, Loss: 0.686993
Train - Epoch 43, Batch: 0, Loss: 0.686957
Train - Epoch 44, Batch: 0, Loss: 0.686557
Train - Epoch 45, Batch: 0, Loss: 0.686798
Train - Epoch 46, Batch: 0, Loss: 0.686734
Train - Epoch 47, Batch: 0, Loss: 0.686298
Train - Epoch 48, Batch: 0, Loss: 0.686316
Train - Epoch 49, Batch: 0, Loss: 0.685764
Train - Epoch 50, Batch: 0, Loss: 0.685730
Train - Epoch 51, Batch: 0, Loss: 0.685652
Train - Epoch 52, Batch: 0, Loss: 0.685716
Train - Epoch 53, Batch: 0, Loss: 0.685802
Train - Epoch 54, Batch: 0, Loss: 0.685339
Train - Epoch 55, Batch: 0, Loss: 0.685283
Train - Epoch 56, Batch: 0, Loss: 0.685214
Train - Epoch 57, Batch: 0, Loss: 0.685251
Train - Epoch 58, Batch: 0, Loss: 0.684632
Train - Epoch 59, Batch: 0, Loss: 0.684869
Train - Epoch 60, Batch: 0, Loss: 0.684643
Train - Epoch 61, Batch: 0, Loss: 0.684229
Train - Epoch 62, Batch: 0, Loss: 0.684438
Train - Epoch 63, Batch: 0, Loss: 0.684197
Train - Epoch 64, Batch: 0, Loss: 0.684247
Train - Epoch 65, Batch: 0, Loss: 0.684309
Train - Epoch 66, Batch: 0, Loss: 0.683858
Train - Epoch 67, Batch: 0, Loss: 0.683600
Train - Epoch 68, Batch: 0, Loss: 0.683374
Train - Epoch 69, Batch: 0, Loss: 0.683554
Train - Epoch 70, Batch: 0, Loss: 0.683205
Train - Epoch 71, Batch: 0, Loss: 0.683209
Train - Epoch 72, Batch: 0, Loss: 0.683631
Train - Epoch 73, Batch: 0, Loss: 0.683093
Train - Epoch 74, Batch: 0, Loss: 0.682767
Train - Epoch 75, Batch: 0, Loss: 0.682705
Train - Epoch 76, Batch: 0, Loss: 0.682930
Train - Epoch 77, Batch: 0, Loss: 0.682525
Train - Epoch 78, Batch: 0, Loss: 0.682194
Train - Epoch 79, Batch: 0, Loss: 0.682236
Train - Epoch 80, Batch: 0, Loss: 0.682189
Train - Epoch 81, Batch: 0, Loss: 0.682146
Train - Epoch 82, Batch: 0, Loss: 0.682083
Train - Epoch 83, Batch: 0, Loss: 0.682017
Train - Epoch 84, Batch: 0, Loss: 0.681167
Train - Epoch 85, Batch: 0, Loss: 0.681956
Train - Epoch 86, Batch: 0, Loss: 0.681747
Train - Epoch 87, Batch: 0, Loss: 0.681409
Train - Epoch 88, Batch: 0, Loss: 0.681270
Train - Epoch 89, Batch: 0, Loss: 0.681277
Train - Epoch 90, Batch: 0, Loss: 0.680749
Train - Epoch 91, Batch: 0, Loss: 0.680901
Train - Epoch 92, Batch: 0, Loss: 0.680543
Train - Epoch 93, Batch: 0, Loss: 0.680950
Train - Epoch 94, Batch: 0, Loss: 0.680215
Train - Epoch 95, Batch: 0, Loss: 0.680644
Train - Epoch 96, Batch: 0, Loss: 0.680736
Train - Epoch 97, Batch: 0, Loss: 0.680091
Train - Epoch 98, Batch: 0, Loss: 0.679945
Train - Epoch 99, Batch: 0, Loss: 0.679836
Train - Epoch 100, Batch: 0, Loss: 0.679534
Train - Epoch 101, Batch: 0, Loss: 0.680122
Train - Epoch 102, Batch: 0, Loss: 0.679597
Train - Epoch 103, Batch: 0, Loss: 0.679308
Train - Epoch 104, Batch: 0, Loss: 0.679563
Train - Epoch 105, Batch: 0, Loss: 0.679394
Train - Epoch 106, Batch: 0, Loss: 0.679488
Train - Epoch 107, Batch: 0, Loss: 0.679262
Train - Epoch 108, Batch: 0, Loss: 0.679079
Train - Epoch 109, Batch: 0, Loss: 0.679361
Train - Epoch 110, Batch: 0, Loss: 0.679254
Train - Epoch 111, Batch: 0, Loss: 0.678949
Train - Epoch 112, Batch: 0, Loss: 0.678523
Train - Epoch 113, Batch: 0, Loss: 0.678427
Train - Epoch 114, Batch: 0, Loss: 0.678635
Train - Epoch 115, Batch: 0, Loss: 0.678089
Train - Epoch 116, Batch: 0, Loss: 0.678159
Train - Epoch 117, Batch: 0, Loss: 0.677551
Train - Epoch 118, Batch: 0, Loss: 0.678202
Train - Epoch 119, Batch: 0, Loss: 0.678198
Train - Epoch 120, Batch: 0, Loss: 0.678130
Train - Epoch 121, Batch: 0, Loss: 0.678040
Train - Epoch 122, Batch: 0, Loss: 0.677582
Train - Epoch 123, Batch: 0, Loss: 0.678263
Train - Epoch 124, Batch: 0, Loss: 0.677236
Train - Epoch 125, Batch: 0, Loss: 0.676923
Train - Epoch 126, Batch: 0, Loss: 0.677431
Train - Epoch 127, Batch: 0, Loss: 0.677223
Train - Epoch 128, Batch: 0, Loss: 0.677617
Train - Epoch 129, Batch: 0, Loss: 0.676991
Train - Epoch 130, Batch: 0, Loss: 0.677173
Train - Epoch 131, Batch: 0, Loss: 0.676165
Train - Epoch 132, Batch: 0, Loss: 0.676247
Train - Epoch 133, Batch: 0, Loss: 0.676254
Train - Epoch 134, Batch: 0, Loss: 0.677398
Train - Epoch 135, Batch: 0, Loss: 0.676286
Train - Epoch 136, Batch: 0, Loss: 0.676888
Train - Epoch 137, Batch: 0, Loss: 0.676442
Train - Epoch 138, Batch: 0, Loss: 0.676321
Train - Epoch 139, Batch: 0, Loss: 0.676265
Train - Epoch 140, Batch: 0, Loss: 0.676014
Train - Epoch 141, Batch: 0, Loss: 0.676158
Train - Epoch 142, Batch: 0, Loss: 0.675790
Train - Epoch 143, Batch: 0, Loss: 0.674916
Train - Epoch 144, Batch: 0, Loss: 0.675695
Train - Epoch 145, Batch: 0, Loss: 0.675351
Train - Epoch 146, Batch: 0, Loss: 0.675677
Train - Epoch 147, Batch: 0, Loss: 0.675325
Train - Epoch 148, Batch: 0, Loss: 0.675212
Train - Epoch 149, Batch: 0, Loss: 0.675494
Train - Epoch 150, Batch: 0, Loss: 0.675405
Train - Epoch 151, Batch: 0, Loss: 0.675751
Train - Epoch 152, Batch: 0, Loss: 0.674882
Train - Epoch 153, Batch: 0, Loss: 0.674545
Train - Epoch 154, Batch: 0, Loss: 0.673933
Train - Epoch 155, Batch: 0, Loss: 0.674522
Train - Epoch 156, Batch: 0, Loss: 0.674331
Train - Epoch 157, Batch: 0, Loss: 0.674909
Train - Epoch 158, Batch: 0, Loss: 0.673840
Train - Epoch 159, Batch: 0, Loss: 0.674624
Train - Epoch 160, Batch: 0, Loss: 0.674741
Train - Epoch 161, Batch: 0, Loss: 0.673495
Train - Epoch 162, Batch: 0, Loss: 0.675160
Train - Epoch 163, Batch: 0, Loss: 0.673759
Train - Epoch 164, Batch: 0, Loss: 0.674332
Train - Epoch 165, Batch: 0, Loss: 0.674331
Train - Epoch 166, Batch: 0, Loss: 0.673524
Train - Epoch 167, Batch: 0, Loss: 0.673472
Train - Epoch 168, Batch: 0, Loss: 0.673630
Train - Epoch 169, Batch: 0, Loss: 0.673190
Train - Epoch 170, Batch: 0, Loss: 0.673224
Train - Epoch 171, Batch: 0, Loss: 0.674494
Train - Epoch 172, Batch: 0, Loss: 0.671791
Train - Epoch 173, Batch: 0, Loss: 0.672592
Train - Epoch 174, Batch: 0, Loss: 0.673318
Train - Epoch 175, Batch: 0, Loss: 0.673437
Train - Epoch 176, Batch: 0, Loss: 0.673205
Train - Epoch 177, Batch: 0, Loss: 0.672893
Train - Epoch 178, Batch: 0, Loss: 0.672701
Train - Epoch 179, Batch: 0, Loss: 0.673524
Train - Epoch 180, Batch: 0, Loss: 0.672329
Train - Epoch 181, Batch: 0, Loss: 0.672139
Train - Epoch 182, Batch: 0, Loss: 0.672476
Train - Epoch 183, Batch: 0, Loss: 0.671934
Train - Epoch 184, Batch: 0, Loss: 0.672495
Train - Epoch 185, Batch: 0, Loss: 0.672233
Train - Epoch 186, Batch: 0, Loss: 0.672387/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 187, Batch: 0, Loss: 0.672786
Train - Epoch 188, Batch: 0, Loss: 0.672348
Train - Epoch 189, Batch: 0, Loss: 0.672147
Train - Epoch 190, Batch: 0, Loss: 0.672148
Train - Epoch 191, Batch: 0, Loss: 0.671796
Train - Epoch 192, Batch: 0, Loss: 0.670997
Train - Epoch 193, Batch: 0, Loss: 0.671091
Train - Epoch 194, Batch: 0, Loss: 0.671683
Train - Epoch 195, Batch: 0, Loss: 0.670923
Train - Epoch 196, Batch: 0, Loss: 0.671352
Train - Epoch 197, Batch: 0, Loss: 0.671545
Train - Epoch 198, Batch: 0, Loss: 0.671102
Train - Epoch 199, Batch: 0, Loss: 0.670252
training_time:: 353.49310779571533
training time full:: 353.49317479133606
provenance prepare time:: 4.76837158203125e-07
here
start test::
Test Avg. Loss: 0.000067, Accuracy: 0.482018
adding noise deletion rate:: 0.01
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
tensor([0.5324, 0.5460, 0.5140,  ..., 0.5361, 0.5219, 0.5210],
       dtype=torch.float64, grad_fn=<MaxBackward0>)
tensor(0.8685, dtype=torch.float64) tensor(0., dtype=torch.float64)
Train - Epoch 0, Batch: 0, Loss: 0.693079
Train - Epoch 1, Batch: 0, Loss: 0.692983
Train - Epoch 2, Batch: 0, Loss: 0.692870
Train - Epoch 3, Batch: 0, Loss: 0.692785
Train - Epoch 4, Batch: 0, Loss: 0.692648
Train - Epoch 5, Batch: 0, Loss: 0.692536
Train - Epoch 6, Batch: 0, Loss: 0.692443
Train - Epoch 7, Batch: 0, Loss: 0.692340
Train - Epoch 8, Batch: 0, Loss: 0.692211
Train - Epoch 9, Batch: 0, Loss: 0.692128
Train - Epoch 10, Batch: 0, Loss: 0.692075
Train - Epoch 11, Batch: 0, Loss: 0.691971
Train - Epoch 12, Batch: 0, Loss: 0.691870
Train - Epoch 13, Batch: 0, Loss: 0.691616
Train - Epoch 14, Batch: 0, Loss: 0.691563
Train - Epoch 15, Batch: 0, Loss: 0.691504
Train - Epoch 16, Batch: 0, Loss: 0.691303
Train - Epoch 17, Batch: 0, Loss: 0.691366
Train - Epoch 18, Batch: 0, Loss: 0.691276
Train - Epoch 19, Batch: 0, Loss: 0.691018
Train - Epoch 20, Batch: 0, Loss: 0.690919
Train - Epoch 21, Batch: 0, Loss: 0.690848
Train - Epoch 22, Batch: 0, Loss: 0.690807
Train - Epoch 23, Batch: 0, Loss: 0.690658
Train - Epoch 24, Batch: 0, Loss: 0.690567
Train - Epoch 25, Batch: 0, Loss: 0.690676
Train - Epoch 26, Batch: 0, Loss: 0.690533
Train - Epoch 27, Batch: 0, Loss: 0.690281
Train - Epoch 28, Batch: 0, Loss: 0.690309
Train - Epoch 29, Batch: 0, Loss: 0.690056
Train - Epoch 30, Batch: 0, Loss: 0.690038
Train - Epoch 31, Batch: 0, Loss: 0.689935
Train - Epoch 32, Batch: 0, Loss: 0.689833
Train - Epoch 33, Batch: 0, Loss: 0.689614
Train - Epoch 34, Batch: 0, Loss: 0.689782
Train - Epoch 35, Batch: 0, Loss: 0.689701
Train - Epoch 36, Batch: 0, Loss: 0.689363
Train - Epoch 37, Batch: 0, Loss: 0.689458
Train - Epoch 38, Batch: 0, Loss: 0.689281
Train - Epoch 39, Batch: 0, Loss: 0.689213
Train - Epoch 40, Batch: 0, Loss: 0.689077
Train - Epoch 41, Batch: 0, Loss: 0.688960
Train - Epoch 42, Batch: 0, Loss: 0.688970
Train - Epoch 43, Batch: 0, Loss: 0.688903
Train - Epoch 44, Batch: 0, Loss: 0.688543
Train - Epoch 45, Batch: 0, Loss: 0.688755
Train - Epoch 46, Batch: 0, Loss: 0.688591
Train - Epoch 47, Batch: 0, Loss: 0.688464
Train - Epoch 48, Batch: 0, Loss: 0.688479
Train - Epoch 49, Batch: 0, Loss: 0.688012
Train - Epoch 50, Batch: 0, Loss: 0.688132
Train - Epoch 51, Batch: 0, Loss: 0.687949
Train - Epoch 52, Batch: 0, Loss: 0.687977
Train - Epoch 53, Batch: 0, Loss: 0.688052
Train - Epoch 54, Batch: 0, Loss: 0.687733
Train - Epoch 55, Batch: 0, Loss: 0.687708
Train - Epoch 56, Batch: 0, Loss: 0.687632
Train - Epoch 57, Batch: 0, Loss: 0.687701
Train - Epoch 58, Batch: 0, Loss: 0.687323
Train - Epoch 59, Batch: 0, Loss: 0.687353
Train - Epoch 60, Batch: 0, Loss: 0.687279
Train - Epoch 61, Batch: 0, Loss: 0.687000
Train - Epoch 62, Batch: 0, Loss: 0.687322
Train - Epoch 63, Batch: 0, Loss: 0.687040
Train - Epoch 64, Batch: 0, Loss: 0.687109
Train - Epoch 65, Batch: 0, Loss: 0.687085
Train - Epoch 66, Batch: 0, Loss: 0.686882
Train - Epoch 67, Batch: 0, Loss: 0.686606
Train - Epoch 68, Batch: 0, Loss: 0.686418
Train - Epoch 69, Batch: 0, Loss: 0.686624
Train - Epoch 70, Batch: 0, Loss: 0.686391
Train - Epoch 71, Batch: 0, Loss: 0.686443
Train - Epoch 72, Batch: 0, Loss: 0.686606
Train - Epoch 73, Batch: 0, Loss: 0.686327
Train - Epoch 74, Batch: 0, Loss: 0.686051
Train - Epoch 75, Batch: 0, Loss: 0.686045
Train - Epoch 76, Batch: 0, Loss: 0.686135
Train - Epoch 77, Batch: 0, Loss: 0.685927
Train - Epoch 78, Batch: 0, Loss: 0.685807
Train - Epoch 79, Batch: 0, Loss: 0.685728
Train - Epoch 80, Batch: 0, Loss: 0.685551
Train - Epoch 81, Batch: 0, Loss: 0.685432
Train - Epoch 82, Batch: 0, Loss: 0.685768
Train - Epoch 83, Batch: 0, Loss: 0.685322
Train - Epoch 84, Batch: 0, Loss: 0.684944
Train - Epoch 85, Batch: 0, Loss: 0.685087
Train - Epoch 86, Batch: 0, Loss: 0.685392
Train - Epoch 87, Batch: 0, Loss: 0.685192
Train - Epoch 88, Batch: 0, Loss: 0.685035
Train - Epoch 89, Batch: 0, Loss: 0.685182
Train - Epoch 90, Batch: 0, Loss: 0.684843
Train - Epoch 91, Batch: 0, Loss: 0.684785
Train - Epoch 92, Batch: 0, Loss: 0.684853
Train - Epoch 93, Batch: 0, Loss: 0.684731
Train - Epoch 94, Batch: 0, Loss: 0.684033
Train - Epoch 95, Batch: 0, Loss: 0.684497
Train - Epoch 96, Batch: 0, Loss: 0.684747
Train - Epoch 97, Batch: 0, Loss: 0.684425
Train - Epoch 98, Batch: 0, Loss: 0.684204
Train - Epoch 99, Batch: 0, Loss: 0.684072
Train - Epoch 100, Batch: 0, Loss: 0.683580
Train - Epoch 101, Batch: 0, Loss: 0.684326
Train - Epoch 102, Batch: 0, Loss: 0.683916
Train - Epoch 103, Batch: 0, Loss: 0.683830
Train - Epoch 104, Batch: 0, Loss: 0.683844
Train - Epoch 105, Batch: 0, Loss: 0.683881
Train - Epoch 106, Batch: 0, Loss: 0.683624
Train - Epoch 107, Batch: 0, Loss: 0.683805
Train - Epoch 108, Batch: 0, Loss: 0.683639
Train - Epoch 109, Batch: 0, Loss: 0.683605
Train - Epoch 110, Batch: 0, Loss: 0.683734
Train - Epoch 111, Batch: 0, Loss: 0.683108
Train - Epoch 112, Batch: 0, Loss: 0.683211
Train - Epoch 113, Batch: 0, Loss: 0.683077
Train - Epoch 114, Batch: 0, Loss: 0.683206
Train - Epoch 115, Batch: 0, Loss: 0.683113
Train - Epoch 116, Batch: 0, Loss: 0.682855
Train - Epoch 117, Batch: 0, Loss: 0.682534
Train - Epoch 118, Batch: 0, Loss: 0.682823
Train - Epoch 119, Batch: 0, Loss: 0.682971
Train - Epoch 120, Batch: 0, Loss: 0.683197
Train - Epoch 121, Batch: 0, Loss: 0.682554
Train - Epoch 122, Batch: 0, Loss: 0.682716
Train - Epoch 123, Batch: 0, Loss: 0.683149
Train - Epoch 124, Batch: 0, Loss: 0.682139
Train - Epoch 125, Batch: 0, Loss: 0.682049
Train - Epoch 126, Batch: 0, Loss: 0.682219
Train - Epoch 127, Batch: 0, Loss: 0.682352
Train - Epoch 128, Batch: 0, Loss: 0.682684
Train - Epoch 129, Batch: 0, Loss: 0.682027
Train - Epoch 130, Batch: 0, Loss: 0.682346
Train - Epoch 131, Batch: 0, Loss: 0.681872
Train - Epoch 132, Batch: 0, Loss: 0.681538
Train - Epoch 133, Batch: 0, Loss: 0.681511
Train - Epoch 134, Batch: 0, Loss: 0.682438
Train - Epoch 135, Batch: 0, Loss: 0.681734
Train - Epoch 136, Batch: 0, Loss: 0.682250
Train - Epoch 137, Batch: 0, Loss: 0.681772
Train - Epoch 138, Batch: 0, Loss: 0.681801
Train - Epoch 139, Batch: 0, Loss: 0.681444
Train - Epoch 140, Batch: 0, Loss: 0.681462
Train - Epoch 141, Batch: 0, Loss: 0.681677
Train - Epoch 142, Batch: 0, Loss: 0.681202
Train - Epoch 143, Batch: 0, Loss: 0.680907
Train - Epoch 144, Batch: 0, Loss: 0.681325
Train - Epoch 145, Batch: 0, Loss: 0.680891
Train - Epoch 146, Batch: 0, Loss: 0.681468
Train - Epoch 147, Batch: 0, Loss: 0.681195
Train - Epoch 148, Batch: 0, Loss: 0.680975
Train - Epoch 149, Batch: 0, Loss: 0.681317
Train - Epoch 150, Batch: 0, Loss: 0.680956
Train - Epoch 151, Batch: 0, Loss: 0.681826
Train - Epoch 152, Batch: 0, Loss: 0.680577
Train - Epoch 153, Batch: 0, Loss: 0.680473
Train - Epoch 154, Batch: 0, Loss: 0.680086
Train - Epoch 155, Batch: 0, Loss: 0.680748
Train - Epoch 156, Batch: 0, Loss: 0.680074
Train - Epoch 157, Batch: 0, Loss: 0.680223
Train - Epoch 158, Batch: 0, Loss: 0.679981
Train - Epoch 159, Batch: 0, Loss: 0.680645
Train - Epoch 160, Batch: 0, Loss: 0.680489
Train - Epoch 161, Batch: 0, Loss: 0.679233
Train - Epoch 162, Batch: 0, Loss: 0.680986
Train - Epoch 163, Batch: 0, Loss: 0.679847
Train - Epoch 164, Batch: 0, Loss: 0.680267
Train - Epoch 165, Batch: 0, Loss: 0.680607
Train - Epoch 166, Batch: 0, Loss: 0.679943
Train - Epoch 167, Batch: 0, Loss: 0.680161
Train - Epoch 168, Batch: 0, Loss: 0.679385
Train - Epoch 169, Batch: 0, Loss: 0.679310
Train - Epoch 170, Batch: 0, Loss: 0.679025
Train - Epoch 171, Batch: 0, Loss: 0.680280
Train - Epoch 172, Batch: 0, Loss: 0.678167
Train - Epoch 173, Batch: 0, Loss: 0.679161
Train - Epoch 174, Batch: 0, Loss: 0.679557
Train - Epoch 175, Batch: 0, Loss: 0.679751
Train - Epoch 176, Batch: 0, Loss: 0.679428
Train - Epoch 177, Batch: 0, Loss: 0.678687
Train - Epoch 178, Batch: 0, Loss: 0.679240
Train - Epoch 179, Batch: 0, Loss: 0.679738
Train - Epoch 180, Batch: 0, Loss: 0.678858
Train - Epoch 181, Batch: 0, Loss: 0.678693
Train - Epoch 182, Batch: 0, Loss: 0.679048/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 183, Batch: 0, Loss: 0.678927
Train - Epoch 184, Batch: 0, Loss: 0.678596
Train - Epoch 185, Batch: 0, Loss: 0.678932
Train - Epoch 186, Batch: 0, Loss: 0.679109
Train - Epoch 187, Batch: 0, Loss: 0.679065
Train - Epoch 188, Batch: 0, Loss: 0.678961
Train - Epoch 189, Batch: 0, Loss: 0.678924
Train - Epoch 190, Batch: 0, Loss: 0.678960
Train - Epoch 191, Batch: 0, Loss: 0.678552
Train - Epoch 192, Batch: 0, Loss: 0.678091
Train - Epoch 193, Batch: 0, Loss: 0.678212
Train - Epoch 194, Batch: 0, Loss: 0.678321
Train - Epoch 195, Batch: 0, Loss: 0.677930
Train - Epoch 196, Batch: 0, Loss: 0.678632
Train - Epoch 197, Batch: 0, Loss: 0.678442
Train - Epoch 198, Batch: 0, Loss: 0.677936
Train - Epoch 199, Batch: 0, Loss: 0.677633
training_time:: 350.75061559677124
training time full:: 350.75065779685974
provenance prepare time:: 0.0
here
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.483055
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 202
max_epoch:: 200
batch_size:: 10200
epoch:: 0
tensor([  0,   2,   3,   5,   6,   7,   9,  12,  14,  15,  18,  20,  22,  25,
         28,  29,  31,  32,  34,  36,  37,  38,  39,  40,  42,  43,  44,  45,
         46,  47,  49,  51,  52,  53,  54,  55,  56,  59,  60,  66,  68,  70,
         77,  90,  94, 101, 102, 103, 104, 105])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
epoch:: 60
epoch:: 61
epoch:: 62
epoch:: 63
epoch:: 64
epoch:: 65
epoch:: 66
epoch:: 67
epoch:: 68
epoch:: 69
epoch:: 70
epoch:: 71
epoch:: 72
epoch:: 73
epoch:: 74
epoch:: 75
epoch:: 76
epoch:: 77
epoch:: 78
epoch:: 79
epoch:: 80
epoch:: 81
epoch:: 82
epoch:: 83
epoch:: 84
epoch:: 85
epoch:: 86
epoch:: 87
epoch:: 88
epoch:: 89
epoch:: 90
epoch:: 91
epoch:: 92
epoch:: 93
epoch:: 94
epoch:: 95
epoch:: 96
epoch:: 97
epoch:: 98
epoch:: 99
epoch:: 100
epoch:: 101
epoch:: 102
epoch:: 103
epoch:: 104
epoch:: 105
epoch:: 106
epoch:: 107
epoch:: 108
epoch:: 109
epoch:: 110
epoch:: 111
epoch:: 112
epoch:: 113
epoch:: 114
epoch:: 115
epoch:: 116
epoch:: 117
epoch:: 118
epoch:: 119
epoch:: 120
epoch:: 121
epoch:: 122
epoch:: 123
epoch:: 124
epoch:: 125
epoch:: 126
epoch:: 127
epoch:: 128
epoch:: 129
epoch:: 130
epoch:: 131
epoch:: 132
epoch:: 133
epoch:: 134
epoch:: 135
epoch:: 136
epoch:: 137
epoch:: 138
epoch:: 139
epoch:: 140
epoch:: 141
epoch:: 142
epoch:: 143
epoch:: 144
epoch:: 145
epoch:: 146
epoch:: 147
epoch:: 148
epoch:: 149
epoch:: 150
epoch:: 151
epoch:: 152
epoch:: 153
epoch:: 154
epoch:: 155
epoch:: 156
epoch:: 157
epoch:: 158
epoch:: 159
epoch:: 160
epoch:: 161
epoch:: 162
epoch:: 163
epoch:: 164
epoch:: 165
epoch:: 166
epoch:: 167
epoch:: 168
epoch:: 169
epoch:: 170
epoch:: 171
epoch:: 172
epoch:: 173
epoch:: 174
epoch:: 175
epoch:: 176
epoch:: 177
epoch:: 178
epoch:: 179
epoch:: 180
epoch:: 181
epoch:: 182
epoch:: 183
epoch:: 184
epoch:: 185
epoch:: 186
epoch:: 187
epoch:: 188
epoch:: 189
epoch:: 190
epoch:: 191
epoch:: 192
epoch:: 193
epoch:: 194
epoch:: 195
epoch:: 196
epoch:: 197
epoch:: 198
epoch:: 199
training time is 273.14820408821106
overhead:: 0
overhead2:: 0.401261568069458
overhead3:: 0
time_baseline:: 273.14824056625366
curr_diff: 0 tensor(0.1187, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1187, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9956, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000067, Accuracy: 0.482215
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 202
max_epoch:: 200
overhead:: 0
overhead2:: 0.021225690841674805
overhead3:: 0.11905980110168457
overhead4:: 32.18420481681824
overhead5:: 0
memory usage:: 26639273984
time_provenance:: 47.55240797996521
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1186, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1186, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9956, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000067, Accuracy: 0.482215
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 202
max_epoch:: 200
overhead:: 0
overhead2:: 0.024318933486938477
overhead3:: 0.12610554695129395
overhead4:: 37.98679852485657
overhead5:: 0
memory usage:: 26634735616
time_provenance:: 54.97039723396301
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1186, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1186, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9956, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000067, Accuracy: 0.482215
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 202
max_epoch:: 200
overhead:: 0
overhead2:: 0.028676509857177734
overhead3:: 0.133378267288208
overhead4:: 43.9473032951355
overhead5:: 0
memory usage:: 26642305024
time_provenance:: 62.62898063659668
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1186, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1186, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9956, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000067, Accuracy: 0.482215
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 202
max_epoch:: 200
overhead:: 0
overhead2:: 0.03835487365722656
overhead3:: 0.15566182136535645
overhead4:: 57.72850203514099
overhead5:: 0
memory usage:: 26651975680
time_provenance:: 80.6801609992981
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1186, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1186, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9956, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000067, Accuracy: 0.482215
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 202
max_epoch:: 200
overhead:: 0
overhead2:: 0.043112993240356445
overhead3:: 0.1646711826324463
overhead4:: 63.355286836624146
overhead5:: 0
memory usage:: 26651365376
time_provenance:: 87.80216693878174
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1186, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1186, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9956, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000067, Accuracy: 0.482215
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 202
max_epoch:: 200
overhead:: 0
overhead2:: 0.045655012130737305
overhead3:: 0.17304182052612305
overhead4:: 68.37081718444824
overhead5:: 0
memory usage:: 26702528512
time_provenance:: 94.34059047698975
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1186, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1186, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9956, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000067, Accuracy: 0.482215
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 202
max_epoch:: 200
overhead:: 0
overhead2:: 0.09378385543823242
overhead3:: 0.2777254581451416
overhead4:: 138.95241045951843
overhead5:: 0
memory usage:: 26639032320
time_provenance:: 184.93364453315735
curr_diff: 0 tensor(9.2342e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.2342e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1186, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1186, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9956, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000067, Accuracy: 0.482215
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 202
max_epoch:: 200
overhead:: 0
overhead2:: 0.09406113624572754
overhead3:: 0.2799663543701172
overhead4:: 140.7937731742859
overhead5:: 0
memory usage:: 26652282880
time_provenance:: 187.72728824615479
curr_diff: 0 tensor(9.2348e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.2348e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1186, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1186, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9956, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000067, Accuracy: 0.482215
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 202
max_epoch:: 200
overhead:: 0
overhead2:: 0.09691500663757324
overhead3:: 0.28284239768981934
overhead4:: 144.52477717399597
overhead5:: 0
memory usage:: 26641956864
time_provenance:: 192.41428661346436
curr_diff: 0 tensor(9.2092e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.2092e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1186, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1186, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9956, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000067, Accuracy: 0.482215
adding noise deletion rate:: 0.02
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
tensor([0.5274, 0.5363, 0.5113,  ..., 0.5310, 0.5168, 0.5184],
       dtype=torch.float64, grad_fn=<MaxBackward0>)
tensor(0.8685, dtype=torch.float64) tensor(0., dtype=torch.float64)
Train - Epoch 0, Batch: 0, Loss: 0.693101
Train - Epoch 1, Batch: 0, Loss: 0.693040
Train - Epoch 2, Batch: 0, Loss: 0.692995
Train - Epoch 3, Batch: 0, Loss: 0.692948
Train - Epoch 4, Batch: 0, Loss: 0.692889
Train - Epoch 5, Batch: 0, Loss: 0.692844
Train - Epoch 6, Batch: 0, Loss: 0.692799
Train - Epoch 7, Batch: 0, Loss: 0.692770
Train - Epoch 8, Batch: 0, Loss: 0.692728
Train - Epoch 9, Batch: 0, Loss: 0.692678
Train - Epoch 10, Batch: 0, Loss: 0.692617
Train - Epoch 11, Batch: 0, Loss: 0.692554
Train - Epoch 12, Batch: 0, Loss: 0.692522
Train - Epoch 13, Batch: 0, Loss: 0.692451
Train - Epoch 14, Batch: 0, Loss: 0.692374
Train - Epoch 15, Batch: 0, Loss: 0.692388
Train - Epoch 16, Batch: 0, Loss: 0.692248
Train - Epoch 17, Batch: 0, Loss: 0.692352
Train - Epoch 18, Batch: 0, Loss: 0.692248
Train - Epoch 19, Batch: 0, Loss: 0.692146
Train - Epoch 20, Batch: 0, Loss: 0.692071
Train - Epoch 21, Batch: 0, Loss: 0.692053
Train - Epoch 22, Batch: 0, Loss: 0.692054
Train - Epoch 23, Batch: 0, Loss: 0.692022
Train - Epoch 24, Batch: 0, Loss: 0.691945
Train - Epoch 25, Batch: 0, Loss: 0.692038
Train - Epoch 26, Batch: 0, Loss: 0.691927
Train - Epoch 27, Batch: 0, Loss: 0.691818
Train - Epoch 28, Batch: 0, Loss: 0.691782
Train - Epoch 29, Batch: 0, Loss: 0.691686
Train - Epoch 30, Batch: 0, Loss: 0.691689
Train - Epoch 31, Batch: 0, Loss: 0.691701
Train - Epoch 32, Batch: 0, Loss: 0.691589
Train - Epoch 33, Batch: 0, Loss: 0.691514
Train - Epoch 34, Batch: 0, Loss: 0.691584
Train - Epoch 35, Batch: 0, Loss: 0.691583
Train - Epoch 36, Batch: 0, Loss: 0.691383
Train - Epoch 37, Batch: 0, Loss: 0.691491
Train - Epoch 38, Batch: 0, Loss: 0.691328
Train - Epoch 39, Batch: 0, Loss: 0.691369
Train - Epoch 40, Batch: 0, Loss: 0.691283
Train - Epoch 41, Batch: 0, Loss: 0.691278
Train - Epoch 42, Batch: 0, Loss: 0.691226
Train - Epoch 43, Batch: 0, Loss: 0.691244
Train - Epoch 44, Batch: 0, Loss: 0.690980
Train - Epoch 45, Batch: 0, Loss: 0.691105
Train - Epoch 46, Batch: 0, Loss: 0.691065
Train - Epoch 47, Batch: 0, Loss: 0.691042
Train - Epoch 48, Batch: 0, Loss: 0.690948
Train - Epoch 49, Batch: 0, Loss: 0.690786
Train - Epoch 50, Batch: 0, Loss: 0.690766
Train - Epoch 51, Batch: 0, Loss: 0.690675
Train - Epoch 52, Batch: 0, Loss: 0.690820
Train - Epoch 53, Batch: 0, Loss: 0.690758
Train - Epoch 54, Batch: 0, Loss: 0.690585
Train - Epoch 55, Batch: 0, Loss: 0.690611
Train - Epoch 56, Batch: 0, Loss: 0.690619
Train - Epoch 57, Batch: 0, Loss: 0.690689
Train - Epoch 58, Batch: 0, Loss: 0.690457
Train - Epoch 59, Batch: 0, Loss: 0.690447
Train - Epoch 60, Batch: 0, Loss: 0.690477
Train - Epoch 61, Batch: 0, Loss: 0.690192
Train - Epoch 62, Batch: 0, Loss: 0.690450
Train - Epoch 63, Batch: 0, Loss: 0.690374
Train - Epoch 64, Batch: 0, Loss: 0.690344
Train - Epoch 65, Batch: 0, Loss: 0.690262
Train - Epoch 66, Batch: 0, Loss: 0.690226
Train - Epoch 67, Batch: 0, Loss: 0.690069
Train - Epoch 68, Batch: 0, Loss: 0.690038
Train - Epoch 69, Batch: 0, Loss: 0.690153
Train - Epoch 70, Batch: 0, Loss: 0.690154
Train - Epoch 71, Batch: 0, Loss: 0.690055
Train - Epoch 72, Batch: 0, Loss: 0.690001
Train - Epoch 73, Batch: 0, Loss: 0.689892
Train - Epoch 74, Batch: 0, Loss: 0.689950
Train - Epoch 75, Batch: 0, Loss: 0.689923
Train - Epoch 76, Batch: 0, Loss: 0.689882
Train - Epoch 77, Batch: 0, Loss: 0.689831
Train - Epoch 78, Batch: 0, Loss: 0.689728
Train - Epoch 79, Batch: 0, Loss: 0.689620
Train - Epoch 80, Batch: 0, Loss: 0.689548
Train - Epoch 81, Batch: 0, Loss: 0.689669
Train - Epoch 82, Batch: 0, Loss: 0.689627
Train - Epoch 83, Batch: 0, Loss: 0.689529
Train - Epoch 84, Batch: 0, Loss: 0.689304
Train - Epoch 85, Batch: 0, Loss: 0.689300
Train - Epoch 86, Batch: 0, Loss: 0.689469
Train - Epoch 87, Batch: 0, Loss: 0.689474
Train - Epoch 88, Batch: 0, Loss: 0.689260
Train - Epoch 89, Batch: 0, Loss: 0.689248
Train - Epoch 90, Batch: 0, Loss: 0.689325
Train - Epoch 91, Batch: 0, Loss: 0.689204
Train - Epoch 92, Batch: 0, Loss: 0.689190
Train - Epoch 93, Batch: 0, Loss: 0.689271
Train - Epoch 94, Batch: 0, Loss: 0.688890
Train - Epoch 95, Batch: 0, Loss: 0.689173
Train - Epoch 96, Batch: 0, Loss: 0.689213
Train - Epoch 97, Batch: 0, Loss: 0.689060
Train - Epoch 98, Batch: 0, Loss: 0.689011
Train - Epoch 99, Batch: 0, Loss: 0.688843
Train - Epoch 100, Batch: 0, Loss: 0.688721
Train - Epoch 101, Batch: 0, Loss: 0.689049
Train - Epoch 102, Batch: 0, Loss: 0.688868
Train - Epoch 103, Batch: 0, Loss: 0.688703
Train - Epoch 104, Batch: 0, Loss: 0.688757
Train - Epoch 105, Batch: 0, Loss: 0.688754
Train - Epoch 106, Batch: 0, Loss: 0.688565
Train - Epoch 107, Batch: 0, Loss: 0.688591
Train - Epoch 108, Batch: 0, Loss: 0.688795
Train - Epoch 109, Batch: 0, Loss: 0.688654
Train - Epoch 110, Batch: 0, Loss: 0.688741
Train - Epoch 111, Batch: 0, Loss: 0.688310
Train - Epoch 112, Batch: 0, Loss: 0.688529
Train - Epoch 113, Batch: 0, Loss: 0.688391
Train - Epoch 114, Batch: 0, Loss: 0.688406
Train - Epoch 115, Batch: 0, Loss: 0.688594
Train - Epoch 116, Batch: 0, Loss: 0.688211
Train - Epoch 117, Batch: 0, Loss: 0.688229
Train - Epoch 118, Batch: 0, Loss: 0.688439
Train - Epoch 119, Batch: 0, Loss: 0.688255
Train - Epoch 120, Batch: 0, Loss: 0.688504
Train - Epoch 121, Batch: 0, Loss: 0.688096
Train - Epoch 122, Batch: 0, Loss: 0.688348
Train - Epoch 123, Batch: 0, Loss: 0.688512
Train - Epoch 124, Batch: 0, Loss: 0.688205
Train - Epoch 125, Batch: 0, Loss: 0.687722
Train - Epoch 126, Batch: 0, Loss: 0.687938
Train - Epoch 127, Batch: 0, Loss: 0.688028
Train - Epoch 128, Batch: 0, Loss: 0.688254
Train - Epoch 129, Batch: 0, Loss: 0.687884
Train - Epoch 130, Batch: 0, Loss: 0.688105
Train - Epoch 131, Batch: 0, Loss: 0.687896
Train - Epoch 132, Batch: 0, Loss: 0.687695
Train - Epoch 133, Batch: 0, Loss: 0.687516
Train - Epoch 134, Batch: 0, Loss: 0.687982
Train - Epoch 135, Batch: 0, Loss: 0.687830
Train - Epoch 136, Batch: 0, Loss: 0.688046
Train - Epoch 137, Batch: 0, Loss: 0.687808
Train - Epoch 138, Batch: 0, Loss: 0.687987
Train - Epoch 139, Batch: 0, Loss: 0.687578
Train - Epoch 140, Batch: 0, Loss: 0.687670
Train - Epoch 141, Batch: 0, Loss: 0.687954
Train - Epoch 142, Batch: 0, Loss: 0.687717
Train - Epoch 143, Batch: 0, Loss: 0.687327
Train - Epoch 144, Batch: 0, Loss: 0.687488
Train - Epoch 145, Batch: 0, Loss: 0.687193
Train - Epoch 146, Batch: 0, Loss: 0.687570
Train - Epoch 147, Batch: 0, Loss: 0.687493
Train - Epoch 148, Batch: 0, Loss: 0.687461
Train - Epoch 149, Batch: 0, Loss: 0.687619
Train - Epoch 150, Batch: 0, Loss: 0.687178
Train - Epoch 151, Batch: 0, Loss: 0.687989
Train - Epoch 152, Batch: 0, Loss: 0.687141
Train - Epoch 153, Batch: 0, Loss: 0.687029
Train - Epoch 154, Batch: 0, Loss: 0.687248
Train - Epoch 155, Batch: 0, Loss: 0.687399
Train - Epoch 156, Batch: 0, Loss: 0.686969
Train - Epoch 157, Batch: 0, Loss: 0.686989
Train - Epoch 158, Batch: 0, Loss: 0.687071
Train - Epoch 159, Batch: 0, Loss: 0.687315
Train - Epoch 160, Batch: 0, Loss: 0.687107
Train - Epoch 161, Batch: 0, Loss: 0.686692
Train - Epoch 162, Batch: 0, Loss: 0.687578
Train - Epoch 163, Batch: 0, Loss: 0.686753
Train - Epoch 164, Batch: 0, Loss: 0.687175
Train - Epoch 165, Batch: 0, Loss: 0.687463
Train - Epoch 166, Batch: 0, Loss: 0.686711
Train - Epoch 167, Batch: 0, Loss: 0.686990
Train - Epoch 168, Batch: 0, Loss: 0.686424
Train - Epoch 169, Batch: 0, Loss: 0.686730
Train - Epoch 170, Batch: 0, Loss: 0.686331
Train - Epoch 171, Batch: 0, Loss: 0.687015
Train - Epoch 172, Batch: 0, Loss: 0.685801
Train - Epoch 173, Batch: 0, Loss: 0.686525
Train - Epoch 174, Batch: 0, Loss: 0.686663
Train - Epoch 175, Batch: 0, Loss: 0.686809
Train - Epoch 176, Batch: 0, Loss: 0.686353
Train - Epoch 177, Batch: 0, Loss: 0.685951
Train - Epoch 178, Batch: 0, Loss: 0.686569
Train - Epoch 179, Batch: 0, Loss: 0.686606
Train - Epoch 180, Batch: 0, Loss: 0.686338
Train - Epoch 181, Batch: 0, Loss: 0.686059
Train - Epoch 182, Batch: 0, Loss: 0.686518/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 183, Batch: 0, Loss: 0.686258
Train - Epoch 184, Batch: 0, Loss: 0.686433
Train - Epoch 185, Batch: 0, Loss: 0.686483
Train - Epoch 186, Batch: 0, Loss: 0.686485
Train - Epoch 187, Batch: 0, Loss: 0.686398
Train - Epoch 188, Batch: 0, Loss: 0.686512
Train - Epoch 189, Batch: 0, Loss: 0.686484
Train - Epoch 190, Batch: 0, Loss: 0.686571
Train - Epoch 191, Batch: 0, Loss: 0.686344
Train - Epoch 192, Batch: 0, Loss: 0.686097
Train - Epoch 193, Batch: 0, Loss: 0.686110
Train - Epoch 194, Batch: 0, Loss: 0.685890
Train - Epoch 195, Batch: 0, Loss: 0.685890
Train - Epoch 196, Batch: 0, Loss: 0.686128
Train - Epoch 197, Batch: 0, Loss: 0.685864
Train - Epoch 198, Batch: 0, Loss: 0.685844
Train - Epoch 199, Batch: 0, Loss: 0.685642
training_time:: 350.8796739578247
training time full:: 350.879714012146
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.502322
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 404
max_epoch:: 200
batch_size:: 10200
epoch:: 0
tensor([  0,   2,   4,   5,   6,   9,  12,  14,  15,  18,  20,  22,  25,  28,
         29,  31,  32,  36,  37,  38,  39,  40,  42,  43,  44,  46,  49,  51,
         52,  53,  54,  55,  56,  59,  60,  66,  68,  70,  77,  90,  94, 101,
        102, 103, 104, 105, 107, 108, 109, 114])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
epoch:: 60
epoch:: 61
epoch:: 62
epoch:: 63
epoch:: 64
epoch:: 65
epoch:: 66
epoch:: 67
epoch:: 68
epoch:: 69
epoch:: 70
epoch:: 71
epoch:: 72
epoch:: 73
epoch:: 74
epoch:: 75
epoch:: 76
epoch:: 77
epoch:: 78
epoch:: 79
epoch:: 80
epoch:: 81
epoch:: 82
epoch:: 83
epoch:: 84
epoch:: 85
epoch:: 86
epoch:: 87
epoch:: 88
epoch:: 89
epoch:: 90
epoch:: 91
epoch:: 92
epoch:: 93
epoch:: 94
epoch:: 95
epoch:: 96
epoch:: 97
epoch:: 98
epoch:: 99
epoch:: 100
epoch:: 101
epoch:: 102
epoch:: 103
epoch:: 104
epoch:: 105
epoch:: 106
epoch:: 107
epoch:: 108
epoch:: 109
epoch:: 110
epoch:: 111
epoch:: 112
epoch:: 113
epoch:: 114
epoch:: 115
epoch:: 116
epoch:: 117
epoch:: 118
epoch:: 119
epoch:: 120
epoch:: 121
epoch:: 122
epoch:: 123
epoch:: 124
epoch:: 125
epoch:: 126
epoch:: 127
epoch:: 128
epoch:: 129
epoch:: 130
epoch:: 131
epoch:: 132
epoch:: 133
epoch:: 134
epoch:: 135
epoch:: 136
epoch:: 137
epoch:: 138
epoch:: 139
epoch:: 140
epoch:: 141
epoch:: 142
epoch:: 143
epoch:: 144
epoch:: 145
epoch:: 146
epoch:: 147
epoch:: 148
epoch:: 149
epoch:: 150
epoch:: 151
epoch:: 152
epoch:: 153
epoch:: 154
epoch:: 155
epoch:: 156
epoch:: 157
epoch:: 158
epoch:: 159
epoch:: 160
epoch:: 161
epoch:: 162
epoch:: 163
epoch:: 164
epoch:: 165
epoch:: 166
epoch:: 167
epoch:: 168
epoch:: 169
epoch:: 170
epoch:: 171
epoch:: 172
epoch:: 173
epoch:: 174
epoch:: 175
epoch:: 176
epoch:: 177
epoch:: 178
epoch:: 179
epoch:: 180
epoch:: 181
epoch:: 182
epoch:: 183
epoch:: 184
epoch:: 185
epoch:: 186
epoch:: 187
epoch:: 188
epoch:: 189
epoch:: 190
epoch:: 191
epoch:: 192
epoch:: 193
epoch:: 194
epoch:: 195
epoch:: 196
epoch:: 197
epoch:: 198
epoch:: 199
training time is 270.83145475387573
overhead:: 0
overhead2:: 0.4067244529724121
overhead3:: 0
time_baseline:: 270.83149886131287
curr_diff: 0 tensor(0.1624, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1624, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9873, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.487155
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 3 0.02 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 404
max_epoch:: 200
overhead:: 0
overhead2:: 0.02135777473449707
overhead3:: 0.12500953674316406
overhead4:: 31.812525749206543
overhead5:: 0
memory usage:: 26607312896
time_provenance:: 50.256587982177734
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1621, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1621, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9874, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.487155
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 3 0.02 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 404
max_epoch:: 200
overhead:: 0
overhead2:: 0.027170896530151367
overhead3:: 0.1339879035949707
overhead4:: 37.73885726928711
overhead5:: 0
memory usage:: 26610024448
time_provenance:: 57.79036903381348
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1621, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1621, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9874, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.487155
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 3 0.02 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 404
max_epoch:: 200
overhead:: 0
overhead2:: 0.030778169631958008
overhead3:: 0.14295101165771484
overhead4:: 43.70377206802368
overhead5:: 0
memory usage:: 26598629376
time_provenance:: 65.38096594810486
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1621, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1621, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9874, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.487155
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 3 0.02 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 404
max_epoch:: 200
overhead:: 0
overhead2:: 0.03777575492858887
overhead3:: 0.16191768646240234
overhead4:: 57.60875201225281
overhead5:: 0
memory usage:: 26602291200
time_provenance:: 83.49175572395325
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1621, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1621, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9873, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.487155
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 3 0.02 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 404
max_epoch:: 200
overhead:: 0
overhead2:: 0.044788360595703125
overhead3:: 0.17063045501708984
overhead4:: 62.51695132255554
overhead5:: 0
memory usage:: 26619092992
time_provenance:: 89.94921374320984
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1621, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1621, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9873, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.487155
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 3 0.02 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 200
delta_size:: 404
max_epoch:: 200
overhead:: 0
overhead2:: 0.047139644622802734
overhead3:: 0.17287564277648926
overhead4:: 68.54189038276672
overhead5:: 0
memory usage:: 26616074240
time_provenance:: 97.42986750602722
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1621, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1621, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9873, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000068, Accuracy: 0.487155
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 3 0.02 6000
