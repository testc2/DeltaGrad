period::
init_iters::
varied deletion rate::
varied number of samples::
python3 generate_dataset_train_test_transfer.py resnet152 cifar10_2 4096 500 10 1 1
Files already downloaded and verified
Files already downloaded and verified
0 100
torch.Size([100, 2048])
1 100
torch.Size([100, 2048])
2 100
torch.Size([100, 2048])
3 100
torch.Size([100, 2048])
4 100
torch.Size([100, 2048])
5 100
torch.Size([100, 2048])
6 100
torch.Size([100, 2048])
7 100
torch.Size([100, 2048])
8 100
torch.Size([100, 2048])
9 100
torch.Size([100, 2048])
10 100
torch.Size([100, 2048])
11 100
torch.Size([100, 2048])
12 100
torch.Size([100, 2048])
13 100
torch.Size([100, 2048])
14 100
torch.Size([100, 2048])
15 100
torch.Size([100, 2048])
16 100
torch.Size([100, 2048])
17 100
torch.Size([100, 2048])
18 100
torch.Size([100, 2048])
19 100
torch.Size([100, 2048])
20 100
torch.Size([100, 2048])
21 100
torch.Size([100, 2048])
22 100
torch.Size([100, 2048])
23 100
torch.Size([100, 2048])
24 100
torch.Size([100, 2048])
25 100
torch.Size([100, 2048])
26 100
torch.Size([100, 2048])
27 100
torch.Size([100, 2048])
28 100
torch.Size([100, 2048])
29 100
torch.Size([100, 2048])
30 100
torch.Size([100, 2048])
31 100
torch.Size([100, 2048])
32 100
torch.Size([100, 2048])
33 100
torch.Size([100, 2048])
34 100
torch.Size([100, 2048])
35 100
torch.Size([100, 2048])
36 100
torch.Size([100, 2048])
37 100
torch.Size([100, 2048])
38 100
torch.Size([100, 2048])
39 100
torch.Size([100, 2048])
40 100
torch.Size([100, 2048])
41 100
torch.Size([100, 2048])
42 100
torch.Size([100, 2048])
43 100
torch.Size([100, 2048])
44 100
torch.Size([100, 2048])
45 100
torch.Size([100, 2048])
46 100
torch.Size([100, 2048])
47 100
torch.Size([100, 2048])
48 100
torch.Size([100, 2048])
49 100
torch.Size([100, 2048])
50 100
torch.Size([100, 2048])
51 100
torch.Size([100, 2048])
52 100
torch.Size([100, 2048])
53 100
torch.Size([100, 2048])
54 100
torch.Size([100, 2048])
55 100
torch.Size([100, 2048])
56 100
torch.Size([100, 2048])
57 100
torch.Size([100, 2048])
58 100
torch.Size([100, 2048])
59 100
torch.Size([100, 2048])
60 100
torch.Size([100, 2048])
61 100
torch.Size([100, 2048])
62 100
torch.Size([100, 2048])
63 100
torch.Size([100, 2048])
64 100
torch.Size([100, 2048])
65 100
torch.Size([100, 2048])
66 100
torch.Size([100, 2048])
67 100
torch.Size([100, 2048])
68 100
torch.Size([100, 2048])
69 100
torch.Size([100, 2048])
70 100
torch.Size([100, 2048])
71 100
torch.Size([100, 2048])
72 100
torch.Size([100, 2048])
73 100
torch.Size([100, 2048])
74 100
torch.Size([100, 2048])
75 100
torch.Size([100, 2048])
76 100
torch.Size([100, 2048])
77 100
torch.Size([100, 2048])
78 100
torch.Size([100, 2048])
79 100
torch.Size([100, 2048])
80 100
torch.Size([100, 2048])
81 100
torch.Size([100, 2048])
82 100
torch.Size([100, 2048])
83 100
torch.Size([100, 2048])
84 100
torch.Size([100, 2048])
85 100
torch.Size([100, 2048])
86 100
torch.Size([100, 2048])
87 100
torch.Size([100, 2048])
88 100
torch.Size([100, 2048])
89 100
torch.Size([100, 2048])
90 100
torch.Size([100, 2048])
91 100
torch.Size([100, 2048])
92 100
torch.Size([100, 2048])
93 100
torch.Size([100, 2048])
94 100
torch.Size([100, 2048])
95 100
torch.Size([100, 2048])
96 100
torch.Size([100, 2048])
97 100
torch.Size([100, 2048])
98 100
torch.Size([100, 2048])
99 100
torch.Size([100, 2048])
100 100
torch.Size([100, 2048])
101 100
torch.Size([100, 2048])
102 100
torch.Size([100, 2048])
103 100
torch.Size([100, 2048])
104 100
torch.Size([100, 2048])
105 100
torch.Size([100, 2048])
106 100
torch.Size([100, 2048])
107 100
torch.Size([100, 2048])
108 100
torch.Size([100, 2048])
109 100
torch.Size([100, 2048])
110 100
torch.Size([100, 2048])
111 100
torch.Size([100, 2048])
112 100
torch.Size([100, 2048])
113 100
torch.Size([100, 2048])
114 100
torch.Size([100, 2048])
115 100
torch.Size([100, 2048])
116 100
torch.Size([100, 2048])
117 100
torch.Size([100, 2048])
118 100
torch.Size([100, 2048])
119 100
torch.Size([100, 2048])
120 100
torch.Size([100, 2048])
121 100
torch.Size([100, 2048])
122 100
torch.Size([100, 2048])
123 100
torch.Size([100, 2048])
124 100
torch.Size([100, 2048])
125 100
torch.Size([100, 2048])
126 100
torch.Size([100, 2048])
127 100
torch.Size([100, 2048])
128 100
torch.Size([100, 2048])
129 100
torch.Size([100, 2048])
130 100
torch.Size([100, 2048])
131 100
torch.Size([100, 2048])
132 100
torch.Size([100, 2048])
133 100
torch.Size([100, 2048])
134 100
torch.Size([100, 2048])
135 100
torch.Size([100, 2048])
136 100
torch.Size([100, 2048])
137 100
torch.Size([100, 2048])
138 100
torch.Size([100, 2048])
139 100
torch.Size([100, 2048])
140 100
torch.Size([100, 2048])
141 100
torch.Size([100, 2048])
142 100
torch.Size([100, 2048])
143 100
torch.Size([100, 2048])
144 100
torch.Size([100, 2048])
145 100
torch.Size([100, 2048])
146 100
torch.Size([100, 2048])
147 100
torch.Size([100, 2048])
148 100
torch.Size([100, 2048])
149 100
torch.Size([100, 2048])
150 100
torch.Size([100, 2048])
151 100
torch.Size([100, 2048])
152 100
torch.Size([100, 2048])
153 100
torch.Size([100, 2048])
154 100
torch.Size([100, 2048])
155 100
torch.Size([100, 2048])
156 100
torch.Size([100, 2048])
157 100
torch.Size([100, 2048])
158 100
torch.Size([100, 2048])
159 100
torch.Size([100, 2048])
160 100
torch.Size([100, 2048])
161 100
torch.Size([100, 2048])
162 100
torch.Size([100, 2048])
163 100
torch.Size([100, 2048])
164 100
torch.Size([100, 2048])
165 100
torch.Size([100, 2048])
166 100
torch.Size([100, 2048])
167 100
torch.Size([100, 2048])
168 100
torch.Size([100, 2048])
169 100
torch.Size([100, 2048])
170 100
torch.Size([100, 2048])
171 100
torch.Size([100, 2048])
172 100
torch.Size([100, 2048])
173 100
torch.Size([100, 2048])
174 100
torch.Size([100, 2048])
175 100
torch.Size([100, 2048])
176 100
torch.Size([100, 2048])
177 100
torch.Size([100, 2048])
178 100
torch.Size([100, 2048])
179 100
torch.Size([100, 2048])
180 100
torch.Size([100, 2048])
181 100
torch.Size([100, 2048])
182 100
torch.Size([100, 2048])
183 100
torch.Size([100, 2048])
184 100
torch.Size([100, 2048])
185 100
torch.Size([100, 2048])
186 100
torch.Size([100, 2048])
187 100
torch.Size([100, 2048])
188 100
torch.Size([100, 2048])
189 100
torch.Size([100, 2048])
190 100
torch.Size([100, 2048])
191 100
torch.Size([100, 2048])
192 100
torch.Size([100, 2048])
193 100
torch.Size([100, 2048])
194 100
torch.Size([100, 2048])
195 100
torch.Size([100, 2048])
196 100
torch.Size([100, 2048])
197 100
torch.Size([100, 2048])
198 100
torch.Size([100, 2048])
199 100
torch.Size([100, 2048])
200 100
torch.Size([100, 2048])
201 100
torch.Size([100, 2048])
202 100
torch.Size([100, 2048])
203 100
torch.Size([100, 2048])
204 100
torch.Size([100, 2048])
205 100
torch.Size([100, 2048])
206 100
torch.Size([100, 2048])
207 100
torch.Size([100, 2048])
208 100
torch.Size([100, 2048])
209 100
torch.Size([100, 2048])
210 100
torch.Size([100, 2048])
211 100
torch.Size([100, 2048])
212 100
torch.Size([100, 2048])
213 100
torch.Size([100, 2048])
214 100
torch.Size([100, 2048])
215 100
torch.Size([100, 2048])
216 100
torch.Size([100, 2048])
217 100
torch.Size([100, 2048])
218 100
torch.Size([100, 2048])
219 100
torch.Size([100, 2048])
220 100
torch.Size([100, 2048])
221 100
torch.Size([100, 2048])
222 100
torch.Size([100, 2048])
223 100
torch.Size([100, 2048])
224 100
torch.Size([100, 2048])
225 100
torch.Size([100, 2048])
226 100
torch.Size([100, 2048])
227 100
torch.Size([100, 2048])
228 100
torch.Size([100, 2048])
229 100
torch.Size([100, 2048])
230 100
torch.Size([100, 2048])
231 100
torch.Size([100, 2048])
232 100
torch.Size([100, 2048])
233 100
torch.Size([100, 2048])
234 100
torch.Size([100, 2048])
235 100
torch.Size([100, 2048])
236 100
torch.Size([100, 2048])
237 100
torch.Size([100, 2048])
238 100
torch.Size([100, 2048])
239 100
torch.Size([100, 2048])
240 100
torch.Size([100, 2048])
241 100
torch.Size([100, 2048])
242 100
torch.Size([100, 2048])
243 100
torch.Size([100, 2048])
244 100
torch.Size([100, 2048])
245 100
torch.Size([100, 2048])
246 100
torch.Size([100, 2048])
247 100
torch.Size([100, 2048])
248 100
torch.Size([100, 2048])
249 100
torch.Size([100, 2048])
250 100
torch.Size([100, 2048])
251 100
torch.Size([100, 2048])
252 100
torch.Size([100, 2048])
253 100
torch.Size([100, 2048])
254 100
torch.Size([100, 2048])
255 100
torch.Size([100, 2048])
256 100
torch.Size([100, 2048])
257 100
torch.Size([100, 2048])
258 100
torch.Size([100, 2048])
259 100
torch.Size([100, 2048])
260 100
torch.Size([100, 2048])
261 100
torch.Size([100, 2048])
262 100
torch.Size([100, 2048])
263 100
torch.Size([100, 2048])
264 100
torch.Size([100, 2048])
265 100
torch.Size([100, 2048])
266 100
torch.Size([100, 2048])
267 100
torch.Size([100, 2048])
268 100
torch.Size([100, 2048])
269 100
torch.Size([100, 2048])
270 100
torch.Size([100, 2048])
271 100
torch.Size([100, 2048])
272 100
torch.Size([100, 2048])
273 100
torch.Size([100, 2048])
274 100
torch.Size([100, 2048])
275 100
torch.Size([100, 2048])
276 100
torch.Size([100, 2048])
277 100
torch.Size([100, 2048])
278 100
torch.Size([100, 2048])
279 100
torch.Size([100, 2048])
280 100
torch.Size([100, 2048])
281 100
torch.Size([100, 2048])
282 100
torch.Size([100, 2048])
283 100
torch.Size([100, 2048])
284 100
torch.Size([100, 2048])
285 100
torch.Size([100, 2048])
286 100
torch.Size([100, 2048])
287 100
torch.Size([100, 2048])
288 100
torch.Size([100, 2048])
289 100
torch.Size([100, 2048])
290 100
torch.Size([100, 2048])
291 100
torch.Size([100, 2048])
292 100
torch.Size([100, 2048])
293 100
torch.Size([100, 2048])
294 100
torch.Size([100, 2048])
295 100
torch.Size([100, 2048])
296 100
torch.Size([100, 2048])
297 100
torch.Size([100, 2048])
298 100
torch.Size([100, 2048])
299 100
torch.Size([100, 2048])
300 100
torch.Size([100, 2048])
301 100
torch.Size([100, 2048])
302 100
torch.Size([100, 2048])
303 100
torch.Size([100, 2048])
304 100
torch.Size([100, 2048])
305 100
torch.Size([100, 2048])
306 100
torch.Size([100, 2048])
307 100
torch.Size([100, 2048])
308 100
torch.Size([100, 2048])
309 100
torch.Size([100, 2048])
310 100
torch.Size([100, 2048])
311 100
torch.Size([100, 2048])
312 100
torch.Size([100, 2048])
313 100
torch.Size([100, 2048])
314 100
torch.Size([100, 2048])
315 100
torch.Size([100, 2048])
316 100
torch.Size([100, 2048])
317 100
torch.Size([100, 2048])
318 100
torch.Size([100, 2048])
319 100
torch.Size([100, 2048])
320 100
torch.Size([100, 2048])
321 100
torch.Size([100, 2048])
322 100
torch.Size([100, 2048])
323 100
torch.Size([100, 2048])
324 100
torch.Size([100, 2048])
325 100
torch.Size([100, 2048])
326 100
torch.Size([100, 2048])
327 100
torch.Size([100, 2048])
328 100
torch.Size([100, 2048])
329 100
torch.Size([100, 2048])
330 100
torch.Size([100, 2048])
331 100
torch.Size([100, 2048])
332 100
torch.Size([100, 2048])
333 100
torch.Size([100, 2048])
334 100
torch.Size([100, 2048])
335 100
torch.Size([100, 2048])
336 100
torch.Size([100, 2048])
337 100
torch.Size([100, 2048])
338 100
torch.Size([100, 2048])
339 100
torch.Size([100, 2048])
340 100
torch.Size([100, 2048])
341 100
torch.Size([100, 2048])
342 100
torch.Size([100, 2048])
343 100
torch.Size([100, 2048])
344 100
torch.Size([100, 2048])
345 100
torch.Size([100, 2048])
346 100
torch.Size([100, 2048])
347 100
torch.Size([100, 2048])
348 100
torch.Size([100, 2048])
349 100
torch.Size([100, 2048])
350 100
torch.Size([100, 2048])
351 100
torch.Size([100, 2048])
352 100
torch.Size([100, 2048])
353 100
torch.Size([100, 2048])
354 100
torch.Size([100, 2048])
355 100
torch.Size([100, 2048])
356 100
torch.Size([100, 2048])
357 100
torch.Size([100, 2048])
358 100
torch.Size([100, 2048])
359 100
torch.Size([100, 2048])
360 100
torch.Size([100, 2048])
361 100
torch.Size([100, 2048])
362 100
torch.Size([100, 2048])
363 100
torch.Size([100, 2048])
364 100
torch.Size([100, 2048])
365 100
torch.Size([100, 2048])
366 100
torch.Size([100, 2048])
367 100
torch.Size([100, 2048])
368 100
torch.Size([100, 2048])
369 100
torch.Size([100, 2048])
370 100
torch.Size([100, 2048])
371 100
torch.Size([100, 2048])
372 100
torch.Size([100, 2048])
373 100
torch.Size([100, 2048])
374 100
torch.Size([100, 2048])
375 100
torch.Size([100, 2048])
376 100
torch.Size([100, 2048])
377 100
torch.Size([100, 2048])
378 100
torch.Size([100, 2048])
379 100
torch.Size([100, 2048])
380 100
torch.Size([100, 2048])
381 100
torch.Size([100, 2048])
382 100
torch.Size([100, 2048])
383 100
torch.Size([100, 2048])
384 100
torch.Size([100, 2048])
385 100
torch.Size([100, 2048])
386 100
torch.Size([100, 2048])
387 100
torch.Size([100, 2048])
388 100
torch.Size([100, 2048])
389 100
torch.Size([100, 2048])
390 100
torch.Size([100, 2048])
391 100
torch.Size([100, 2048])
392 100
torch.Size([100, 2048])
393 100
torch.Size([100, 2048])
394 100
torch.Size([100, 2048])
395 100
torch.Size([100, 2048])
396 100
torch.Size([100, 2048])
397 100
torch.Size([100, 2048])
398 100
torch.Size([100, 2048])
399 100
torch.Size([100, 2048])
400 100
torch.Size([100, 2048])
401 100
torch.Size([100, 2048])
402 100
torch.Size([100, 2048])
403 100
torch.Size([100, 2048])
404 100
torch.Size([100, 2048])
405 100
torch.Size([100, 2048])
406 100
torch.Size([100, 2048])
407 100
torch.Size([100, 2048])
408 100
torch.Size([100, 2048])
409 100
torch.Size([100, 2048])
410 100
torch.Size([100, 2048])
411 100
torch.Size([100, 2048])
412 100
torch.Size([100, 2048])
413 100
torch.Size([100, 2048])
414 100
torch.Size([100, 2048])
415 100
torch.Size([100, 2048])
416 100
torch.Size([100, 2048])
417 100
torch.Size([100, 2048])
418 100
torch.Size([100, 2048])
419 100
torch.Size([100, 2048])
420 100
torch.Size([100, 2048])
421 100
torch.Size([100, 2048])
422 100
torch.Size([100, 2048])
423 100
torch.Size([100, 2048])
424 100
torch.Size([100, 2048])
425 100
torch.Size([100, 2048])
426 100
torch.Size([100, 2048])
427 100
torch.Size([100, 2048])
428 100
torch.Size([100, 2048])
429 100
torch.Size([100, 2048])
430 100
torch.Size([100, 2048])
431 100
torch.Size([100, 2048])
432 100
torch.Size([100, 2048])
433 100
torch.Size([100, 2048])
434 100
torch.Size([100, 2048])
435 100
torch.Size([100, 2048])
436 100
torch.Size([100, 2048])
437 100
torch.Size([100, 2048])
438 100
torch.Size([100, 2048])
439 100
torch.Size([100, 2048])
440 100
torch.Size([100, 2048])
441 100
torch.Size([100, 2048])
442 100
torch.Size([100, 2048])
443 100
torch.Size([100, 2048])
444 100
torch.Size([100, 2048])
445 100
torch.Size([100, 2048])
446 100
torch.Size([100, 2048])
447 100
torch.Size([100, 2048])
448 100
torch.Size([100, 2048])
449 100
torch.Size([100, 2048])
450 100
torch.Size([100, 2048])
451 100
torch.Size([100, 2048])
452 100
torch.Size([100, 2048])
453 100
torch.Size([100, 2048])
454 100
torch.Size([100, 2048])
455 100
torch.Size([100, 2048])
456 100
torch.Size([100, 2048])
457 100
torch.Size([100, 2048])
458 100
torch.Size([100, 2048])
459 100
torch.Size([100, 2048])
460 100
torch.Size([100, 2048])
461 100
torch.Size([100, 2048])
462 100
torch.Size([100, 2048])
463 100
torch.Size([100, 2048])
464 100
torch.Size([100, 2048])
465 100
torch.Size([100, 2048])
466 100
torch.Size([100, 2048])
467 100
torch.Size([100, 2048])
468 100
torch.Size([100, 2048])
469 100
torch.Size([100, 2048])
470 100
torch.Size([100, 2048])
471 100
torch.Size([100, 2048])
472 100
torch.Size([100, 2048])
473 100
torch.Size([100, 2048])
474 100
torch.Size([100, 2048])
475 100
torch.Size([100, 2048])
476 100
torch.Size([100, 2048])
477 100
torch.Size([100, 2048])
478 100
torch.Size([100, 2048])
479 100
torch.Size([100, 2048])
480 100
torch.Size([100, 2048])
481 100
torch.Size([100, 2048])
482 100
torch.Size([100, 2048])
483 100
torch.Size([100, 2048])
484 100
torch.Size([100, 2048])
485 100
torch.Size([100, 2048])
486 100
torch.Size([100, 2048])
487 100
torch.Size([100, 2048])
488 100
torch.Size([100, 2048])
489 100
torch.Size([100, 2048])
490 100
torch.Size([100, 2048])
491 100
torch.Size([100, 2048])
492 100
torch.Size([100, 2048])
493 100
torch.Size([100, 2048])
494 100
torch.Size([100, 2048])
495 100
torch.Size([100, 2048])
496 100
torch.Size([100, 2048])
497 100
torch.Size([100, 2048])
498 100
torch.Size([100, 2048])
499 100
torch.Size([100, 2048])
torch.Size([50000, 2048])
normalization start!!
0 100
torch.Size([100, 2048])
1 100
torch.Size([100, 2048])
2 100
torch.Size([100, 2048])
3 100
torch.Size([100, 2048])
4 100
torch.Size([100, 2048])
5 100
torch.Size([100, 2048])
6 100
torch.Size([100, 2048])
7 100
torch.Size([100, 2048])
8 100
torch.Size([100, 2048])
9 100
torch.Size([100, 2048])
10 100
torch.Size([100, 2048])
11 100
torch.Size([100, 2048])
12 100
torch.Size([100, 2048])
13 100
torch.Size([100, 2048])
14 100
torch.Size([100, 2048])
15 100
torch.Size([100, 2048])
16 100
torch.Size([100, 2048])
17 100
torch.Size([100, 2048])
18 100
torch.Size([100, 2048])
19 100
torch.Size([100, 2048])
20 100
torch.Size([100, 2048])
21 100
torch.Size([100, 2048])
22 100
torch.Size([100, 2048])
23 100
torch.Size([100, 2048])
24 100
torch.Size([100, 2048])
25 100
torch.Size([100, 2048])
26 100
torch.Size([100, 2048])
27 100
torch.Size([100, 2048])
28 100
torch.Size([100, 2048])
29 100
torch.Size([100, 2048])
30 100
torch.Size([100, 2048])
31 100
torch.Size([100, 2048])
32 100
torch.Size([100, 2048])
33 100
torch.Size([100, 2048])
34 100
torch.Size([100, 2048])
35 100
torch.Size([100, 2048])
36 100
torch.Size([100, 2048])
37 100
torch.Size([100, 2048])
38 100
torch.Size([100, 2048])
39 100
torch.Size([100, 2048])
40 100
torch.Size([100, 2048])
41 100
torch.Size([100, 2048])
42 100
torch.Size([100, 2048])
43 100
torch.Size([100, 2048])
44 100
torch.Size([100, 2048])
45 100
torch.Size([100, 2048])
46 100
torch.Size([100, 2048])
47 100
torch.Size([100, 2048])
48 100
torch.Size([100, 2048])
49 100
torch.Size([100, 2048])
50 100
torch.Size([100, 2048])
51 100
torch.Size([100, 2048])
52 100
torch.Size([100, 2048])
53 100
torch.Size([100, 2048])
54 100
torch.Size([100, 2048])
55 100
torch.Size([100, 2048])
56 100
torch.Size([100, 2048])
57 100
torch.Size([100, 2048])
58 100
torch.Size([100, 2048])
59 100
torch.Size([100, 2048])
60 100
torch.Size([100, 2048])
61 100
torch.Size([100, 2048])
62 100
torch.Size([100, 2048])
63 100
torch.Size([100, 2048])
64 100
torch.Size([100, 2048])
65 100
torch.Size([100, 2048])
66 100
torch.Size([100, 2048])
67 100
torch.Size([100, 2048])
68 100
torch.Size([100, 2048])
69 100
torch.Size([100, 2048])
70 100
torch.Size([100, 2048])
71 100
torch.Size([100, 2048])
72 100
torch.Size([100, 2048])
73 100
torch.Size([100, 2048])
74 100
torch.Size([100, 2048])
75 100
torch.Size([100, 2048])
76 100
torch.Size([100, 2048])
77 100
torch.Size([100, 2048])
78 100
torch.Size([100, 2048])
79 100
torch.Size([100, 2048])
80 100
torch.Size([100, 2048])
81 100
torch.Size([100, 2048])
82 100
torch.Size([100, 2048])
83 100
torch.Size([100, 2048])
84 100
torch.Size([100, 2048])
85 100
torch.Size([100, 2048])
86 100
torch.Size([100, 2048])
87 100
torch.Size([100, 2048])
88 100
torch.Size([100, 2048])
89 100
torch.Size([100, 2048])
90 100
torch.Size([100, 2048])
91 100
torch.Size([100, 2048])
92 100
torch.Size([100, 2048])
93 100
torch.Size([100, 2048])
94 100
torch.Size([100, 2048])
95 100
torch.Size([100, 2048])
96 100
torch.Size([100, 2048])
97 100
torch.Size([100, 2048])
98 100
torch.Size([100, 2048])
99 100
torch.Size([100, 2048])
torch.Size([10000, 2048])
normalization start!!
deletion rate:: 0.00005
python3 generate_rand_ids 0.00005  cifar10_2 1
torch.Size([50000, 2048])
tensor([12278, 33087])
batch size:: 10000
repetition 0
python3 benchmark_exp_lr.py 0.001 10000 500 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05] Logistic_regression cifar10_2 0 0.0001 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 2.313616
Train - Epoch 1, Batch: 0, Loss: 2.249170
Train - Epoch 2, Batch: 0, Loss: 2.196910
Train - Epoch 3, Batch: 0, Loss: 2.145447
Train - Epoch 4, Batch: 0, Loss: 2.094617
Train - Epoch 5, Batch: 0, Loss: 2.052156
Train - Epoch 6, Batch: 0, Loss: 2.005518
Train - Epoch 7, Batch: 0, Loss: 1.971159
Train - Epoch 8, Batch: 0, Loss: 1.933961
Train - Epoch 9, Batch: 0, Loss: 1.896567
Train - Epoch 10, Batch: 0, Loss: 1.873422
Train - Epoch 11, Batch: 0, Loss: 1.841517
Train - Epoch 12, Batch: 0, Loss: 1.811470
Train - Epoch 13, Batch: 0, Loss: 1.789120
Train - Epoch 14, Batch: 0, Loss: 1.760071
Train - Epoch 15, Batch: 0, Loss: 1.740067
Train - Epoch 16, Batch: 0, Loss: 1.723482
Train - Epoch 17, Batch: 0, Loss: 1.695288
Train - Epoch 18, Batch: 0, Loss: 1.676758
Train - Epoch 19, Batch: 0, Loss: 1.664903
Train - Epoch 20, Batch: 0, Loss: 1.637558
Train - Epoch 21, Batch: 0, Loss: 1.624701
Train - Epoch 22, Batch: 0, Loss: 1.607712
Train - Epoch 23, Batch: 0, Loss: 1.608335
Train - Epoch 24, Batch: 0, Loss: 1.576037
Train - Epoch 25, Batch: 0, Loss: 1.565519
Train - Epoch 26, Batch: 0, Loss: 1.546216
Train - Epoch 27, Batch: 0, Loss: 1.548488
Train - Epoch 28, Batch: 0, Loss: 1.528615
Train - Epoch 29, Batch: 0, Loss: 1.514318
Train - Epoch 30, Batch: 0, Loss: 1.506870
Train - Epoch 31, Batch: 0, Loss: 1.501833
Train - Epoch 32, Batch: 0, Loss: 1.477310
Train - Epoch 33, Batch: 0, Loss: 1.475678
Train - Epoch 34, Batch: 0, Loss: 1.477794
Train - Epoch 35, Batch: 0, Loss: 1.464503
Train - Epoch 36, Batch: 0, Loss: 1.448127
Train - Epoch 37, Batch: 0, Loss: 1.453023
Train - Epoch 38, Batch: 0, Loss: 1.436515
Train - Epoch 39, Batch: 0, Loss: 1.432133
Train - Epoch 40, Batch: 0, Loss: 1.430912
Train - Epoch 41, Batch: 0, Loss: 1.414921
Train - Epoch 42, Batch: 0, Loss: 1.407479
Train - Epoch 43, Batch: 0, Loss: 1.405282
Train - Epoch 44, Batch: 0, Loss: 1.400492
Train - Epoch 45, Batch: 0, Loss: 1.380965
Train - Epoch 46, Batch: 0, Loss: 1.382100
Train - Epoch 47, Batch: 0, Loss: 1.372053
Train - Epoch 48, Batch: 0, Loss: 1.374717
Train - Epoch 49, Batch: 0, Loss: 1.366974
Train - Epoch 50, Batch: 0, Loss: 1.361737
Train - Epoch 51, Batch: 0, Loss: 1.361678
Train - Epoch 52, Batch: 0, Loss: 1.362943
Train - Epoch 53, Batch: 0, Loss: 1.343026
Train - Epoch 54, Batch: 0, Loss: 1.337558
Train - Epoch 55, Batch: 0, Loss: 1.334646
Train - Epoch 56, Batch: 0, Loss: 1.340070
Train - Epoch 57, Batch: 0, Loss: 1.326720
Train - Epoch 58, Batch: 0, Loss: 1.318064
Train - Epoch 59, Batch: 0, Loss: 1.307190
Train - Epoch 60, Batch: 0, Loss: 1.321080
Train - Epoch 61, Batch: 0, Loss: 1.303703
Train - Epoch 62, Batch: 0, Loss: 1.312034
Train - Epoch 63, Batch: 0, Loss: 1.300181
Train - Epoch 64, Batch: 0, Loss: 1.297883
Train - Epoch 65, Batch: 0, Loss: 1.299300
Train - Epoch 66, Batch: 0, Loss: 1.296588
Train - Epoch 67, Batch: 0, Loss: 1.298817
Train - Epoch 68, Batch: 0, Loss: 1.301348
Train - Epoch 69, Batch: 0, Loss: 1.291766
Train - Epoch 70, Batch: 0, Loss: 1.284106
Train - Epoch 71, Batch: 0, Loss: 1.286009
Train - Epoch 72, Batch: 0, Loss: 1.278164
Train - Epoch 73, Batch: 0, Loss: 1.280927
Train - Epoch 74, Batch: 0, Loss: 1.281609
Train - Epoch 75, Batch: 0, Loss: 1.271158
Train - Epoch 76, Batch: 0, Loss: 1.265848
Train - Epoch 77, Batch: 0, Loss: 1.271597
Train - Epoch 78, Batch: 0, Loss: 1.258284
Train - Epoch 79, Batch: 0, Loss: 1.271454
Train - Epoch 80, Batch: 0, Loss: 1.260724
Train - Epoch 81, Batch: 0, Loss: 1.249547
Train - Epoch 82, Batch: 0, Loss: 1.242811
Train - Epoch 83, Batch: 0, Loss: 1.256328
Train - Epoch 84, Batch: 0, Loss: 1.258015
Train - Epoch 85, Batch: 0, Loss: 1.241598
Train - Epoch 86, Batch: 0, Loss: 1.244954
Train - Epoch 87, Batch: 0, Loss: 1.239084
Train - Epoch 88, Batch: 0, Loss: 1.235534
Train - Epoch 89, Batch: 0, Loss: 1.232572
Train - Epoch 90, Batch: 0, Loss: 1.227786
Train - Epoch 91, Batch: 0, Loss: 1.230421
Train - Epoch 92, Batch: 0, Loss: 1.221379
Train - Epoch 93, Batch: 0, Loss: 1.222677
Train - Epoch 94, Batch: 0, Loss: 1.230866
Train - Epoch 95, Batch: 0, Loss: 1.222992
Train - Epoch 96, Batch: 0, Loss: 1.219165
Train - Epoch 97, Batch: 0, Loss: 1.221347
Train - Epoch 98, Batch: 0, Loss: 1.218699
Train - Epoch 99, Batch: 0, Loss: 1.219823
Train - Epoch 100, Batch: 0, Loss: 1.224529
Train - Epoch 101, Batch: 0, Loss: 1.217138
Train - Epoch 102, Batch: 0, Loss: 1.223798
Train - Epoch 103, Batch: 0, Loss: 1.203376
Train - Epoch 104, Batch: 0, Loss: 1.206682
Train - Epoch 105, Batch: 0, Loss: 1.208667
Train - Epoch 106, Batch: 0, Loss: 1.205820
Train - Epoch 107, Batch: 0, Loss: 1.199018
Train - Epoch 108, Batch: 0, Loss: 1.195568
Train - Epoch 109, Batch: 0, Loss: 1.208968
Train - Epoch 110, Batch: 0, Loss: 1.207661
Train - Epoch 111, Batch: 0, Loss: 1.200919
Train - Epoch 112, Batch: 0, Loss: 1.187247
Train - Epoch 113, Batch: 0, Loss: 1.182185
Train - Epoch 114, Batch: 0, Loss: 1.192660
Train - Epoch 115, Batch: 0, Loss: 1.201074
Train - Epoch 116, Batch: 0, Loss: 1.188338
Train - Epoch 117, Batch: 0, Loss: 1.189135
Train - Epoch 118, Batch: 0, Loss: 1.174182
Train - Epoch 119, Batch: 0, Loss: 1.179689
Train - Epoch 120, Batch: 0, Loss: 1.184086
Train - Epoch 121, Batch: 0, Loss: 1.163282
Train - Epoch 122, Batch: 0, Loss: 1.183211
Train - Epoch 123, Batch: 0, Loss: 1.181114
Train - Epoch 124, Batch: 0, Loss: 1.187673
Train - Epoch 125, Batch: 0, Loss: 1.178777
Train - Epoch 126, Batch: 0, Loss: 1.184971
Train - Epoch 127, Batch: 0, Loss: 1.190367
Train - Epoch 128, Batch: 0, Loss: 1.176089
Train - Epoch 129, Batch: 0, Loss: 1.186064
Train - Epoch 130, Batch: 0, Loss: 1.179104
Train - Epoch 131, Batch: 0, Loss: 1.176250
Train - Epoch 132, Batch: 0, Loss: 1.166124
Train - Epoch 133, Batch: 0, Loss: 1.168381
Train - Epoch 134, Batch: 0, Loss: 1.175398
Train - Epoch 135, Batch: 0, Loss: 1.181004
Train - Epoch 136, Batch: 0, Loss: 1.171012
Train - Epoch 137, Batch: 0, Loss: 1.168662
Train - Epoch 138, Batch: 0, Loss: 1.162360
Train - Epoch 139, Batch: 0, Loss: 1.149119
Train - Epoch 140, Batch: 0, Loss: 1.164363
Train - Epoch 141, Batch: 0, Loss: 1.170114
Train - Epoch 142, Batch: 0, Loss: 1.145554
Train - Epoch 143, Batch: 0, Loss: 1.157920
Train - Epoch 144, Batch: 0, Loss: 1.169941
Train - Epoch 145, Batch: 0, Loss: 1.161821
Train - Epoch 146, Batch: 0, Loss: 1.162082
Train - Epoch 147, Batch: 0, Loss: 1.143823
Train - Epoch 148, Batch: 0, Loss: 1.154226
Train - Epoch 149, Batch: 0, Loss: 1.143197
Train - Epoch 150, Batch: 0, Loss: 1.157063
Train - Epoch 151, Batch: 0, Loss: 1.157515
Train - Epoch 152, Batch: 0, Loss: 1.159654
Train - Epoch 153, Batch: 0, Loss: 1.158372
Train - Epoch 154, Batch: 0, Loss: 1.155400
Train - Epoch 155, Batch: 0, Loss: 1.146435
Train - Epoch 156, Batch: 0, Loss: 1.148831
Train - Epoch 157, Batch: 0, Loss: 1.145821
Train - Epoch 158, Batch: 0, Loss: 1.152147
Train - Epoch 159, Batch: 0, Loss: 1.159222
Train - Epoch 160, Batch: 0, Loss: 1.147803
Train - Epoch 161, Batch: 0, Loss: 1.126723
Train - Epoch 162, Batch: 0, Loss: 1.149520
Train - Epoch 163, Batch: 0, Loss: 1.143023
Train - Epoch 164, Batch: 0, Loss: 1.154860
Train - Epoch 165, Batch: 0, Loss: 1.133867
Train - Epoch 166, Batch: 0, Loss: 1.144875
Train - Epoch 167, Batch: 0, Loss: 1.140562
Train - Epoch 168, Batch: 0, Loss: 1.125347
Train - Epoch 169, Batch: 0, Loss: 1.133667
Train - Epoch 170, Batch: 0, Loss: 1.141170
Train - Epoch 171, Batch: 0, Loss: 1.141714
Train - Epoch 172, Batch: 0, Loss: 1.136766
Train - Epoch 173, Batch: 0, Loss: 1.134106
Train - Epoch 174, Batch: 0, Loss: 1.134364
Train - Epoch 175, Batch: 0, Loss: 1.129962
Train - Epoch 176, Batch: 0, Loss: 1.139599
Train - Epoch 177, Batch: 0, Loss: 1.131007
Train - Epoch 178, Batch: 0, Loss: 1.129052
Train - Epoch 179, Batch: 0, Loss: 1.131405
Train - Epoch 180, Batch: 0, Loss: 1.133257
Train - Epoch 181, Batch: 0, Loss: 1.142263
Train - Epoch 182, Batch: 0, Loss: 1.131228
Train - Epoch 183, Batch: 0, Loss: 1.126001
Train - Epoch 184, Batch: 0, Loss: 1.103814
Train - Epoch 185, Batch: 0, Loss: 1.129754
Train - Epoch 186, Batch: 0, Loss: 1.143000
Train - Epoch 187, Batch: 0, Loss: 1.124506
Train - Epoch 188, Batch: 0, Loss: 1.127445
Train - Epoch 189, Batch: 0, Loss: 1.148200
Train - Epoch 190, Batch: 0, Loss: 1.131249
Train - Epoch 191, Batch: 0, Loss: 1.127217
Train - Epoch 192, Batch: 0, Loss: 1.117672
Train - Epoch 193, Batch: 0, Loss: 1.131306
Train - Epoch 194, Batch: 0, Loss: 1.114763
Train - Epoch 195, Batch: 0, Loss: 1.116103
Train - Epoch 196, Batch: 0, Loss: 1.118053
Train - Epoch 197, Batch: 0, Loss: 1.106562
Train - Epoch 198, Batch: 0, Loss: 1.122652
Train - Epoch 199, Batch: 0, Loss: 1.117522
Train - Epoch 200, Batch: 0, Loss: 1.126941
Train - Epoch 201, Batch: 0, Loss: 1.131322
Train - Epoch 202, Batch: 0, Loss: 1.125791
Train - Epoch 203, Batch: 0, Loss: 1.115975
Train - Epoch 204, Batch: 0, Loss: 1.132854
Train - Epoch 205, Batch: 0, Loss: 1.116318
Train - Epoch 206, Batch: 0, Loss: 1.108963
Train - Epoch 207, Batch: 0, Loss: 1.121109
Train - Epoch 208, Batch: 0, Loss: 1.108968
Train - Epoch 209, Batch: 0, Loss: 1.102559
Train - Epoch 210, Batch: 0, Loss: 1.109563
Train - Epoch 211, Batch: 0, Loss: 1.121807
Train - Epoch 212, Batch: 0, Loss: 1.103377
Train - Epoch 213, Batch: 0, Loss: 1.090778
Train - Epoch 214, Batch: 0, Loss: 1.109334
Train - Epoch 215, Batch: 0, Loss: 1.107493
Train - Epoch 216, Batch: 0, Loss: 1.109907
Train - Epoch 217, Batch: 0, Loss: 1.124221
Train - Epoch 218, Batch: 0, Loss: 1.108805
Train - Epoch 219, Batch: 0, Loss: 1.099160
Train - Epoch 220, Batch: 0, Loss: 1.114608
Train - Epoch 221, Batch: 0, Loss: 1.106910
Train - Epoch 222, Batch: 0, Loss: 1.132773
Train - Epoch 223, Batch: 0, Loss: 1.107015
Train - Epoch 224, Batch: 0, Loss: 1.108900
Train - Epoch 225, Batch: 0, Loss: 1.108410
Train - Epoch 226, Batch: 0, Loss: 1.102201
Train - Epoch 227, Batch: 0, Loss: 1.092416
Train - Epoch 228, Batch: 0, Loss: 1.113852
Train - Epoch 229, Batch: 0, Loss: 1.084155
Train - Epoch 230, Batch: 0, Loss: 1.107406
Train - Epoch 231, Batch: 0, Loss: 1.099447
Train - Epoch 232, Batch: 0, Loss: 1.114333
Train - Epoch 233, Batch: 0, Loss: 1.099904
Train - Epoch 234, Batch: 0, Loss: 1.108802
Train - Epoch 235, Batch: 0, Loss: 1.103643
Train - Epoch 236, Batch: 0, Loss: 1.111743
Train - Epoch 237, Batch: 0, Loss: 1.095400
Train - Epoch 238, Batch: 0, Loss: 1.108027
Train - Epoch 239, Batch: 0, Loss: 1.089619
Train - Epoch 240, Batch: 0, Loss: 1.107354
Train - Epoch 241, Batch: 0, Loss: 1.099644
Train - Epoch 242, Batch: 0, Loss: 1.100926
Train - Epoch 243, Batch: 0, Loss: 1.089169
Train - Epoch 244, Batch: 0, Loss: 1.100139
Train - Epoch 245, Batch: 0, Loss: 1.093222
Train - Epoch 246, Batch: 0, Loss: 1.096861
Train - Epoch 247, Batch: 0, Loss: 1.091344
Train - Epoch 248, Batch: 0, Loss: 1.085651
Train - Epoch 249, Batch: 0, Loss: 1.093970
Train - Epoch 250, Batch: 0, Loss: 1.091712
Train - Epoch 251, Batch: 0, Loss: 1.083187
Train - Epoch 252, Batch: 0, Loss: 1.092710
Train - Epoch 253, Batch: 0, Loss: 1.083475
Train - Epoch 254, Batch: 0, Loss: 1.083295
Train - Epoch 255, Batch: 0, Loss: 1.102244
Train - Epoch 256, Batch: 0, Loss: 1.084546
Train - Epoch 257, Batch: 0, Loss: 1.097913
Train - Epoch 258, Batch: 0, Loss: 1.082431
Train - Epoch 259, Batch: 0, Loss: 1.092703
Train - Epoch 260, Batch: 0, Loss: 1.095802
Train - Epoch 261, Batch: 0, Loss: 1.088317
Train - Epoch 262, Batch: 0, Loss: 1.098674
Train - Epoch 263, Batch: 0, Loss: 1.107889
Train - Epoch 264, Batch: 0, Loss: 1.078846
Train - Epoch 265, Batch: 0, Loss: 1.091252
Train - Epoch 266, Batch: 0, Loss: 1.080464
Train - Epoch 267, Batch: 0, Loss: 1.102376
Train - Epoch 268, Batch: 0, Loss: 1.085434
Train - Epoch 269, Batch: 0, Loss: 1.080701
Train - Epoch 270, Batch: 0, Loss: 1.096533
Train - Epoch 271, Batch: 0, Loss: 1.085829
Train - Epoch 272, Batch: 0, Loss: 1.090117
Train - Epoch 273, Batch: 0, Loss: 1.084249
Train - Epoch 274, Batch: 0, Loss: 1.086787
Train - Epoch 275, Batch: 0, Loss: 1.083372
Train - Epoch 276, Batch: 0, Loss: 1.089747
Train - Epoch 277, Batch: 0, Loss: 1.095064
Train - Epoch 278, Batch: 0, Loss: 1.088159
Train - Epoch 279, Batch: 0, Loss: 1.090234
Train - Epoch 280, Batch: 0, Loss: 1.110267
Train - Epoch 281, Batch: 0, Loss: 1.086124
Train - Epoch 282, Batch: 0, Loss: 1.075653
Train - Epoch 283, Batch: 0, Loss: 1.084215
Train - Epoch 284, Batch: 0, Loss: 1.086892
Train - Epoch 285, Batch: 0, Loss: 1.080664
Train - Epoch 286, Batch: 0, Loss: 1.084589
Train - Epoch 287, Batch: 0, Loss: 1.091654
Train - Epoch 288, Batch: 0, Loss: 1.072018
Train - Epoch 289, Batch: 0, Loss: 1.069196
Train - Epoch 290, Batch: 0, Loss: 1.089006
Train - Epoch 291, Batch: 0, Loss: 1.075947
Train - Epoch 292, Batch: 0, Loss: 1.084594
Train - Epoch 293, Batch: 0, Loss: 1.105581
Train - Epoch 294, Batch: 0, Loss: 1.094495
Train - Epoch 295, Batch: 0, Loss: 1.094275
Train - Epoch 296, Batch: 0, Loss: 1.078417
Train - Epoch 297, Batch: 0, Loss: 1.098855
Train - Epoch 298, Batch: 0, Loss: 1.073422
Train - Epoch 299, Batch: 0, Loss: 1.075192
Train - Epoch 300, Batch: 0, Loss: 1.081386
Train - Epoch 301, Batch: 0, Loss: 1.078458
Train - Epoch 302, Batch: 0, Loss: 1.081359
Train - Epoch 303, Batch: 0, Loss: 1.079013
Train - Epoch 304, Batch: 0, Loss: 1.085461
Train - Epoch 305, Batch: 0, Loss: 1.071416
Train - Epoch 306, Batch: 0, Loss: 1.082047
Train - Epoch 307, Batch: 0, Loss: 1.068330
Train - Epoch 308, Batch: 0, Loss: 1.077382
Train - Epoch 309, Batch: 0, Loss: 1.084725
Train - Epoch 310, Batch: 0, Loss: 1.068516
Train - Epoch 311, Batch: 0, Loss: 1.077645
Train - Epoch 312, Batch: 0, Loss: 1.070187
Train - Epoch 313, Batch: 0, Loss: 1.078237
Train - Epoch 314, Batch: 0, Loss: 1.085138
Train - Epoch 315, Batch: 0, Loss: 1.059341
Train - Epoch 316, Batch: 0, Loss: 1.066919
Train - Epoch 317, Batch: 0, Loss: 1.080912
Train - Epoch 318, Batch: 0, Loss: 1.084322
Train - Epoch 319, Batch: 0, Loss: 1.079199
Train - Epoch 320, Batch: 0, Loss: 1.075712
Train - Epoch 321, Batch: 0, Loss: 1.077684
Train - Epoch 322, Batch: 0, Loss: 1.061010
Train - Epoch 323, Batch: 0, Loss: 1.051177
Train - Epoch 324, Batch: 0, Loss: 1.059283
Train - Epoch 325, Batch: 0, Loss: 1.059922
Train - Epoch 326, Batch: 0, Loss: 1.056334
Train - Epoch 327, Batch: 0, Loss: 1.068522
Train - Epoch 328, Batch: 0, Loss: 1.067717
Train - Epoch 329, Batch: 0, Loss: 1.078143
Train - Epoch 330, Batch: 0, Loss: 1.086875
Train - Epoch 331, Batch: 0, Loss: 1.056148
Train - Epoch 332, Batch: 0, Loss: 1.075261
Train - Epoch 333, Batch: 0, Loss: 1.082646
Train - Epoch 334, Batch: 0, Loss: 1.055694
Train - Epoch 335, Batch: 0, Loss: 1.064455
Train - Epoch 336, Batch: 0, Loss: 1.079337
Train - Epoch 337, Batch: 0, Loss: 1.059074
Train - Epoch 338, Batch: 0, Loss: 1.073651
Train - Epoch 339, Batch: 0, Loss: 1.063858
Train - Epoch 340, Batch: 0, Loss: 1.063765
Train - Epoch 341, Batch: 0, Loss: 1.057754
Train - Epoch 342, Batch: 0, Loss: 1.064144
Train - Epoch 343, Batch: 0, Loss: 1.058615
Train - Epoch 344, Batch: 0, Loss: 1.071043
Train - Epoch 345, Batch: 0, Loss: 1.058185
Train - Epoch 346, Batch: 0, Loss: 1.066960
Train - Epoch 347, Batch: 0, Loss: 1.063167
Train - Epoch 348, Batch: 0, Loss: 1.062523
Train - Epoch 349, Batch: 0, Loss: 1.049970
Train - Epoch 350, Batch: 0, Loss: 1.067530
Train - Epoch 351, Batch: 0, Loss: 1.068560
Train - Epoch 352, Batch: 0, Loss: 1.070458
Train - Epoch 353, Batch: 0, Loss: 1.066108
Train - Epoch 354, Batch: 0, Loss: 1.070554
Train - Epoch 355, Batch: 0, Loss: 1.076048
Train - Epoch 356, Batch: 0, Loss: 1.073887
Train - Epoch 357, Batch: 0, Loss: 1.061037
Train - Epoch 358, Batch: 0, Loss: 1.048017
Train - Epoch 359, Batch: 0, Loss: 1.060681
Train - Epoch 360, Batch: 0, Loss: 1.071340
Train - Epoch 361, Batch: 0, Loss: 1.046152
Train - Epoch 362, Batch: 0, Loss: 1.062307
Train - Epoch 363, Batch: 0, Loss: 1.055800
Train - Epoch 364, Batch: 0, Loss: 1.055708
Train - Epoch 365, Batch: 0, Loss: 1.054214
Train - Epoch 366, Batch: 0, Loss: 1.045588
Train - Epoch 367, Batch: 0, Loss: 1.064633
Train - Epoch 368, Batch: 0, Loss: 1.047090
Train - Epoch 369, Batch: 0, Loss: 1.071341
Train - Epoch 370, Batch: 0, Loss: 1.060320
Train - Epoch 371, Batch: 0, Loss: 1.063002
Train - Epoch 372, Batch: 0, Loss: 1.055602
Train - Epoch 373, Batch: 0, Loss: 1.051472/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 374, Batch: 0, Loss: 1.046419
Train - Epoch 375, Batch: 0, Loss: 1.054172
Train - Epoch 376, Batch: 0, Loss: 1.052330
Train - Epoch 377, Batch: 0, Loss: 1.060153
Train - Epoch 378, Batch: 0, Loss: 1.060243
Train - Epoch 379, Batch: 0, Loss: 1.041023
Train - Epoch 380, Batch: 0, Loss: 1.051816
Train - Epoch 381, Batch: 0, Loss: 1.054587
Train - Epoch 382, Batch: 0, Loss: 1.054422
Train - Epoch 383, Batch: 0, Loss: 1.064698
Train - Epoch 384, Batch: 0, Loss: 1.049039
Train - Epoch 385, Batch: 0, Loss: 1.052795
Train - Epoch 386, Batch: 0, Loss: 1.068924
Train - Epoch 387, Batch: 0, Loss: 1.033458
Train - Epoch 388, Batch: 0, Loss: 1.053239
Train - Epoch 389, Batch: 0, Loss: 1.061532
Train - Epoch 390, Batch: 0, Loss: 1.058044
Train - Epoch 391, Batch: 0, Loss: 1.048762
Train - Epoch 392, Batch: 0, Loss: 1.050431
Train - Epoch 393, Batch: 0, Loss: 1.072187
Train - Epoch 394, Batch: 0, Loss: 1.065612
Train - Epoch 395, Batch: 0, Loss: 1.059200
Train - Epoch 396, Batch: 0, Loss: 1.052208
Train - Epoch 397, Batch: 0, Loss: 1.067524
Train - Epoch 398, Batch: 0, Loss: 1.063531
Train - Epoch 399, Batch: 0, Loss: 1.061605
Train - Epoch 400, Batch: 0, Loss: 1.050304
Train - Epoch 401, Batch: 0, Loss: 1.051837
Train - Epoch 402, Batch: 0, Loss: 1.065749
Train - Epoch 403, Batch: 0, Loss: 1.055864
Train - Epoch 404, Batch: 0, Loss: 1.047540
Train - Epoch 405, Batch: 0, Loss: 1.043768
Train - Epoch 406, Batch: 0, Loss: 1.056810
Train - Epoch 407, Batch: 0, Loss: 1.059315
Train - Epoch 408, Batch: 0, Loss: 1.038852
Train - Epoch 409, Batch: 0, Loss: 1.055911
Train - Epoch 410, Batch: 0, Loss: 1.066702
Train - Epoch 411, Batch: 0, Loss: 1.050747
Train - Epoch 412, Batch: 0, Loss: 1.039231
Train - Epoch 413, Batch: 0, Loss: 1.036614
Train - Epoch 414, Batch: 0, Loss: 1.048217
Train - Epoch 415, Batch: 0, Loss: 1.052406
Train - Epoch 416, Batch: 0, Loss: 1.054398
Train - Epoch 417, Batch: 0, Loss: 1.059557
Train - Epoch 418, Batch: 0, Loss: 1.064661
Train - Epoch 419, Batch: 0, Loss: 1.056073
Train - Epoch 420, Batch: 0, Loss: 1.060665
Train - Epoch 421, Batch: 0, Loss: 1.054368
Train - Epoch 422, Batch: 0, Loss: 1.058948
Train - Epoch 423, Batch: 0, Loss: 1.054305
Train - Epoch 424, Batch: 0, Loss: 1.051333
Train - Epoch 425, Batch: 0, Loss: 1.057716
Train - Epoch 426, Batch: 0, Loss: 1.037706
Train - Epoch 427, Batch: 0, Loss: 1.035703
Train - Epoch 428, Batch: 0, Loss: 1.061834
Train - Epoch 429, Batch: 0, Loss: 1.060872
Train - Epoch 430, Batch: 0, Loss: 1.048461
Train - Epoch 431, Batch: 0, Loss: 1.052255
Train - Epoch 432, Batch: 0, Loss: 1.046752
Train - Epoch 433, Batch: 0, Loss: 1.040919
Train - Epoch 434, Batch: 0, Loss: 1.038620
Train - Epoch 435, Batch: 0, Loss: 1.046586
Train - Epoch 436, Batch: 0, Loss: 1.035519
Train - Epoch 437, Batch: 0, Loss: 1.051871
Train - Epoch 438, Batch: 0, Loss: 1.054596
Train - Epoch 439, Batch: 0, Loss: 1.055797
Train - Epoch 440, Batch: 0, Loss: 1.051944
Train - Epoch 441, Batch: 0, Loss: 1.053044
Train - Epoch 442, Batch: 0, Loss: 1.046811
Train - Epoch 443, Batch: 0, Loss: 1.047424
Train - Epoch 444, Batch: 0, Loss: 1.033750
Train - Epoch 445, Batch: 0, Loss: 1.030190
Train - Epoch 446, Batch: 0, Loss: 1.038869
Train - Epoch 447, Batch: 0, Loss: 1.039923
Train - Epoch 448, Batch: 0, Loss: 1.041078
Train - Epoch 449, Batch: 0, Loss: 1.047678
Train - Epoch 450, Batch: 0, Loss: 1.055796
Train - Epoch 451, Batch: 0, Loss: 1.032587
Train - Epoch 452, Batch: 0, Loss: 1.053278
Train - Epoch 453, Batch: 0, Loss: 1.045296
Train - Epoch 454, Batch: 0, Loss: 1.040744
Train - Epoch 455, Batch: 0, Loss: 1.046733
Train - Epoch 456, Batch: 0, Loss: 1.044317
Train - Epoch 457, Batch: 0, Loss: 1.039231
Train - Epoch 458, Batch: 0, Loss: 1.062722
Train - Epoch 459, Batch: 0, Loss: 1.052530
Train - Epoch 460, Batch: 0, Loss: 1.056045
Train - Epoch 461, Batch: 0, Loss: 1.040278
Train - Epoch 462, Batch: 0, Loss: 1.033459
Train - Epoch 463, Batch: 0, Loss: 1.053725
Train - Epoch 464, Batch: 0, Loss: 1.052176
Train - Epoch 465, Batch: 0, Loss: 1.044646
Train - Epoch 466, Batch: 0, Loss: 1.037951
Train - Epoch 467, Batch: 0, Loss: 1.050698
Train - Epoch 468, Batch: 0, Loss: 1.048756
Train - Epoch 469, Batch: 0, Loss: 1.049271
Train - Epoch 470, Batch: 0, Loss: 1.058322
Train - Epoch 471, Batch: 0, Loss: 1.042323
Train - Epoch 472, Batch: 0, Loss: 1.049252
Train - Epoch 473, Batch: 0, Loss: 1.046430
Train - Epoch 474, Batch: 0, Loss: 1.032269
Train - Epoch 475, Batch: 0, Loss: 1.048191
Train - Epoch 476, Batch: 0, Loss: 1.036117
Train - Epoch 477, Batch: 0, Loss: 1.044024
Train - Epoch 478, Batch: 0, Loss: 1.032929
Train - Epoch 479, Batch: 0, Loss: 1.048561
Train - Epoch 480, Batch: 0, Loss: 1.046082
Train - Epoch 481, Batch: 0, Loss: 1.051167
Train - Epoch 482, Batch: 0, Loss: 1.049163
Train - Epoch 483, Batch: 0, Loss: 1.033621
Train - Epoch 484, Batch: 0, Loss: 1.050899
Train - Epoch 485, Batch: 0, Loss: 1.035613
Train - Epoch 486, Batch: 0, Loss: 1.046266
Train - Epoch 487, Batch: 0, Loss: 1.025020
Train - Epoch 488, Batch: 0, Loss: 1.036321
Train - Epoch 489, Batch: 0, Loss: 1.052841
Train - Epoch 490, Batch: 0, Loss: 1.056532
Train - Epoch 491, Batch: 0, Loss: 1.018046
Train - Epoch 492, Batch: 0, Loss: 1.053351
Train - Epoch 493, Batch: 0, Loss: 1.054892
Train - Epoch 494, Batch: 0, Loss: 1.037358
Train - Epoch 495, Batch: 0, Loss: 1.046256
Train - Epoch 496, Batch: 0, Loss: 1.055979
Train - Epoch 497, Batch: 0, Loss: 1.029072
Train - Epoch 498, Batch: 0, Loss: 1.020827
Train - Epoch 499, Batch: 0, Loss: 1.037427
training_time:: 108.05745506286621
training time full:: 108.0575225353241
provenance prepare time:: 0.0
here
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632600
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 2
max_epoch:: 500
batch_size:: 10000
epoch:: 0
tensor([    3, 32773,     7,    13,    17,    22,    24,    25,    32, 32801,
        32802,    35, 32803,    36, 32806, 32810, 32812, 32815,    49,    53,
           54, 32823,    61,    67, 32835,    68,    71, 32840, 32841, 32842,
        32843, 32848,    81, 32850, 32852,    85,    86,    91, 32860, 32862,
           95,    98,    99,   100,   102,   104,   105,   107, 32876,   117])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
epoch:: 60
epoch:: 61
epoch:: 62
epoch:: 63
epoch:: 64
epoch:: 65
epoch:: 66
epoch:: 67
epoch:: 68
epoch:: 69
epoch:: 70
epoch:: 71
epoch:: 72
epoch:: 73
epoch:: 74
epoch:: 75
epoch:: 76
epoch:: 77
epoch:: 78
epoch:: 79
epoch:: 80
epoch:: 81
epoch:: 82
epoch:: 83
epoch:: 84
epoch:: 85
epoch:: 86
epoch:: 87
epoch:: 88
epoch:: 89
epoch:: 90
epoch:: 91
epoch:: 92
epoch:: 93
epoch:: 94
epoch:: 95
epoch:: 96
epoch:: 97
epoch:: 98
epoch:: 99
epoch:: 100
epoch:: 101
epoch:: 102
epoch:: 103
epoch:: 104
epoch:: 105
epoch:: 106
epoch:: 107
epoch:: 108
epoch:: 109
epoch:: 110
epoch:: 111
epoch:: 112
epoch:: 113
epoch:: 114
epoch:: 115
epoch:: 116
epoch:: 117
epoch:: 118
epoch:: 119
epoch:: 120
epoch:: 121
epoch:: 122
epoch:: 123
epoch:: 124
epoch:: 125
epoch:: 126
epoch:: 127
epoch:: 128
epoch:: 129
epoch:: 130
epoch:: 131
epoch:: 132
epoch:: 133
epoch:: 134
epoch:: 135
epoch:: 136
epoch:: 137
epoch:: 138
epoch:: 139
epoch:: 140
epoch:: 141
epoch:: 142
epoch:: 143
epoch:: 144
epoch:: 145
epoch:: 146
epoch:: 147
epoch:: 148
epoch:: 149
epoch:: 150
epoch:: 151
epoch:: 152
epoch:: 153
epoch:: 154
epoch:: 155
epoch:: 156
epoch:: 157
epoch:: 158
epoch:: 159
epoch:: 160
epoch:: 161
epoch:: 162
epoch:: 163
epoch:: 164
epoch:: 165
epoch:: 166
epoch:: 167
epoch:: 168
epoch:: 169
epoch:: 170
epoch:: 171
epoch:: 172
epoch:: 173
epoch:: 174
epoch:: 175
epoch:: 176
epoch:: 177
epoch:: 178
epoch:: 179
epoch:: 180
epoch:: 181
epoch:: 182
epoch:: 183
epoch:: 184
epoch:: 185
epoch:: 186
epoch:: 187
epoch:: 188
epoch:: 189
epoch:: 190
epoch:: 191
epoch:: 192
epoch:: 193
epoch:: 194
epoch:: 195
epoch:: 196
epoch:: 197
epoch:: 198
epoch:: 199
epoch:: 200
epoch:: 201
epoch:: 202
epoch:: 203
epoch:: 204
epoch:: 205
epoch:: 206
epoch:: 207
epoch:: 208
epoch:: 209
epoch:: 210
epoch:: 211
epoch:: 212
epoch:: 213
epoch:: 214
epoch:: 215
epoch:: 216
epoch:: 217
epoch:: 218
epoch:: 219
epoch:: 220
epoch:: 221
epoch:: 222
epoch:: 223
epoch:: 224
epoch:: 225
epoch:: 226
epoch:: 227
epoch:: 228
epoch:: 229
epoch:: 230
epoch:: 231
epoch:: 232
epoch:: 233
epoch:: 234
epoch:: 235
epoch:: 236
epoch:: 237
epoch:: 238
epoch:: 239
epoch:: 240
epoch:: 241
epoch:: 242
epoch:: 243
epoch:: 244
epoch:: 245
epoch:: 246
epoch:: 247
epoch:: 248
epoch:: 249
epoch:: 250
epoch:: 251
epoch:: 252
epoch:: 253
epoch:: 254
epoch:: 255
epoch:: 256
epoch:: 257
epoch:: 258
epoch:: 259
epoch:: 260
epoch:: 261
epoch:: 262
epoch:: 263
epoch:: 264
epoch:: 265
epoch:: 266
epoch:: 267
epoch:: 268
epoch:: 269
epoch:: 270
epoch:: 271
epoch:: 272
epoch:: 273
epoch:: 274
epoch:: 275
epoch:: 276
epoch:: 277
epoch:: 278
epoch:: 279
epoch:: 280
epoch:: 281
epoch:: 282
epoch:: 283
epoch:: 284
epoch:: 285
epoch:: 286
epoch:: 287
epoch:: 288
epoch:: 289
epoch:: 290
epoch:: 291
epoch:: 292
epoch:: 293
epoch:: 294
epoch:: 295
epoch:: 296
epoch:: 297
epoch:: 298
epoch:: 299
epoch:: 300
epoch:: 301
epoch:: 302
epoch:: 303
epoch:: 304
epoch:: 305
epoch:: 306
epoch:: 307
epoch:: 308
epoch:: 309
epoch:: 310
epoch:: 311
epoch:: 312
epoch:: 313
epoch:: 314
epoch:: 315
epoch:: 316
epoch:: 317
epoch:: 318
epoch:: 319
epoch:: 320
epoch:: 321
epoch:: 322
epoch:: 323
epoch:: 324
epoch:: 325
epoch:: 326
epoch:: 327
epoch:: 328
epoch:: 329
epoch:: 330
epoch:: 331
epoch:: 332
epoch:: 333
epoch:: 334
epoch:: 335
epoch:: 336
epoch:: 337
epoch:: 338
epoch:: 339
epoch:: 340
epoch:: 341
epoch:: 342
epoch:: 343
epoch:: 344
epoch:: 345
epoch:: 346
epoch:: 347
epoch:: 348
epoch:: 349
epoch:: 350
epoch:: 351
epoch:: 352
epoch:: 353
epoch:: 354
epoch:: 355
epoch:: 356
epoch:: 357
epoch:: 358
epoch:: 359
epoch:: 360
epoch:: 361
epoch:: 362
epoch:: 363
epoch:: 364
epoch:: 365
epoch:: 366
epoch:: 367
epoch:: 368
epoch:: 369
epoch:: 370
epoch:: 371
epoch:: 372
epoch:: 373
epoch:: 374
epoch:: 375
epoch:: 376
epoch:: 377
epoch:: 378
epoch:: 379
epoch:: 380
epoch:: 381
epoch:: 382
epoch:: 383
epoch:: 384
epoch:: 385
epoch:: 386
epoch:: 387
epoch:: 388
epoch:: 389
epoch:: 390
epoch:: 391
epoch:: 392
epoch:: 393
epoch:: 394
epoch:: 395
epoch:: 396
epoch:: 397
epoch:: 398
epoch:: 399
epoch:: 400
epoch:: 401
epoch:: 402
epoch:: 403
epoch:: 404
epoch:: 405
epoch:: 406
epoch:: 407
epoch:: 408
epoch:: 409
epoch:: 410
epoch:: 411
epoch:: 412
epoch:: 413
epoch:: 414
epoch:: 415
epoch:: 416
epoch:: 417
epoch:: 418
epoch:: 419
epoch:: 420
epoch:: 421
epoch:: 422
epoch:: 423
epoch:: 424
epoch:: 425
epoch:: 426
epoch:: 427
epoch:: 428
epoch:: 429
epoch:: 430
epoch:: 431
epoch:: 432
epoch:: 433
epoch:: 434
epoch:: 435
epoch:: 436
epoch:: 437
epoch:: 438
epoch:: 439
epoch:: 440
epoch:: 441
epoch:: 442
epoch:: 443
epoch:: 444
epoch:: 445
epoch:: 446
epoch:: 447
epoch:: 448
epoch:: 449
epoch:: 450
epoch:: 451
epoch:: 452
epoch:: 453
epoch:: 454
epoch:: 455
epoch:: 456
epoch:: 457
epoch:: 458
epoch:: 459
epoch:: 460
epoch:: 461
epoch:: 462
epoch:: 463
epoch:: 464
epoch:: 465
epoch:: 466
epoch:: 467
epoch:: 468
epoch:: 469
epoch:: 470
epoch:: 471
epoch:: 472
epoch:: 473
epoch:: 474
epoch:: 475
epoch:: 476
epoch:: 477
epoch:: 478
epoch:: 479
epoch:: 480
epoch:: 481
epoch:: 482
epoch:: 483
epoch:: 484
epoch:: 485
epoch:: 486
epoch:: 487
epoch:: 488
epoch:: 489
epoch:: 490
epoch:: 491
epoch:: 492
epoch:: 493
epoch:: 494
epoch:: 495
epoch:: 496
epoch:: 497
epoch:: 498
epoch:: 499
training time is 77.88246560096741
overhead:: 0
overhead2:: 0.9782028198242188
overhead3:: 0
time_baseline:: 77.88249635696411
curr_diff: 0 tensor(0.0089, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0089, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632600
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 2
max_epoch:: 500
overhead:: 0
overhead2:: 0.02672553062438965
overhead3:: 0.23806357383728027
overhead4:: 9.235287189483643
overhead5:: 0
memory usage:: 5650358272
time_provenance:: 15.618513345718384
curr_diff: 0 tensor(0.0021, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0021, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0104, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0104, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632700
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 2
max_epoch:: 500
overhead:: 0
overhead2:: 0.027211666107177734
overhead3:: 0.2601199150085449
overhead4:: 9.78423523902893
overhead5:: 0
memory usage:: 5691457536
time_provenance:: 16.25138545036316
curr_diff: 0 tensor(0.0021, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0021, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0104, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0104, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632500
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 2
max_epoch:: 500
overhead:: 0
overhead2:: 0.02774953842163086
overhead3:: 0.2589290142059326
overhead4:: 9.913484573364258
overhead5:: 0
memory usage:: 5631066112
time_provenance:: 16.36835265159607
curr_diff: 0 tensor(0.0021, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0021, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0104, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0104, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632700
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 2
max_epoch:: 500
overhead:: 0
overhead2:: 0.05868959426879883
overhead3:: 0.4127953052520752
overhead4:: 17.414777755737305
overhead5:: 0
memory usage:: 5627789312
time_provenance:: 25.85772156715393
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0090, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0090, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632500
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 2
max_epoch:: 500
overhead:: 0
overhead2:: 0.05236196517944336
overhead3:: 0.4516584873199463
overhead4:: 17.410683155059814
overhead5:: 0
memory usage:: 5627727872
time_provenance:: 25.92106342315674
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0090, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0090, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632500
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 2
max_epoch:: 500
overhead:: 0
overhead2:: 0.055007219314575195
overhead3:: 0.4550330638885498
overhead4:: 17.75379490852356
overhead5:: 0
memory usage:: 5624528896
time_provenance:: 26.24875807762146
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0090, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0090, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632500
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 2
max_epoch:: 500
overhead:: 0
overhead2:: 0.14219021797180176
overhead3:: 1.0272557735443115
overhead4:: 42.68393611907959
overhead5:: 0
memory usage:: 5628878848
time_provenance:: 57.2921302318573
curr_diff: 0 tensor(2.8700e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.8700e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0089, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0089, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632600
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 2
max_epoch:: 500
overhead:: 0
overhead2:: 0.14026594161987305
overhead3:: 1.0447888374328613
overhead4:: 42.24649739265442
overhead5:: 0
memory usage:: 5626535936
time_provenance:: 56.83386707305908
curr_diff: 0 tensor(2.8726e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.8726e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0089, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0089, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632600
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 2
max_epoch:: 500
overhead:: 0
overhead2:: 0.14155292510986328
overhead3:: 1.0333662033081055
overhead4:: 42.723488330841064
overhead5:: 0
memory usage:: 5626343424
time_provenance:: 57.26564073562622
curr_diff: 0 tensor(2.8741e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.8741e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0089, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0089, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632600
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 0 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 2
max_epoch:: 500
overhead:: 0
overhead2:: 0.27085280418395996
overhead3:: 2.07610821723938
overhead4:: 76.62784934043884
overhead5:: 0
memory usage:: 5628850176
time_provenance:: 86.81856942176819
curr_diff: 0 tensor(5.0004e-13, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.0004e-13, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0089, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0089, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632600
repetition 1
python3 benchmark_exp_lr.py 0.001 10000 500 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05] Logistic_regression cifar10_2 1 0.0001 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 2.306757
Train - Epoch 1, Batch: 0, Loss: 2.245326
Train - Epoch 2, Batch: 0, Loss: 2.189869
Train - Epoch 3, Batch: 0, Loss: 2.139684
Train - Epoch 4, Batch: 0, Loss: 2.091307
Train - Epoch 5, Batch: 0, Loss: 2.049656
Train - Epoch 6, Batch: 0, Loss: 2.007872
Train - Epoch 7, Batch: 0, Loss: 1.970810
Train - Epoch 8, Batch: 0, Loss: 1.931714
Train - Epoch 9, Batch: 0, Loss: 1.899749
Train - Epoch 10, Batch: 0, Loss: 1.870774
Train - Epoch 11, Batch: 0, Loss: 1.844084
Train - Epoch 12, Batch: 0, Loss: 1.811688
Train - Epoch 13, Batch: 0, Loss: 1.788523
Train - Epoch 14, Batch: 0, Loss: 1.755767
Train - Epoch 15, Batch: 0, Loss: 1.730377
Train - Epoch 16, Batch: 0, Loss: 1.721021
Train - Epoch 17, Batch: 0, Loss: 1.690793
Train - Epoch 18, Batch: 0, Loss: 1.682686
Train - Epoch 19, Batch: 0, Loss: 1.659155
Train - Epoch 20, Batch: 0, Loss: 1.639400
Train - Epoch 21, Batch: 0, Loss: 1.628810
Train - Epoch 22, Batch: 0, Loss: 1.607502
Train - Epoch 23, Batch: 0, Loss: 1.597973
Train - Epoch 24, Batch: 0, Loss: 1.581035
Train - Epoch 25, Batch: 0, Loss: 1.563511
Train - Epoch 26, Batch: 0, Loss: 1.552286
Train - Epoch 27, Batch: 0, Loss: 1.548033
Train - Epoch 28, Batch: 0, Loss: 1.526749
Train - Epoch 29, Batch: 0, Loss: 1.521149
Train - Epoch 30, Batch: 0, Loss: 1.505247
Train - Epoch 31, Batch: 0, Loss: 1.497428
Train - Epoch 32, Batch: 0, Loss: 1.493416
Train - Epoch 33, Batch: 0, Loss: 1.481228
Train - Epoch 34, Batch: 0, Loss: 1.479230
Train - Epoch 35, Batch: 0, Loss: 1.466749
Train - Epoch 36, Batch: 0, Loss: 1.441063
Train - Epoch 37, Batch: 0, Loss: 1.447264
Train - Epoch 38, Batch: 0, Loss: 1.436567
Train - Epoch 39, Batch: 0, Loss: 1.430980
Train - Epoch 40, Batch: 0, Loss: 1.421258
Train - Epoch 41, Batch: 0, Loss: 1.414770
Train - Epoch 42, Batch: 0, Loss: 1.419636
Train - Epoch 43, Batch: 0, Loss: 1.405358
Train - Epoch 44, Batch: 0, Loss: 1.393619
Train - Epoch 45, Batch: 0, Loss: 1.386619
Train - Epoch 46, Batch: 0, Loss: 1.383237
Train - Epoch 47, Batch: 0, Loss: 1.386047
Train - Epoch 48, Batch: 0, Loss: 1.370988
Train - Epoch 49, Batch: 0, Loss: 1.361919
Train - Epoch 50, Batch: 0, Loss: 1.353832
Train - Epoch 51, Batch: 0, Loss: 1.361968
Train - Epoch 52, Batch: 0, Loss: 1.354809
Train - Epoch 53, Batch: 0, Loss: 1.346556
Train - Epoch 54, Batch: 0, Loss: 1.333661
Train - Epoch 55, Batch: 0, Loss: 1.338302
Train - Epoch 56, Batch: 0, Loss: 1.334529
Train - Epoch 57, Batch: 0, Loss: 1.335507
Train - Epoch 58, Batch: 0, Loss: 1.330976
Train - Epoch 59, Batch: 0, Loss: 1.322425
Train - Epoch 60, Batch: 0, Loss: 1.311456
Train - Epoch 61, Batch: 0, Loss: 1.317433
Train - Epoch 62, Batch: 0, Loss: 1.314944
Train - Epoch 63, Batch: 0, Loss: 1.305053
Train - Epoch 64, Batch: 0, Loss: 1.300967
Train - Epoch 65, Batch: 0, Loss: 1.298373
Train - Epoch 66, Batch: 0, Loss: 1.300043
Train - Epoch 67, Batch: 0, Loss: 1.290208
Train - Epoch 68, Batch: 0, Loss: 1.292781
Train - Epoch 69, Batch: 0, Loss: 1.292481
Train - Epoch 70, Batch: 0, Loss: 1.281836
Train - Epoch 71, Batch: 0, Loss: 1.271578
Train - Epoch 72, Batch: 0, Loss: 1.278139
Train - Epoch 73, Batch: 0, Loss: 1.276289
Train - Epoch 74, Batch: 0, Loss: 1.264005
Train - Epoch 75, Batch: 0, Loss: 1.264871
Train - Epoch 76, Batch: 0, Loss: 1.268642
Train - Epoch 77, Batch: 0, Loss: 1.264586
Train - Epoch 78, Batch: 0, Loss: 1.268527
Train - Epoch 79, Batch: 0, Loss: 1.261730
Train - Epoch 80, Batch: 0, Loss: 1.252661
Train - Epoch 81, Batch: 0, Loss: 1.265484
Train - Epoch 82, Batch: 0, Loss: 1.249399
Train - Epoch 83, Batch: 0, Loss: 1.246754
Train - Epoch 84, Batch: 0, Loss: 1.235295
Train - Epoch 85, Batch: 0, Loss: 1.248909
Train - Epoch 86, Batch: 0, Loss: 1.254431
Train - Epoch 87, Batch: 0, Loss: 1.246350
Train - Epoch 88, Batch: 0, Loss: 1.231934
Train - Epoch 89, Batch: 0, Loss: 1.234292
Train - Epoch 90, Batch: 0, Loss: 1.233376
Train - Epoch 91, Batch: 0, Loss: 1.236345
Train - Epoch 92, Batch: 0, Loss: 1.233197
Train - Epoch 93, Batch: 0, Loss: 1.231056
Train - Epoch 94, Batch: 0, Loss: 1.233845
Train - Epoch 95, Batch: 0, Loss: 1.228871
Train - Epoch 96, Batch: 0, Loss: 1.214818
Train - Epoch 97, Batch: 0, Loss: 1.218821
Train - Epoch 98, Batch: 0, Loss: 1.220767
Train - Epoch 99, Batch: 0, Loss: 1.209355
Train - Epoch 100, Batch: 0, Loss: 1.210095
Train - Epoch 101, Batch: 0, Loss: 1.217318
Train - Epoch 102, Batch: 0, Loss: 1.214160
Train - Epoch 103, Batch: 0, Loss: 1.217777
Train - Epoch 104, Batch: 0, Loss: 1.222160
Train - Epoch 105, Batch: 0, Loss: 1.210762
Train - Epoch 106, Batch: 0, Loss: 1.212017
Train - Epoch 107, Batch: 0, Loss: 1.208005
Train - Epoch 108, Batch: 0, Loss: 1.208063
Train - Epoch 109, Batch: 0, Loss: 1.189183
Train - Epoch 110, Batch: 0, Loss: 1.194817
Train - Epoch 111, Batch: 0, Loss: 1.196559
Train - Epoch 112, Batch: 0, Loss: 1.204884
Train - Epoch 113, Batch: 0, Loss: 1.189033
Train - Epoch 114, Batch: 0, Loss: 1.205736
Train - Epoch 115, Batch: 0, Loss: 1.191549
Train - Epoch 116, Batch: 0, Loss: 1.177102
Train - Epoch 117, Batch: 0, Loss: 1.187720
Train - Epoch 118, Batch: 0, Loss: 1.189437
Train - Epoch 119, Batch: 0, Loss: 1.188564
Train - Epoch 120, Batch: 0, Loss: 1.182678
Train - Epoch 121, Batch: 0, Loss: 1.186285
Train - Epoch 122, Batch: 0, Loss: 1.189714
Train - Epoch 123, Batch: 0, Loss: 1.196330
Train - Epoch 124, Batch: 0, Loss: 1.172433
Train - Epoch 125, Batch: 0, Loss: 1.174324
Train - Epoch 126, Batch: 0, Loss: 1.200050
Train - Epoch 127, Batch: 0, Loss: 1.177836
Train - Epoch 128, Batch: 0, Loss: 1.175904
Train - Epoch 129, Batch: 0, Loss: 1.161121
Train - Epoch 130, Batch: 0, Loss: 1.172720
Train - Epoch 131, Batch: 0, Loss: 1.163789
Train - Epoch 132, Batch: 0, Loss: 1.164492
Train - Epoch 133, Batch: 0, Loss: 1.170585
Train - Epoch 134, Batch: 0, Loss: 1.166035
Train - Epoch 135, Batch: 0, Loss: 1.167586
Train - Epoch 136, Batch: 0, Loss: 1.173314
Train - Epoch 137, Batch: 0, Loss: 1.160117
Train - Epoch 138, Batch: 0, Loss: 1.162455
Train - Epoch 139, Batch: 0, Loss: 1.178733
Train - Epoch 140, Batch: 0, Loss: 1.162344
Train - Epoch 141, Batch: 0, Loss: 1.163395
Train - Epoch 142, Batch: 0, Loss: 1.176187
Train - Epoch 143, Batch: 0, Loss: 1.166052
Train - Epoch 144, Batch: 0, Loss: 1.156580
Train - Epoch 145, Batch: 0, Loss: 1.155476
Train - Epoch 146, Batch: 0, Loss: 1.167566
Train - Epoch 147, Batch: 0, Loss: 1.158980
Train - Epoch 148, Batch: 0, Loss: 1.165427
Train - Epoch 149, Batch: 0, Loss: 1.167856
Train - Epoch 150, Batch: 0, Loss: 1.153161
Train - Epoch 151, Batch: 0, Loss: 1.159169
Train - Epoch 152, Batch: 0, Loss: 1.139108
Train - Epoch 153, Batch: 0, Loss: 1.154583
Train - Epoch 154, Batch: 0, Loss: 1.155123
Train - Epoch 155, Batch: 0, Loss: 1.154784
Train - Epoch 156, Batch: 0, Loss: 1.139923
Train - Epoch 157, Batch: 0, Loss: 1.165136
Train - Epoch 158, Batch: 0, Loss: 1.144730
Train - Epoch 159, Batch: 0, Loss: 1.161580
Train - Epoch 160, Batch: 0, Loss: 1.149314
Train - Epoch 161, Batch: 0, Loss: 1.144255
Train - Epoch 162, Batch: 0, Loss: 1.153855
Train - Epoch 163, Batch: 0, Loss: 1.153011
Train - Epoch 164, Batch: 0, Loss: 1.143665
Train - Epoch 165, Batch: 0, Loss: 1.146382
Train - Epoch 166, Batch: 0, Loss: 1.152172
Train - Epoch 167, Batch: 0, Loss: 1.133313
Train - Epoch 168, Batch: 0, Loss: 1.142044
Train - Epoch 169, Batch: 0, Loss: 1.127724
Train - Epoch 170, Batch: 0, Loss: 1.145444
Train - Epoch 171, Batch: 0, Loss: 1.131980
Train - Epoch 172, Batch: 0, Loss: 1.136381
Train - Epoch 173, Batch: 0, Loss: 1.129228
Train - Epoch 174, Batch: 0, Loss: 1.132817
Train - Epoch 175, Batch: 0, Loss: 1.144569
Train - Epoch 176, Batch: 0, Loss: 1.136365
Train - Epoch 177, Batch: 0, Loss: 1.144379
Train - Epoch 178, Batch: 0, Loss: 1.145354
Train - Epoch 179, Batch: 0, Loss: 1.132868
Train - Epoch 180, Batch: 0, Loss: 1.131289
Train - Epoch 181, Batch: 0, Loss: 1.126854
Train - Epoch 182, Batch: 0, Loss: 1.124500
Train - Epoch 183, Batch: 0, Loss: 1.129706
Train - Epoch 184, Batch: 0, Loss: 1.137971
Train - Epoch 185, Batch: 0, Loss: 1.127802
Train - Epoch 186, Batch: 0, Loss: 1.152134
Train - Epoch 187, Batch: 0, Loss: 1.121915
Train - Epoch 188, Batch: 0, Loss: 1.129265
Train - Epoch 189, Batch: 0, Loss: 1.127055
Train - Epoch 190, Batch: 0, Loss: 1.132045
Train - Epoch 191, Batch: 0, Loss: 1.132747
Train - Epoch 192, Batch: 0, Loss: 1.130827
Train - Epoch 193, Batch: 0, Loss: 1.114781
Train - Epoch 194, Batch: 0, Loss: 1.130824
Train - Epoch 195, Batch: 0, Loss: 1.121850
Train - Epoch 196, Batch: 0, Loss: 1.126831
Train - Epoch 197, Batch: 0, Loss: 1.125263
Train - Epoch 198, Batch: 0, Loss: 1.114641
Train - Epoch 199, Batch: 0, Loss: 1.107951
Train - Epoch 200, Batch: 0, Loss: 1.119052
Train - Epoch 201, Batch: 0, Loss: 1.110486
Train - Epoch 202, Batch: 0, Loss: 1.118874
Train - Epoch 203, Batch: 0, Loss: 1.118378
Train - Epoch 204, Batch: 0, Loss: 1.123881
Train - Epoch 205, Batch: 0, Loss: 1.125634
Train - Epoch 206, Batch: 0, Loss: 1.133518
Train - Epoch 207, Batch: 0, Loss: 1.104114
Train - Epoch 208, Batch: 0, Loss: 1.109881
Train - Epoch 209, Batch: 0, Loss: 1.098533
Train - Epoch 210, Batch: 0, Loss: 1.111295
Train - Epoch 211, Batch: 0, Loss: 1.116325
Train - Epoch 212, Batch: 0, Loss: 1.109293
Train - Epoch 213, Batch: 0, Loss: 1.115189
Train - Epoch 214, Batch: 0, Loss: 1.117930
Train - Epoch 215, Batch: 0, Loss: 1.112608
Train - Epoch 216, Batch: 0, Loss: 1.109207
Train - Epoch 217, Batch: 0, Loss: 1.114318
Train - Epoch 218, Batch: 0, Loss: 1.105449
Train - Epoch 219, Batch: 0, Loss: 1.097488
Train - Epoch 220, Batch: 0, Loss: 1.103926
Train - Epoch 221, Batch: 0, Loss: 1.114777
Train - Epoch 222, Batch: 0, Loss: 1.108245
Train - Epoch 223, Batch: 0, Loss: 1.110971
Train - Epoch 224, Batch: 0, Loss: 1.096852
Train - Epoch 225, Batch: 0, Loss: 1.117430
Train - Epoch 226, Batch: 0, Loss: 1.105066
Train - Epoch 227, Batch: 0, Loss: 1.111094
Train - Epoch 228, Batch: 0, Loss: 1.089766
Train - Epoch 229, Batch: 0, Loss: 1.105772
Train - Epoch 230, Batch: 0, Loss: 1.100113
Train - Epoch 231, Batch: 0, Loss: 1.106289
Train - Epoch 232, Batch: 0, Loss: 1.099752
Train - Epoch 233, Batch: 0, Loss: 1.081772
Train - Epoch 234, Batch: 0, Loss: 1.098471
Train - Epoch 235, Batch: 0, Loss: 1.104545
Train - Epoch 236, Batch: 0, Loss: 1.105684
Train - Epoch 237, Batch: 0, Loss: 1.116692
Train - Epoch 238, Batch: 0, Loss: 1.097803
Train - Epoch 239, Batch: 0, Loss: 1.101594
Train - Epoch 240, Batch: 0, Loss: 1.081374
Train - Epoch 241, Batch: 0, Loss: 1.115553
Train - Epoch 242, Batch: 0, Loss: 1.110286
Train - Epoch 243, Batch: 0, Loss: 1.097914
Train - Epoch 244, Batch: 0, Loss: 1.102671
Train - Epoch 245, Batch: 0, Loss: 1.099278
Train - Epoch 246, Batch: 0, Loss: 1.093167
Train - Epoch 247, Batch: 0, Loss: 1.090232
Train - Epoch 248, Batch: 0, Loss: 1.097959
Train - Epoch 249, Batch: 0, Loss: 1.092632
Train - Epoch 250, Batch: 0, Loss: 1.087164
Train - Epoch 251, Batch: 0, Loss: 1.090777
Train - Epoch 252, Batch: 0, Loss: 1.088914
Train - Epoch 253, Batch: 0, Loss: 1.101245
Train - Epoch 254, Batch: 0, Loss: 1.084238
Train - Epoch 255, Batch: 0, Loss: 1.092901
Train - Epoch 256, Batch: 0, Loss: 1.095173
Train - Epoch 257, Batch: 0, Loss: 1.096418
Train - Epoch 258, Batch: 0, Loss: 1.095666
Train - Epoch 259, Batch: 0, Loss: 1.083773
Train - Epoch 260, Batch: 0, Loss: 1.086083
Train - Epoch 261, Batch: 0, Loss: 1.077044
Train - Epoch 262, Batch: 0, Loss: 1.092938
Train - Epoch 263, Batch: 0, Loss: 1.085554
Train - Epoch 264, Batch: 0, Loss: 1.090702
Train - Epoch 265, Batch: 0, Loss: 1.081606
Train - Epoch 266, Batch: 0, Loss: 1.089173
Train - Epoch 267, Batch: 0, Loss: 1.082358
Train - Epoch 268, Batch: 0, Loss: 1.089315
Train - Epoch 269, Batch: 0, Loss: 1.089413
Train - Epoch 270, Batch: 0, Loss: 1.082388
Train - Epoch 271, Batch: 0, Loss: 1.094617
Train - Epoch 272, Batch: 0, Loss: 1.094774
Train - Epoch 273, Batch: 0, Loss: 1.086258
Train - Epoch 274, Batch: 0, Loss: 1.081031
Train - Epoch 275, Batch: 0, Loss: 1.081189
Train - Epoch 276, Batch: 0, Loss: 1.090985
Train - Epoch 277, Batch: 0, Loss: 1.082188
Train - Epoch 278, Batch: 0, Loss: 1.087561
Train - Epoch 279, Batch: 0, Loss: 1.078468
Train - Epoch 280, Batch: 0, Loss: 1.088064
Train - Epoch 281, Batch: 0, Loss: 1.089736
Train - Epoch 282, Batch: 0, Loss: 1.079927
Train - Epoch 283, Batch: 0, Loss: 1.084784
Train - Epoch 284, Batch: 0, Loss: 1.094091
Train - Epoch 285, Batch: 0, Loss: 1.076706
Train - Epoch 286, Batch: 0, Loss: 1.082899
Train - Epoch 287, Batch: 0, Loss: 1.076850
Train - Epoch 288, Batch: 0, Loss: 1.079150
Train - Epoch 289, Batch: 0, Loss: 1.077525
Train - Epoch 290, Batch: 0, Loss: 1.078142
Train - Epoch 291, Batch: 0, Loss: 1.092812
Train - Epoch 292, Batch: 0, Loss: 1.079459
Train - Epoch 293, Batch: 0, Loss: 1.068365
Train - Epoch 294, Batch: 0, Loss: 1.083180
Train - Epoch 295, Batch: 0, Loss: 1.078287
Train - Epoch 296, Batch: 0, Loss: 1.083260
Train - Epoch 297, Batch: 0, Loss: 1.079074
Train - Epoch 298, Batch: 0, Loss: 1.081670
Train - Epoch 299, Batch: 0, Loss: 1.064538
Train - Epoch 300, Batch: 0, Loss: 1.074761
Train - Epoch 301, Batch: 0, Loss: 1.081114
Train - Epoch 302, Batch: 0, Loss: 1.074773
Train - Epoch 303, Batch: 0, Loss: 1.088423
Train - Epoch 304, Batch: 0, Loss: 1.065048
Train - Epoch 305, Batch: 0, Loss: 1.089947
Train - Epoch 306, Batch: 0, Loss: 1.089891
Train - Epoch 307, Batch: 0, Loss: 1.063306
Train - Epoch 308, Batch: 0, Loss: 1.062975
Train - Epoch 309, Batch: 0, Loss: 1.070411
Train - Epoch 310, Batch: 0, Loss: 1.067114
Train - Epoch 311, Batch: 0, Loss: 1.066874
Train - Epoch 312, Batch: 0, Loss: 1.091849
Train - Epoch 313, Batch: 0, Loss: 1.080737
Train - Epoch 314, Batch: 0, Loss: 1.066419
Train - Epoch 315, Batch: 0, Loss: 1.074004
Train - Epoch 316, Batch: 0, Loss: 1.082281
Train - Epoch 317, Batch: 0, Loss: 1.067987
Train - Epoch 318, Batch: 0, Loss: 1.076067
Train - Epoch 319, Batch: 0, Loss: 1.068735
Train - Epoch 320, Batch: 0, Loss: 1.085083
Train - Epoch 321, Batch: 0, Loss: 1.070335
Train - Epoch 322, Batch: 0, Loss: 1.079615
Train - Epoch 323, Batch: 0, Loss: 1.074671
Train - Epoch 324, Batch: 0, Loss: 1.058778
Train - Epoch 325, Batch: 0, Loss: 1.073793
Train - Epoch 326, Batch: 0, Loss: 1.072136
Train - Epoch 327, Batch: 0, Loss: 1.078582
Train - Epoch 328, Batch: 0, Loss: 1.048595
Train - Epoch 329, Batch: 0, Loss: 1.077083
Train - Epoch 330, Batch: 0, Loss: 1.058211
Train - Epoch 331, Batch: 0, Loss: 1.080733
Train - Epoch 332, Batch: 0, Loss: 1.076820
Train - Epoch 333, Batch: 0, Loss: 1.066286
Train - Epoch 334, Batch: 0, Loss: 1.062232
Train - Epoch 335, Batch: 0, Loss: 1.059552
Train - Epoch 336, Batch: 0, Loss: 1.067932
Train - Epoch 337, Batch: 0, Loss: 1.074393
Train - Epoch 338, Batch: 0, Loss: 1.071370
Train - Epoch 339, Batch: 0, Loss: 1.061427
Train - Epoch 340, Batch: 0, Loss: 1.070558
Train - Epoch 341, Batch: 0, Loss: 1.067291
Train - Epoch 342, Batch: 0, Loss: 1.058772
Train - Epoch 343, Batch: 0, Loss: 1.059212
Train - Epoch 344, Batch: 0, Loss: 1.077418
Train - Epoch 345, Batch: 0, Loss: 1.075981
Train - Epoch 346, Batch: 0, Loss: 1.070608
Train - Epoch 347, Batch: 0, Loss: 1.060626
Train - Epoch 348, Batch: 0, Loss: 1.063539
Train - Epoch 349, Batch: 0, Loss: 1.071587
Train - Epoch 350, Batch: 0, Loss: 1.076764
Train - Epoch 351, Batch: 0, Loss: 1.059044
Train - Epoch 352, Batch: 0, Loss: 1.068556
Train - Epoch 353, Batch: 0, Loss: 1.061144
Train - Epoch 354, Batch: 0, Loss: 1.045545
Train - Epoch 355, Batch: 0, Loss: 1.063867
Train - Epoch 356, Batch: 0, Loss: 1.062688
Train - Epoch 357, Batch: 0, Loss: 1.060894
Train - Epoch 358, Batch: 0, Loss: 1.071723
Train - Epoch 359, Batch: 0, Loss: 1.060200
Train - Epoch 360, Batch: 0, Loss: 1.064493
Train - Epoch 361, Batch: 0, Loss: 1.066387
Train - Epoch 362, Batch: 0, Loss: 1.076198
Train - Epoch 363, Batch: 0, Loss: 1.053559
Train - Epoch 364, Batch: 0, Loss: 1.044471
Train - Epoch 365, Batch: 0, Loss: 1.060311
Train - Epoch 366, Batch: 0, Loss: 1.049752
Train - Epoch 367, Batch: 0, Loss: 1.045965
Train - Epoch 368, Batch: 0, Loss: 1.061102
Train - Epoch 369, Batch: 0, Loss: 1.057671
Train - Epoch 370, Batch: 0, Loss: 1.069841
Train - Epoch 371, Batch: 0, Loss: 1.052656
Train - Epoch 372, Batch: 0, Loss: 1.059712
Train - Epoch 373, Batch: 0, Loss: 1.049048/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 374, Batch: 0, Loss: 1.068504
Train - Epoch 375, Batch: 0, Loss: 1.067122
Train - Epoch 376, Batch: 0, Loss: 1.046198
Train - Epoch 377, Batch: 0, Loss: 1.067710
Train - Epoch 378, Batch: 0, Loss: 1.065172
Train - Epoch 379, Batch: 0, Loss: 1.058718
Train - Epoch 380, Batch: 0, Loss: 1.041389
Train - Epoch 381, Batch: 0, Loss: 1.054995
Train - Epoch 382, Batch: 0, Loss: 1.058703
Train - Epoch 383, Batch: 0, Loss: 1.058132
Train - Epoch 384, Batch: 0, Loss: 1.052201
Train - Epoch 385, Batch: 0, Loss: 1.062683
Train - Epoch 386, Batch: 0, Loss: 1.078366
Train - Epoch 387, Batch: 0, Loss: 1.050465
Train - Epoch 388, Batch: 0, Loss: 1.062015
Train - Epoch 389, Batch: 0, Loss: 1.059640
Train - Epoch 390, Batch: 0, Loss: 1.056780
Train - Epoch 391, Batch: 0, Loss: 1.060407
Train - Epoch 392, Batch: 0, Loss: 1.059780
Train - Epoch 393, Batch: 0, Loss: 1.061691
Train - Epoch 394, Batch: 0, Loss: 1.069969
Train - Epoch 395, Batch: 0, Loss: 1.055459
Train - Epoch 396, Batch: 0, Loss: 1.052136
Train - Epoch 397, Batch: 0, Loss: 1.047509
Train - Epoch 398, Batch: 0, Loss: 1.069305
Train - Epoch 399, Batch: 0, Loss: 1.041127
Train - Epoch 400, Batch: 0, Loss: 1.057069
Train - Epoch 401, Batch: 0, Loss: 1.058459
Train - Epoch 402, Batch: 0, Loss: 1.054512
Train - Epoch 403, Batch: 0, Loss: 1.061218
Train - Epoch 404, Batch: 0, Loss: 1.052970
Train - Epoch 405, Batch: 0, Loss: 1.053143
Train - Epoch 406, Batch: 0, Loss: 1.052143
Train - Epoch 407, Batch: 0, Loss: 1.057142
Train - Epoch 408, Batch: 0, Loss: 1.049523
Train - Epoch 409, Batch: 0, Loss: 1.052473
Train - Epoch 410, Batch: 0, Loss: 1.045241
Train - Epoch 411, Batch: 0, Loss: 1.040833
Train - Epoch 412, Batch: 0, Loss: 1.060525
Train - Epoch 413, Batch: 0, Loss: 1.070278
Train - Epoch 414, Batch: 0, Loss: 1.044560
Train - Epoch 415, Batch: 0, Loss: 1.046403
Train - Epoch 416, Batch: 0, Loss: 1.067689
Train - Epoch 417, Batch: 0, Loss: 1.052969
Train - Epoch 418, Batch: 0, Loss: 1.055158
Train - Epoch 419, Batch: 0, Loss: 1.051746
Train - Epoch 420, Batch: 0, Loss: 1.038984
Train - Epoch 421, Batch: 0, Loss: 1.043047
Train - Epoch 422, Batch: 0, Loss: 1.055173
Train - Epoch 423, Batch: 0, Loss: 1.064516
Train - Epoch 424, Batch: 0, Loss: 1.041806
Train - Epoch 425, Batch: 0, Loss: 1.046070
Train - Epoch 426, Batch: 0, Loss: 1.053706
Train - Epoch 427, Batch: 0, Loss: 1.049768
Train - Epoch 428, Batch: 0, Loss: 1.045329
Train - Epoch 429, Batch: 0, Loss: 1.061232
Train - Epoch 430, Batch: 0, Loss: 1.042244
Train - Epoch 431, Batch: 0, Loss: 1.043353
Train - Epoch 432, Batch: 0, Loss: 1.042854
Train - Epoch 433, Batch: 0, Loss: 1.044586
Train - Epoch 434, Batch: 0, Loss: 1.052433
Train - Epoch 435, Batch: 0, Loss: 1.055190
Train - Epoch 436, Batch: 0, Loss: 1.049307
Train - Epoch 437, Batch: 0, Loss: 1.050788
Train - Epoch 438, Batch: 0, Loss: 1.032952
Train - Epoch 439, Batch: 0, Loss: 1.057629
Train - Epoch 440, Batch: 0, Loss: 1.044873
Train - Epoch 441, Batch: 0, Loss: 1.049980
Train - Epoch 442, Batch: 0, Loss: 1.048359
Train - Epoch 443, Batch: 0, Loss: 1.046835
Train - Epoch 444, Batch: 0, Loss: 1.051337
Train - Epoch 445, Batch: 0, Loss: 1.042179
Train - Epoch 446, Batch: 0, Loss: 1.036463
Train - Epoch 447, Batch: 0, Loss: 1.053086
Train - Epoch 448, Batch: 0, Loss: 1.046811
Train - Epoch 449, Batch: 0, Loss: 1.045972
Train - Epoch 450, Batch: 0, Loss: 1.039807
Train - Epoch 451, Batch: 0, Loss: 1.060149
Train - Epoch 452, Batch: 0, Loss: 1.046834
Train - Epoch 453, Batch: 0, Loss: 1.053552
Train - Epoch 454, Batch: 0, Loss: 1.026941
Train - Epoch 455, Batch: 0, Loss: 1.055864
Train - Epoch 456, Batch: 0, Loss: 1.049017
Train - Epoch 457, Batch: 0, Loss: 1.057454
Train - Epoch 458, Batch: 0, Loss: 1.043348
Train - Epoch 459, Batch: 0, Loss: 1.047990
Train - Epoch 460, Batch: 0, Loss: 1.043166
Train - Epoch 461, Batch: 0, Loss: 1.044719
Train - Epoch 462, Batch: 0, Loss: 1.034618
Train - Epoch 463, Batch: 0, Loss: 1.049600
Train - Epoch 464, Batch: 0, Loss: 1.040436
Train - Epoch 465, Batch: 0, Loss: 1.053375
Train - Epoch 466, Batch: 0, Loss: 1.026982
Train - Epoch 467, Batch: 0, Loss: 1.040343
Train - Epoch 468, Batch: 0, Loss: 1.037140
Train - Epoch 469, Batch: 0, Loss: 1.030719
Train - Epoch 470, Batch: 0, Loss: 1.047538
Train - Epoch 471, Batch: 0, Loss: 1.043968
Train - Epoch 472, Batch: 0, Loss: 1.048297
Train - Epoch 473, Batch: 0, Loss: 1.055712
Train - Epoch 474, Batch: 0, Loss: 1.046870
Train - Epoch 475, Batch: 0, Loss: 1.035173
Train - Epoch 476, Batch: 0, Loss: 1.044114
Train - Epoch 477, Batch: 0, Loss: 1.050517
Train - Epoch 478, Batch: 0, Loss: 1.039281
Train - Epoch 479, Batch: 0, Loss: 1.031810
Train - Epoch 480, Batch: 0, Loss: 1.038467
Train - Epoch 481, Batch: 0, Loss: 1.057253
Train - Epoch 482, Batch: 0, Loss: 1.033770
Train - Epoch 483, Batch: 0, Loss: 1.030623
Train - Epoch 484, Batch: 0, Loss: 1.039324
Train - Epoch 485, Batch: 0, Loss: 1.040922
Train - Epoch 486, Batch: 0, Loss: 1.050097
Train - Epoch 487, Batch: 0, Loss: 1.041720
Train - Epoch 488, Batch: 0, Loss: 1.027858
Train - Epoch 489, Batch: 0, Loss: 1.022212
Train - Epoch 490, Batch: 0, Loss: 1.039706
Train - Epoch 491, Batch: 0, Loss: 1.029294
Train - Epoch 492, Batch: 0, Loss: 1.034819
Train - Epoch 493, Batch: 0, Loss: 1.019609
Train - Epoch 494, Batch: 0, Loss: 1.056725
Train - Epoch 495, Batch: 0, Loss: 1.041379
Train - Epoch 496, Batch: 0, Loss: 1.033199
Train - Epoch 497, Batch: 0, Loss: 1.040970
Train - Epoch 498, Batch: 0, Loss: 1.035946
Train - Epoch 499, Batch: 0, Loss: 1.042185
training_time:: 106.33611154556274
training time full:: 106.33618021011353
provenance prepare time:: 4.76837158203125e-07
here
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633400
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 2
max_epoch:: 500
batch_size:: 10000
epoch:: 0
tensor([41733, 28306, 42832, 33565, 19376, 45909, 40281, 34015, 19918, 36656,
        24073, 13774, 13826, 10801, 10759, 40602, 38007, 40791, 49202, 14778,
        44560,  4925,  8083,  1823,  4417,  2801, 23859, 37866,  1803, 25303,
        46084, 11278,  8761,  2154, 19088, 28536, 43370, 25201, 24504, 11833,
        40699, 11141, 23785, 28955,  6773, 44071, 27844, 31939, 20835, 36176])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
epoch:: 60
epoch:: 61
epoch:: 62
epoch:: 63
epoch:: 64
epoch:: 65
epoch:: 66
epoch:: 67
epoch:: 68
epoch:: 69
epoch:: 70
epoch:: 71
epoch:: 72
epoch:: 73
epoch:: 74
epoch:: 75
epoch:: 76
epoch:: 77
epoch:: 78
epoch:: 79
epoch:: 80
epoch:: 81
epoch:: 82
epoch:: 83
epoch:: 84
epoch:: 85
epoch:: 86
epoch:: 87
epoch:: 88
epoch:: 89
epoch:: 90
epoch:: 91
epoch:: 92
epoch:: 93
epoch:: 94
epoch:: 95
epoch:: 96
epoch:: 97
epoch:: 98
epoch:: 99
epoch:: 100
epoch:: 101
epoch:: 102
epoch:: 103
epoch:: 104
epoch:: 105
epoch:: 106
epoch:: 107
epoch:: 108
epoch:: 109
epoch:: 110
epoch:: 111
epoch:: 112
epoch:: 113
epoch:: 114
epoch:: 115
epoch:: 116
epoch:: 117
epoch:: 118
epoch:: 119
epoch:: 120
epoch:: 121
epoch:: 122
epoch:: 123
epoch:: 124
epoch:: 125
epoch:: 126
epoch:: 127
epoch:: 128
epoch:: 129
epoch:: 130
epoch:: 131
epoch:: 132
epoch:: 133
epoch:: 134
epoch:: 135
epoch:: 136
epoch:: 137
epoch:: 138
epoch:: 139
epoch:: 140
epoch:: 141
epoch:: 142
epoch:: 143
epoch:: 144
epoch:: 145
epoch:: 146
epoch:: 147
epoch:: 148
epoch:: 149
epoch:: 150
epoch:: 151
epoch:: 152
epoch:: 153
epoch:: 154
epoch:: 155
epoch:: 156
epoch:: 157
epoch:: 158
epoch:: 159
epoch:: 160
epoch:: 161
epoch:: 162
epoch:: 163
epoch:: 164
epoch:: 165
epoch:: 166
epoch:: 167
epoch:: 168
epoch:: 169
epoch:: 170
epoch:: 171
epoch:: 172
epoch:: 173
epoch:: 174
epoch:: 175
epoch:: 176
epoch:: 177
epoch:: 178
epoch:: 179
epoch:: 180
epoch:: 181
epoch:: 182
epoch:: 183
epoch:: 184
epoch:: 185
epoch:: 186
epoch:: 187
epoch:: 188
epoch:: 189
epoch:: 190
epoch:: 191
epoch:: 192
epoch:: 193
epoch:: 194
epoch:: 195
epoch:: 196
epoch:: 197
epoch:: 198
epoch:: 199
epoch:: 200
epoch:: 201
epoch:: 202
epoch:: 203
epoch:: 204
epoch:: 205
epoch:: 206
epoch:: 207
epoch:: 208
epoch:: 209
epoch:: 210
epoch:: 211
epoch:: 212
epoch:: 213
epoch:: 214
epoch:: 215
epoch:: 216
epoch:: 217
epoch:: 218
epoch:: 219
epoch:: 220
epoch:: 221
epoch:: 222
epoch:: 223
epoch:: 224
epoch:: 225
epoch:: 226
epoch:: 227
epoch:: 228
epoch:: 229
epoch:: 230
epoch:: 231
epoch:: 232
epoch:: 233
epoch:: 234
epoch:: 235
epoch:: 236
epoch:: 237
epoch:: 238
epoch:: 239
epoch:: 240
epoch:: 241
epoch:: 242
epoch:: 243
epoch:: 244
epoch:: 245
epoch:: 246
epoch:: 247
epoch:: 248
epoch:: 249
epoch:: 250
epoch:: 251
epoch:: 252
epoch:: 253
epoch:: 254
epoch:: 255
epoch:: 256
epoch:: 257
epoch:: 258
epoch:: 259
epoch:: 260
epoch:: 261
epoch:: 262
epoch:: 263
epoch:: 264
epoch:: 265
epoch:: 266
epoch:: 267
epoch:: 268
epoch:: 269
epoch:: 270
epoch:: 271
epoch:: 272
epoch:: 273
epoch:: 274
epoch:: 275
epoch:: 276
epoch:: 277
epoch:: 278
epoch:: 279
epoch:: 280
epoch:: 281
epoch:: 282
epoch:: 283
epoch:: 284
epoch:: 285
epoch:: 286
epoch:: 287
epoch:: 288
epoch:: 289
epoch:: 290
epoch:: 291
epoch:: 292
epoch:: 293
epoch:: 294
epoch:: 295
epoch:: 296
epoch:: 297
epoch:: 298
epoch:: 299
epoch:: 300
epoch:: 301
epoch:: 302
epoch:: 303
epoch:: 304
epoch:: 305
epoch:: 306
epoch:: 307
epoch:: 308
epoch:: 309
epoch:: 310
epoch:: 311
epoch:: 312
epoch:: 313
epoch:: 314
epoch:: 315
epoch:: 316
epoch:: 317
epoch:: 318
epoch:: 319
epoch:: 320
epoch:: 321
epoch:: 322
epoch:: 323
epoch:: 324
epoch:: 325
epoch:: 326
epoch:: 327
epoch:: 328
epoch:: 329
epoch:: 330
epoch:: 331
epoch:: 332
epoch:: 333
epoch:: 334
epoch:: 335
epoch:: 336
epoch:: 337
epoch:: 338
epoch:: 339
epoch:: 340
epoch:: 341
epoch:: 342
epoch:: 343
epoch:: 344
epoch:: 345
epoch:: 346
epoch:: 347
epoch:: 348
epoch:: 349
epoch:: 350
epoch:: 351
epoch:: 352
epoch:: 353
epoch:: 354
epoch:: 355
epoch:: 356
epoch:: 357
epoch:: 358
epoch:: 359
epoch:: 360
epoch:: 361
epoch:: 362
epoch:: 363
epoch:: 364
epoch:: 365
epoch:: 366
epoch:: 367
epoch:: 368
epoch:: 369
epoch:: 370
epoch:: 371
epoch:: 372
epoch:: 373
epoch:: 374
epoch:: 375
epoch:: 376
epoch:: 377
epoch:: 378
epoch:: 379
epoch:: 380
epoch:: 381
epoch:: 382
epoch:: 383
epoch:: 384
epoch:: 385
epoch:: 386
epoch:: 387
epoch:: 388
epoch:: 389
epoch:: 390
epoch:: 391
epoch:: 392
epoch:: 393
epoch:: 394
epoch:: 395
epoch:: 396
epoch:: 397
epoch:: 398
epoch:: 399
epoch:: 400
epoch:: 401
epoch:: 402
epoch:: 403
epoch:: 404
epoch:: 405
epoch:: 406
epoch:: 407
epoch:: 408
epoch:: 409
epoch:: 410
epoch:: 411
epoch:: 412
epoch:: 413
epoch:: 414
epoch:: 415
epoch:: 416
epoch:: 417
epoch:: 418
epoch:: 419
epoch:: 420
epoch:: 421
epoch:: 422
epoch:: 423
epoch:: 424
epoch:: 425
epoch:: 426
epoch:: 427
epoch:: 428
epoch:: 429
epoch:: 430
epoch:: 431
epoch:: 432
epoch:: 433
epoch:: 434
epoch:: 435
epoch:: 436
epoch:: 437
epoch:: 438
epoch:: 439
epoch:: 440
epoch:: 441
epoch:: 442
epoch:: 443
epoch:: 444
epoch:: 445
epoch:: 446
epoch:: 447
epoch:: 448
epoch:: 449
epoch:: 450
epoch:: 451
epoch:: 452
epoch:: 453
epoch:: 454
epoch:: 455
epoch:: 456
epoch:: 457
epoch:: 458
epoch:: 459
epoch:: 460
epoch:: 461
epoch:: 462
epoch:: 463
epoch:: 464
epoch:: 465
epoch:: 466
epoch:: 467
epoch:: 468
epoch:: 469
epoch:: 470
epoch:: 471
epoch:: 472
epoch:: 473
epoch:: 474
epoch:: 475
epoch:: 476
epoch:: 477
epoch:: 478
epoch:: 479
epoch:: 480
epoch:: 481
epoch:: 482
epoch:: 483
epoch:: 484
epoch:: 485
epoch:: 486
epoch:: 487
epoch:: 488
epoch:: 489
epoch:: 490
epoch:: 491
epoch:: 492
epoch:: 493
epoch:: 494
epoch:: 495
epoch:: 496
epoch:: 497
epoch:: 498
epoch:: 499
training time is 78.05359840393066
overhead:: 0
overhead2:: 1.0111923217773438
overhead3:: 0
time_baseline:: 78.05362939834595
curr_diff: 0 tensor(0.0090, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0090, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633400
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 2
max_epoch:: 500
overhead:: 0
overhead2:: 0.029700279235839844
overhead3:: 0.24782109260559082
overhead4:: 9.167431354522705
overhead5:: 0
memory usage:: 5641998336
time_provenance:: 15.596072673797607
curr_diff: 0 tensor(0.0021, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0021, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0105, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0105, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633300
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 2
max_epoch:: 500
overhead:: 0
overhead2:: 0.03248143196105957
overhead3:: 0.2503662109375
overhead4:: 9.639179944992065
overhead5:: 0
memory usage:: 5660004352
time_provenance:: 16.071018934249878
curr_diff: 0 tensor(0.0021, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0021, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0106, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0106, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633300
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 2
max_epoch:: 500
overhead:: 0
overhead2:: 0.03154325485229492
overhead3:: 0.2603461742401123
overhead4:: 10.10487174987793
overhead5:: 0
memory usage:: 5633839104
time_provenance:: 16.589121103286743
curr_diff: 0 tensor(0.0021, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0021, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0106, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0106, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633300
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 2
max_epoch:: 500
overhead:: 0
overhead2:: 0.05416369438171387
overhead3:: 0.4529604911804199
overhead4:: 17.617696285247803
overhead5:: 0
memory usage:: 5703110656
time_provenance:: 26.128735303878784
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0091, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0091, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633400
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 2
max_epoch:: 500
overhead:: 0
overhead2:: 0.05735278129577637
overhead3:: 0.4404449462890625
overhead4:: 17.625074863433838
overhead5:: 0
memory usage:: 5628915712
time_provenance:: 26.10925054550171
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0091, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0091, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633400
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 2
max_epoch:: 500
overhead:: 0
overhead2:: 0.05737709999084473
overhead3:: 0.4519989490509033
overhead4:: 17.839126348495483
overhead5:: 0
memory usage:: 5639974912
time_provenance:: 26.358909130096436
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0091, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0091, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633400
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 2
max_epoch:: 500
overhead:: 0
overhead2:: 0.1390838623046875
overhead3:: 1.0174744129180908
overhead4:: 42.17237091064453
overhead5:: 0
memory usage:: 5629816832
time_provenance:: 56.78033781051636
curr_diff: 0 tensor(2.8152e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.8152e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0090, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0090, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633400
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 2
max_epoch:: 500
overhead:: 0
overhead2:: 0.13720178604125977
overhead3:: 1.0239925384521484
overhead4:: 41.81727170944214
overhead5:: 0
memory usage:: 5632172032
time_provenance:: 56.39699745178223
curr_diff: 0 tensor(2.8116e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.8116e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0090, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0090, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633400
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 2
max_epoch:: 500
overhead:: 0
overhead2:: 0.1405038833618164
overhead3:: 1.0210916996002197
overhead4:: 41.37788367271423
overhead5:: 0
memory usage:: 5623869440
time_provenance:: 55.93185257911682
curr_diff: 0 tensor(2.8092e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.8092e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0090, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0090, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633400
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 1 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 2
max_epoch:: 500
overhead:: 0
overhead2:: 0.28222060203552246
overhead3:: 2.071298599243164
overhead4:: 76.90705132484436
overhead5:: 0
memory usage:: 5625049088
time_provenance:: 87.28487873077393
curr_diff: 0 tensor(4.9951e-13, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.9951e-13, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0090, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0090, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633400
repetition 2
python3 benchmark_exp_lr.py 0.001 10000 500 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05] Logistic_regression cifar10_2 2 0.0001 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 2.301022
Train - Epoch 1, Batch: 0, Loss: 2.236974
Train - Epoch 2, Batch: 0, Loss: 2.181868
Train - Epoch 3, Batch: 0, Loss: 2.130809
Train - Epoch 4, Batch: 0, Loss: 2.085286
Train - Epoch 5, Batch: 0, Loss: 2.046894
Train - Epoch 6, Batch: 0, Loss: 1.999479
Train - Epoch 7, Batch: 0, Loss: 1.964496
Train - Epoch 8, Batch: 0, Loss: 1.925124
Train - Epoch 9, Batch: 0, Loss: 1.894502
Train - Epoch 10, Batch: 0, Loss: 1.860402
Train - Epoch 11, Batch: 0, Loss: 1.829926
Train - Epoch 12, Batch: 0, Loss: 1.810829
Train - Epoch 13, Batch: 0, Loss: 1.785917
Train - Epoch 14, Batch: 0, Loss: 1.760962
Train - Epoch 15, Batch: 0, Loss: 1.733778
Train - Epoch 16, Batch: 0, Loss: 1.705967
Train - Epoch 17, Batch: 0, Loss: 1.693372
Train - Epoch 18, Batch: 0, Loss: 1.667899
Train - Epoch 19, Batch: 0, Loss: 1.656006
Train - Epoch 20, Batch: 0, Loss: 1.641842
Train - Epoch 21, Batch: 0, Loss: 1.619853
Train - Epoch 22, Batch: 0, Loss: 1.604715
Train - Epoch 23, Batch: 0, Loss: 1.584305
Train - Epoch 24, Batch: 0, Loss: 1.580781
Train - Epoch 25, Batch: 0, Loss: 1.565781
Train - Epoch 26, Batch: 0, Loss: 1.557691
Train - Epoch 27, Batch: 0, Loss: 1.545399
Train - Epoch 28, Batch: 0, Loss: 1.523159
Train - Epoch 29, Batch: 0, Loss: 1.524228
Train - Epoch 30, Batch: 0, Loss: 1.498409
Train - Epoch 31, Batch: 0, Loss: 1.496355
Train - Epoch 32, Batch: 0, Loss: 1.480846
Train - Epoch 33, Batch: 0, Loss: 1.481275
Train - Epoch 34, Batch: 0, Loss: 1.454246
Train - Epoch 35, Batch: 0, Loss: 1.462041
Train - Epoch 36, Batch: 0, Loss: 1.446588
Train - Epoch 37, Batch: 0, Loss: 1.452716
Train - Epoch 38, Batch: 0, Loss: 1.427764
Train - Epoch 39, Batch: 0, Loss: 1.421835
Train - Epoch 40, Batch: 0, Loss: 1.420494
Train - Epoch 41, Batch: 0, Loss: 1.417136
Train - Epoch 42, Batch: 0, Loss: 1.410750
Train - Epoch 43, Batch: 0, Loss: 1.394369
Train - Epoch 44, Batch: 0, Loss: 1.399202
Train - Epoch 45, Batch: 0, Loss: 1.390418
Train - Epoch 46, Batch: 0, Loss: 1.377681
Train - Epoch 47, Batch: 0, Loss: 1.379564
Train - Epoch 48, Batch: 0, Loss: 1.370849
Train - Epoch 49, Batch: 0, Loss: 1.363043
Train - Epoch 50, Batch: 0, Loss: 1.351305
Train - Epoch 51, Batch: 0, Loss: 1.356699
Train - Epoch 52, Batch: 0, Loss: 1.358364
Train - Epoch 53, Batch: 0, Loss: 1.358605
Train - Epoch 54, Batch: 0, Loss: 1.342514
Train - Epoch 55, Batch: 0, Loss: 1.334776
Train - Epoch 56, Batch: 0, Loss: 1.337085
Train - Epoch 57, Batch: 0, Loss: 1.333272
Train - Epoch 58, Batch: 0, Loss: 1.311825
Train - Epoch 59, Batch: 0, Loss: 1.318643
Train - Epoch 60, Batch: 0, Loss: 1.319100
Train - Epoch 61, Batch: 0, Loss: 1.320389
Train - Epoch 62, Batch: 0, Loss: 1.309112
Train - Epoch 63, Batch: 0, Loss: 1.313759
Train - Epoch 64, Batch: 0, Loss: 1.302892
Train - Epoch 65, Batch: 0, Loss: 1.301051
Train - Epoch 66, Batch: 0, Loss: 1.292862
Train - Epoch 67, Batch: 0, Loss: 1.295669
Train - Epoch 68, Batch: 0, Loss: 1.283345
Train - Epoch 69, Batch: 0, Loss: 1.293943
Train - Epoch 70, Batch: 0, Loss: 1.265113
Train - Epoch 71, Batch: 0, Loss: 1.279017
Train - Epoch 72, Batch: 0, Loss: 1.271024
Train - Epoch 73, Batch: 0, Loss: 1.279864
Train - Epoch 74, Batch: 0, Loss: 1.287888
Train - Epoch 75, Batch: 0, Loss: 1.259666
Train - Epoch 76, Batch: 0, Loss: 1.259818
Train - Epoch 77, Batch: 0, Loss: 1.254796
Train - Epoch 78, Batch: 0, Loss: 1.267138
Train - Epoch 79, Batch: 0, Loss: 1.256923
Train - Epoch 80, Batch: 0, Loss: 1.251111
Train - Epoch 81, Batch: 0, Loss: 1.264816
Train - Epoch 82, Batch: 0, Loss: 1.259082
Train - Epoch 83, Batch: 0, Loss: 1.246745
Train - Epoch 84, Batch: 0, Loss: 1.251524
Train - Epoch 85, Batch: 0, Loss: 1.250708
Train - Epoch 86, Batch: 0, Loss: 1.250384
Train - Epoch 87, Batch: 0, Loss: 1.238889
Train - Epoch 88, Batch: 0, Loss: 1.241234
Train - Epoch 89, Batch: 0, Loss: 1.239556
Train - Epoch 90, Batch: 0, Loss: 1.222253
Train - Epoch 91, Batch: 0, Loss: 1.234075
Train - Epoch 92, Batch: 0, Loss: 1.209842
Train - Epoch 93, Batch: 0, Loss: 1.233643
Train - Epoch 94, Batch: 0, Loss: 1.219673
Train - Epoch 95, Batch: 0, Loss: 1.248401
Train - Epoch 96, Batch: 0, Loss: 1.217788
Train - Epoch 97, Batch: 0, Loss: 1.234901
Train - Epoch 98, Batch: 0, Loss: 1.202988
Train - Epoch 99, Batch: 0, Loss: 1.207285
Train - Epoch 100, Batch: 0, Loss: 1.222573
Train - Epoch 101, Batch: 0, Loss: 1.205769
Train - Epoch 102, Batch: 0, Loss: 1.232415
Train - Epoch 103, Batch: 0, Loss: 1.202630
Train - Epoch 104, Batch: 0, Loss: 1.210457
Train - Epoch 105, Batch: 0, Loss: 1.203963
Train - Epoch 106, Batch: 0, Loss: 1.207237
Train - Epoch 107, Batch: 0, Loss: 1.194432
Train - Epoch 108, Batch: 0, Loss: 1.202367
Train - Epoch 109, Batch: 0, Loss: 1.215830
Train - Epoch 110, Batch: 0, Loss: 1.191234
Train - Epoch 111, Batch: 0, Loss: 1.195840
Train - Epoch 112, Batch: 0, Loss: 1.195499
Train - Epoch 113, Batch: 0, Loss: 1.195855
Train - Epoch 114, Batch: 0, Loss: 1.183912
Train - Epoch 115, Batch: 0, Loss: 1.193641
Train - Epoch 116, Batch: 0, Loss: 1.188911
Train - Epoch 117, Batch: 0, Loss: 1.181337
Train - Epoch 118, Batch: 0, Loss: 1.175778
Train - Epoch 119, Batch: 0, Loss: 1.182430
Train - Epoch 120, Batch: 0, Loss: 1.178920
Train - Epoch 121, Batch: 0, Loss: 1.180389
Train - Epoch 122, Batch: 0, Loss: 1.177577
Train - Epoch 123, Batch: 0, Loss: 1.184358
Train - Epoch 124, Batch: 0, Loss: 1.169015
Train - Epoch 125, Batch: 0, Loss: 1.176639
Train - Epoch 126, Batch: 0, Loss: 1.173692
Train - Epoch 127, Batch: 0, Loss: 1.185794
Train - Epoch 128, Batch: 0, Loss: 1.169969
Train - Epoch 129, Batch: 0, Loss: 1.172560
Train - Epoch 130, Batch: 0, Loss: 1.167558
Train - Epoch 131, Batch: 0, Loss: 1.163056
Train - Epoch 132, Batch: 0, Loss: 1.159957
Train - Epoch 133, Batch: 0, Loss: 1.166171
Train - Epoch 134, Batch: 0, Loss: 1.186687
Train - Epoch 135, Batch: 0, Loss: 1.156176
Train - Epoch 136, Batch: 0, Loss: 1.171609
Train - Epoch 137, Batch: 0, Loss: 1.165434
Train - Epoch 138, Batch: 0, Loss: 1.167389
Train - Epoch 139, Batch: 0, Loss: 1.158523
Train - Epoch 140, Batch: 0, Loss: 1.157164
Train - Epoch 141, Batch: 0, Loss: 1.174644
Train - Epoch 142, Batch: 0, Loss: 1.165041
Train - Epoch 143, Batch: 0, Loss: 1.163175
Train - Epoch 144, Batch: 0, Loss: 1.165291
Train - Epoch 145, Batch: 0, Loss: 1.170764
Train - Epoch 146, Batch: 0, Loss: 1.151386
Train - Epoch 147, Batch: 0, Loss: 1.158621
Train - Epoch 148, Batch: 0, Loss: 1.153951
Train - Epoch 149, Batch: 0, Loss: 1.156088
Train - Epoch 150, Batch: 0, Loss: 1.160884
Train - Epoch 151, Batch: 0, Loss: 1.145219
Train - Epoch 152, Batch: 0, Loss: 1.141775
Train - Epoch 153, Batch: 0, Loss: 1.156300
Train - Epoch 154, Batch: 0, Loss: 1.163700
Train - Epoch 155, Batch: 0, Loss: 1.151426
Train - Epoch 156, Batch: 0, Loss: 1.148626
Train - Epoch 157, Batch: 0, Loss: 1.152944
Train - Epoch 158, Batch: 0, Loss: 1.156384
Train - Epoch 159, Batch: 0, Loss: 1.139196
Train - Epoch 160, Batch: 0, Loss: 1.145359
Train - Epoch 161, Batch: 0, Loss: 1.137576
Train - Epoch 162, Batch: 0, Loss: 1.142401
Train - Epoch 163, Batch: 0, Loss: 1.145169
Train - Epoch 164, Batch: 0, Loss: 1.147090
Train - Epoch 165, Batch: 0, Loss: 1.135605
Train - Epoch 166, Batch: 0, Loss: 1.129924
Train - Epoch 167, Batch: 0, Loss: 1.133479
Train - Epoch 168, Batch: 0, Loss: 1.128527
Train - Epoch 169, Batch: 0, Loss: 1.123502
Train - Epoch 170, Batch: 0, Loss: 1.138946
Train - Epoch 171, Batch: 0, Loss: 1.137695
Train - Epoch 172, Batch: 0, Loss: 1.136866
Train - Epoch 173, Batch: 0, Loss: 1.124189
Train - Epoch 174, Batch: 0, Loss: 1.124577
Train - Epoch 175, Batch: 0, Loss: 1.139269
Train - Epoch 176, Batch: 0, Loss: 1.145886
Train - Epoch 177, Batch: 0, Loss: 1.124242
Train - Epoch 178, Batch: 0, Loss: 1.127308
Train - Epoch 179, Batch: 0, Loss: 1.131549
Train - Epoch 180, Batch: 0, Loss: 1.129961
Train - Epoch 181, Batch: 0, Loss: 1.132319
Train - Epoch 182, Batch: 0, Loss: 1.115686
Train - Epoch 183, Batch: 0, Loss: 1.122783
Train - Epoch 184, Batch: 0, Loss: 1.139448
Train - Epoch 185, Batch: 0, Loss: 1.138332
Train - Epoch 186, Batch: 0, Loss: 1.130662
Train - Epoch 187, Batch: 0, Loss: 1.121041
Train - Epoch 188, Batch: 0, Loss: 1.130275
Train - Epoch 189, Batch: 0, Loss: 1.127820
Train - Epoch 190, Batch: 0, Loss: 1.123678
Train - Epoch 191, Batch: 0, Loss: 1.129723
Train - Epoch 192, Batch: 0, Loss: 1.120032
Train - Epoch 193, Batch: 0, Loss: 1.126847
Train - Epoch 194, Batch: 0, Loss: 1.128964
Train - Epoch 195, Batch: 0, Loss: 1.129630
Train - Epoch 196, Batch: 0, Loss: 1.112789
Train - Epoch 197, Batch: 0, Loss: 1.122197
Train - Epoch 198, Batch: 0, Loss: 1.118638
Train - Epoch 199, Batch: 0, Loss: 1.119043
Train - Epoch 200, Batch: 0, Loss: 1.111847
Train - Epoch 201, Batch: 0, Loss: 1.124302
Train - Epoch 202, Batch: 0, Loss: 1.109141
Train - Epoch 203, Batch: 0, Loss: 1.100184
Train - Epoch 204, Batch: 0, Loss: 1.115769
Train - Epoch 205, Batch: 0, Loss: 1.102196
Train - Epoch 206, Batch: 0, Loss: 1.113697
Train - Epoch 207, Batch: 0, Loss: 1.116064
Train - Epoch 208, Batch: 0, Loss: 1.112136
Train - Epoch 209, Batch: 0, Loss: 1.123791
Train - Epoch 210, Batch: 0, Loss: 1.120603
Train - Epoch 211, Batch: 0, Loss: 1.127569
Train - Epoch 212, Batch: 0, Loss: 1.110348
Train - Epoch 213, Batch: 0, Loss: 1.107144
Train - Epoch 214, Batch: 0, Loss: 1.119625
Train - Epoch 215, Batch: 0, Loss: 1.111560
Train - Epoch 216, Batch: 0, Loss: 1.116672
Train - Epoch 217, Batch: 0, Loss: 1.104315
Train - Epoch 218, Batch: 0, Loss: 1.104296
Train - Epoch 219, Batch: 0, Loss: 1.108669
Train - Epoch 220, Batch: 0, Loss: 1.120206
Train - Epoch 221, Batch: 0, Loss: 1.105941
Train - Epoch 222, Batch: 0, Loss: 1.111113
Train - Epoch 223, Batch: 0, Loss: 1.105495
Train - Epoch 224, Batch: 0, Loss: 1.104766
Train - Epoch 225, Batch: 0, Loss: 1.097387
Train - Epoch 226, Batch: 0, Loss: 1.114774
Train - Epoch 227, Batch: 0, Loss: 1.102976
Train - Epoch 228, Batch: 0, Loss: 1.099903
Train - Epoch 229, Batch: 0, Loss: 1.084323
Train - Epoch 230, Batch: 0, Loss: 1.107640
Train - Epoch 231, Batch: 0, Loss: 1.095811
Train - Epoch 232, Batch: 0, Loss: 1.105445
Train - Epoch 233, Batch: 0, Loss: 1.095663
Train - Epoch 234, Batch: 0, Loss: 1.103595
Train - Epoch 235, Batch: 0, Loss: 1.108622
Train - Epoch 236, Batch: 0, Loss: 1.112228
Train - Epoch 237, Batch: 0, Loss: 1.111409
Train - Epoch 238, Batch: 0, Loss: 1.107405
Train - Epoch 239, Batch: 0, Loss: 1.097750
Train - Epoch 240, Batch: 0, Loss: 1.093226
Train - Epoch 241, Batch: 0, Loss: 1.103090
Train - Epoch 242, Batch: 0, Loss: 1.115062
Train - Epoch 243, Batch: 0, Loss: 1.088722
Train - Epoch 244, Batch: 0, Loss: 1.101167
Train - Epoch 245, Batch: 0, Loss: 1.088490
Train - Epoch 246, Batch: 0, Loss: 1.080454
Train - Epoch 247, Batch: 0, Loss: 1.087019
Train - Epoch 248, Batch: 0, Loss: 1.090711
Train - Epoch 249, Batch: 0, Loss: 1.095238
Train - Epoch 250, Batch: 0, Loss: 1.103872
Train - Epoch 251, Batch: 0, Loss: 1.107663
Train - Epoch 252, Batch: 0, Loss: 1.073886
Train - Epoch 253, Batch: 0, Loss: 1.089858
Train - Epoch 254, Batch: 0, Loss: 1.088654
Train - Epoch 255, Batch: 0, Loss: 1.097912
Train - Epoch 256, Batch: 0, Loss: 1.099819
Train - Epoch 257, Batch: 0, Loss: 1.087304
Train - Epoch 258, Batch: 0, Loss: 1.091642
Train - Epoch 259, Batch: 0, Loss: 1.091731
Train - Epoch 260, Batch: 0, Loss: 1.090292
Train - Epoch 261, Batch: 0, Loss: 1.091992
Train - Epoch 262, Batch: 0, Loss: 1.102053
Train - Epoch 263, Batch: 0, Loss: 1.081525
Train - Epoch 264, Batch: 0, Loss: 1.083319
Train - Epoch 265, Batch: 0, Loss: 1.084864
Train - Epoch 266, Batch: 0, Loss: 1.095745
Train - Epoch 267, Batch: 0, Loss: 1.103931
Train - Epoch 268, Batch: 0, Loss: 1.089436
Train - Epoch 269, Batch: 0, Loss: 1.091104
Train - Epoch 270, Batch: 0, Loss: 1.095003
Train - Epoch 271, Batch: 0, Loss: 1.072122
Train - Epoch 272, Batch: 0, Loss: 1.091361
Train - Epoch 273, Batch: 0, Loss: 1.089055
Train - Epoch 274, Batch: 0, Loss: 1.072000
Train - Epoch 275, Batch: 0, Loss: 1.070580
Train - Epoch 276, Batch: 0, Loss: 1.081219
Train - Epoch 277, Batch: 0, Loss: 1.087646
Train - Epoch 278, Batch: 0, Loss: 1.079134
Train - Epoch 279, Batch: 0, Loss: 1.092360
Train - Epoch 280, Batch: 0, Loss: 1.083787
Train - Epoch 281, Batch: 0, Loss: 1.087712
Train - Epoch 282, Batch: 0, Loss: 1.088943
Train - Epoch 283, Batch: 0, Loss: 1.085238
Train - Epoch 284, Batch: 0, Loss: 1.085406
Train - Epoch 285, Batch: 0, Loss: 1.079301
Train - Epoch 286, Batch: 0, Loss: 1.083094
Train - Epoch 287, Batch: 0, Loss: 1.084351
Train - Epoch 288, Batch: 0, Loss: 1.082211
Train - Epoch 289, Batch: 0, Loss: 1.079104
Train - Epoch 290, Batch: 0, Loss: 1.077395
Train - Epoch 291, Batch: 0, Loss: 1.074040
Train - Epoch 292, Batch: 0, Loss: 1.075805
Train - Epoch 293, Batch: 0, Loss: 1.082995
Train - Epoch 294, Batch: 0, Loss: 1.080850
Train - Epoch 295, Batch: 0, Loss: 1.085774
Train - Epoch 296, Batch: 0, Loss: 1.080242
Train - Epoch 297, Batch: 0, Loss: 1.067308
Train - Epoch 298, Batch: 0, Loss: 1.068169
Train - Epoch 299, Batch: 0, Loss: 1.082582
Train - Epoch 300, Batch: 0, Loss: 1.086665
Train - Epoch 301, Batch: 0, Loss: 1.079291
Train - Epoch 302, Batch: 0, Loss: 1.075161
Train - Epoch 303, Batch: 0, Loss: 1.078292
Train - Epoch 304, Batch: 0, Loss: 1.088921
Train - Epoch 305, Batch: 0, Loss: 1.068905
Train - Epoch 306, Batch: 0, Loss: 1.079359
Train - Epoch 307, Batch: 0, Loss: 1.055948
Train - Epoch 308, Batch: 0, Loss: 1.083759
Train - Epoch 309, Batch: 0, Loss: 1.071398
Train - Epoch 310, Batch: 0, Loss: 1.076424
Train - Epoch 311, Batch: 0, Loss: 1.074580
Train - Epoch 312, Batch: 0, Loss: 1.099166
Train - Epoch 313, Batch: 0, Loss: 1.081728
Train - Epoch 314, Batch: 0, Loss: 1.085021
Train - Epoch 315, Batch: 0, Loss: 1.063000
Train - Epoch 316, Batch: 0, Loss: 1.078632
Train - Epoch 317, Batch: 0, Loss: 1.060790
Train - Epoch 318, Batch: 0, Loss: 1.078503
Train - Epoch 319, Batch: 0, Loss: 1.084041
Train - Epoch 320, Batch: 0, Loss: 1.077688
Train - Epoch 321, Batch: 0, Loss: 1.079215
Train - Epoch 322, Batch: 0, Loss: 1.071250
Train - Epoch 323, Batch: 0, Loss: 1.075208
Train - Epoch 324, Batch: 0, Loss: 1.074690
Train - Epoch 325, Batch: 0, Loss: 1.076602
Train - Epoch 326, Batch: 0, Loss: 1.068068
Train - Epoch 327, Batch: 0, Loss: 1.073840
Train - Epoch 328, Batch: 0, Loss: 1.053849
Train - Epoch 329, Batch: 0, Loss: 1.065437
Train - Epoch 330, Batch: 0, Loss: 1.076662
Train - Epoch 331, Batch: 0, Loss: 1.071204
Train - Epoch 332, Batch: 0, Loss: 1.068575
Train - Epoch 333, Batch: 0, Loss: 1.059829
Train - Epoch 334, Batch: 0, Loss: 1.045001
Train - Epoch 335, Batch: 0, Loss: 1.061879
Train - Epoch 336, Batch: 0, Loss: 1.065996
Train - Epoch 337, Batch: 0, Loss: 1.078265
Train - Epoch 338, Batch: 0, Loss: 1.063300
Train - Epoch 339, Batch: 0, Loss: 1.072854
Train - Epoch 340, Batch: 0, Loss: 1.072103
Train - Epoch 341, Batch: 0, Loss: 1.061157
Train - Epoch 342, Batch: 0, Loss: 1.077310
Train - Epoch 343, Batch: 0, Loss: 1.054648
Train - Epoch 344, Batch: 0, Loss: 1.064919
Train - Epoch 345, Batch: 0, Loss: 1.058765
Train - Epoch 346, Batch: 0, Loss: 1.070653
Train - Epoch 347, Batch: 0, Loss: 1.070326
Train - Epoch 348, Batch: 0, Loss: 1.074170
Train - Epoch 349, Batch: 0, Loss: 1.068319
Train - Epoch 350, Batch: 0, Loss: 1.056524
Train - Epoch 351, Batch: 0, Loss: 1.061794
Train - Epoch 352, Batch: 0, Loss: 1.079601
Train - Epoch 353, Batch: 0, Loss: 1.075411
Train - Epoch 354, Batch: 0, Loss: 1.073420
Train - Epoch 355, Batch: 0, Loss: 1.075509
Train - Epoch 356, Batch: 0, Loss: 1.077319
Train - Epoch 357, Batch: 0, Loss: 1.063949
Train - Epoch 358, Batch: 0, Loss: 1.079832
Train - Epoch 359, Batch: 0, Loss: 1.052789
Train - Epoch 360, Batch: 0, Loss: 1.057617
Train - Epoch 361, Batch: 0, Loss: 1.051063
Train - Epoch 362, Batch: 0, Loss: 1.068752
Train - Epoch 363, Batch: 0, Loss: 1.072091
Train - Epoch 364, Batch: 0, Loss: 1.050598
Train - Epoch 365, Batch: 0, Loss: 1.074473
Train - Epoch 366, Batch: 0, Loss: 1.053740
Train - Epoch 367, Batch: 0, Loss: 1.047783
Train - Epoch 368, Batch: 0, Loss: 1.058661
Train - Epoch 369, Batch: 0, Loss: 1.061290
Train - Epoch 370, Batch: 0, Loss: 1.054848
Train - Epoch 371, Batch: 0, Loss: 1.053508
Train - Epoch 372, Batch: 0, Loss: 1.071946
Train - Epoch 373, Batch: 0, Loss: 1.055696/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 374, Batch: 0, Loss: 1.076550
Train - Epoch 375, Batch: 0, Loss: 1.059072
Train - Epoch 376, Batch: 0, Loss: 1.053553
Train - Epoch 377, Batch: 0, Loss: 1.068288
Train - Epoch 378, Batch: 0, Loss: 1.074573
Train - Epoch 379, Batch: 0, Loss: 1.059232
Train - Epoch 380, Batch: 0, Loss: 1.057990
Train - Epoch 381, Batch: 0, Loss: 1.056963
Train - Epoch 382, Batch: 0, Loss: 1.061197
Train - Epoch 383, Batch: 0, Loss: 1.066162
Train - Epoch 384, Batch: 0, Loss: 1.047363
Train - Epoch 385, Batch: 0, Loss: 1.068255
Train - Epoch 386, Batch: 0, Loss: 1.068376
Train - Epoch 387, Batch: 0, Loss: 1.056139
Train - Epoch 388, Batch: 0, Loss: 1.051194
Train - Epoch 389, Batch: 0, Loss: 1.061742
Train - Epoch 390, Batch: 0, Loss: 1.054432
Train - Epoch 391, Batch: 0, Loss: 1.045675
Train - Epoch 392, Batch: 0, Loss: 1.061700
Train - Epoch 393, Batch: 0, Loss: 1.057479
Train - Epoch 394, Batch: 0, Loss: 1.079308
Train - Epoch 395, Batch: 0, Loss: 1.057630
Train - Epoch 396, Batch: 0, Loss: 1.053792
Train - Epoch 397, Batch: 0, Loss: 1.045471
Train - Epoch 398, Batch: 0, Loss: 1.059346
Train - Epoch 399, Batch: 0, Loss: 1.051028
Train - Epoch 400, Batch: 0, Loss: 1.057925
Train - Epoch 401, Batch: 0, Loss: 1.063513
Train - Epoch 402, Batch: 0, Loss: 1.069532
Train - Epoch 403, Batch: 0, Loss: 1.056722
Train - Epoch 404, Batch: 0, Loss: 1.056647
Train - Epoch 405, Batch: 0, Loss: 1.061625
Train - Epoch 406, Batch: 0, Loss: 1.065037
Train - Epoch 407, Batch: 0, Loss: 1.029468
Train - Epoch 408, Batch: 0, Loss: 1.058408
Train - Epoch 409, Batch: 0, Loss: 1.051927
Train - Epoch 410, Batch: 0, Loss: 1.055573
Train - Epoch 411, Batch: 0, Loss: 1.060408
Train - Epoch 412, Batch: 0, Loss: 1.056441
Train - Epoch 413, Batch: 0, Loss: 1.045116
Train - Epoch 414, Batch: 0, Loss: 1.040538
Train - Epoch 415, Batch: 0, Loss: 1.063212
Train - Epoch 416, Batch: 0, Loss: 1.054783
Train - Epoch 417, Batch: 0, Loss: 1.056728
Train - Epoch 418, Batch: 0, Loss: 1.054513
Train - Epoch 419, Batch: 0, Loss: 1.044411
Train - Epoch 420, Batch: 0, Loss: 1.043628
Train - Epoch 421, Batch: 0, Loss: 1.041449
Train - Epoch 422, Batch: 0, Loss: 1.047839
Train - Epoch 423, Batch: 0, Loss: 1.063666
Train - Epoch 424, Batch: 0, Loss: 1.060203
Train - Epoch 425, Batch: 0, Loss: 1.051969
Train - Epoch 426, Batch: 0, Loss: 1.044103
Train - Epoch 427, Batch: 0, Loss: 1.063684
Train - Epoch 428, Batch: 0, Loss: 1.056101
Train - Epoch 429, Batch: 0, Loss: 1.039641
Train - Epoch 430, Batch: 0, Loss: 1.053818
Train - Epoch 431, Batch: 0, Loss: 1.039449
Train - Epoch 432, Batch: 0, Loss: 1.025217
Train - Epoch 433, Batch: 0, Loss: 1.055779
Train - Epoch 434, Batch: 0, Loss: 1.060807
Train - Epoch 435, Batch: 0, Loss: 1.031819
Train - Epoch 436, Batch: 0, Loss: 1.040083
Train - Epoch 437, Batch: 0, Loss: 1.056047
Train - Epoch 438, Batch: 0, Loss: 1.038532
Train - Epoch 439, Batch: 0, Loss: 1.061996
Train - Epoch 440, Batch: 0, Loss: 1.047212
Train - Epoch 441, Batch: 0, Loss: 1.051325
Train - Epoch 442, Batch: 0, Loss: 1.046231
Train - Epoch 443, Batch: 0, Loss: 1.060271
Train - Epoch 444, Batch: 0, Loss: 1.048258
Train - Epoch 445, Batch: 0, Loss: 1.050169
Train - Epoch 446, Batch: 0, Loss: 1.034506
Train - Epoch 447, Batch: 0, Loss: 1.043138
Train - Epoch 448, Batch: 0, Loss: 1.047772
Train - Epoch 449, Batch: 0, Loss: 1.040811
Train - Epoch 450, Batch: 0, Loss: 1.036370
Train - Epoch 451, Batch: 0, Loss: 1.039547
Train - Epoch 452, Batch: 0, Loss: 1.024745
Train - Epoch 453, Batch: 0, Loss: 1.045210
Train - Epoch 454, Batch: 0, Loss: 1.047107
Train - Epoch 455, Batch: 0, Loss: 1.036865
Train - Epoch 456, Batch: 0, Loss: 1.035440
Train - Epoch 457, Batch: 0, Loss: 1.033724
Train - Epoch 458, Batch: 0, Loss: 1.040849
Train - Epoch 459, Batch: 0, Loss: 1.054384
Train - Epoch 460, Batch: 0, Loss: 1.040344
Train - Epoch 461, Batch: 0, Loss: 1.044697
Train - Epoch 462, Batch: 0, Loss: 1.029782
Train - Epoch 463, Batch: 0, Loss: 1.035159
Train - Epoch 464, Batch: 0, Loss: 1.047263
Train - Epoch 465, Batch: 0, Loss: 1.067851
Train - Epoch 466, Batch: 0, Loss: 1.035365
Train - Epoch 467, Batch: 0, Loss: 1.045479
Train - Epoch 468, Batch: 0, Loss: 1.047932
Train - Epoch 469, Batch: 0, Loss: 1.052306
Train - Epoch 470, Batch: 0, Loss: 1.031493
Train - Epoch 471, Batch: 0, Loss: 1.045010
Train - Epoch 472, Batch: 0, Loss: 1.045789
Train - Epoch 473, Batch: 0, Loss: 1.038531
Train - Epoch 474, Batch: 0, Loss: 1.032368
Train - Epoch 475, Batch: 0, Loss: 1.043162
Train - Epoch 476, Batch: 0, Loss: 1.051401
Train - Epoch 477, Batch: 0, Loss: 1.042577
Train - Epoch 478, Batch: 0, Loss: 1.055794
Train - Epoch 479, Batch: 0, Loss: 1.029821
Train - Epoch 480, Batch: 0, Loss: 1.069171
Train - Epoch 481, Batch: 0, Loss: 1.049427
Train - Epoch 482, Batch: 0, Loss: 1.043486
Train - Epoch 483, Batch: 0, Loss: 1.049517
Train - Epoch 484, Batch: 0, Loss: 1.037952
Train - Epoch 485, Batch: 0, Loss: 1.036179
Train - Epoch 486, Batch: 0, Loss: 1.035164
Train - Epoch 487, Batch: 0, Loss: 1.023303
Train - Epoch 488, Batch: 0, Loss: 1.037744
Train - Epoch 489, Batch: 0, Loss: 1.031659
Train - Epoch 490, Batch: 0, Loss: 1.041245
Train - Epoch 491, Batch: 0, Loss: 1.041220
Train - Epoch 492, Batch: 0, Loss: 1.027287
Train - Epoch 493, Batch: 0, Loss: 1.033821
Train - Epoch 494, Batch: 0, Loss: 1.044153
Train - Epoch 495, Batch: 0, Loss: 1.041279
Train - Epoch 496, Batch: 0, Loss: 1.042953
Train - Epoch 497, Batch: 0, Loss: 1.040829
Train - Epoch 498, Batch: 0, Loss: 1.023148
Train - Epoch 499, Batch: 0, Loss: 1.028304
training_time:: 107.34299826622009
training time full:: 107.34306740760803
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632600
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 2
max_epoch:: 500
batch_size:: 10000
epoch:: 0
tensor([34294, 41724,  7727,  2182, 44783, 44823, 29950, 27423, 30966,  4723,
         5063, 43302, 21045, 48405, 13790, 35691, 29608,  1947, 40072,  9773,
        45520,  4629,  1414, 11176, 25014,  1346,  5585, 30222,  6773, 38904,
        27142, 36745,  4467, 47824, 33434,  1309, 41106, 13952,  6764,  7082,
        43398, 36472, 22881, 22307,  5477, 17341, 24981, 26907,  2833, 33413])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
epoch:: 60
epoch:: 61
epoch:: 62
epoch:: 63
epoch:: 64
epoch:: 65
epoch:: 66
epoch:: 67
epoch:: 68
epoch:: 69
epoch:: 70
epoch:: 71
epoch:: 72
epoch:: 73
epoch:: 74
epoch:: 75
epoch:: 76
epoch:: 77
epoch:: 78
epoch:: 79
epoch:: 80
epoch:: 81
epoch:: 82
epoch:: 83
epoch:: 84
epoch:: 85
epoch:: 86
epoch:: 87
epoch:: 88
epoch:: 89
epoch:: 90
epoch:: 91
epoch:: 92
epoch:: 93
epoch:: 94
epoch:: 95
epoch:: 96
epoch:: 97
epoch:: 98
epoch:: 99
epoch:: 100
epoch:: 101
epoch:: 102
epoch:: 103
epoch:: 104
epoch:: 105
epoch:: 106
epoch:: 107
epoch:: 108
epoch:: 109
epoch:: 110
epoch:: 111
epoch:: 112
epoch:: 113
epoch:: 114
epoch:: 115
epoch:: 116
epoch:: 117
epoch:: 118
epoch:: 119
epoch:: 120
epoch:: 121
epoch:: 122
epoch:: 123
epoch:: 124
epoch:: 125
epoch:: 126
epoch:: 127
epoch:: 128
epoch:: 129
epoch:: 130
epoch:: 131
epoch:: 132
epoch:: 133
epoch:: 134
epoch:: 135
epoch:: 136
epoch:: 137
epoch:: 138
epoch:: 139
epoch:: 140
epoch:: 141
epoch:: 142
epoch:: 143
epoch:: 144
epoch:: 145
epoch:: 146
epoch:: 147
epoch:: 148
epoch:: 149
epoch:: 150
epoch:: 151
epoch:: 152
epoch:: 153
epoch:: 154
epoch:: 155
epoch:: 156
epoch:: 157
epoch:: 158
epoch:: 159
epoch:: 160
epoch:: 161
epoch:: 162
epoch:: 163
epoch:: 164
epoch:: 165
epoch:: 166
epoch:: 167
epoch:: 168
epoch:: 169
epoch:: 170
epoch:: 171
epoch:: 172
epoch:: 173
epoch:: 174
epoch:: 175
epoch:: 176
epoch:: 177
epoch:: 178
epoch:: 179
epoch:: 180
epoch:: 181
epoch:: 182
epoch:: 183
epoch:: 184
epoch:: 185
epoch:: 186
epoch:: 187
epoch:: 188
epoch:: 189
epoch:: 190
epoch:: 191
epoch:: 192
epoch:: 193
epoch:: 194
epoch:: 195
epoch:: 196
epoch:: 197
epoch:: 198
epoch:: 199
epoch:: 200
epoch:: 201
epoch:: 202
epoch:: 203
epoch:: 204
epoch:: 205
epoch:: 206
epoch:: 207
epoch:: 208
epoch:: 209
epoch:: 210
epoch:: 211
epoch:: 212
epoch:: 213
epoch:: 214
epoch:: 215
epoch:: 216
epoch:: 217
epoch:: 218
epoch:: 219
epoch:: 220
epoch:: 221
epoch:: 222
epoch:: 223
epoch:: 224
epoch:: 225
epoch:: 226
epoch:: 227
epoch:: 228
epoch:: 229
epoch:: 230
epoch:: 231
epoch:: 232
epoch:: 233
epoch:: 234
epoch:: 235
epoch:: 236
epoch:: 237
epoch:: 238
epoch:: 239
epoch:: 240
epoch:: 241
epoch:: 242
epoch:: 243
epoch:: 244
epoch:: 245
epoch:: 246
epoch:: 247
epoch:: 248
epoch:: 249
epoch:: 250
epoch:: 251
epoch:: 252
epoch:: 253
epoch:: 254
epoch:: 255
epoch:: 256
epoch:: 257
epoch:: 258
epoch:: 259
epoch:: 260
epoch:: 261
epoch:: 262
epoch:: 263
epoch:: 264
epoch:: 265
epoch:: 266
epoch:: 267
epoch:: 268
epoch:: 269
epoch:: 270
epoch:: 271
epoch:: 272
epoch:: 273
epoch:: 274
epoch:: 275
epoch:: 276
epoch:: 277
epoch:: 278
epoch:: 279
epoch:: 280
epoch:: 281
epoch:: 282
epoch:: 283
epoch:: 284
epoch:: 285
epoch:: 286
epoch:: 287
epoch:: 288
epoch:: 289
epoch:: 290
epoch:: 291
epoch:: 292
epoch:: 293
epoch:: 294
epoch:: 295
epoch:: 296
epoch:: 297
epoch:: 298
epoch:: 299
epoch:: 300
epoch:: 301
epoch:: 302
epoch:: 303
epoch:: 304
epoch:: 305
epoch:: 306
epoch:: 307
epoch:: 308
epoch:: 309
epoch:: 310
epoch:: 311
epoch:: 312
epoch:: 313
epoch:: 314
epoch:: 315
epoch:: 316
epoch:: 317
epoch:: 318
epoch:: 319
epoch:: 320
epoch:: 321
epoch:: 322
epoch:: 323
epoch:: 324
epoch:: 325
epoch:: 326
epoch:: 327
epoch:: 328
epoch:: 329
epoch:: 330
epoch:: 331
epoch:: 332
epoch:: 333
epoch:: 334
epoch:: 335
epoch:: 336
epoch:: 337
epoch:: 338
epoch:: 339
epoch:: 340
epoch:: 341
epoch:: 342
epoch:: 343
epoch:: 344
epoch:: 345
epoch:: 346
epoch:: 347
epoch:: 348
epoch:: 349
epoch:: 350
epoch:: 351
epoch:: 352
epoch:: 353
epoch:: 354
epoch:: 355
epoch:: 356
epoch:: 357
epoch:: 358
epoch:: 359
epoch:: 360
epoch:: 361
epoch:: 362
epoch:: 363
epoch:: 364
epoch:: 365
epoch:: 366
epoch:: 367
epoch:: 368
epoch:: 369
epoch:: 370
epoch:: 371
epoch:: 372
epoch:: 373
epoch:: 374
epoch:: 375
epoch:: 376
epoch:: 377
epoch:: 378
epoch:: 379
epoch:: 380
epoch:: 381
epoch:: 382
epoch:: 383
epoch:: 384
epoch:: 385
epoch:: 386
epoch:: 387
epoch:: 388
epoch:: 389
epoch:: 390
epoch:: 391
epoch:: 392
epoch:: 393
epoch:: 394
epoch:: 395
epoch:: 396
epoch:: 397
epoch:: 398
epoch:: 399
epoch:: 400
epoch:: 401
epoch:: 402
epoch:: 403
epoch:: 404
epoch:: 405
epoch:: 406
epoch:: 407
epoch:: 408
epoch:: 409
epoch:: 410
epoch:: 411
epoch:: 412
epoch:: 413
epoch:: 414
epoch:: 415
epoch:: 416
epoch:: 417
epoch:: 418
epoch:: 419
epoch:: 420
epoch:: 421
epoch:: 422
epoch:: 423
epoch:: 424
epoch:: 425
epoch:: 426
epoch:: 427
epoch:: 428
epoch:: 429
epoch:: 430
epoch:: 431
epoch:: 432
epoch:: 433
epoch:: 434
epoch:: 435
epoch:: 436
epoch:: 437
epoch:: 438
epoch:: 439
epoch:: 440
epoch:: 441
epoch:: 442
epoch:: 443
epoch:: 444
epoch:: 445
epoch:: 446
epoch:: 447
epoch:: 448
epoch:: 449
epoch:: 450
epoch:: 451
epoch:: 452
epoch:: 453
epoch:: 454
epoch:: 455
epoch:: 456
epoch:: 457
epoch:: 458
epoch:: 459
epoch:: 460
epoch:: 461
epoch:: 462
epoch:: 463
epoch:: 464
epoch:: 465
epoch:: 466
epoch:: 467
epoch:: 468
epoch:: 469
epoch:: 470
epoch:: 471
epoch:: 472
epoch:: 473
epoch:: 474
epoch:: 475
epoch:: 476
epoch:: 477
epoch:: 478
epoch:: 479
epoch:: 480
epoch:: 481
epoch:: 482
epoch:: 483
epoch:: 484
epoch:: 485
epoch:: 486
epoch:: 487
epoch:: 488
epoch:: 489
epoch:: 490
epoch:: 491
epoch:: 492
epoch:: 493
epoch:: 494
epoch:: 495
epoch:: 496
epoch:: 497
epoch:: 498
epoch:: 499
training time is 78.03267979621887
overhead:: 0
overhead2:: 0.9889404773712158
overhead3:: 0
time_baseline:: 78.032710313797
curr_diff: 0 tensor(0.0090, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0090, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632700
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 2
max_epoch:: 500
overhead:: 0
overhead2:: 0.02837824821472168
overhead3:: 0.2532505989074707
overhead4:: 9.306884050369263
overhead5:: 0
memory usage:: 5623091200
time_provenance:: 15.752362251281738
curr_diff: 0 tensor(0.0021, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0021, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0105, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0105, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632700
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 2
max_epoch:: 500
overhead:: 0
overhead2:: 0.029781341552734375
overhead3:: 0.25528812408447266
overhead4:: 9.699146509170532
overhead5:: 0
memory usage:: 5626155008
time_provenance:: 16.17156195640564
curr_diff: 0 tensor(0.0021, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0021, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0105, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0105, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632700
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 2
max_epoch:: 500
overhead:: 0
overhead2:: 0.02974081039428711
overhead3:: 0.2650773525238037
overhead4:: 9.889003992080688
overhead5:: 0
memory usage:: 5647495168
time_provenance:: 16.35278296470642
curr_diff: 0 tensor(0.0021, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0021, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0105, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0105, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632700
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 2
max_epoch:: 500
overhead:: 0
overhead2:: 0.059781551361083984
overhead3:: 0.4062373638153076
overhead4:: 17.612019062042236
overhead5:: 0
memory usage:: 5631787008
time_provenance:: 26.034530878067017
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0091, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0091, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632700
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 2
max_epoch:: 500
overhead:: 0
overhead2:: 0.05439114570617676
overhead3:: 0.4414207935333252
overhead4:: 17.44281029701233
overhead5:: 0
memory usage:: 5625421824
time_provenance:: 25.899591207504272
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0091, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0091, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632700
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 2
max_epoch:: 500
overhead:: 0
overhead2:: 0.06184053421020508
overhead3:: 0.43817734718322754
overhead4:: 18.654293298721313
overhead5:: 0
memory usage:: 5627428864
time_provenance:: 27.157530784606934
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0091, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0091, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632700
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 2
max_epoch:: 500
overhead:: 0
overhead2:: 0.1359546184539795
overhead3:: 0.996842622756958
overhead4:: 41.276010513305664
overhead5:: 0
memory usage:: 5639905280
time_provenance:: 55.8577721118927
curr_diff: 0 tensor(2.8909e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.8909e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0090, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0090, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632700
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 2
max_epoch:: 500
overhead:: 0
overhead2:: 0.13578414916992188
overhead3:: 1.0330944061279297
overhead4:: 41.241661071777344
overhead5:: 0
memory usage:: 5627064320
time_provenance:: 55.7980694770813
curr_diff: 0 tensor(2.8963e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.8963e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0090, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0090, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632700
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 2
max_epoch:: 500
overhead:: 0
overhead2:: 0.14013886451721191
overhead3:: 1.0377497673034668
overhead4:: 42.69379425048828
overhead5:: 0
memory usage:: 5656543232
time_provenance:: 57.274449586868286
curr_diff: 0 tensor(2.8993e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.8993e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0090, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0090, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632700
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 2 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 2
max_epoch:: 500
overhead:: 0
overhead2:: 0.27788424491882324
overhead3:: 2.0417861938476562
overhead4:: 76.24087405204773
overhead5:: 0
memory usage:: 5629104128
time_provenance:: 86.51942920684814
curr_diff: 0 tensor(4.9942e-13, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.9942e-13, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0090, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0090, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632700
repetition 3
python3 benchmark_exp_lr.py 0.001 10000 500 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05] Logistic_regression cifar10_2 3 0.0001 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 2.310784
Train - Epoch 1, Batch: 0, Loss: 2.245709
Train - Epoch 2, Batch: 0, Loss: 2.190502
Train - Epoch 3, Batch: 0, Loss: 2.139442
Train - Epoch 4, Batch: 0, Loss: 2.092987
Train - Epoch 5, Batch: 0, Loss: 2.048568
Train - Epoch 6, Batch: 0, Loss: 2.006264
Train - Epoch 7, Batch: 0, Loss: 1.970507
Train - Epoch 8, Batch: 0, Loss: 1.933841
Train - Epoch 9, Batch: 0, Loss: 1.900135
Train - Epoch 10, Batch: 0, Loss: 1.869895
Train - Epoch 11, Batch: 0, Loss: 1.839246
Train - Epoch 12, Batch: 0, Loss: 1.811762
Train - Epoch 13, Batch: 0, Loss: 1.786910
Train - Epoch 14, Batch: 0, Loss: 1.752850
Train - Epoch 15, Batch: 0, Loss: 1.736589
Train - Epoch 16, Batch: 0, Loss: 1.717034
Train - Epoch 17, Batch: 0, Loss: 1.689876
Train - Epoch 18, Batch: 0, Loss: 1.673379
Train - Epoch 19, Batch: 0, Loss: 1.661187
Train - Epoch 20, Batch: 0, Loss: 1.642226
Train - Epoch 21, Batch: 0, Loss: 1.619547
Train - Epoch 22, Batch: 0, Loss: 1.615110
Train - Epoch 23, Batch: 0, Loss: 1.589923
Train - Epoch 24, Batch: 0, Loss: 1.578853
Train - Epoch 25, Batch: 0, Loss: 1.567698
Train - Epoch 26, Batch: 0, Loss: 1.542972
Train - Epoch 27, Batch: 0, Loss: 1.544822
Train - Epoch 28, Batch: 0, Loss: 1.529539
Train - Epoch 29, Batch: 0, Loss: 1.525621
Train - Epoch 30, Batch: 0, Loss: 1.507549
Train - Epoch 31, Batch: 0, Loss: 1.499997
Train - Epoch 32, Batch: 0, Loss: 1.488063
Train - Epoch 33, Batch: 0, Loss: 1.478426
Train - Epoch 34, Batch: 0, Loss: 1.483127
Train - Epoch 35, Batch: 0, Loss: 1.456977
Train - Epoch 36, Batch: 0, Loss: 1.459225
Train - Epoch 37, Batch: 0, Loss: 1.442478
Train - Epoch 38, Batch: 0, Loss: 1.439297
Train - Epoch 39, Batch: 0, Loss: 1.433079
Train - Epoch 40, Batch: 0, Loss: 1.417991
Train - Epoch 41, Batch: 0, Loss: 1.417920
Train - Epoch 42, Batch: 0, Loss: 1.400456
Train - Epoch 43, Batch: 0, Loss: 1.408330
Train - Epoch 44, Batch: 0, Loss: 1.405493
Train - Epoch 45, Batch: 0, Loss: 1.383810
Train - Epoch 46, Batch: 0, Loss: 1.380353
Train - Epoch 47, Batch: 0, Loss: 1.371261
Train - Epoch 48, Batch: 0, Loss: 1.375226
Train - Epoch 49, Batch: 0, Loss: 1.363753
Train - Epoch 50, Batch: 0, Loss: 1.356356
Train - Epoch 51, Batch: 0, Loss: 1.355415
Train - Epoch 52, Batch: 0, Loss: 1.349560
Train - Epoch 53, Batch: 0, Loss: 1.368649
Train - Epoch 54, Batch: 0, Loss: 1.338517
Train - Epoch 55, Batch: 0, Loss: 1.344188
Train - Epoch 56, Batch: 0, Loss: 1.335850
Train - Epoch 57, Batch: 0, Loss: 1.323825
Train - Epoch 58, Batch: 0, Loss: 1.323045
Train - Epoch 59, Batch: 0, Loss: 1.317579
Train - Epoch 60, Batch: 0, Loss: 1.312961
Train - Epoch 61, Batch: 0, Loss: 1.309038
Train - Epoch 62, Batch: 0, Loss: 1.314524
Train - Epoch 63, Batch: 0, Loss: 1.304741
Train - Epoch 64, Batch: 0, Loss: 1.301223
Train - Epoch 65, Batch: 0, Loss: 1.306727
Train - Epoch 66, Batch: 0, Loss: 1.290428
Train - Epoch 67, Batch: 0, Loss: 1.291907
Train - Epoch 68, Batch: 0, Loss: 1.286010
Train - Epoch 69, Batch: 0, Loss: 1.289198
Train - Epoch 70, Batch: 0, Loss: 1.288792
Train - Epoch 71, Batch: 0, Loss: 1.269907
Train - Epoch 72, Batch: 0, Loss: 1.268183
Train - Epoch 73, Batch: 0, Loss: 1.260176
Train - Epoch 74, Batch: 0, Loss: 1.268365
Train - Epoch 75, Batch: 0, Loss: 1.273567
Train - Epoch 76, Batch: 0, Loss: 1.271749
Train - Epoch 77, Batch: 0, Loss: 1.265603
Train - Epoch 78, Batch: 0, Loss: 1.265709
Train - Epoch 79, Batch: 0, Loss: 1.270241
Train - Epoch 80, Batch: 0, Loss: 1.251085
Train - Epoch 81, Batch: 0, Loss: 1.243338
Train - Epoch 82, Batch: 0, Loss: 1.252969
Train - Epoch 83, Batch: 0, Loss: 1.243434
Train - Epoch 84, Batch: 0, Loss: 1.243949
Train - Epoch 85, Batch: 0, Loss: 1.250555
Train - Epoch 86, Batch: 0, Loss: 1.254885
Train - Epoch 87, Batch: 0, Loss: 1.245559
Train - Epoch 88, Batch: 0, Loss: 1.228114
Train - Epoch 89, Batch: 0, Loss: 1.238043
Train - Epoch 90, Batch: 0, Loss: 1.230374
Train - Epoch 91, Batch: 0, Loss: 1.229277
Train - Epoch 92, Batch: 0, Loss: 1.240414
Train - Epoch 93, Batch: 0, Loss: 1.217746
Train - Epoch 94, Batch: 0, Loss: 1.224428
Train - Epoch 95, Batch: 0, Loss: 1.241256
Train - Epoch 96, Batch: 0, Loss: 1.232658
Train - Epoch 97, Batch: 0, Loss: 1.220145
Train - Epoch 98, Batch: 0, Loss: 1.222981
Train - Epoch 99, Batch: 0, Loss: 1.226431
Train - Epoch 100, Batch: 0, Loss: 1.207288
Train - Epoch 101, Batch: 0, Loss: 1.199208
Train - Epoch 102, Batch: 0, Loss: 1.214370
Train - Epoch 103, Batch: 0, Loss: 1.211501
Train - Epoch 104, Batch: 0, Loss: 1.202643
Train - Epoch 105, Batch: 0, Loss: 1.195684
Train - Epoch 106, Batch: 0, Loss: 1.213414
Train - Epoch 107, Batch: 0, Loss: 1.206356
Train - Epoch 108, Batch: 0, Loss: 1.203098
Train - Epoch 109, Batch: 0, Loss: 1.188596
Train - Epoch 110, Batch: 0, Loss: 1.197959
Train - Epoch 111, Batch: 0, Loss: 1.197737
Train - Epoch 112, Batch: 0, Loss: 1.187148
Train - Epoch 113, Batch: 0, Loss: 1.187619
Train - Epoch 114, Batch: 0, Loss: 1.193439
Train - Epoch 115, Batch: 0, Loss: 1.196511
Train - Epoch 116, Batch: 0, Loss: 1.182186
Train - Epoch 117, Batch: 0, Loss: 1.193181
Train - Epoch 118, Batch: 0, Loss: 1.187502
Train - Epoch 119, Batch: 0, Loss: 1.170827
Train - Epoch 120, Batch: 0, Loss: 1.187184
Train - Epoch 121, Batch: 0, Loss: 1.191808
Train - Epoch 122, Batch: 0, Loss: 1.195988
Train - Epoch 123, Batch: 0, Loss: 1.188653
Train - Epoch 124, Batch: 0, Loss: 1.172009
Train - Epoch 125, Batch: 0, Loss: 1.186497
Train - Epoch 126, Batch: 0, Loss: 1.185698
Train - Epoch 127, Batch: 0, Loss: 1.195394
Train - Epoch 128, Batch: 0, Loss: 1.185657
Train - Epoch 129, Batch: 0, Loss: 1.158290
Train - Epoch 130, Batch: 0, Loss: 1.162074
Train - Epoch 131, Batch: 0, Loss: 1.165456
Train - Epoch 132, Batch: 0, Loss: 1.178050
Train - Epoch 133, Batch: 0, Loss: 1.165581
Train - Epoch 134, Batch: 0, Loss: 1.157542
Train - Epoch 135, Batch: 0, Loss: 1.168996
Train - Epoch 136, Batch: 0, Loss: 1.160608
Train - Epoch 137, Batch: 0, Loss: 1.172981
Train - Epoch 138, Batch: 0, Loss: 1.159945
Train - Epoch 139, Batch: 0, Loss: 1.154270
Train - Epoch 140, Batch: 0, Loss: 1.168875
Train - Epoch 141, Batch: 0, Loss: 1.174281
Train - Epoch 142, Batch: 0, Loss: 1.153078
Train - Epoch 143, Batch: 0, Loss: 1.162715
Train - Epoch 144, Batch: 0, Loss: 1.154849
Train - Epoch 145, Batch: 0, Loss: 1.164496
Train - Epoch 146, Batch: 0, Loss: 1.163019
Train - Epoch 147, Batch: 0, Loss: 1.152773
Train - Epoch 148, Batch: 0, Loss: 1.162861
Train - Epoch 149, Batch: 0, Loss: 1.154847
Train - Epoch 150, Batch: 0, Loss: 1.151905
Train - Epoch 151, Batch: 0, Loss: 1.157618
Train - Epoch 152, Batch: 0, Loss: 1.142326
Train - Epoch 153, Batch: 0, Loss: 1.143874
Train - Epoch 154, Batch: 0, Loss: 1.156234
Train - Epoch 155, Batch: 0, Loss: 1.154985
Train - Epoch 156, Batch: 0, Loss: 1.153835
Train - Epoch 157, Batch: 0, Loss: 1.148700
Train - Epoch 158, Batch: 0, Loss: 1.140123
Train - Epoch 159, Batch: 0, Loss: 1.144377
Train - Epoch 160, Batch: 0, Loss: 1.143388
Train - Epoch 161, Batch: 0, Loss: 1.139180
Train - Epoch 162, Batch: 0, Loss: 1.139351
Train - Epoch 163, Batch: 0, Loss: 1.143824
Train - Epoch 164, Batch: 0, Loss: 1.155326
Train - Epoch 165, Batch: 0, Loss: 1.144993
Train - Epoch 166, Batch: 0, Loss: 1.148939
Train - Epoch 167, Batch: 0, Loss: 1.144227
Train - Epoch 168, Batch: 0, Loss: 1.144732
Train - Epoch 169, Batch: 0, Loss: 1.136450
Train - Epoch 170, Batch: 0, Loss: 1.147725
Train - Epoch 171, Batch: 0, Loss: 1.138721
Train - Epoch 172, Batch: 0, Loss: 1.129935
Train - Epoch 173, Batch: 0, Loss: 1.143063
Train - Epoch 174, Batch: 0, Loss: 1.132263
Train - Epoch 175, Batch: 0, Loss: 1.133263
Train - Epoch 176, Batch: 0, Loss: 1.126128
Train - Epoch 177, Batch: 0, Loss: 1.118147
Train - Epoch 178, Batch: 0, Loss: 1.126359
Train - Epoch 179, Batch: 0, Loss: 1.122447
Train - Epoch 180, Batch: 0, Loss: 1.131682
Train - Epoch 181, Batch: 0, Loss: 1.113001
Train - Epoch 182, Batch: 0, Loss: 1.118409
Train - Epoch 183, Batch: 0, Loss: 1.133046
Train - Epoch 184, Batch: 0, Loss: 1.139999
Train - Epoch 185, Batch: 0, Loss: 1.110346
Train - Epoch 186, Batch: 0, Loss: 1.122729
Train - Epoch 187, Batch: 0, Loss: 1.125573
Train - Epoch 188, Batch: 0, Loss: 1.115755
Train - Epoch 189, Batch: 0, Loss: 1.134052
Train - Epoch 190, Batch: 0, Loss: 1.123527
Train - Epoch 191, Batch: 0, Loss: 1.123058
Train - Epoch 192, Batch: 0, Loss: 1.125328
Train - Epoch 193, Batch: 0, Loss: 1.126530
Train - Epoch 194, Batch: 0, Loss: 1.125948
Train - Epoch 195, Batch: 0, Loss: 1.126085
Train - Epoch 196, Batch: 0, Loss: 1.124865
Train - Epoch 197, Batch: 0, Loss: 1.113787
Train - Epoch 198, Batch: 0, Loss: 1.124124
Train - Epoch 199, Batch: 0, Loss: 1.118636
Train - Epoch 200, Batch: 0, Loss: 1.112789
Train - Epoch 201, Batch: 0, Loss: 1.111819
Train - Epoch 202, Batch: 0, Loss: 1.117065
Train - Epoch 203, Batch: 0, Loss: 1.129830
Train - Epoch 204, Batch: 0, Loss: 1.126016
Train - Epoch 205, Batch: 0, Loss: 1.107801
Train - Epoch 206, Batch: 0, Loss: 1.128965
Train - Epoch 207, Batch: 0, Loss: 1.116774
Train - Epoch 208, Batch: 0, Loss: 1.118167
Train - Epoch 209, Batch: 0, Loss: 1.110666
Train - Epoch 210, Batch: 0, Loss: 1.114866
Train - Epoch 211, Batch: 0, Loss: 1.107950
Train - Epoch 212, Batch: 0, Loss: 1.123069
Train - Epoch 213, Batch: 0, Loss: 1.109513
Train - Epoch 214, Batch: 0, Loss: 1.111767
Train - Epoch 215, Batch: 0, Loss: 1.122625
Train - Epoch 216, Batch: 0, Loss: 1.111039
Train - Epoch 217, Batch: 0, Loss: 1.105669
Train - Epoch 218, Batch: 0, Loss: 1.113254
Train - Epoch 219, Batch: 0, Loss: 1.113778
Train - Epoch 220, Batch: 0, Loss: 1.110813
Train - Epoch 221, Batch: 0, Loss: 1.105147
Train - Epoch 222, Batch: 0, Loss: 1.096271
Train - Epoch 223, Batch: 0, Loss: 1.108752
Train - Epoch 224, Batch: 0, Loss: 1.100839
Train - Epoch 225, Batch: 0, Loss: 1.109814
Train - Epoch 226, Batch: 0, Loss: 1.109328
Train - Epoch 227, Batch: 0, Loss: 1.111032
Train - Epoch 228, Batch: 0, Loss: 1.097947
Train - Epoch 229, Batch: 0, Loss: 1.111498
Train - Epoch 230, Batch: 0, Loss: 1.090707
Train - Epoch 231, Batch: 0, Loss: 1.090104
Train - Epoch 232, Batch: 0, Loss: 1.096712
Train - Epoch 233, Batch: 0, Loss: 1.097246
Train - Epoch 234, Batch: 0, Loss: 1.090062
Train - Epoch 235, Batch: 0, Loss: 1.098148
Train - Epoch 236, Batch: 0, Loss: 1.098926
Train - Epoch 237, Batch: 0, Loss: 1.105546
Train - Epoch 238, Batch: 0, Loss: 1.097905
Train - Epoch 239, Batch: 0, Loss: 1.096991
Train - Epoch 240, Batch: 0, Loss: 1.111041
Train - Epoch 241, Batch: 0, Loss: 1.099783
Train - Epoch 242, Batch: 0, Loss: 1.094319
Train - Epoch 243, Batch: 0, Loss: 1.115763
Train - Epoch 244, Batch: 0, Loss: 1.096224
Train - Epoch 245, Batch: 0, Loss: 1.086725
Train - Epoch 246, Batch: 0, Loss: 1.093434
Train - Epoch 247, Batch: 0, Loss: 1.090468
Train - Epoch 248, Batch: 0, Loss: 1.089091
Train - Epoch 249, Batch: 0, Loss: 1.083917
Train - Epoch 250, Batch: 0, Loss: 1.085099
Train - Epoch 251, Batch: 0, Loss: 1.106580
Train - Epoch 252, Batch: 0, Loss: 1.085636
Train - Epoch 253, Batch: 0, Loss: 1.100387
Train - Epoch 254, Batch: 0, Loss: 1.092606
Train - Epoch 255, Batch: 0, Loss: 1.106891
Train - Epoch 256, Batch: 0, Loss: 1.086020
Train - Epoch 257, Batch: 0, Loss: 1.085161
Train - Epoch 258, Batch: 0, Loss: 1.080766
Train - Epoch 259, Batch: 0, Loss: 1.082117
Train - Epoch 260, Batch: 0, Loss: 1.093135
Train - Epoch 261, Batch: 0, Loss: 1.086082
Train - Epoch 262, Batch: 0, Loss: 1.088279
Train - Epoch 263, Batch: 0, Loss: 1.084751
Train - Epoch 264, Batch: 0, Loss: 1.072400
Train - Epoch 265, Batch: 0, Loss: 1.081796
Train - Epoch 266, Batch: 0, Loss: 1.083849
Train - Epoch 267, Batch: 0, Loss: 1.090729
Train - Epoch 268, Batch: 0, Loss: 1.087424
Train - Epoch 269, Batch: 0, Loss: 1.080099
Train - Epoch 270, Batch: 0, Loss: 1.074319
Train - Epoch 271, Batch: 0, Loss: 1.087297
Train - Epoch 272, Batch: 0, Loss: 1.088862
Train - Epoch 273, Batch: 0, Loss: 1.092132
Train - Epoch 274, Batch: 0, Loss: 1.093701
Train - Epoch 275, Batch: 0, Loss: 1.079355
Train - Epoch 276, Batch: 0, Loss: 1.079269
Train - Epoch 277, Batch: 0, Loss: 1.080589
Train - Epoch 278, Batch: 0, Loss: 1.066417
Train - Epoch 279, Batch: 0, Loss: 1.092115
Train - Epoch 280, Batch: 0, Loss: 1.080656
Train - Epoch 281, Batch: 0, Loss: 1.091148
Train - Epoch 282, Batch: 0, Loss: 1.086821
Train - Epoch 283, Batch: 0, Loss: 1.104260
Train - Epoch 284, Batch: 0, Loss: 1.084715
Train - Epoch 285, Batch: 0, Loss: 1.071638
Train - Epoch 286, Batch: 0, Loss: 1.090739
Train - Epoch 287, Batch: 0, Loss: 1.089303
Train - Epoch 288, Batch: 0, Loss: 1.074332
Train - Epoch 289, Batch: 0, Loss: 1.086877
Train - Epoch 290, Batch: 0, Loss: 1.078391
Train - Epoch 291, Batch: 0, Loss: 1.082658
Train - Epoch 292, Batch: 0, Loss: 1.061008
Train - Epoch 293, Batch: 0, Loss: 1.083053
Train - Epoch 294, Batch: 0, Loss: 1.074979
Train - Epoch 295, Batch: 0, Loss: 1.078066
Train - Epoch 296, Batch: 0, Loss: 1.079389
Train - Epoch 297, Batch: 0, Loss: 1.075653
Train - Epoch 298, Batch: 0, Loss: 1.068290
Train - Epoch 299, Batch: 0, Loss: 1.072193
Train - Epoch 300, Batch: 0, Loss: 1.077182
Train - Epoch 301, Batch: 0, Loss: 1.091313
Train - Epoch 302, Batch: 0, Loss: 1.080058
Train - Epoch 303, Batch: 0, Loss: 1.077566
Train - Epoch 304, Batch: 0, Loss: 1.081352
Train - Epoch 305, Batch: 0, Loss: 1.066879
Train - Epoch 306, Batch: 0, Loss: 1.071911
Train - Epoch 307, Batch: 0, Loss: 1.081398
Train - Epoch 308, Batch: 0, Loss: 1.079290
Train - Epoch 309, Batch: 0, Loss: 1.062235
Train - Epoch 310, Batch: 0, Loss: 1.075599
Train - Epoch 311, Batch: 0, Loss: 1.062331
Train - Epoch 312, Batch: 0, Loss: 1.062869
Train - Epoch 313, Batch: 0, Loss: 1.066558
Train - Epoch 314, Batch: 0, Loss: 1.080241
Train - Epoch 315, Batch: 0, Loss: 1.080776
Train - Epoch 316, Batch: 0, Loss: 1.071529
Train - Epoch 317, Batch: 0, Loss: 1.076051
Train - Epoch 318, Batch: 0, Loss: 1.084691
Train - Epoch 319, Batch: 0, Loss: 1.077710
Train - Epoch 320, Batch: 0, Loss: 1.066454
Train - Epoch 321, Batch: 0, Loss: 1.083451
Train - Epoch 322, Batch: 0, Loss: 1.057890
Train - Epoch 323, Batch: 0, Loss: 1.072194
Train - Epoch 324, Batch: 0, Loss: 1.071611
Train - Epoch 325, Batch: 0, Loss: 1.073950
Train - Epoch 326, Batch: 0, Loss: 1.053942
Train - Epoch 327, Batch: 0, Loss: 1.064313
Train - Epoch 328, Batch: 0, Loss: 1.067663
Train - Epoch 329, Batch: 0, Loss: 1.062109
Train - Epoch 330, Batch: 0, Loss: 1.069477
Train - Epoch 331, Batch: 0, Loss: 1.081348
Train - Epoch 332, Batch: 0, Loss: 1.064717
Train - Epoch 333, Batch: 0, Loss: 1.081475
Train - Epoch 334, Batch: 0, Loss: 1.087871
Train - Epoch 335, Batch: 0, Loss: 1.086720
Train - Epoch 336, Batch: 0, Loss: 1.082885
Train - Epoch 337, Batch: 0, Loss: 1.069857
Train - Epoch 338, Batch: 0, Loss: 1.071921
Train - Epoch 339, Batch: 0, Loss: 1.056170
Train - Epoch 340, Batch: 0, Loss: 1.080300
Train - Epoch 341, Batch: 0, Loss: 1.068089
Train - Epoch 342, Batch: 0, Loss: 1.058577
Train - Epoch 343, Batch: 0, Loss: 1.061049
Train - Epoch 344, Batch: 0, Loss: 1.059444
Train - Epoch 345, Batch: 0, Loss: 1.071377
Train - Epoch 346, Batch: 0, Loss: 1.071326
Train - Epoch 347, Batch: 0, Loss: 1.063230
Train - Epoch 348, Batch: 0, Loss: 1.072681
Train - Epoch 349, Batch: 0, Loss: 1.069952
Train - Epoch 350, Batch: 0, Loss: 1.060551
Train - Epoch 351, Batch: 0, Loss: 1.070338
Train - Epoch 352, Batch: 0, Loss: 1.055266
Train - Epoch 353, Batch: 0, Loss: 1.061506
Train - Epoch 354, Batch: 0, Loss: 1.067547
Train - Epoch 355, Batch: 0, Loss: 1.058875
Train - Epoch 356, Batch: 0, Loss: 1.047756
Train - Epoch 357, Batch: 0, Loss: 1.062443
Train - Epoch 358, Batch: 0, Loss: 1.068309
Train - Epoch 359, Batch: 0, Loss: 1.067531
Train - Epoch 360, Batch: 0, Loss: 1.061007
Train - Epoch 361, Batch: 0, Loss: 1.056114
Train - Epoch 362, Batch: 0, Loss: 1.069582
Train - Epoch 363, Batch: 0, Loss: 1.062340
Train - Epoch 364, Batch: 0, Loss: 1.078345
Train - Epoch 365, Batch: 0, Loss: 1.061441
Train - Epoch 366, Batch: 0, Loss: 1.067450
Train - Epoch 367, Batch: 0, Loss: 1.069032
Train - Epoch 368, Batch: 0, Loss: 1.061420
Train - Epoch 369, Batch: 0, Loss: 1.074028
Train - Epoch 370, Batch: 0, Loss: 1.073582
Train - Epoch 371, Batch: 0, Loss: 1.059905
Train - Epoch 372, Batch: 0, Loss: 1.057775
Train - Epoch 373, Batch: 0, Loss: 1.060523/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 374, Batch: 0, Loss: 1.057263
Train - Epoch 375, Batch: 0, Loss: 1.056561
Train - Epoch 376, Batch: 0, Loss: 1.063289
Train - Epoch 377, Batch: 0, Loss: 1.067543
Train - Epoch 378, Batch: 0, Loss: 1.061505
Train - Epoch 379, Batch: 0, Loss: 1.045347
Train - Epoch 380, Batch: 0, Loss: 1.041127
Train - Epoch 381, Batch: 0, Loss: 1.056925
Train - Epoch 382, Batch: 0, Loss: 1.057102
Train - Epoch 383, Batch: 0, Loss: 1.057781
Train - Epoch 384, Batch: 0, Loss: 1.060781
Train - Epoch 385, Batch: 0, Loss: 1.054614
Train - Epoch 386, Batch: 0, Loss: 1.058479
Train - Epoch 387, Batch: 0, Loss: 1.061439
Train - Epoch 388, Batch: 0, Loss: 1.054182
Train - Epoch 389, Batch: 0, Loss: 1.071132
Train - Epoch 390, Batch: 0, Loss: 1.051287
Train - Epoch 391, Batch: 0, Loss: 1.058995
Train - Epoch 392, Batch: 0, Loss: 1.057562
Train - Epoch 393, Batch: 0, Loss: 1.057694
Train - Epoch 394, Batch: 0, Loss: 1.063825
Train - Epoch 395, Batch: 0, Loss: 1.062827
Train - Epoch 396, Batch: 0, Loss: 1.047306
Train - Epoch 397, Batch: 0, Loss: 1.072806
Train - Epoch 398, Batch: 0, Loss: 1.051135
Train - Epoch 399, Batch: 0, Loss: 1.067477
Train - Epoch 400, Batch: 0, Loss: 1.066210
Train - Epoch 401, Batch: 0, Loss: 1.042672
Train - Epoch 402, Batch: 0, Loss: 1.043062
Train - Epoch 403, Batch: 0, Loss: 1.063751
Train - Epoch 404, Batch: 0, Loss: 1.071976
Train - Epoch 405, Batch: 0, Loss: 1.046852
Train - Epoch 406, Batch: 0, Loss: 1.053361
Train - Epoch 407, Batch: 0, Loss: 1.055216
Train - Epoch 408, Batch: 0, Loss: 1.051569
Train - Epoch 409, Batch: 0, Loss: 1.055343
Train - Epoch 410, Batch: 0, Loss: 1.039617
Train - Epoch 411, Batch: 0, Loss: 1.062131
Train - Epoch 412, Batch: 0, Loss: 1.050800
Train - Epoch 413, Batch: 0, Loss: 1.050648
Train - Epoch 414, Batch: 0, Loss: 1.058520
Train - Epoch 415, Batch: 0, Loss: 1.043203
Train - Epoch 416, Batch: 0, Loss: 1.042888
Train - Epoch 417, Batch: 0, Loss: 1.049375
Train - Epoch 418, Batch: 0, Loss: 1.050945
Train - Epoch 419, Batch: 0, Loss: 1.040308
Train - Epoch 420, Batch: 0, Loss: 1.049040
Train - Epoch 421, Batch: 0, Loss: 1.054217
Train - Epoch 422, Batch: 0, Loss: 1.055947
Train - Epoch 423, Batch: 0, Loss: 1.041449
Train - Epoch 424, Batch: 0, Loss: 1.047184
Train - Epoch 425, Batch: 0, Loss: 1.046673
Train - Epoch 426, Batch: 0, Loss: 1.033031
Train - Epoch 427, Batch: 0, Loss: 1.041694
Train - Epoch 428, Batch: 0, Loss: 1.043501
Train - Epoch 429, Batch: 0, Loss: 1.048604
Train - Epoch 430, Batch: 0, Loss: 1.051961
Train - Epoch 431, Batch: 0, Loss: 1.053735
Train - Epoch 432, Batch: 0, Loss: 1.061738
Train - Epoch 433, Batch: 0, Loss: 1.055975
Train - Epoch 434, Batch: 0, Loss: 1.046347
Train - Epoch 435, Batch: 0, Loss: 1.062616
Train - Epoch 436, Batch: 0, Loss: 1.038992
Train - Epoch 437, Batch: 0, Loss: 1.050675
Train - Epoch 438, Batch: 0, Loss: 1.050442
Train - Epoch 439, Batch: 0, Loss: 1.037090
Train - Epoch 440, Batch: 0, Loss: 1.034429
Train - Epoch 441, Batch: 0, Loss: 1.057319
Train - Epoch 442, Batch: 0, Loss: 1.072324
Train - Epoch 443, Batch: 0, Loss: 1.033524
Train - Epoch 444, Batch: 0, Loss: 1.058050
Train - Epoch 445, Batch: 0, Loss: 1.041857
Train - Epoch 446, Batch: 0, Loss: 1.040248
Train - Epoch 447, Batch: 0, Loss: 1.046400
Train - Epoch 448, Batch: 0, Loss: 1.057485
Train - Epoch 449, Batch: 0, Loss: 1.037240
Train - Epoch 450, Batch: 0, Loss: 1.036665
Train - Epoch 451, Batch: 0, Loss: 1.049364
Train - Epoch 452, Batch: 0, Loss: 1.046923
Train - Epoch 453, Batch: 0, Loss: 1.035401
Train - Epoch 454, Batch: 0, Loss: 1.054032
Train - Epoch 455, Batch: 0, Loss: 1.047668
Train - Epoch 456, Batch: 0, Loss: 1.042188
Train - Epoch 457, Batch: 0, Loss: 1.056861
Train - Epoch 458, Batch: 0, Loss: 1.044019
Train - Epoch 459, Batch: 0, Loss: 1.045941
Train - Epoch 460, Batch: 0, Loss: 1.049157
Train - Epoch 461, Batch: 0, Loss: 1.047861
Train - Epoch 462, Batch: 0, Loss: 1.045155
Train - Epoch 463, Batch: 0, Loss: 1.037001
Train - Epoch 464, Batch: 0, Loss: 1.041160
Train - Epoch 465, Batch: 0, Loss: 1.047292
Train - Epoch 466, Batch: 0, Loss: 1.050893
Train - Epoch 467, Batch: 0, Loss: 1.048679
Train - Epoch 468, Batch: 0, Loss: 1.059109
Train - Epoch 469, Batch: 0, Loss: 1.050294
Train - Epoch 470, Batch: 0, Loss: 1.035941
Train - Epoch 471, Batch: 0, Loss: 1.040827
Train - Epoch 472, Batch: 0, Loss: 1.041204
Train - Epoch 473, Batch: 0, Loss: 1.030489
Train - Epoch 474, Batch: 0, Loss: 1.041011
Train - Epoch 475, Batch: 0, Loss: 1.044007
Train - Epoch 476, Batch: 0, Loss: 1.045253
Train - Epoch 477, Batch: 0, Loss: 1.038179
Train - Epoch 478, Batch: 0, Loss: 1.024815
Train - Epoch 479, Batch: 0, Loss: 1.032451
Train - Epoch 480, Batch: 0, Loss: 1.035067
Train - Epoch 481, Batch: 0, Loss: 1.037483
Train - Epoch 482, Batch: 0, Loss: 1.059712
Train - Epoch 483, Batch: 0, Loss: 1.038104
Train - Epoch 484, Batch: 0, Loss: 1.036752
Train - Epoch 485, Batch: 0, Loss: 1.037413
Train - Epoch 486, Batch: 0, Loss: 1.032167
Train - Epoch 487, Batch: 0, Loss: 1.032576
Train - Epoch 488, Batch: 0, Loss: 1.021110
Train - Epoch 489, Batch: 0, Loss: 1.036350
Train - Epoch 490, Batch: 0, Loss: 1.041450
Train - Epoch 491, Batch: 0, Loss: 1.045970
Train - Epoch 492, Batch: 0, Loss: 1.036380
Train - Epoch 493, Batch: 0, Loss: 1.043405
Train - Epoch 494, Batch: 0, Loss: 1.042068
Train - Epoch 495, Batch: 0, Loss: 1.049226
Train - Epoch 496, Batch: 0, Loss: 1.033986
Train - Epoch 497, Batch: 0, Loss: 1.040424
Train - Epoch 498, Batch: 0, Loss: 1.039831
Train - Epoch 499, Batch: 0, Loss: 1.049242
training_time:: 107.19683718681335
training time full:: 107.19690656661987
provenance prepare time:: 0.0
here
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.631900
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 2
max_epoch:: 500
batch_size:: 10000
epoch:: 0
tensor([32768,     4, 32774,    10, 32780,    13,    15, 32784,    20,    28,
        32798,    31,    45, 32816,    50, 32820,    54, 32831,    65, 32834,
        32836,    69,    75, 32847,    81,    82, 32849,    84,    85, 32853,
        32855, 32861, 32864,    97, 32865, 32866, 32869,   110,   111, 32881,
        32882, 32883,   118,   127,   129, 32899, 32901,   136, 32909, 32914])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
epoch:: 60
epoch:: 61
epoch:: 62
epoch:: 63
epoch:: 64
epoch:: 65
epoch:: 66
epoch:: 67
epoch:: 68
epoch:: 69
epoch:: 70
epoch:: 71
epoch:: 72
epoch:: 73
epoch:: 74
epoch:: 75
epoch:: 76
epoch:: 77
epoch:: 78
epoch:: 79
epoch:: 80
epoch:: 81
epoch:: 82
epoch:: 83
epoch:: 84
epoch:: 85
epoch:: 86
epoch:: 87
epoch:: 88
epoch:: 89
epoch:: 90
epoch:: 91
epoch:: 92
epoch:: 93
epoch:: 94
epoch:: 95
epoch:: 96
epoch:: 97
epoch:: 98
epoch:: 99
epoch:: 100
epoch:: 101
epoch:: 102
epoch:: 103
epoch:: 104
epoch:: 105
epoch:: 106
epoch:: 107
epoch:: 108
epoch:: 109
epoch:: 110
epoch:: 111
epoch:: 112
epoch:: 113
epoch:: 114
epoch:: 115
epoch:: 116
epoch:: 117
epoch:: 118
epoch:: 119
epoch:: 120
epoch:: 121
epoch:: 122
epoch:: 123
epoch:: 124
epoch:: 125
epoch:: 126
epoch:: 127
epoch:: 128
epoch:: 129
epoch:: 130
epoch:: 131
epoch:: 132
epoch:: 133
epoch:: 134
epoch:: 135
epoch:: 136
epoch:: 137
epoch:: 138
epoch:: 139
epoch:: 140
epoch:: 141
epoch:: 142
epoch:: 143
epoch:: 144
epoch:: 145
epoch:: 146
epoch:: 147
epoch:: 148
epoch:: 149
epoch:: 150
epoch:: 151
epoch:: 152
epoch:: 153
epoch:: 154
epoch:: 155
epoch:: 156
epoch:: 157
epoch:: 158
epoch:: 159
epoch:: 160
epoch:: 161
epoch:: 162
epoch:: 163
epoch:: 164
epoch:: 165
epoch:: 166
epoch:: 167
epoch:: 168
epoch:: 169
epoch:: 170
epoch:: 171
epoch:: 172
epoch:: 173
epoch:: 174
epoch:: 175
epoch:: 176
epoch:: 177
epoch:: 178
epoch:: 179
epoch:: 180
epoch:: 181
epoch:: 182
epoch:: 183
epoch:: 184
epoch:: 185
epoch:: 186
epoch:: 187
epoch:: 188
epoch:: 189
epoch:: 190
epoch:: 191
epoch:: 192
epoch:: 193
epoch:: 194
epoch:: 195
epoch:: 196
epoch:: 197
epoch:: 198
epoch:: 199
epoch:: 200
epoch:: 201
epoch:: 202
epoch:: 203
epoch:: 204
epoch:: 205
epoch:: 206
epoch:: 207
epoch:: 208
epoch:: 209
epoch:: 210
epoch:: 211
epoch:: 212
epoch:: 213
epoch:: 214
epoch:: 215
epoch:: 216
epoch:: 217
epoch:: 218
epoch:: 219
epoch:: 220
epoch:: 221
epoch:: 222
epoch:: 223
epoch:: 224
epoch:: 225
epoch:: 226
epoch:: 227
epoch:: 228
epoch:: 229
epoch:: 230
epoch:: 231
epoch:: 232
epoch:: 233
epoch:: 234
epoch:: 235
epoch:: 236
epoch:: 237
epoch:: 238
epoch:: 239
epoch:: 240
epoch:: 241
epoch:: 242
epoch:: 243
epoch:: 244
epoch:: 245
epoch:: 246
epoch:: 247
epoch:: 248
epoch:: 249
epoch:: 250
epoch:: 251
epoch:: 252
epoch:: 253
epoch:: 254
epoch:: 255
epoch:: 256
epoch:: 257
epoch:: 258
epoch:: 259
epoch:: 260
epoch:: 261
epoch:: 262
epoch:: 263
epoch:: 264
epoch:: 265
epoch:: 266
epoch:: 267
epoch:: 268
epoch:: 269
epoch:: 270
epoch:: 271
epoch:: 272
epoch:: 273
epoch:: 274
epoch:: 275
epoch:: 276
epoch:: 277
epoch:: 278
epoch:: 279
epoch:: 280
epoch:: 281
epoch:: 282
epoch:: 283
epoch:: 284
epoch:: 285
epoch:: 286
epoch:: 287
epoch:: 288
epoch:: 289
epoch:: 290
epoch:: 291
epoch:: 292
epoch:: 293
epoch:: 294
epoch:: 295
epoch:: 296
epoch:: 297
epoch:: 298
epoch:: 299
epoch:: 300
epoch:: 301
epoch:: 302
epoch:: 303
epoch:: 304
epoch:: 305
epoch:: 306
epoch:: 307
epoch:: 308
epoch:: 309
epoch:: 310
epoch:: 311
epoch:: 312
epoch:: 313
epoch:: 314
epoch:: 315
epoch:: 316
epoch:: 317
epoch:: 318
epoch:: 319
epoch:: 320
epoch:: 321
epoch:: 322
epoch:: 323
epoch:: 324
epoch:: 325
epoch:: 326
epoch:: 327
epoch:: 328
epoch:: 329
epoch:: 330
epoch:: 331
epoch:: 332
epoch:: 333
epoch:: 334
epoch:: 335
epoch:: 336
epoch:: 337
epoch:: 338
epoch:: 339
epoch:: 340
epoch:: 341
epoch:: 342
epoch:: 343
epoch:: 344
epoch:: 345
epoch:: 346
epoch:: 347
epoch:: 348
epoch:: 349
epoch:: 350
epoch:: 351
epoch:: 352
epoch:: 353
epoch:: 354
epoch:: 355
epoch:: 356
epoch:: 357
epoch:: 358
epoch:: 359
epoch:: 360
epoch:: 361
epoch:: 362
epoch:: 363
epoch:: 364
epoch:: 365
epoch:: 366
epoch:: 367
epoch:: 368
epoch:: 369
epoch:: 370
epoch:: 371
epoch:: 372
epoch:: 373
epoch:: 374
epoch:: 375
epoch:: 376
epoch:: 377
epoch:: 378
epoch:: 379
epoch:: 380
epoch:: 381
epoch:: 382
epoch:: 383
epoch:: 384
epoch:: 385
epoch:: 386
epoch:: 387
epoch:: 388
epoch:: 389
epoch:: 390
epoch:: 391
epoch:: 392
epoch:: 393
epoch:: 394
epoch:: 395
epoch:: 396
epoch:: 397
epoch:: 398
epoch:: 399
epoch:: 400
epoch:: 401
epoch:: 402
epoch:: 403
epoch:: 404
epoch:: 405
epoch:: 406
epoch:: 407
epoch:: 408
epoch:: 409
epoch:: 410
epoch:: 411
epoch:: 412
epoch:: 413
epoch:: 414
epoch:: 415
epoch:: 416
epoch:: 417
epoch:: 418
epoch:: 419
epoch:: 420
epoch:: 421
epoch:: 422
epoch:: 423
epoch:: 424
epoch:: 425
epoch:: 426
epoch:: 427
epoch:: 428
epoch:: 429
epoch:: 430
epoch:: 431
epoch:: 432
epoch:: 433
epoch:: 434
epoch:: 435
epoch:: 436
epoch:: 437
epoch:: 438
epoch:: 439
epoch:: 440
epoch:: 441
epoch:: 442
epoch:: 443
epoch:: 444
epoch:: 445
epoch:: 446
epoch:: 447
epoch:: 448
epoch:: 449
epoch:: 450
epoch:: 451
epoch:: 452
epoch:: 453
epoch:: 454
epoch:: 455
epoch:: 456
epoch:: 457
epoch:: 458
epoch:: 459
epoch:: 460
epoch:: 461
epoch:: 462
epoch:: 463
epoch:: 464
epoch:: 465
epoch:: 466
epoch:: 467
epoch:: 468
epoch:: 469
epoch:: 470
epoch:: 471
epoch:: 472
epoch:: 473
epoch:: 474
epoch:: 475
epoch:: 476
epoch:: 477
epoch:: 478
epoch:: 479
epoch:: 480
epoch:: 481
epoch:: 482
epoch:: 483
epoch:: 484
epoch:: 485
epoch:: 486
epoch:: 487
epoch:: 488
epoch:: 489
epoch:: 490
epoch:: 491
epoch:: 492
epoch:: 493
epoch:: 494
epoch:: 495
epoch:: 496
epoch:: 497
epoch:: 498
epoch:: 499
training time is 78.07138991355896
overhead:: 0
overhead2:: 0.9926693439483643
overhead3:: 0
time_baseline:: 78.07142066955566
curr_diff: 0 tensor(0.0090, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0090, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632100
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 2
max_epoch:: 500
overhead:: 0
overhead2:: 0.030400753021240234
overhead3:: 0.248246431350708
overhead4:: 9.561111688613892
overhead5:: 0
memory usage:: 5643931648
time_provenance:: 15.975426197052002
curr_diff: 0 tensor(0.0021, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0021, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0105, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0105, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632100
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 2
max_epoch:: 500
overhead:: 0
overhead2:: 0.031110525131225586
overhead3:: 0.25531697273254395
overhead4:: 9.80569839477539
overhead5:: 0
memory usage:: 5644034048
time_provenance:: 16.225658178329468
curr_diff: 0 tensor(0.0021, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0021, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0105, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0105, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632100
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 2
max_epoch:: 500
overhead:: 0
overhead2:: 0.0328981876373291
overhead3:: 0.26189470291137695
overhead4:: 9.96368408203125
overhead5:: 0
memory usage:: 5623783424
time_provenance:: 16.42015314102173
curr_diff: 0 tensor(0.0021, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0021, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0105, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0105, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632100
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 2
max_epoch:: 500
overhead:: 0
overhead2:: 0.05957937240600586
overhead3:: 0.44161200523376465
overhead4:: 17.149102210998535
overhead5:: 0
memory usage:: 5630447616
time_provenance:: 25.605635166168213
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0091, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0091, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632100
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 2
max_epoch:: 500
overhead:: 0
overhead2:: 0.060450077056884766
overhead3:: 0.45212626457214355
overhead4:: 17.623356103897095
overhead5:: 0
memory usage:: 5630681088
time_provenance:: 26.08909296989441
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0091, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0091, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632100
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 2
max_epoch:: 500
overhead:: 0
overhead2:: 0.06109976768493652
overhead3:: 0.4531097412109375
overhead4:: 17.70850896835327
overhead5:: 0
memory usage:: 5627953152
time_provenance:: 26.18461561203003
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0091, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0091, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632100
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 2
max_epoch:: 500
overhead:: 0
overhead2:: 0.14830780029296875
overhead3:: 0.9407002925872803
overhead4:: 41.90776801109314
overhead5:: 0
memory usage:: 5668687872
time_provenance:: 56.48343753814697
curr_diff: 0 tensor(3.0391e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.0391e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0090, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0090, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632100
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 2
max_epoch:: 500
overhead:: 0
overhead2:: 0.12984061241149902
overhead3:: 1.0392892360687256
overhead4:: 41.39594578742981
overhead5:: 0
memory usage:: 5623521280
time_provenance:: 55.96703052520752
curr_diff: 0 tensor(3.0450e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.0450e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0090, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0090, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632100
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 2
max_epoch:: 500
overhead:: 0
overhead2:: 0.14012908935546875
overhead3:: 1.0438745021820068
overhead4:: 42.332534313201904
overhead5:: 0
memory usage:: 5706387456
time_provenance:: 56.93572235107422
curr_diff: 0 tensor(3.0496e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.0496e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0090, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0090, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632100
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 3 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 2
max_epoch:: 500
overhead:: 0
overhead2:: 0.2820765972137451
overhead3:: 2.080629587173462
overhead4:: 76.76181745529175
overhead5:: 0
memory usage:: 5624172544
time_provenance:: 87.00844430923462
curr_diff: 0 tensor(5.0157e-13, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.0157e-13, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0090, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0090, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632100
repetition 4
python3 benchmark_exp_lr.py 0.001 10000 500 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05] Logistic_regression cifar10_2 4 0.0001 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 2.305029
Train - Epoch 1, Batch: 0, Loss: 2.245756
Train - Epoch 2, Batch: 0, Loss: 2.191309
Train - Epoch 3, Batch: 0, Loss: 2.141398
Train - Epoch 4, Batch: 0, Loss: 2.091516
Train - Epoch 5, Batch: 0, Loss: 2.048362
Train - Epoch 6, Batch: 0, Loss: 2.007927
Train - Epoch 7, Batch: 0, Loss: 1.970032
Train - Epoch 8, Batch: 0, Loss: 1.932829
Train - Epoch 9, Batch: 0, Loss: 1.897103
Train - Epoch 10, Batch: 0, Loss: 1.860353
Train - Epoch 11, Batch: 0, Loss: 1.837241
Train - Epoch 12, Batch: 0, Loss: 1.804835
Train - Epoch 13, Batch: 0, Loss: 1.784642
Train - Epoch 14, Batch: 0, Loss: 1.764185
Train - Epoch 15, Batch: 0, Loss: 1.735216
Train - Epoch 16, Batch: 0, Loss: 1.709073
Train - Epoch 17, Batch: 0, Loss: 1.697048
Train - Epoch 18, Batch: 0, Loss: 1.679666
Train - Epoch 19, Batch: 0, Loss: 1.657343
Train - Epoch 20, Batch: 0, Loss: 1.645675
Train - Epoch 21, Batch: 0, Loss: 1.620612
Train - Epoch 22, Batch: 0, Loss: 1.605061
Train - Epoch 23, Batch: 0, Loss: 1.591725
Train - Epoch 24, Batch: 0, Loss: 1.576459
Train - Epoch 25, Batch: 0, Loss: 1.571982
Train - Epoch 26, Batch: 0, Loss: 1.552322
Train - Epoch 27, Batch: 0, Loss: 1.537385
Train - Epoch 28, Batch: 0, Loss: 1.524243
Train - Epoch 29, Batch: 0, Loss: 1.520500
Train - Epoch 30, Batch: 0, Loss: 1.502350
Train - Epoch 31, Batch: 0, Loss: 1.496318
Train - Epoch 32, Batch: 0, Loss: 1.477013
Train - Epoch 33, Batch: 0, Loss: 1.482168
Train - Epoch 34, Batch: 0, Loss: 1.465299
Train - Epoch 35, Batch: 0, Loss: 1.461469
Train - Epoch 36, Batch: 0, Loss: 1.444553
Train - Epoch 37, Batch: 0, Loss: 1.438621
Train - Epoch 38, Batch: 0, Loss: 1.441061
Train - Epoch 39, Batch: 0, Loss: 1.423872
Train - Epoch 40, Batch: 0, Loss: 1.419344
Train - Epoch 41, Batch: 0, Loss: 1.415817
Train - Epoch 42, Batch: 0, Loss: 1.399545
Train - Epoch 43, Batch: 0, Loss: 1.404179
Train - Epoch 44, Batch: 0, Loss: 1.397756
Train - Epoch 45, Batch: 0, Loss: 1.387079
Train - Epoch 46, Batch: 0, Loss: 1.386960
Train - Epoch 47, Batch: 0, Loss: 1.380328
Train - Epoch 48, Batch: 0, Loss: 1.377444
Train - Epoch 49, Batch: 0, Loss: 1.378304
Train - Epoch 50, Batch: 0, Loss: 1.351529
Train - Epoch 51, Batch: 0, Loss: 1.349951
Train - Epoch 52, Batch: 0, Loss: 1.348113
Train - Epoch 53, Batch: 0, Loss: 1.346709
Train - Epoch 54, Batch: 0, Loss: 1.346067
Train - Epoch 55, Batch: 0, Loss: 1.340768
Train - Epoch 56, Batch: 0, Loss: 1.347086
Train - Epoch 57, Batch: 0, Loss: 1.331110
Train - Epoch 58, Batch: 0, Loss: 1.324810
Train - Epoch 59, Batch: 0, Loss: 1.313819
Train - Epoch 60, Batch: 0, Loss: 1.320446
Train - Epoch 61, Batch: 0, Loss: 1.317305
Train - Epoch 62, Batch: 0, Loss: 1.307335
Train - Epoch 63, Batch: 0, Loss: 1.308936
Train - Epoch 64, Batch: 0, Loss: 1.303636
Train - Epoch 65, Batch: 0, Loss: 1.295183
Train - Epoch 66, Batch: 0, Loss: 1.303682
Train - Epoch 67, Batch: 0, Loss: 1.290339
Train - Epoch 68, Batch: 0, Loss: 1.281166
Train - Epoch 69, Batch: 0, Loss: 1.268973
Train - Epoch 70, Batch: 0, Loss: 1.276629
Train - Epoch 71, Batch: 0, Loss: 1.291444
Train - Epoch 72, Batch: 0, Loss: 1.272436
Train - Epoch 73, Batch: 0, Loss: 1.277289
Train - Epoch 74, Batch: 0, Loss: 1.262972
Train - Epoch 75, Batch: 0, Loss: 1.272428
Train - Epoch 76, Batch: 0, Loss: 1.262186
Train - Epoch 77, Batch: 0, Loss: 1.275124
Train - Epoch 78, Batch: 0, Loss: 1.261840
Train - Epoch 79, Batch: 0, Loss: 1.253752
Train - Epoch 80, Batch: 0, Loss: 1.255099
Train - Epoch 81, Batch: 0, Loss: 1.244002
Train - Epoch 82, Batch: 0, Loss: 1.246924
Train - Epoch 83, Batch: 0, Loss: 1.247913
Train - Epoch 84, Batch: 0, Loss: 1.248029
Train - Epoch 85, Batch: 0, Loss: 1.255637
Train - Epoch 86, Batch: 0, Loss: 1.238858
Train - Epoch 87, Batch: 0, Loss: 1.247146
Train - Epoch 88, Batch: 0, Loss: 1.245056
Train - Epoch 89, Batch: 0, Loss: 1.237602
Train - Epoch 90, Batch: 0, Loss: 1.217862
Train - Epoch 91, Batch: 0, Loss: 1.231323
Train - Epoch 92, Batch: 0, Loss: 1.218397
Train - Epoch 93, Batch: 0, Loss: 1.230489
Train - Epoch 94, Batch: 0, Loss: 1.212579
Train - Epoch 95, Batch: 0, Loss: 1.229448
Train - Epoch 96, Batch: 0, Loss: 1.221304
Train - Epoch 97, Batch: 0, Loss: 1.214001
Train - Epoch 98, Batch: 0, Loss: 1.217434
Train - Epoch 99, Batch: 0, Loss: 1.218806
Train - Epoch 100, Batch: 0, Loss: 1.216538
Train - Epoch 101, Batch: 0, Loss: 1.200195
Train - Epoch 102, Batch: 0, Loss: 1.204480
Train - Epoch 103, Batch: 0, Loss: 1.208261
Train - Epoch 104, Batch: 0, Loss: 1.212641
Train - Epoch 105, Batch: 0, Loss: 1.223529
Train - Epoch 106, Batch: 0, Loss: 1.203351
Train - Epoch 107, Batch: 0, Loss: 1.207935
Train - Epoch 108, Batch: 0, Loss: 1.207926
Train - Epoch 109, Batch: 0, Loss: 1.196553
Train - Epoch 110, Batch: 0, Loss: 1.195425
Train - Epoch 111, Batch: 0, Loss: 1.191251
Train - Epoch 112, Batch: 0, Loss: 1.198244
Train - Epoch 113, Batch: 0, Loss: 1.182854
Train - Epoch 114, Batch: 0, Loss: 1.191332
Train - Epoch 115, Batch: 0, Loss: 1.197927
Train - Epoch 116, Batch: 0, Loss: 1.186360
Train - Epoch 117, Batch: 0, Loss: 1.190783
Train - Epoch 118, Batch: 0, Loss: 1.186560
Train - Epoch 119, Batch: 0, Loss: 1.192071
Train - Epoch 120, Batch: 0, Loss: 1.177784
Train - Epoch 121, Batch: 0, Loss: 1.188300
Train - Epoch 122, Batch: 0, Loss: 1.176209
Train - Epoch 123, Batch: 0, Loss: 1.191310
Train - Epoch 124, Batch: 0, Loss: 1.177743
Train - Epoch 125, Batch: 0, Loss: 1.175431
Train - Epoch 126, Batch: 0, Loss: 1.173429
Train - Epoch 127, Batch: 0, Loss: 1.160480
Train - Epoch 128, Batch: 0, Loss: 1.177641
Train - Epoch 129, Batch: 0, Loss: 1.178624
Train - Epoch 130, Batch: 0, Loss: 1.171891
Train - Epoch 131, Batch: 0, Loss: 1.170325
Train - Epoch 132, Batch: 0, Loss: 1.177249
Train - Epoch 133, Batch: 0, Loss: 1.180762
Train - Epoch 134, Batch: 0, Loss: 1.173366
Train - Epoch 135, Batch: 0, Loss: 1.172317
Train - Epoch 136, Batch: 0, Loss: 1.155989
Train - Epoch 137, Batch: 0, Loss: 1.160359
Train - Epoch 138, Batch: 0, Loss: 1.150823
Train - Epoch 139, Batch: 0, Loss: 1.174718
Train - Epoch 140, Batch: 0, Loss: 1.164432
Train - Epoch 141, Batch: 0, Loss: 1.167553
Train - Epoch 142, Batch: 0, Loss: 1.163721
Train - Epoch 143, Batch: 0, Loss: 1.166927
Train - Epoch 144, Batch: 0, Loss: 1.157657
Train - Epoch 145, Batch: 0, Loss: 1.156332
Train - Epoch 146, Batch: 0, Loss: 1.159118
Train - Epoch 147, Batch: 0, Loss: 1.156296
Train - Epoch 148, Batch: 0, Loss: 1.159167
Train - Epoch 149, Batch: 0, Loss: 1.155622
Train - Epoch 150, Batch: 0, Loss: 1.143363
Train - Epoch 151, Batch: 0, Loss: 1.136854
Train - Epoch 152, Batch: 0, Loss: 1.150883
Train - Epoch 153, Batch: 0, Loss: 1.147718
Train - Epoch 154, Batch: 0, Loss: 1.145237
Train - Epoch 155, Batch: 0, Loss: 1.143881
Train - Epoch 156, Batch: 0, Loss: 1.141162
Train - Epoch 157, Batch: 0, Loss: 1.146252
Train - Epoch 158, Batch: 0, Loss: 1.144497
Train - Epoch 159, Batch: 0, Loss: 1.146172
Train - Epoch 160, Batch: 0, Loss: 1.146949
Train - Epoch 161, Batch: 0, Loss: 1.146283
Train - Epoch 162, Batch: 0, Loss: 1.160283
Train - Epoch 163, Batch: 0, Loss: 1.129284
Train - Epoch 164, Batch: 0, Loss: 1.138153
Train - Epoch 165, Batch: 0, Loss: 1.129832
Train - Epoch 166, Batch: 0, Loss: 1.137739
Train - Epoch 167, Batch: 0, Loss: 1.134281
Train - Epoch 168, Batch: 0, Loss: 1.144706
Train - Epoch 169, Batch: 0, Loss: 1.135778
Train - Epoch 170, Batch: 0, Loss: 1.128999
Train - Epoch 171, Batch: 0, Loss: 1.136024
Train - Epoch 172, Batch: 0, Loss: 1.138561
Train - Epoch 173, Batch: 0, Loss: 1.136780
Train - Epoch 174, Batch: 0, Loss: 1.128910
Train - Epoch 175, Batch: 0, Loss: 1.131675
Train - Epoch 176, Batch: 0, Loss: 1.139328
Train - Epoch 177, Batch: 0, Loss: 1.139361
Train - Epoch 178, Batch: 0, Loss: 1.132434
Train - Epoch 179, Batch: 0, Loss: 1.122097
Train - Epoch 180, Batch: 0, Loss: 1.134913
Train - Epoch 181, Batch: 0, Loss: 1.120411
Train - Epoch 182, Batch: 0, Loss: 1.133430
Train - Epoch 183, Batch: 0, Loss: 1.137038
Train - Epoch 184, Batch: 0, Loss: 1.128403
Train - Epoch 185, Batch: 0, Loss: 1.102858
Train - Epoch 186, Batch: 0, Loss: 1.133759
Train - Epoch 187, Batch: 0, Loss: 1.129779
Train - Epoch 188, Batch: 0, Loss: 1.139512
Train - Epoch 189, Batch: 0, Loss: 1.118924
Train - Epoch 190, Batch: 0, Loss: 1.126077
Train - Epoch 191, Batch: 0, Loss: 1.129199
Train - Epoch 192, Batch: 0, Loss: 1.127088
Train - Epoch 193, Batch: 0, Loss: 1.119441
Train - Epoch 194, Batch: 0, Loss: 1.111192
Train - Epoch 195, Batch: 0, Loss: 1.118517
Train - Epoch 196, Batch: 0, Loss: 1.141957
Train - Epoch 197, Batch: 0, Loss: 1.119696
Train - Epoch 198, Batch: 0, Loss: 1.115910
Train - Epoch 199, Batch: 0, Loss: 1.115949
Train - Epoch 200, Batch: 0, Loss: 1.122486
Train - Epoch 201, Batch: 0, Loss: 1.116883
Train - Epoch 202, Batch: 0, Loss: 1.118605
Train - Epoch 203, Batch: 0, Loss: 1.098505
Train - Epoch 204, Batch: 0, Loss: 1.124374
Train - Epoch 205, Batch: 0, Loss: 1.118161
Train - Epoch 206, Batch: 0, Loss: 1.116314
Train - Epoch 207, Batch: 0, Loss: 1.110219
Train - Epoch 208, Batch: 0, Loss: 1.127197
Train - Epoch 209, Batch: 0, Loss: 1.108664
Train - Epoch 210, Batch: 0, Loss: 1.101284
Train - Epoch 211, Batch: 0, Loss: 1.114216
Train - Epoch 212, Batch: 0, Loss: 1.129855
Train - Epoch 213, Batch: 0, Loss: 1.120291
Train - Epoch 214, Batch: 0, Loss: 1.116856
Train - Epoch 215, Batch: 0, Loss: 1.103578
Train - Epoch 216, Batch: 0, Loss: 1.113167
Train - Epoch 217, Batch: 0, Loss: 1.101859
Train - Epoch 218, Batch: 0, Loss: 1.114027
Train - Epoch 219, Batch: 0, Loss: 1.108464
Train - Epoch 220, Batch: 0, Loss: 1.121219
Train - Epoch 221, Batch: 0, Loss: 1.109527
Train - Epoch 222, Batch: 0, Loss: 1.098466
Train - Epoch 223, Batch: 0, Loss: 1.111189
Train - Epoch 224, Batch: 0, Loss: 1.106326
Train - Epoch 225, Batch: 0, Loss: 1.109389
Train - Epoch 226, Batch: 0, Loss: 1.099194
Train - Epoch 227, Batch: 0, Loss: 1.100131
Train - Epoch 228, Batch: 0, Loss: 1.114251
Train - Epoch 229, Batch: 0, Loss: 1.109499
Train - Epoch 230, Batch: 0, Loss: 1.094950
Train - Epoch 231, Batch: 0, Loss: 1.095248
Train - Epoch 232, Batch: 0, Loss: 1.102472
Train - Epoch 233, Batch: 0, Loss: 1.107290
Train - Epoch 234, Batch: 0, Loss: 1.110938
Train - Epoch 235, Batch: 0, Loss: 1.110211
Train - Epoch 236, Batch: 0, Loss: 1.105998
Train - Epoch 237, Batch: 0, Loss: 1.103449
Train - Epoch 238, Batch: 0, Loss: 1.099659
Train - Epoch 239, Batch: 0, Loss: 1.102043
Train - Epoch 240, Batch: 0, Loss: 1.099080
Train - Epoch 241, Batch: 0, Loss: 1.093720
Train - Epoch 242, Batch: 0, Loss: 1.109071
Train - Epoch 243, Batch: 0, Loss: 1.082378
Train - Epoch 244, Batch: 0, Loss: 1.093830
Train - Epoch 245, Batch: 0, Loss: 1.096866
Train - Epoch 246, Batch: 0, Loss: 1.101611
Train - Epoch 247, Batch: 0, Loss: 1.094369
Train - Epoch 248, Batch: 0, Loss: 1.097828
Train - Epoch 249, Batch: 0, Loss: 1.077752
Train - Epoch 250, Batch: 0, Loss: 1.084215
Train - Epoch 251, Batch: 0, Loss: 1.103712
Train - Epoch 252, Batch: 0, Loss: 1.084918
Train - Epoch 253, Batch: 0, Loss: 1.088278
Train - Epoch 254, Batch: 0, Loss: 1.106054
Train - Epoch 255, Batch: 0, Loss: 1.096906
Train - Epoch 256, Batch: 0, Loss: 1.085030
Train - Epoch 257, Batch: 0, Loss: 1.094860
Train - Epoch 258, Batch: 0, Loss: 1.108882
Train - Epoch 259, Batch: 0, Loss: 1.089326
Train - Epoch 260, Batch: 0, Loss: 1.094174
Train - Epoch 261, Batch: 0, Loss: 1.085345
Train - Epoch 262, Batch: 0, Loss: 1.096033
Train - Epoch 263, Batch: 0, Loss: 1.087091
Train - Epoch 264, Batch: 0, Loss: 1.084681
Train - Epoch 265, Batch: 0, Loss: 1.089606
Train - Epoch 266, Batch: 0, Loss: 1.083831
Train - Epoch 267, Batch: 0, Loss: 1.088339
Train - Epoch 268, Batch: 0, Loss: 1.093544
Train - Epoch 269, Batch: 0, Loss: 1.096614
Train - Epoch 270, Batch: 0, Loss: 1.096126
Train - Epoch 271, Batch: 0, Loss: 1.092372
Train - Epoch 272, Batch: 0, Loss: 1.092482
Train - Epoch 273, Batch: 0, Loss: 1.093236
Train - Epoch 274, Batch: 0, Loss: 1.095953
Train - Epoch 275, Batch: 0, Loss: 1.074873
Train - Epoch 276, Batch: 0, Loss: 1.090546
Train - Epoch 277, Batch: 0, Loss: 1.064160
Train - Epoch 278, Batch: 0, Loss: 1.067215
Train - Epoch 279, Batch: 0, Loss: 1.092325
Train - Epoch 280, Batch: 0, Loss: 1.085486
Train - Epoch 281, Batch: 0, Loss: 1.090996
Train - Epoch 282, Batch: 0, Loss: 1.084571
Train - Epoch 283, Batch: 0, Loss: 1.083349
Train - Epoch 284, Batch: 0, Loss: 1.077275
Train - Epoch 285, Batch: 0, Loss: 1.067315
Train - Epoch 286, Batch: 0, Loss: 1.083714
Train - Epoch 287, Batch: 0, Loss: 1.065127
Train - Epoch 288, Batch: 0, Loss: 1.084243
Train - Epoch 289, Batch: 0, Loss: 1.080333
Train - Epoch 290, Batch: 0, Loss: 1.082853
Train - Epoch 291, Batch: 0, Loss: 1.077976
Train - Epoch 292, Batch: 0, Loss: 1.069957
Train - Epoch 293, Batch: 0, Loss: 1.080360
Train - Epoch 294, Batch: 0, Loss: 1.081213
Train - Epoch 295, Batch: 0, Loss: 1.090006
Train - Epoch 296, Batch: 0, Loss: 1.076452
Train - Epoch 297, Batch: 0, Loss: 1.057513
Train - Epoch 298, Batch: 0, Loss: 1.079173
Train - Epoch 299, Batch: 0, Loss: 1.068807
Train - Epoch 300, Batch: 0, Loss: 1.080606
Train - Epoch 301, Batch: 0, Loss: 1.100252
Train - Epoch 302, Batch: 0, Loss: 1.077418
Train - Epoch 303, Batch: 0, Loss: 1.078673
Train - Epoch 304, Batch: 0, Loss: 1.078958
Train - Epoch 305, Batch: 0, Loss: 1.074817
Train - Epoch 306, Batch: 0, Loss: 1.067129
Train - Epoch 307, Batch: 0, Loss: 1.084411
Train - Epoch 308, Batch: 0, Loss: 1.077831
Train - Epoch 309, Batch: 0, Loss: 1.087067
Train - Epoch 310, Batch: 0, Loss: 1.075537
Train - Epoch 311, Batch: 0, Loss: 1.077035
Train - Epoch 312, Batch: 0, Loss: 1.079775
Train - Epoch 313, Batch: 0, Loss: 1.072262
Train - Epoch 314, Batch: 0, Loss: 1.061534
Train - Epoch 315, Batch: 0, Loss: 1.072010
Train - Epoch 316, Batch: 0, Loss: 1.058764
Train - Epoch 317, Batch: 0, Loss: 1.079395
Train - Epoch 318, Batch: 0, Loss: 1.068416
Train - Epoch 319, Batch: 0, Loss: 1.078973
Train - Epoch 320, Batch: 0, Loss: 1.065092
Train - Epoch 321, Batch: 0, Loss: 1.078732
Train - Epoch 322, Batch: 0, Loss: 1.079653
Train - Epoch 323, Batch: 0, Loss: 1.079949
Train - Epoch 324, Batch: 0, Loss: 1.057450
Train - Epoch 325, Batch: 0, Loss: 1.073228
Train - Epoch 326, Batch: 0, Loss: 1.064806
Train - Epoch 327, Batch: 0, Loss: 1.067065
Train - Epoch 328, Batch: 0, Loss: 1.068439
Train - Epoch 329, Batch: 0, Loss: 1.059955
Train - Epoch 330, Batch: 0, Loss: 1.075533
Train - Epoch 331, Batch: 0, Loss: 1.078297
Train - Epoch 332, Batch: 0, Loss: 1.072806
Train - Epoch 333, Batch: 0, Loss: 1.072707
Train - Epoch 334, Batch: 0, Loss: 1.069499
Train - Epoch 335, Batch: 0, Loss: 1.072272
Train - Epoch 336, Batch: 0, Loss: 1.072032
Train - Epoch 337, Batch: 0, Loss: 1.072415
Train - Epoch 338, Batch: 0, Loss: 1.076730
Train - Epoch 339, Batch: 0, Loss: 1.073159
Train - Epoch 340, Batch: 0, Loss: 1.073845
Train - Epoch 341, Batch: 0, Loss: 1.063870
Train - Epoch 342, Batch: 0, Loss: 1.069669
Train - Epoch 343, Batch: 0, Loss: 1.069332
Train - Epoch 344, Batch: 0, Loss: 1.073177
Train - Epoch 345, Batch: 0, Loss: 1.066452
Train - Epoch 346, Batch: 0, Loss: 1.065806
Train - Epoch 347, Batch: 0, Loss: 1.074638
Train - Epoch 348, Batch: 0, Loss: 1.090829
Train - Epoch 349, Batch: 0, Loss: 1.080596
Train - Epoch 350, Batch: 0, Loss: 1.065178
Train - Epoch 351, Batch: 0, Loss: 1.056746
Train - Epoch 352, Batch: 0, Loss: 1.058863
Train - Epoch 353, Batch: 0, Loss: 1.053254
Train - Epoch 354, Batch: 0, Loss: 1.073082
Train - Epoch 355, Batch: 0, Loss: 1.062537
Train - Epoch 356, Batch: 0, Loss: 1.067781
Train - Epoch 357, Batch: 0, Loss: 1.056966
Train - Epoch 358, Batch: 0, Loss: 1.064132
Train - Epoch 359, Batch: 0, Loss: 1.060336
Train - Epoch 360, Batch: 0, Loss: 1.058289
Train - Epoch 361, Batch: 0, Loss: 1.065413
Train - Epoch 362, Batch: 0, Loss: 1.060714
Train - Epoch 363, Batch: 0, Loss: 1.058202
Train - Epoch 364, Batch: 0, Loss: 1.047144
Train - Epoch 365, Batch: 0, Loss: 1.065701
Train - Epoch 366, Batch: 0, Loss: 1.065263
Train - Epoch 367, Batch: 0, Loss: 1.057787
Train - Epoch 368, Batch: 0, Loss: 1.060119
Train - Epoch 369, Batch: 0, Loss: 1.069121
Train - Epoch 370, Batch: 0, Loss: 1.060709
Train - Epoch 371, Batch: 0, Loss: 1.057291
Train - Epoch 372, Batch: 0, Loss: 1.081210
Train - Epoch 373, Batch: 0, Loss: 1.059731/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 374, Batch: 0, Loss: 1.052992
Train - Epoch 375, Batch: 0, Loss: 1.056602
Train - Epoch 376, Batch: 0, Loss: 1.063624
Train - Epoch 377, Batch: 0, Loss: 1.066123
Train - Epoch 378, Batch: 0, Loss: 1.054439
Train - Epoch 379, Batch: 0, Loss: 1.063114
Train - Epoch 380, Batch: 0, Loss: 1.064425
Train - Epoch 381, Batch: 0, Loss: 1.058310
Train - Epoch 382, Batch: 0, Loss: 1.059531
Train - Epoch 383, Batch: 0, Loss: 1.054166
Train - Epoch 384, Batch: 0, Loss: 1.054470
Train - Epoch 385, Batch: 0, Loss: 1.060928
Train - Epoch 386, Batch: 0, Loss: 1.063546
Train - Epoch 387, Batch: 0, Loss: 1.063823
Train - Epoch 388, Batch: 0, Loss: 1.061804
Train - Epoch 389, Batch: 0, Loss: 1.051989
Train - Epoch 390, Batch: 0, Loss: 1.066033
Train - Epoch 391, Batch: 0, Loss: 1.048388
Train - Epoch 392, Batch: 0, Loss: 1.051789
Train - Epoch 393, Batch: 0, Loss: 1.058279
Train - Epoch 394, Batch: 0, Loss: 1.054644
Train - Epoch 395, Batch: 0, Loss: 1.040297
Train - Epoch 396, Batch: 0, Loss: 1.055919
Train - Epoch 397, Batch: 0, Loss: 1.042067
Train - Epoch 398, Batch: 0, Loss: 1.054429
Train - Epoch 399, Batch: 0, Loss: 1.061436
Train - Epoch 400, Batch: 0, Loss: 1.053182
Train - Epoch 401, Batch: 0, Loss: 1.049428
Train - Epoch 402, Batch: 0, Loss: 1.057027
Train - Epoch 403, Batch: 0, Loss: 1.052880
Train - Epoch 404, Batch: 0, Loss: 1.064301
Train - Epoch 405, Batch: 0, Loss: 1.058388
Train - Epoch 406, Batch: 0, Loss: 1.061092
Train - Epoch 407, Batch: 0, Loss: 1.063936
Train - Epoch 408, Batch: 0, Loss: 1.057124
Train - Epoch 409, Batch: 0, Loss: 1.066137
Train - Epoch 410, Batch: 0, Loss: 1.077367
Train - Epoch 411, Batch: 0, Loss: 1.050295
Train - Epoch 412, Batch: 0, Loss: 1.048975
Train - Epoch 413, Batch: 0, Loss: 1.055358
Train - Epoch 414, Batch: 0, Loss: 1.050640
Train - Epoch 415, Batch: 0, Loss: 1.058003
Train - Epoch 416, Batch: 0, Loss: 1.060986
Train - Epoch 417, Batch: 0, Loss: 1.058296
Train - Epoch 418, Batch: 0, Loss: 1.042694
Train - Epoch 419, Batch: 0, Loss: 1.047572
Train - Epoch 420, Batch: 0, Loss: 1.052419
Train - Epoch 421, Batch: 0, Loss: 1.039643
Train - Epoch 422, Batch: 0, Loss: 1.048193
Train - Epoch 423, Batch: 0, Loss: 1.076633
Train - Epoch 424, Batch: 0, Loss: 1.050159
Train - Epoch 425, Batch: 0, Loss: 1.054712
Train - Epoch 426, Batch: 0, Loss: 1.051861
Train - Epoch 427, Batch: 0, Loss: 1.049614
Train - Epoch 428, Batch: 0, Loss: 1.058855
Train - Epoch 429, Batch: 0, Loss: 1.060508
Train - Epoch 430, Batch: 0, Loss: 1.066571
Train - Epoch 431, Batch: 0, Loss: 1.042293
Train - Epoch 432, Batch: 0, Loss: 1.056088
Train - Epoch 433, Batch: 0, Loss: 1.050129
Train - Epoch 434, Batch: 0, Loss: 1.056887
Train - Epoch 435, Batch: 0, Loss: 1.035058
Train - Epoch 436, Batch: 0, Loss: 1.046591
Train - Epoch 437, Batch: 0, Loss: 1.055506
Train - Epoch 438, Batch: 0, Loss: 1.042085
Train - Epoch 439, Batch: 0, Loss: 1.057831
Train - Epoch 440, Batch: 0, Loss: 1.055346
Train - Epoch 441, Batch: 0, Loss: 1.045903
Train - Epoch 442, Batch: 0, Loss: 1.026459
Train - Epoch 443, Batch: 0, Loss: 1.053782
Train - Epoch 444, Batch: 0, Loss: 1.044020
Train - Epoch 445, Batch: 0, Loss: 1.050638
Train - Epoch 446, Batch: 0, Loss: 1.041420
Train - Epoch 447, Batch: 0, Loss: 1.033113
Train - Epoch 448, Batch: 0, Loss: 1.026437
Train - Epoch 449, Batch: 0, Loss: 1.039104
Train - Epoch 450, Batch: 0, Loss: 1.053266
Train - Epoch 451, Batch: 0, Loss: 1.039354
Train - Epoch 452, Batch: 0, Loss: 1.057136
Train - Epoch 453, Batch: 0, Loss: 1.039367
Train - Epoch 454, Batch: 0, Loss: 1.036521
Train - Epoch 455, Batch: 0, Loss: 1.045213
Train - Epoch 456, Batch: 0, Loss: 1.056062
Train - Epoch 457, Batch: 0, Loss: 1.056780
Train - Epoch 458, Batch: 0, Loss: 1.047276
Train - Epoch 459, Batch: 0, Loss: 1.042434
Train - Epoch 460, Batch: 0, Loss: 1.051412
Train - Epoch 461, Batch: 0, Loss: 1.047073
Train - Epoch 462, Batch: 0, Loss: 1.053796
Train - Epoch 463, Batch: 0, Loss: 1.040509
Train - Epoch 464, Batch: 0, Loss: 1.042354
Train - Epoch 465, Batch: 0, Loss: 1.043302
Train - Epoch 466, Batch: 0, Loss: 1.057815
Train - Epoch 467, Batch: 0, Loss: 1.065401
Train - Epoch 468, Batch: 0, Loss: 1.032387
Train - Epoch 469, Batch: 0, Loss: 1.054655
Train - Epoch 470, Batch: 0, Loss: 1.038269
Train - Epoch 471, Batch: 0, Loss: 1.044688
Train - Epoch 472, Batch: 0, Loss: 1.025871
Train - Epoch 473, Batch: 0, Loss: 1.037893
Train - Epoch 474, Batch: 0, Loss: 1.040471
Train - Epoch 475, Batch: 0, Loss: 1.052116
Train - Epoch 476, Batch: 0, Loss: 1.040359
Train - Epoch 477, Batch: 0, Loss: 1.044399
Train - Epoch 478, Batch: 0, Loss: 1.048217
Train - Epoch 479, Batch: 0, Loss: 1.060906
Train - Epoch 480, Batch: 0, Loss: 1.046465
Train - Epoch 481, Batch: 0, Loss: 1.058384
Train - Epoch 482, Batch: 0, Loss: 1.039939
Train - Epoch 483, Batch: 0, Loss: 1.041082
Train - Epoch 484, Batch: 0, Loss: 1.041576
Train - Epoch 485, Batch: 0, Loss: 1.060382
Train - Epoch 486, Batch: 0, Loss: 1.039074
Train - Epoch 487, Batch: 0, Loss: 1.041583
Train - Epoch 488, Batch: 0, Loss: 1.039033
Train - Epoch 489, Batch: 0, Loss: 1.047293
Train - Epoch 490, Batch: 0, Loss: 1.036045
Train - Epoch 491, Batch: 0, Loss: 1.046998
Train - Epoch 492, Batch: 0, Loss: 1.040555
Train - Epoch 493, Batch: 0, Loss: 1.037839
Train - Epoch 494, Batch: 0, Loss: 1.024072
Train - Epoch 495, Batch: 0, Loss: 1.034807
Train - Epoch 496, Batch: 0, Loss: 1.028240
Train - Epoch 497, Batch: 0, Loss: 1.045647
Train - Epoch 498, Batch: 0, Loss: 1.033534
Train - Epoch 499, Batch: 0, Loss: 1.049711
training_time:: 107.4196879863739
training time full:: 107.41975450515747
provenance prepare time:: 0.0
here
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633300
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 2
max_epoch:: 500
batch_size:: 10000
epoch:: 0
tensor([36014, 37325,  3378,  7705, 43323,  4669,   853, 49635, 21735,  7046,
        39942,  2175, 15571, 31141,  7027,  2128, 18176, 48801, 49801, 27558,
        44810,  6823, 27326,  7182, 17458,  3524,  8631,   244,  8326, 37809,
        32265, 20555, 48873, 31202, 39495,  8728, 16806, 41631, 40670, 46916,
        33178, 38623,  6722, 19937, 13953, 34869, 41336,  9084, 26464, 46786])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
epoch:: 60
epoch:: 61
epoch:: 62
epoch:: 63
epoch:: 64
epoch:: 65
epoch:: 66
epoch:: 67
epoch:: 68
epoch:: 69
epoch:: 70
epoch:: 71
epoch:: 72
epoch:: 73
epoch:: 74
epoch:: 75
epoch:: 76
epoch:: 77
epoch:: 78
epoch:: 79
epoch:: 80
epoch:: 81
epoch:: 82
epoch:: 83
epoch:: 84
epoch:: 85
epoch:: 86
epoch:: 87
epoch:: 88
epoch:: 89
epoch:: 90
epoch:: 91
epoch:: 92
epoch:: 93
epoch:: 94
epoch:: 95
epoch:: 96
epoch:: 97
epoch:: 98
epoch:: 99
epoch:: 100
epoch:: 101
epoch:: 102
epoch:: 103
epoch:: 104
epoch:: 105
epoch:: 106
epoch:: 107
epoch:: 108
epoch:: 109
epoch:: 110
epoch:: 111
epoch:: 112
epoch:: 113
epoch:: 114
epoch:: 115
epoch:: 116
epoch:: 117
epoch:: 118
epoch:: 119
epoch:: 120
epoch:: 121
epoch:: 122
epoch:: 123
epoch:: 124
epoch:: 125
epoch:: 126
epoch:: 127
epoch:: 128
epoch:: 129
epoch:: 130
epoch:: 131
epoch:: 132
epoch:: 133
epoch:: 134
epoch:: 135
epoch:: 136
epoch:: 137
epoch:: 138
epoch:: 139
epoch:: 140
epoch:: 141
epoch:: 142
epoch:: 143
epoch:: 144
epoch:: 145
epoch:: 146
epoch:: 147
epoch:: 148
epoch:: 149
epoch:: 150
epoch:: 151
epoch:: 152
epoch:: 153
epoch:: 154
epoch:: 155
epoch:: 156
epoch:: 157
epoch:: 158
epoch:: 159
epoch:: 160
epoch:: 161
epoch:: 162
epoch:: 163
epoch:: 164
epoch:: 165
epoch:: 166
epoch:: 167
epoch:: 168
epoch:: 169
epoch:: 170
epoch:: 171
epoch:: 172
epoch:: 173
epoch:: 174
epoch:: 175
epoch:: 176
epoch:: 177
epoch:: 178
epoch:: 179
epoch:: 180
epoch:: 181
epoch:: 182
epoch:: 183
epoch:: 184
epoch:: 185
epoch:: 186
epoch:: 187
epoch:: 188
epoch:: 189
epoch:: 190
epoch:: 191
epoch:: 192
epoch:: 193
epoch:: 194
epoch:: 195
epoch:: 196
epoch:: 197
epoch:: 198
epoch:: 199
epoch:: 200
epoch:: 201
epoch:: 202
epoch:: 203
epoch:: 204
epoch:: 205
epoch:: 206
epoch:: 207
epoch:: 208
epoch:: 209
epoch:: 210
epoch:: 211
epoch:: 212
epoch:: 213
epoch:: 214
epoch:: 215
epoch:: 216
epoch:: 217
epoch:: 218
epoch:: 219
epoch:: 220
epoch:: 221
epoch:: 222
epoch:: 223
epoch:: 224
epoch:: 225
epoch:: 226
epoch:: 227
epoch:: 228
epoch:: 229
epoch:: 230
epoch:: 231
epoch:: 232
epoch:: 233
epoch:: 234
epoch:: 235
epoch:: 236
epoch:: 237
epoch:: 238
epoch:: 239
epoch:: 240
epoch:: 241
epoch:: 242
epoch:: 243
epoch:: 244
epoch:: 245
epoch:: 246
epoch:: 247
epoch:: 248
epoch:: 249
epoch:: 250
epoch:: 251
epoch:: 252
epoch:: 253
epoch:: 254
epoch:: 255
epoch:: 256
epoch:: 257
epoch:: 258
epoch:: 259
epoch:: 260
epoch:: 261
epoch:: 262
epoch:: 263
epoch:: 264
epoch:: 265
epoch:: 266
epoch:: 267
epoch:: 268
epoch:: 269
epoch:: 270
epoch:: 271
epoch:: 272
epoch:: 273
epoch:: 274
epoch:: 275
epoch:: 276
epoch:: 277
epoch:: 278
epoch:: 279
epoch:: 280
epoch:: 281
epoch:: 282
epoch:: 283
epoch:: 284
epoch:: 285
epoch:: 286
epoch:: 287
epoch:: 288
epoch:: 289
epoch:: 290
epoch:: 291
epoch:: 292
epoch:: 293
epoch:: 294
epoch:: 295
epoch:: 296
epoch:: 297
epoch:: 298
epoch:: 299
epoch:: 300
epoch:: 301
epoch:: 302
epoch:: 303
epoch:: 304
epoch:: 305
epoch:: 306
epoch:: 307
epoch:: 308
epoch:: 309
epoch:: 310
epoch:: 311
epoch:: 312
epoch:: 313
epoch:: 314
epoch:: 315
epoch:: 316
epoch:: 317
epoch:: 318
epoch:: 319
epoch:: 320
epoch:: 321
epoch:: 322
epoch:: 323
epoch:: 324
epoch:: 325
epoch:: 326
epoch:: 327
epoch:: 328
epoch:: 329
epoch:: 330
epoch:: 331
epoch:: 332
epoch:: 333
epoch:: 334
epoch:: 335
epoch:: 336
epoch:: 337
epoch:: 338
epoch:: 339
epoch:: 340
epoch:: 341
epoch:: 342
epoch:: 343
epoch:: 344
epoch:: 345
epoch:: 346
epoch:: 347
epoch:: 348
epoch:: 349
epoch:: 350
epoch:: 351
epoch:: 352
epoch:: 353
epoch:: 354
epoch:: 355
epoch:: 356
epoch:: 357
epoch:: 358
epoch:: 359
epoch:: 360
epoch:: 361
epoch:: 362
epoch:: 363
epoch:: 364
epoch:: 365
epoch:: 366
epoch:: 367
epoch:: 368
epoch:: 369
epoch:: 370
epoch:: 371
epoch:: 372
epoch:: 373
epoch:: 374
epoch:: 375
epoch:: 376
epoch:: 377
epoch:: 378
epoch:: 379
epoch:: 380
epoch:: 381
epoch:: 382
epoch:: 383
epoch:: 384
epoch:: 385
epoch:: 386
epoch:: 387
epoch:: 388
epoch:: 389
epoch:: 390
epoch:: 391
epoch:: 392
epoch:: 393
epoch:: 394
epoch:: 395
epoch:: 396
epoch:: 397
epoch:: 398
epoch:: 399
epoch:: 400
epoch:: 401
epoch:: 402
epoch:: 403
epoch:: 404
epoch:: 405
epoch:: 406
epoch:: 407
epoch:: 408
epoch:: 409
epoch:: 410
epoch:: 411
epoch:: 412
epoch:: 413
epoch:: 414
epoch:: 415
epoch:: 416
epoch:: 417
epoch:: 418
epoch:: 419
epoch:: 420
epoch:: 421
epoch:: 422
epoch:: 423
epoch:: 424
epoch:: 425
epoch:: 426
epoch:: 427
epoch:: 428
epoch:: 429
epoch:: 430
epoch:: 431
epoch:: 432
epoch:: 433
epoch:: 434
epoch:: 435
epoch:: 436
epoch:: 437
epoch:: 438
epoch:: 439
epoch:: 440
epoch:: 441
epoch:: 442
epoch:: 443
epoch:: 444
epoch:: 445
epoch:: 446
epoch:: 447
epoch:: 448
epoch:: 449
epoch:: 450
epoch:: 451
epoch:: 452
epoch:: 453
epoch:: 454
epoch:: 455
epoch:: 456
epoch:: 457
epoch:: 458
epoch:: 459
epoch:: 460
epoch:: 461
epoch:: 462
epoch:: 463
epoch:: 464
epoch:: 465
epoch:: 466
epoch:: 467
epoch:: 468
epoch:: 469
epoch:: 470
epoch:: 471
epoch:: 472
epoch:: 473
epoch:: 474
epoch:: 475
epoch:: 476
epoch:: 477
epoch:: 478
epoch:: 479
epoch:: 480
epoch:: 481
epoch:: 482
epoch:: 483
epoch:: 484
epoch:: 485
epoch:: 486
epoch:: 487
epoch:: 488
epoch:: 489
epoch:: 490
epoch:: 491
epoch:: 492
epoch:: 493
epoch:: 494
epoch:: 495
epoch:: 496
epoch:: 497
epoch:: 498
epoch:: 499
training time is 79.51624989509583
overhead:: 0
overhead2:: 0.9971916675567627
overhead3:: 0
time_baseline:: 79.51628017425537
curr_diff: 0 tensor(0.0090, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0090, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633400
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 2
max_epoch:: 500
overhead:: 0
overhead2:: 0.026898622512817383
overhead3:: 0.24382972717285156
overhead4:: 9.139431238174438
overhead5:: 0
memory usage:: 5631754240
time_provenance:: 15.5457022190094
curr_diff: 0 tensor(0.0021, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0021, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0105, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0105, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633500
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 2
max_epoch:: 500
overhead:: 0
overhead2:: 0.029526710510253906
overhead3:: 0.25214576721191406
overhead4:: 9.54028058052063
overhead5:: 0
memory usage:: 5638635520
time_provenance:: 15.962756156921387
curr_diff: 0 tensor(0.0021, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0021, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0105, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0105, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633500
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 2
max_epoch:: 500
overhead:: 0
overhead2:: 0.030036211013793945
overhead3:: 0.2636704444885254
overhead4:: 9.894749402999878
overhead5:: 0
memory usage:: 5635428352
time_provenance:: 16.35263967514038
curr_diff: 0 tensor(0.0021, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0021, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0105, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0105, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633500
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 2
max_epoch:: 500
overhead:: 0
overhead2:: 0.06277298927307129
overhead3:: 0.40714216232299805
overhead4:: 17.18837070465088
overhead5:: 0
memory usage:: 5654712320
time_provenance:: 25.60350513458252
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0091, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0091, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633400
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 2
max_epoch:: 500
overhead:: 0
overhead2:: 0.0588221549987793
overhead3:: 0.44797801971435547
overhead4:: 17.577958583831787
overhead5:: 0
memory usage:: 5645135872
time_provenance:: 26.050036430358887
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0091, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0091, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633400
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 2
max_epoch:: 500
overhead:: 0
overhead2:: 0.05895853042602539
overhead3:: 0.4281036853790283
overhead4:: 17.62333345413208
overhead5:: 0
memory usage:: 5630894080
time_provenance:: 26.069958686828613
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0091, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0091, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633400
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 2
max_epoch:: 500
overhead:: 0
overhead2:: 0.14699029922485352
overhead3:: 0.9777905941009521
overhead4:: 42.5842604637146
overhead5:: 0
memory usage:: 5640704000
time_provenance:: 57.16134071350098
curr_diff: 0 tensor(2.6855e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.6855e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0090, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0090, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633400
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 2
max_epoch:: 500
overhead:: 0
overhead2:: 0.14240813255310059
overhead3:: 1.0430235862731934
overhead4:: 42.64847540855408
overhead5:: 0
memory usage:: 5679529984
time_provenance:: 57.24676156044006
curr_diff: 0 tensor(2.6843e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.6843e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0090, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0090, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633400
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 2
max_epoch:: 500
overhead:: 0
overhead2:: 0.14131498336791992
overhead3:: 1.0168163776397705
overhead4:: 42.78914999961853
overhead5:: 0
memory usage:: 5640417280
time_provenance:: 57.3410210609436
curr_diff: 0 tensor(2.6873e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.6873e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0090, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0090, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633400
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 4 0.00005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 2
max_epoch:: 500
overhead:: 0
overhead2:: 0.2868654727935791
overhead3:: 2.075878620147705
overhead4:: 76.24613070487976
overhead5:: 0
memory usage:: 5616336896
time_provenance:: 86.57884645462036
curr_diff: 0 tensor(5.0068e-13, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.0068e-13, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0090, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0090, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633400
deletion rate:: 0.0001
python3 generate_rand_ids 0.0001  cifar10_2 0
tensor([12278, 35638, 40745,  3131, 33087])
batch size:: 10000
repetition 0
python3 benchmark_exp_lr.py 0.001 10000 500 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05] Logistic_regression cifar10_2 0 0.0001 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 2.298410
Train - Epoch 1, Batch: 0, Loss: 2.238565
Train - Epoch 2, Batch: 0, Loss: 2.186368
Train - Epoch 3, Batch: 0, Loss: 2.135561
Train - Epoch 4, Batch: 0, Loss: 2.087496
Train - Epoch 5, Batch: 0, Loss: 2.044921
Train - Epoch 6, Batch: 0, Loss: 1.998852
Train - Epoch 7, Batch: 0, Loss: 1.964475
Train - Epoch 8, Batch: 0, Loss: 1.928310
Train - Epoch 9, Batch: 0, Loss: 1.892326
Train - Epoch 10, Batch: 0, Loss: 1.868675
Train - Epoch 11, Batch: 0, Loss: 1.836519
Train - Epoch 12, Batch: 0, Loss: 1.806223
Train - Epoch 13, Batch: 0, Loss: 1.785071
Train - Epoch 14, Batch: 0, Loss: 1.755302
Train - Epoch 15, Batch: 0, Loss: 1.736653
Train - Epoch 16, Batch: 0, Loss: 1.721305
Train - Epoch 17, Batch: 0, Loss: 1.692615
Train - Epoch 18, Batch: 0, Loss: 1.672968
Train - Epoch 19, Batch: 0, Loss: 1.661359
Train - Epoch 20, Batch: 0, Loss: 1.634218
Train - Epoch 21, Batch: 0, Loss: 1.621770
Train - Epoch 22, Batch: 0, Loss: 1.605511
Train - Epoch 23, Batch: 0, Loss: 1.606348
Train - Epoch 24, Batch: 0, Loss: 1.574614
Train - Epoch 25, Batch: 0, Loss: 1.563883
Train - Epoch 26, Batch: 0, Loss: 1.544560
Train - Epoch 27, Batch: 0, Loss: 1.546068
Train - Epoch 28, Batch: 0, Loss: 1.526395
Train - Epoch 29, Batch: 0, Loss: 1.512869
Train - Epoch 30, Batch: 0, Loss: 1.505061
Train - Epoch 31, Batch: 0, Loss: 1.499706
Train - Epoch 32, Batch: 0, Loss: 1.474816
Train - Epoch 33, Batch: 0, Loss: 1.474004
Train - Epoch 34, Batch: 0, Loss: 1.476471
Train - Epoch 35, Batch: 0, Loss: 1.462831
Train - Epoch 36, Batch: 0, Loss: 1.446288
Train - Epoch 37, Batch: 0, Loss: 1.450427
Train - Epoch 38, Batch: 0, Loss: 1.434603
Train - Epoch 39, Batch: 0, Loss: 1.430207
Train - Epoch 40, Batch: 0, Loss: 1.429846
Train - Epoch 41, Batch: 0, Loss: 1.413251
Train - Epoch 42, Batch: 0, Loss: 1.406435
Train - Epoch 43, Batch: 0, Loss: 1.403651
Train - Epoch 44, Batch: 0, Loss: 1.399009
Train - Epoch 45, Batch: 0, Loss: 1.380022
Train - Epoch 46, Batch: 0, Loss: 1.380913
Train - Epoch 47, Batch: 0, Loss: 1.370848
Train - Epoch 48, Batch: 0, Loss: 1.373058
Train - Epoch 49, Batch: 0, Loss: 1.365912
Train - Epoch 50, Batch: 0, Loss: 1.360373
Train - Epoch 51, Batch: 0, Loss: 1.360449
Train - Epoch 52, Batch: 0, Loss: 1.362028
Train - Epoch 53, Batch: 0, Loss: 1.341890
Train - Epoch 54, Batch: 0, Loss: 1.336423
Train - Epoch 55, Batch: 0, Loss: 1.333744
Train - Epoch 56, Batch: 0, Loss: 1.340233
Train - Epoch 57, Batch: 0, Loss: 1.326016
Train - Epoch 58, Batch: 0, Loss: 1.316675
Train - Epoch 59, Batch: 0, Loss: 1.306537
Train - Epoch 60, Batch: 0, Loss: 1.320739
Train - Epoch 61, Batch: 0, Loss: 1.303171
Train - Epoch 62, Batch: 0, Loss: 1.311045
Train - Epoch 63, Batch: 0, Loss: 1.299969
Train - Epoch 64, Batch: 0, Loss: 1.296935
Train - Epoch 65, Batch: 0, Loss: 1.298856
Train - Epoch 66, Batch: 0, Loss: 1.296818
Train - Epoch 67, Batch: 0, Loss: 1.297987
Train - Epoch 68, Batch: 0, Loss: 1.300569
Train - Epoch 69, Batch: 0, Loss: 1.291310
Train - Epoch 70, Batch: 0, Loss: 1.283356
Train - Epoch 71, Batch: 0, Loss: 1.285571
Train - Epoch 72, Batch: 0, Loss: 1.277825
Train - Epoch 73, Batch: 0, Loss: 1.279798
Train - Epoch 74, Batch: 0, Loss: 1.280275
Train - Epoch 75, Batch: 0, Loss: 1.270792
Train - Epoch 76, Batch: 0, Loss: 1.265324
Train - Epoch 77, Batch: 0, Loss: 1.271093
Train - Epoch 78, Batch: 0, Loss: 1.257855
Train - Epoch 79, Batch: 0, Loss: 1.271238
Train - Epoch 80, Batch: 0, Loss: 1.259999
Train - Epoch 81, Batch: 0, Loss: 1.249472
Train - Epoch 82, Batch: 0, Loss: 1.242833
Train - Epoch 83, Batch: 0, Loss: 1.256093
Train - Epoch 84, Batch: 0, Loss: 1.257570
Train - Epoch 85, Batch: 0, Loss: 1.241130
Train - Epoch 86, Batch: 0, Loss: 1.244473
Train - Epoch 87, Batch: 0, Loss: 1.238374
Train - Epoch 88, Batch: 0, Loss: 1.234967
Train - Epoch 89, Batch: 0, Loss: 1.232144
Train - Epoch 90, Batch: 0, Loss: 1.227662
Train - Epoch 91, Batch: 0, Loss: 1.229823
Train - Epoch 92, Batch: 0, Loss: 1.220935
Train - Epoch 93, Batch: 0, Loss: 1.222466
Train - Epoch 94, Batch: 0, Loss: 1.230566
Train - Epoch 95, Batch: 0, Loss: 1.222309
Train - Epoch 96, Batch: 0, Loss: 1.218616
Train - Epoch 97, Batch: 0, Loss: 1.220831
Train - Epoch 98, Batch: 0, Loss: 1.218547
Train - Epoch 99, Batch: 0, Loss: 1.218953
Train - Epoch 100, Batch: 0, Loss: 1.224079
Train - Epoch 101, Batch: 0, Loss: 1.216751
Train - Epoch 102, Batch: 0, Loss: 1.223539
Train - Epoch 103, Batch: 0, Loss: 1.203136
Train - Epoch 104, Batch: 0, Loss: 1.206742
Train - Epoch 105, Batch: 0, Loss: 1.208190
Train - Epoch 106, Batch: 0, Loss: 1.205389
Train - Epoch 107, Batch: 0, Loss: 1.197805
Train - Epoch 108, Batch: 0, Loss: 1.194931
Train - Epoch 109, Batch: 0, Loss: 1.207893
Train - Epoch 110, Batch: 0, Loss: 1.207248
Train - Epoch 111, Batch: 0, Loss: 1.200419
Train - Epoch 112, Batch: 0, Loss: 1.187334
Train - Epoch 113, Batch: 0, Loss: 1.182460
Train - Epoch 114, Batch: 0, Loss: 1.192279
Train - Epoch 115, Batch: 0, Loss: 1.200878
Train - Epoch 116, Batch: 0, Loss: 1.187742
Train - Epoch 117, Batch: 0, Loss: 1.189104
Train - Epoch 118, Batch: 0, Loss: 1.173861
Train - Epoch 119, Batch: 0, Loss: 1.179034
Train - Epoch 120, Batch: 0, Loss: 1.183675
Train - Epoch 121, Batch: 0, Loss: 1.161839
Train - Epoch 122, Batch: 0, Loss: 1.182841
Train - Epoch 123, Batch: 0, Loss: 1.181019
Train - Epoch 124, Batch: 0, Loss: 1.187685
Train - Epoch 125, Batch: 0, Loss: 1.177945
Train - Epoch 126, Batch: 0, Loss: 1.184857
Train - Epoch 127, Batch: 0, Loss: 1.190076
Train - Epoch 128, Batch: 0, Loss: 1.176197
Train - Epoch 129, Batch: 0, Loss: 1.185980
Train - Epoch 130, Batch: 0, Loss: 1.179412
Train - Epoch 131, Batch: 0, Loss: 1.175787
Train - Epoch 132, Batch: 0, Loss: 1.166229
Train - Epoch 133, Batch: 0, Loss: 1.168136
Train - Epoch 134, Batch: 0, Loss: 1.175081
Train - Epoch 135, Batch: 0, Loss: 1.180894
Train - Epoch 136, Batch: 0, Loss: 1.170544
Train - Epoch 137, Batch: 0, Loss: 1.168069
Train - Epoch 138, Batch: 0, Loss: 1.161859
Train - Epoch 139, Batch: 0, Loss: 1.148488
Train - Epoch 140, Batch: 0, Loss: 1.164263
Train - Epoch 141, Batch: 0, Loss: 1.169905
Train - Epoch 142, Batch: 0, Loss: 1.145005
Train - Epoch 143, Batch: 0, Loss: 1.157215
Train - Epoch 144, Batch: 0, Loss: 1.169385
Train - Epoch 145, Batch: 0, Loss: 1.161298
Train - Epoch 146, Batch: 0, Loss: 1.161906
Train - Epoch 147, Batch: 0, Loss: 1.143035
Train - Epoch 148, Batch: 0, Loss: 1.153853
Train - Epoch 149, Batch: 0, Loss: 1.142991
Train - Epoch 150, Batch: 0, Loss: 1.157283
Train - Epoch 151, Batch: 0, Loss: 1.157186
Train - Epoch 152, Batch: 0, Loss: 1.159570
Train - Epoch 153, Batch: 0, Loss: 1.158008
Train - Epoch 154, Batch: 0, Loss: 1.155161
Train - Epoch 155, Batch: 0, Loss: 1.146255
Train - Epoch 156, Batch: 0, Loss: 1.148798
Train - Epoch 157, Batch: 0, Loss: 1.145831
Train - Epoch 158, Batch: 0, Loss: 1.151895
Train - Epoch 159, Batch: 0, Loss: 1.159293
Train - Epoch 160, Batch: 0, Loss: 1.147510
Train - Epoch 161, Batch: 0, Loss: 1.126366
Train - Epoch 162, Batch: 0, Loss: 1.149181
Train - Epoch 163, Batch: 0, Loss: 1.143610
Train - Epoch 164, Batch: 0, Loss: 1.154606
Train - Epoch 165, Batch: 0, Loss: 1.133714
Train - Epoch 166, Batch: 0, Loss: 1.145011
Train - Epoch 167, Batch: 0, Loss: 1.140677
Train - Epoch 168, Batch: 0, Loss: 1.125115
Train - Epoch 169, Batch: 0, Loss: 1.133335
Train - Epoch 170, Batch: 0, Loss: 1.141342
Train - Epoch 171, Batch: 0, Loss: 1.141626
Train - Epoch 172, Batch: 0, Loss: 1.136516
Train - Epoch 173, Batch: 0, Loss: 1.134385
Train - Epoch 174, Batch: 0, Loss: 1.134232
Train - Epoch 175, Batch: 0, Loss: 1.129646
Train - Epoch 176, Batch: 0, Loss: 1.139306
Train - Epoch 177, Batch: 0, Loss: 1.130934
Train - Epoch 178, Batch: 0, Loss: 1.128504
Train - Epoch 179, Batch: 0, Loss: 1.131552
Train - Epoch 180, Batch: 0, Loss: 1.133487
Train - Epoch 181, Batch: 0, Loss: 1.142152
Train - Epoch 182, Batch: 0, Loss: 1.130843
Train - Epoch 183, Batch: 0, Loss: 1.125952
Train - Epoch 184, Batch: 0, Loss: 1.103020
Train - Epoch 185, Batch: 0, Loss: 1.129766
Train - Epoch 186, Batch: 0, Loss: 1.142862
Train - Epoch 187, Batch: 0, Loss: 1.124072
Train - Epoch 188, Batch: 0, Loss: 1.127214
Train - Epoch 189, Batch: 0, Loss: 1.148335
Train - Epoch 190, Batch: 0, Loss: 1.131104
Train - Epoch 191, Batch: 0, Loss: 1.127000
Train - Epoch 192, Batch: 0, Loss: 1.117484
Train - Epoch 193, Batch: 0, Loss: 1.131293
Train - Epoch 194, Batch: 0, Loss: 1.114334
Train - Epoch 195, Batch: 0, Loss: 1.115967
Train - Epoch 196, Batch: 0, Loss: 1.118310
Train - Epoch 197, Batch: 0, Loss: 1.106816
Train - Epoch 198, Batch: 0, Loss: 1.123336
Train - Epoch 199, Batch: 0, Loss: 1.117449
Train - Epoch 200, Batch: 0, Loss: 1.127067
Train - Epoch 201, Batch: 0, Loss: 1.131511
Train - Epoch 202, Batch: 0, Loss: 1.125566
Train - Epoch 203, Batch: 0, Loss: 1.115967
Train - Epoch 204, Batch: 0, Loss: 1.132694
Train - Epoch 205, Batch: 0, Loss: 1.115878
Train - Epoch 206, Batch: 0, Loss: 1.109146
Train - Epoch 207, Batch: 0, Loss: 1.120905
Train - Epoch 208, Batch: 0, Loss: 1.109802
Train - Epoch 209, Batch: 0, Loss: 1.102401
Train - Epoch 210, Batch: 0, Loss: 1.109651
Train - Epoch 211, Batch: 0, Loss: 1.121460
Train - Epoch 212, Batch: 0, Loss: 1.103607
Train - Epoch 213, Batch: 0, Loss: 1.091076
Train - Epoch 214, Batch: 0, Loss: 1.109979
Train - Epoch 215, Batch: 0, Loss: 1.107230
Train - Epoch 216, Batch: 0, Loss: 1.109887
Train - Epoch 217, Batch: 0, Loss: 1.123843
Train - Epoch 218, Batch: 0, Loss: 1.108918
Train - Epoch 219, Batch: 0, Loss: 1.099139
Train - Epoch 220, Batch: 0, Loss: 1.115106
Train - Epoch 221, Batch: 0, Loss: 1.107044
Train - Epoch 222, Batch: 0, Loss: 1.133166
Train - Epoch 223, Batch: 0, Loss: 1.107476
Train - Epoch 224, Batch: 0, Loss: 1.108979
Train - Epoch 225, Batch: 0, Loss: 1.108634
Train - Epoch 226, Batch: 0, Loss: 1.102361
Train - Epoch 227, Batch: 0, Loss: 1.092336
Train - Epoch 228, Batch: 0, Loss: 1.113842
Train - Epoch 229, Batch: 0, Loss: 1.083984
Train - Epoch 230, Batch: 0, Loss: 1.107628
Train - Epoch 231, Batch: 0, Loss: 1.099635
Train - Epoch 232, Batch: 0, Loss: 1.114497
Train - Epoch 233, Batch: 0, Loss: 1.099805
Train - Epoch 234, Batch: 0, Loss: 1.108385
Train - Epoch 235, Batch: 0, Loss: 1.103621
Train - Epoch 236, Batch: 0, Loss: 1.112294
Train - Epoch 237, Batch: 0, Loss: 1.095042
Train - Epoch 238, Batch: 0, Loss: 1.107630
Train - Epoch 239, Batch: 0, Loss: 1.089735
Train - Epoch 240, Batch: 0, Loss: 1.107620
Train - Epoch 241, Batch: 0, Loss: 1.099351
Train - Epoch 242, Batch: 0, Loss: 1.100570
Train - Epoch 243, Batch: 0, Loss: 1.089119
Train - Epoch 244, Batch: 0, Loss: 1.100037
Train - Epoch 245, Batch: 0, Loss: 1.092758
Train - Epoch 246, Batch: 0, Loss: 1.096948
Train - Epoch 247, Batch: 0, Loss: 1.091960
Train - Epoch 248, Batch: 0, Loss: 1.085443
Train - Epoch 249, Batch: 0, Loss: 1.094418
Train - Epoch 250, Batch: 0, Loss: 1.091565
Train - Epoch 251, Batch: 0, Loss: 1.083392
Train - Epoch 252, Batch: 0, Loss: 1.092885
Train - Epoch 253, Batch: 0, Loss: 1.083755
Train - Epoch 254, Batch: 0, Loss: 1.083744
Train - Epoch 255, Batch: 0, Loss: 1.102352
Train - Epoch 256, Batch: 0, Loss: 1.084629
Train - Epoch 257, Batch: 0, Loss: 1.098268
Train - Epoch 258, Batch: 0, Loss: 1.082755
Train - Epoch 259, Batch: 0, Loss: 1.092676
Train - Epoch 260, Batch: 0, Loss: 1.096199
Train - Epoch 261, Batch: 0, Loss: 1.088356
Train - Epoch 262, Batch: 0, Loss: 1.098624
Train - Epoch 263, Batch: 0, Loss: 1.107988
Train - Epoch 264, Batch: 0, Loss: 1.078775
Train - Epoch 265, Batch: 0, Loss: 1.091248
Train - Epoch 266, Batch: 0, Loss: 1.080558
Train - Epoch 267, Batch: 0, Loss: 1.102373
Train - Epoch 268, Batch: 0, Loss: 1.085963
Train - Epoch 269, Batch: 0, Loss: 1.080569
Train - Epoch 270, Batch: 0, Loss: 1.096310
Train - Epoch 271, Batch: 0, Loss: 1.086006
Train - Epoch 272, Batch: 0, Loss: 1.090339
Train - Epoch 273, Batch: 0, Loss: 1.084091
Train - Epoch 274, Batch: 0, Loss: 1.086296
Train - Epoch 275, Batch: 0, Loss: 1.082865
Train - Epoch 276, Batch: 0, Loss: 1.089611
Train - Epoch 277, Batch: 0, Loss: 1.095015
Train - Epoch 278, Batch: 0, Loss: 1.087943
Train - Epoch 279, Batch: 0, Loss: 1.090347
Train - Epoch 280, Batch: 0, Loss: 1.110798
Train - Epoch 281, Batch: 0, Loss: 1.085658
Train - Epoch 282, Batch: 0, Loss: 1.075734
Train - Epoch 283, Batch: 0, Loss: 1.084553
Train - Epoch 284, Batch: 0, Loss: 1.086755
Train - Epoch 285, Batch: 0, Loss: 1.080987
Train - Epoch 286, Batch: 0, Loss: 1.084429
Train - Epoch 287, Batch: 0, Loss: 1.091762
Train - Epoch 288, Batch: 0, Loss: 1.072038
Train - Epoch 289, Batch: 0, Loss: 1.069020
Train - Epoch 290, Batch: 0, Loss: 1.088838
Train - Epoch 291, Batch: 0, Loss: 1.075886
Train - Epoch 292, Batch: 0, Loss: 1.084064
Train - Epoch 293, Batch: 0, Loss: 1.105368
Train - Epoch 294, Batch: 0, Loss: 1.094209
Train - Epoch 295, Batch: 0, Loss: 1.094482
Train - Epoch 296, Batch: 0, Loss: 1.078702
Train - Epoch 297, Batch: 0, Loss: 1.099062
Train - Epoch 298, Batch: 0, Loss: 1.073385
Train - Epoch 299, Batch: 0, Loss: 1.075333
Train - Epoch 300, Batch: 0, Loss: 1.081680
Train - Epoch 301, Batch: 0, Loss: 1.078634
Train - Epoch 302, Batch: 0, Loss: 1.081289
Train - Epoch 303, Batch: 0, Loss: 1.079456
Train - Epoch 304, Batch: 0, Loss: 1.085511
Train - Epoch 305, Batch: 0, Loss: 1.071506
Train - Epoch 306, Batch: 0, Loss: 1.081971
Train - Epoch 307, Batch: 0, Loss: 1.068257
Train - Epoch 308, Batch: 0, Loss: 1.077219
Train - Epoch 309, Batch: 0, Loss: 1.084838
Train - Epoch 310, Batch: 0, Loss: 1.068660
Train - Epoch 311, Batch: 0, Loss: 1.078408
Train - Epoch 312, Batch: 0, Loss: 1.070267
Train - Epoch 313, Batch: 0, Loss: 1.078391
Train - Epoch 314, Batch: 0, Loss: 1.084763
Train - Epoch 315, Batch: 0, Loss: 1.059534
Train - Epoch 316, Batch: 0, Loss: 1.067244
Train - Epoch 317, Batch: 0, Loss: 1.080852
Train - Epoch 318, Batch: 0, Loss: 1.084141
Train - Epoch 319, Batch: 0, Loss: 1.079074
Train - Epoch 320, Batch: 0, Loss: 1.075956
Train - Epoch 321, Batch: 0, Loss: 1.077173
Train - Epoch 322, Batch: 0, Loss: 1.061158
Train - Epoch 323, Batch: 0, Loss: 1.050895
Train - Epoch 324, Batch: 0, Loss: 1.059879
Train - Epoch 325, Batch: 0, Loss: 1.059975
Train - Epoch 326, Batch: 0, Loss: 1.056567
Train - Epoch 327, Batch: 0, Loss: 1.068495
Train - Epoch 328, Batch: 0, Loss: 1.068400
Train - Epoch 329, Batch: 0, Loss: 1.078471
Train - Epoch 330, Batch: 0, Loss: 1.086870
Train - Epoch 331, Batch: 0, Loss: 1.056630
Train - Epoch 332, Batch: 0, Loss: 1.075219
Train - Epoch 333, Batch: 0, Loss: 1.082646
Train - Epoch 334, Batch: 0, Loss: 1.055361
Train - Epoch 335, Batch: 0, Loss: 1.064612
Train - Epoch 336, Batch: 0, Loss: 1.079338
Train - Epoch 337, Batch: 0, Loss: 1.059314
Train - Epoch 338, Batch: 0, Loss: 1.073837
Train - Epoch 339, Batch: 0, Loss: 1.064088
Train - Epoch 340, Batch: 0, Loss: 1.063734
Train - Epoch 341, Batch: 0, Loss: 1.057717
Train - Epoch 342, Batch: 0, Loss: 1.064067
Train - Epoch 343, Batch: 0, Loss: 1.058553
Train - Epoch 344, Batch: 0, Loss: 1.071239
Train - Epoch 345, Batch: 0, Loss: 1.058362
Train - Epoch 346, Batch: 0, Loss: 1.067178
Train - Epoch 347, Batch: 0, Loss: 1.063025
Train - Epoch 348, Batch: 0, Loss: 1.062381
Train - Epoch 349, Batch: 0, Loss: 1.050235
Train - Epoch 350, Batch: 0, Loss: 1.067454
Train - Epoch 351, Batch: 0, Loss: 1.068088
Train - Epoch 352, Batch: 0, Loss: 1.070474
Train - Epoch 353, Batch: 0, Loss: 1.066338
Train - Epoch 354, Batch: 0, Loss: 1.071000
Train - Epoch 355, Batch: 0, Loss: 1.075884
Train - Epoch 356, Batch: 0, Loss: 1.073858
Train - Epoch 357, Batch: 0, Loss: 1.060819
Train - Epoch 358, Batch: 0, Loss: 1.048239
Train - Epoch 359, Batch: 0, Loss: 1.061143
Train - Epoch 360, Batch: 0, Loss: 1.071005
Train - Epoch 361, Batch: 0, Loss: 1.045993
Train - Epoch 362, Batch: 0, Loss: 1.062687
Train - Epoch 363, Batch: 0, Loss: 1.055448
Train - Epoch 364, Batch: 0, Loss: 1.055711
Train - Epoch 365, Batch: 0, Loss: 1.054502
Train - Epoch 366, Batch: 0, Loss: 1.045898
Train - Epoch 367, Batch: 0, Loss: 1.064594
Train - Epoch 368, Batch: 0, Loss: 1.047054
Train - Epoch 369, Batch: 0, Loss: 1.071714
Train - Epoch 370, Batch: 0, Loss: 1.059952
Train - Epoch 371, Batch: 0, Loss: 1.062989
Train - Epoch 372, Batch: 0, Loss: 1.055338
Train - Epoch 373, Batch: 0, Loss: 1.051439/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 374, Batch: 0, Loss: 1.046292
Train - Epoch 375, Batch: 0, Loss: 1.053685
Train - Epoch 376, Batch: 0, Loss: 1.052258
Train - Epoch 377, Batch: 0, Loss: 1.060408
Train - Epoch 378, Batch: 0, Loss: 1.060467
Train - Epoch 379, Batch: 0, Loss: 1.041301
Train - Epoch 380, Batch: 0, Loss: 1.051907
Train - Epoch 381, Batch: 0, Loss: 1.054532
Train - Epoch 382, Batch: 0, Loss: 1.054609
Train - Epoch 383, Batch: 0, Loss: 1.065016
Train - Epoch 384, Batch: 0, Loss: 1.049322
Train - Epoch 385, Batch: 0, Loss: 1.052964
Train - Epoch 386, Batch: 0, Loss: 1.069297
Train - Epoch 387, Batch: 0, Loss: 1.033661
Train - Epoch 388, Batch: 0, Loss: 1.053180
Train - Epoch 389, Batch: 0, Loss: 1.061335
Train - Epoch 390, Batch: 0, Loss: 1.058067
Train - Epoch 391, Batch: 0, Loss: 1.048633
Train - Epoch 392, Batch: 0, Loss: 1.050465
Train - Epoch 393, Batch: 0, Loss: 1.072500
Train - Epoch 394, Batch: 0, Loss: 1.065934
Train - Epoch 395, Batch: 0, Loss: 1.059383
Train - Epoch 396, Batch: 0, Loss: 1.052178
Train - Epoch 397, Batch: 0, Loss: 1.067547
Train - Epoch 398, Batch: 0, Loss: 1.063861
Train - Epoch 399, Batch: 0, Loss: 1.061752
Train - Epoch 400, Batch: 0, Loss: 1.050480
Train - Epoch 401, Batch: 0, Loss: 1.052075
Train - Epoch 402, Batch: 0, Loss: 1.066034
Train - Epoch 403, Batch: 0, Loss: 1.056206
Train - Epoch 404, Batch: 0, Loss: 1.047855
Train - Epoch 405, Batch: 0, Loss: 1.043900
Train - Epoch 406, Batch: 0, Loss: 1.056975
Train - Epoch 407, Batch: 0, Loss: 1.059087
Train - Epoch 408, Batch: 0, Loss: 1.038967
Train - Epoch 409, Batch: 0, Loss: 1.055903
Train - Epoch 410, Batch: 0, Loss: 1.066796
Train - Epoch 411, Batch: 0, Loss: 1.050732
Train - Epoch 412, Batch: 0, Loss: 1.039331
Train - Epoch 413, Batch: 0, Loss: 1.036521
Train - Epoch 414, Batch: 0, Loss: 1.048389
Train - Epoch 415, Batch: 0, Loss: 1.052561
Train - Epoch 416, Batch: 0, Loss: 1.054456
Train - Epoch 417, Batch: 0, Loss: 1.059495
Train - Epoch 418, Batch: 0, Loss: 1.064424
Train - Epoch 419, Batch: 0, Loss: 1.056421
Train - Epoch 420, Batch: 0, Loss: 1.060942
Train - Epoch 421, Batch: 0, Loss: 1.054769
Train - Epoch 422, Batch: 0, Loss: 1.058944
Train - Epoch 423, Batch: 0, Loss: 1.054563
Train - Epoch 424, Batch: 0, Loss: 1.051396
Train - Epoch 425, Batch: 0, Loss: 1.057458
Train - Epoch 426, Batch: 0, Loss: 1.037472
Train - Epoch 427, Batch: 0, Loss: 1.035997
Train - Epoch 428, Batch: 0, Loss: 1.061963
Train - Epoch 429, Batch: 0, Loss: 1.060856
Train - Epoch 430, Batch: 0, Loss: 1.048822
Train - Epoch 431, Batch: 0, Loss: 1.052212
Train - Epoch 432, Batch: 0, Loss: 1.046899
Train - Epoch 433, Batch: 0, Loss: 1.040906
Train - Epoch 434, Batch: 0, Loss: 1.039058
Train - Epoch 435, Batch: 0, Loss: 1.046480
Train - Epoch 436, Batch: 0, Loss: 1.035363
Train - Epoch 437, Batch: 0, Loss: 1.051991
Train - Epoch 438, Batch: 0, Loss: 1.054843
Train - Epoch 439, Batch: 0, Loss: 1.055614
Train - Epoch 440, Batch: 0, Loss: 1.052292
Train - Epoch 441, Batch: 0, Loss: 1.052755
Train - Epoch 442, Batch: 0, Loss: 1.047145
Train - Epoch 443, Batch: 0, Loss: 1.047070
Train - Epoch 444, Batch: 0, Loss: 1.033663
Train - Epoch 445, Batch: 0, Loss: 1.030292
Train - Epoch 446, Batch: 0, Loss: 1.038854
Train - Epoch 447, Batch: 0, Loss: 1.039884
Train - Epoch 448, Batch: 0, Loss: 1.041240
Train - Epoch 449, Batch: 0, Loss: 1.048211
Train - Epoch 450, Batch: 0, Loss: 1.056121
Train - Epoch 451, Batch: 0, Loss: 1.032883
Train - Epoch 452, Batch: 0, Loss: 1.053506
Train - Epoch 453, Batch: 0, Loss: 1.044922
Train - Epoch 454, Batch: 0, Loss: 1.040516
Train - Epoch 455, Batch: 0, Loss: 1.046856
Train - Epoch 456, Batch: 0, Loss: 1.044458
Train - Epoch 457, Batch: 0, Loss: 1.039263
Train - Epoch 458, Batch: 0, Loss: 1.062711
Train - Epoch 459, Batch: 0, Loss: 1.052431
Train - Epoch 460, Batch: 0, Loss: 1.056305
Train - Epoch 461, Batch: 0, Loss: 1.040359
Train - Epoch 462, Batch: 0, Loss: 1.033525
Train - Epoch 463, Batch: 0, Loss: 1.053554
Train - Epoch 464, Batch: 0, Loss: 1.052513
Train - Epoch 465, Batch: 0, Loss: 1.044493
Train - Epoch 466, Batch: 0, Loss: 1.038092
Train - Epoch 467, Batch: 0, Loss: 1.050708
Train - Epoch 468, Batch: 0, Loss: 1.048837
Train - Epoch 469, Batch: 0, Loss: 1.049163
Train - Epoch 470, Batch: 0, Loss: 1.058369
Train - Epoch 471, Batch: 0, Loss: 1.042197
Train - Epoch 472, Batch: 0, Loss: 1.049052
Train - Epoch 473, Batch: 0, Loss: 1.046451
Train - Epoch 474, Batch: 0, Loss: 1.032608
Train - Epoch 475, Batch: 0, Loss: 1.048287
Train - Epoch 476, Batch: 0, Loss: 1.036032
Train - Epoch 477, Batch: 0, Loss: 1.043924
Train - Epoch 478, Batch: 0, Loss: 1.032712
Train - Epoch 479, Batch: 0, Loss: 1.048747
Train - Epoch 480, Batch: 0, Loss: 1.045990
Train - Epoch 481, Batch: 0, Loss: 1.051058
Train - Epoch 482, Batch: 0, Loss: 1.049266
Train - Epoch 483, Batch: 0, Loss: 1.033449
Train - Epoch 484, Batch: 0, Loss: 1.050818
Train - Epoch 485, Batch: 0, Loss: 1.035972
Train - Epoch 486, Batch: 0, Loss: 1.046389
Train - Epoch 487, Batch: 0, Loss: 1.025350
Train - Epoch 488, Batch: 0, Loss: 1.036383
Train - Epoch 489, Batch: 0, Loss: 1.053261
Train - Epoch 490, Batch: 0, Loss: 1.056866
Train - Epoch 491, Batch: 0, Loss: 1.017967
Train - Epoch 492, Batch: 0, Loss: 1.053003
Train - Epoch 493, Batch: 0, Loss: 1.055294
Train - Epoch 494, Batch: 0, Loss: 1.037364
Train - Epoch 495, Batch: 0, Loss: 1.046150
Train - Epoch 496, Batch: 0, Loss: 1.056252
Train - Epoch 497, Batch: 0, Loss: 1.029368
Train - Epoch 498, Batch: 0, Loss: 1.020928
Train - Epoch 499, Batch: 0, Loss: 1.037415
training_time:: 107.44762992858887
training time full:: 107.44770073890686
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632000
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 5
max_epoch:: 500
batch_size:: 10000
epoch:: 0
tensor([    3, 32773,     7,    13,    17,    22,    24,    25,    32, 32801,
        32802,    35, 32803,    36, 32806, 32810, 32812, 32815,    49,    53,
           54, 32823,    61,    67, 32835,    68,    71, 32840, 32841, 32842,
        32843, 32848,    81, 32850, 32852,    85,    86,    91, 32860, 32862,
           95,    98,    99,   100,   102,   104,   105,   107, 32876,   117])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
epoch:: 60
epoch:: 61
epoch:: 62
epoch:: 63
epoch:: 64
epoch:: 65
epoch:: 66
epoch:: 67
epoch:: 68
epoch:: 69
epoch:: 70
epoch:: 71
epoch:: 72
epoch:: 73
epoch:: 74
epoch:: 75
epoch:: 76
epoch:: 77
epoch:: 78
epoch:: 79
epoch:: 80
epoch:: 81
epoch:: 82
epoch:: 83
epoch:: 84
epoch:: 85
epoch:: 86
epoch:: 87
epoch:: 88
epoch:: 89
epoch:: 90
epoch:: 91
epoch:: 92
epoch:: 93
epoch:: 94
epoch:: 95
epoch:: 96
epoch:: 97
epoch:: 98
epoch:: 99
epoch:: 100
epoch:: 101
epoch:: 102
epoch:: 103
epoch:: 104
epoch:: 105
epoch:: 106
epoch:: 107
epoch:: 108
epoch:: 109
epoch:: 110
epoch:: 111
epoch:: 112
epoch:: 113
epoch:: 114
epoch:: 115
epoch:: 116
epoch:: 117
epoch:: 118
epoch:: 119
epoch:: 120
epoch:: 121
epoch:: 122
epoch:: 123
epoch:: 124
epoch:: 125
epoch:: 126
epoch:: 127
epoch:: 128
epoch:: 129
epoch:: 130
epoch:: 131
epoch:: 132
epoch:: 133
epoch:: 134
epoch:: 135
epoch:: 136
epoch:: 137
epoch:: 138
epoch:: 139
epoch:: 140
epoch:: 141
epoch:: 142
epoch:: 143
epoch:: 144
epoch:: 145
epoch:: 146
epoch:: 147
epoch:: 148
epoch:: 149
epoch:: 150
epoch:: 151
epoch:: 152
epoch:: 153
epoch:: 154
epoch:: 155
epoch:: 156
epoch:: 157
epoch:: 158
epoch:: 159
epoch:: 160
epoch:: 161
epoch:: 162
epoch:: 163
epoch:: 164
epoch:: 165
epoch:: 166
epoch:: 167
epoch:: 168
epoch:: 169
epoch:: 170
epoch:: 171
epoch:: 172
epoch:: 173
epoch:: 174
epoch:: 175
epoch:: 176
epoch:: 177
epoch:: 178
epoch:: 179
epoch:: 180
epoch:: 181
epoch:: 182
epoch:: 183
epoch:: 184
epoch:: 185
epoch:: 186
epoch:: 187
epoch:: 188
epoch:: 189
epoch:: 190
epoch:: 191
epoch:: 192
epoch:: 193
epoch:: 194
epoch:: 195
epoch:: 196
epoch:: 197
epoch:: 198
epoch:: 199
epoch:: 200
epoch:: 201
epoch:: 202
epoch:: 203
epoch:: 204
epoch:: 205
epoch:: 206
epoch:: 207
epoch:: 208
epoch:: 209
epoch:: 210
epoch:: 211
epoch:: 212
epoch:: 213
epoch:: 214
epoch:: 215
epoch:: 216
epoch:: 217
epoch:: 218
epoch:: 219
epoch:: 220
epoch:: 221
epoch:: 222
epoch:: 223
epoch:: 224
epoch:: 225
epoch:: 226
epoch:: 227
epoch:: 228
epoch:: 229
epoch:: 230
epoch:: 231
epoch:: 232
epoch:: 233
epoch:: 234
epoch:: 235
epoch:: 236
epoch:: 237
epoch:: 238
epoch:: 239
epoch:: 240
epoch:: 241
epoch:: 242
epoch:: 243
epoch:: 244
epoch:: 245
epoch:: 246
epoch:: 247
epoch:: 248
epoch:: 249
epoch:: 250
epoch:: 251
epoch:: 252
epoch:: 253
epoch:: 254
epoch:: 255
epoch:: 256
epoch:: 257
epoch:: 258
epoch:: 259
epoch:: 260
epoch:: 261
epoch:: 262
epoch:: 263
epoch:: 264
epoch:: 265
epoch:: 266
epoch:: 267
epoch:: 268
epoch:: 269
epoch:: 270
epoch:: 271
epoch:: 272
epoch:: 273
epoch:: 274
epoch:: 275
epoch:: 276
epoch:: 277
epoch:: 278
epoch:: 279
epoch:: 280
epoch:: 281
epoch:: 282
epoch:: 283
epoch:: 284
epoch:: 285
epoch:: 286
epoch:: 287
epoch:: 288
epoch:: 289
epoch:: 290
epoch:: 291
epoch:: 292
epoch:: 293
epoch:: 294
epoch:: 295
epoch:: 296
epoch:: 297
epoch:: 298
epoch:: 299
epoch:: 300
epoch:: 301
epoch:: 302
epoch:: 303
epoch:: 304
epoch:: 305
epoch:: 306
epoch:: 307
epoch:: 308
epoch:: 309
epoch:: 310
epoch:: 311
epoch:: 312
epoch:: 313
epoch:: 314
epoch:: 315
epoch:: 316
epoch:: 317
epoch:: 318
epoch:: 319
epoch:: 320
epoch:: 321
epoch:: 322
epoch:: 323
epoch:: 324
epoch:: 325
epoch:: 326
epoch:: 327
epoch:: 328
epoch:: 329
epoch:: 330
epoch:: 331
epoch:: 332
epoch:: 333
epoch:: 334
epoch:: 335
epoch:: 336
epoch:: 337
epoch:: 338
epoch:: 339
epoch:: 340
epoch:: 341
epoch:: 342
epoch:: 343
epoch:: 344
epoch:: 345
epoch:: 346
epoch:: 347
epoch:: 348
epoch:: 349
epoch:: 350
epoch:: 351
epoch:: 352
epoch:: 353
epoch:: 354
epoch:: 355
epoch:: 356
epoch:: 357
epoch:: 358
epoch:: 359
epoch:: 360
epoch:: 361
epoch:: 362
epoch:: 363
epoch:: 364
epoch:: 365
epoch:: 366
epoch:: 367
epoch:: 368
epoch:: 369
epoch:: 370
epoch:: 371
epoch:: 372
epoch:: 373
epoch:: 374
epoch:: 375
epoch:: 376
epoch:: 377
epoch:: 378
epoch:: 379
epoch:: 380
epoch:: 381
epoch:: 382
epoch:: 383
epoch:: 384
epoch:: 385
epoch:: 386
epoch:: 387
epoch:: 388
epoch:: 389
epoch:: 390
epoch:: 391
epoch:: 392
epoch:: 393
epoch:: 394
epoch:: 395
epoch:: 396
epoch:: 397
epoch:: 398
epoch:: 399
epoch:: 400
epoch:: 401
epoch:: 402
epoch:: 403
epoch:: 404
epoch:: 405
epoch:: 406
epoch:: 407
epoch:: 408
epoch:: 409
epoch:: 410
epoch:: 411
epoch:: 412
epoch:: 413
epoch:: 414
epoch:: 415
epoch:: 416
epoch:: 417
epoch:: 418
epoch:: 419
epoch:: 420
epoch:: 421
epoch:: 422
epoch:: 423
epoch:: 424
epoch:: 425
epoch:: 426
epoch:: 427
epoch:: 428
epoch:: 429
epoch:: 430
epoch:: 431
epoch:: 432
epoch:: 433
epoch:: 434
epoch:: 435
epoch:: 436
epoch:: 437
epoch:: 438
epoch:: 439
epoch:: 440
epoch:: 441
epoch:: 442
epoch:: 443
epoch:: 444
epoch:: 445
epoch:: 446
epoch:: 447
epoch:: 448
epoch:: 449
epoch:: 450
epoch:: 451
epoch:: 452
epoch:: 453
epoch:: 454
epoch:: 455
epoch:: 456
epoch:: 457
epoch:: 458
epoch:: 459
epoch:: 460
epoch:: 461
epoch:: 462
epoch:: 463
epoch:: 464
epoch:: 465
epoch:: 466
epoch:: 467
epoch:: 468
epoch:: 469
epoch:: 470
epoch:: 471
epoch:: 472
epoch:: 473
epoch:: 474
epoch:: 475
epoch:: 476
epoch:: 477
epoch:: 478
epoch:: 479
epoch:: 480
epoch:: 481
epoch:: 482
epoch:: 483
epoch:: 484
epoch:: 485
epoch:: 486
epoch:: 487
epoch:: 488
epoch:: 489
epoch:: 490
epoch:: 491
epoch:: 492
epoch:: 493
epoch:: 494
epoch:: 495
epoch:: 496
epoch:: 497
epoch:: 498
epoch:: 499
training time is 80.05425667762756
overhead:: 0
overhead2:: 1.8062841892242432
overhead3:: 0
time_baseline:: 80.05428647994995
curr_diff: 0 tensor(0.0193, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0193, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632000
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 5
max_epoch:: 500
overhead:: 0
overhead2:: 0.05018877983093262
overhead3:: 0.24898672103881836
overhead4:: 9.717868089675903
overhead5:: 0
memory usage:: 5631139840
time_provenance:: 16.632269382476807
curr_diff: 0 tensor(0.0048, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0048, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0230, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0230, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632200
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 5
max_epoch:: 500
overhead:: 0
overhead2:: 0.051636695861816406
overhead3:: 0.2580866813659668
overhead4:: 10.149041414260864
overhead5:: 0
memory usage:: 5627371520
time_provenance:: 17.0893337726593
curr_diff: 0 tensor(0.0048, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0048, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0230, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0230, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632200
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 5
max_epoch:: 500
overhead:: 0
overhead2:: 0.05444192886352539
overhead3:: 0.2632169723510742
overhead4:: 10.40728759765625
overhead5:: 0
memory usage:: 5643698176
time_provenance:: 17.403873920440674
curr_diff: 0 tensor(0.0048, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0048, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0230, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0230, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632200
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 5
max_epoch:: 500
overhead:: 0
overhead2:: 0.11651396751403809
overhead3:: 0.44074487686157227
overhead4:: 17.75010323524475
overhead5:: 0
memory usage:: 5640155136
time_provenance:: 26.64548921585083
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0196, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0196, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632100
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 5
max_epoch:: 500
overhead:: 0
overhead2:: 0.10564494132995605
overhead3:: 0.4465446472167969
overhead4:: 17.882290601730347
overhead5:: 0
memory usage:: 5652217856
time_provenance:: 26.824899911880493
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0196, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0196, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632100
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 5
max_epoch:: 500
overhead:: 0
overhead2:: 0.12618041038513184
overhead3:: 0.41678667068481445
overhead4:: 17.502305030822754
overhead5:: 0
memory usage:: 5631610880
time_provenance:: 26.353431701660156
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0196, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0196, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632100
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 5
max_epoch:: 500
overhead:: 0
overhead2:: 0.26080846786499023
overhead3:: 1.0373687744140625
overhead4:: 43.378825664520264
overhead5:: 0
memory usage:: 5647835136
time_provenance:: 58.32083296775818
curr_diff: 0 tensor(6.9034e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.9034e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0194, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0194, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632000
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 5
max_epoch:: 500
overhead:: 0
overhead2:: 0.26753973960876465
overhead3:: 1.0190889835357666
overhead4:: 43.071317195892334
overhead5:: 0
memory usage:: 5632131072
time_provenance:: 57.9928560256958
curr_diff: 0 tensor(6.9038e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.9038e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0194, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0194, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632000
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 5
max_epoch:: 500
overhead:: 0
overhead2:: 0.260326623916626
overhead3:: 1.0323352813720703
overhead4:: 43.36787796020508
overhead5:: 0
memory usage:: 5646548992
time_provenance:: 58.315662145614624
curr_diff: 0 tensor(6.9075e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.9075e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0194, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0194, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632000
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 0 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 5
max_epoch:: 500
overhead:: 0
overhead2:: 0.5737123489379883
overhead3:: 1.915818452835083
overhead4:: 78.72849655151367
overhead5:: 0
memory usage:: 5615304704
time_provenance:: 95.51616168022156
curr_diff: 0 tensor(4.9971e-13, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.9971e-13, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0193, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0193, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632000
repetition 1
python3 benchmark_exp_lr.py 0.001 10000 500 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05] Logistic_regression cifar10_2 1 0.0001 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 2.314227
Train - Epoch 1, Batch: 0, Loss: 2.252407
Train - Epoch 2, Batch: 0, Loss: 2.196350
Train - Epoch 3, Batch: 0, Loss: 2.145059
Train - Epoch 4, Batch: 0, Loss: 2.097089
Train - Epoch 5, Batch: 0, Loss: 2.055069
Train - Epoch 6, Batch: 0, Loss: 2.012680
Train - Epoch 7, Batch: 0, Loss: 1.974761
Train - Epoch 8, Batch: 0, Loss: 1.935004
Train - Epoch 9, Batch: 0, Loss: 1.903510
Train - Epoch 10, Batch: 0, Loss: 1.874670
Train - Epoch 11, Batch: 0, Loss: 1.847450
Train - Epoch 12, Batch: 0, Loss: 1.814675
Train - Epoch 13, Batch: 0, Loss: 1.791252
Train - Epoch 14, Batch: 0, Loss: 1.758220
Train - Epoch 15, Batch: 0, Loss: 1.732494
Train - Epoch 16, Batch: 0, Loss: 1.723244
Train - Epoch 17, Batch: 0, Loss: 1.693204
Train - Epoch 18, Batch: 0, Loss: 1.683309
Train - Epoch 19, Batch: 0, Loss: 1.661785
Train - Epoch 20, Batch: 0, Loss: 1.641023
Train - Epoch 21, Batch: 0, Loss: 1.630319
Train - Epoch 22, Batch: 0, Loss: 1.610284
Train - Epoch 23, Batch: 0, Loss: 1.599153
Train - Epoch 24, Batch: 0, Loss: 1.582410
Train - Epoch 25, Batch: 0, Loss: 1.564778
Train - Epoch 26, Batch: 0, Loss: 1.555030
Train - Epoch 27, Batch: 0, Loss: 1.549717
Train - Epoch 28, Batch: 0, Loss: 1.528038
Train - Epoch 29, Batch: 0, Loss: 1.520892
Train - Epoch 30, Batch: 0, Loss: 1.506015
Train - Epoch 31, Batch: 0, Loss: 1.498481
Train - Epoch 32, Batch: 0, Loss: 1.493775
Train - Epoch 33, Batch: 0, Loss: 1.481810
Train - Epoch 34, Batch: 0, Loss: 1.478919
Train - Epoch 35, Batch: 0, Loss: 1.467603
Train - Epoch 36, Batch: 0, Loss: 1.442254
Train - Epoch 37, Batch: 0, Loss: 1.448005
Train - Epoch 38, Batch: 0, Loss: 1.436143
Train - Epoch 39, Batch: 0, Loss: 1.431381
Train - Epoch 40, Batch: 0, Loss: 1.421955
Train - Epoch 41, Batch: 0, Loss: 1.414558
Train - Epoch 42, Batch: 0, Loss: 1.419809
Train - Epoch 43, Batch: 0, Loss: 1.404988
Train - Epoch 44, Batch: 0, Loss: 1.394161
Train - Epoch 45, Batch: 0, Loss: 1.386259
Train - Epoch 46, Batch: 0, Loss: 1.383025
Train - Epoch 47, Batch: 0, Loss: 1.386152
Train - Epoch 48, Batch: 0, Loss: 1.370971
Train - Epoch 49, Batch: 0, Loss: 1.361818
Train - Epoch 50, Batch: 0, Loss: 1.353754
Train - Epoch 51, Batch: 0, Loss: 1.361393
Train - Epoch 52, Batch: 0, Loss: 1.355118
Train - Epoch 53, Batch: 0, Loss: 1.346671
Train - Epoch 54, Batch: 0, Loss: 1.334220
Train - Epoch 55, Batch: 0, Loss: 1.337993
Train - Epoch 56, Batch: 0, Loss: 1.334578
Train - Epoch 57, Batch: 0, Loss: 1.335567
Train - Epoch 58, Batch: 0, Loss: 1.330286
Train - Epoch 59, Batch: 0, Loss: 1.322093
Train - Epoch 60, Batch: 0, Loss: 1.311037
Train - Epoch 61, Batch: 0, Loss: 1.317051
Train - Epoch 62, Batch: 0, Loss: 1.314986
Train - Epoch 63, Batch: 0, Loss: 1.304268
Train - Epoch 64, Batch: 0, Loss: 1.300522
Train - Epoch 65, Batch: 0, Loss: 1.298143
Train - Epoch 66, Batch: 0, Loss: 1.299242
Train - Epoch 67, Batch: 0, Loss: 1.289330
Train - Epoch 68, Batch: 0, Loss: 1.291812
Train - Epoch 69, Batch: 0, Loss: 1.293070
Train - Epoch 70, Batch: 0, Loss: 1.281633
Train - Epoch 71, Batch: 0, Loss: 1.271083
Train - Epoch 72, Batch: 0, Loss: 1.277945
Train - Epoch 73, Batch: 0, Loss: 1.275245
Train - Epoch 74, Batch: 0, Loss: 1.263693
Train - Epoch 75, Batch: 0, Loss: 1.264401
Train - Epoch 76, Batch: 0, Loss: 1.268421
Train - Epoch 77, Batch: 0, Loss: 1.264399
Train - Epoch 78, Batch: 0, Loss: 1.267566
Train - Epoch 79, Batch: 0, Loss: 1.261770
Train - Epoch 80, Batch: 0, Loss: 1.252215
Train - Epoch 81, Batch: 0, Loss: 1.265387
Train - Epoch 82, Batch: 0, Loss: 1.248653
Train - Epoch 83, Batch: 0, Loss: 1.245927
Train - Epoch 84, Batch: 0, Loss: 1.234830
Train - Epoch 85, Batch: 0, Loss: 1.248195
Train - Epoch 86, Batch: 0, Loss: 1.253890
Train - Epoch 87, Batch: 0, Loss: 1.246016
Train - Epoch 88, Batch: 0, Loss: 1.231659
Train - Epoch 89, Batch: 0, Loss: 1.233971
Train - Epoch 90, Batch: 0, Loss: 1.232705
Train - Epoch 91, Batch: 0, Loss: 1.235833
Train - Epoch 92, Batch: 0, Loss: 1.232277
Train - Epoch 93, Batch: 0, Loss: 1.230217
Train - Epoch 94, Batch: 0, Loss: 1.233535
Train - Epoch 95, Batch: 0, Loss: 1.228530
Train - Epoch 96, Batch: 0, Loss: 1.214180
Train - Epoch 97, Batch: 0, Loss: 1.218099
Train - Epoch 98, Batch: 0, Loss: 1.220279
Train - Epoch 99, Batch: 0, Loss: 1.209345
Train - Epoch 100, Batch: 0, Loss: 1.209802
Train - Epoch 101, Batch: 0, Loss: 1.217103
Train - Epoch 102, Batch: 0, Loss: 1.213652
Train - Epoch 103, Batch: 0, Loss: 1.216849
Train - Epoch 104, Batch: 0, Loss: 1.221966
Train - Epoch 105, Batch: 0, Loss: 1.209267
Train - Epoch 106, Batch: 0, Loss: 1.211718
Train - Epoch 107, Batch: 0, Loss: 1.207254
Train - Epoch 108, Batch: 0, Loss: 1.207449
Train - Epoch 109, Batch: 0, Loss: 1.189264
Train - Epoch 110, Batch: 0, Loss: 1.193995
Train - Epoch 111, Batch: 0, Loss: 1.195855
Train - Epoch 112, Batch: 0, Loss: 1.204104
Train - Epoch 113, Batch: 0, Loss: 1.188387
Train - Epoch 114, Batch: 0, Loss: 1.205086
Train - Epoch 115, Batch: 0, Loss: 1.191095
Train - Epoch 116, Batch: 0, Loss: 1.176825
Train - Epoch 117, Batch: 0, Loss: 1.186742
Train - Epoch 118, Batch: 0, Loss: 1.188854
Train - Epoch 119, Batch: 0, Loss: 1.188030
Train - Epoch 120, Batch: 0, Loss: 1.182437
Train - Epoch 121, Batch: 0, Loss: 1.185549
Train - Epoch 122, Batch: 0, Loss: 1.189058
Train - Epoch 123, Batch: 0, Loss: 1.195625
Train - Epoch 124, Batch: 0, Loss: 1.171672
Train - Epoch 125, Batch: 0, Loss: 1.173888
Train - Epoch 126, Batch: 0, Loss: 1.199381
Train - Epoch 127, Batch: 0, Loss: 1.177237
Train - Epoch 128, Batch: 0, Loss: 1.174515
Train - Epoch 129, Batch: 0, Loss: 1.160940
Train - Epoch 130, Batch: 0, Loss: 1.172030
Train - Epoch 131, Batch: 0, Loss: 1.163533
Train - Epoch 132, Batch: 0, Loss: 1.163938
Train - Epoch 133, Batch: 0, Loss: 1.170501
Train - Epoch 134, Batch: 0, Loss: 1.165613
Train - Epoch 135, Batch: 0, Loss: 1.167128
Train - Epoch 136, Batch: 0, Loss: 1.172252
Train - Epoch 137, Batch: 0, Loss: 1.159221
Train - Epoch 138, Batch: 0, Loss: 1.161568
Train - Epoch 139, Batch: 0, Loss: 1.178266
Train - Epoch 140, Batch: 0, Loss: 1.161684
Train - Epoch 141, Batch: 0, Loss: 1.162628
Train - Epoch 142, Batch: 0, Loss: 1.175807
Train - Epoch 143, Batch: 0, Loss: 1.165157
Train - Epoch 144, Batch: 0, Loss: 1.156292
Train - Epoch 145, Batch: 0, Loss: 1.155113
Train - Epoch 146, Batch: 0, Loss: 1.167015
Train - Epoch 147, Batch: 0, Loss: 1.158350
Train - Epoch 148, Batch: 0, Loss: 1.165114
Train - Epoch 149, Batch: 0, Loss: 1.167493
Train - Epoch 150, Batch: 0, Loss: 1.152372
Train - Epoch 151, Batch: 0, Loss: 1.159426
Train - Epoch 152, Batch: 0, Loss: 1.139013
Train - Epoch 153, Batch: 0, Loss: 1.154241
Train - Epoch 154, Batch: 0, Loss: 1.154614
Train - Epoch 155, Batch: 0, Loss: 1.154747
Train - Epoch 156, Batch: 0, Loss: 1.138934
Train - Epoch 157, Batch: 0, Loss: 1.164469
Train - Epoch 158, Batch: 0, Loss: 1.144002
Train - Epoch 159, Batch: 0, Loss: 1.161829
Train - Epoch 160, Batch: 0, Loss: 1.148600
Train - Epoch 161, Batch: 0, Loss: 1.144180
Train - Epoch 162, Batch: 0, Loss: 1.153037
Train - Epoch 163, Batch: 0, Loss: 1.152265
Train - Epoch 164, Batch: 0, Loss: 1.142951
Train - Epoch 165, Batch: 0, Loss: 1.145951
Train - Epoch 166, Batch: 0, Loss: 1.151428
Train - Epoch 167, Batch: 0, Loss: 1.132964
Train - Epoch 168, Batch: 0, Loss: 1.141507
Train - Epoch 169, Batch: 0, Loss: 1.127122
Train - Epoch 170, Batch: 0, Loss: 1.144533
Train - Epoch 171, Batch: 0, Loss: 1.131508
Train - Epoch 172, Batch: 0, Loss: 1.136371
Train - Epoch 173, Batch: 0, Loss: 1.128695
Train - Epoch 174, Batch: 0, Loss: 1.132078
Train - Epoch 175, Batch: 0, Loss: 1.143748
Train - Epoch 176, Batch: 0, Loss: 1.135821
Train - Epoch 177, Batch: 0, Loss: 1.143711
Train - Epoch 178, Batch: 0, Loss: 1.144393
Train - Epoch 179, Batch: 0, Loss: 1.132115
Train - Epoch 180, Batch: 0, Loss: 1.130558
Train - Epoch 181, Batch: 0, Loss: 1.126549
Train - Epoch 182, Batch: 0, Loss: 1.124292
Train - Epoch 183, Batch: 0, Loss: 1.129033
Train - Epoch 184, Batch: 0, Loss: 1.137475
Train - Epoch 185, Batch: 0, Loss: 1.127387
Train - Epoch 186, Batch: 0, Loss: 1.151805
Train - Epoch 187, Batch: 0, Loss: 1.121637
Train - Epoch 188, Batch: 0, Loss: 1.128470
Train - Epoch 189, Batch: 0, Loss: 1.126284
Train - Epoch 190, Batch: 0, Loss: 1.131372
Train - Epoch 191, Batch: 0, Loss: 1.132407
Train - Epoch 192, Batch: 0, Loss: 1.130412
Train - Epoch 193, Batch: 0, Loss: 1.114038
Train - Epoch 194, Batch: 0, Loss: 1.130377
Train - Epoch 195, Batch: 0, Loss: 1.121112
Train - Epoch 196, Batch: 0, Loss: 1.126243
Train - Epoch 197, Batch: 0, Loss: 1.124583
Train - Epoch 198, Batch: 0, Loss: 1.113918
Train - Epoch 199, Batch: 0, Loss: 1.107447
Train - Epoch 200, Batch: 0, Loss: 1.118745
Train - Epoch 201, Batch: 0, Loss: 1.109688
Train - Epoch 202, Batch: 0, Loss: 1.118579
Train - Epoch 203, Batch: 0, Loss: 1.117950
Train - Epoch 204, Batch: 0, Loss: 1.123484
Train - Epoch 205, Batch: 0, Loss: 1.124534
Train - Epoch 206, Batch: 0, Loss: 1.133136
Train - Epoch 207, Batch: 0, Loss: 1.103536
Train - Epoch 208, Batch: 0, Loss: 1.109180
Train - Epoch 209, Batch: 0, Loss: 1.098001
Train - Epoch 210, Batch: 0, Loss: 1.110727
Train - Epoch 211, Batch: 0, Loss: 1.115732
Train - Epoch 212, Batch: 0, Loss: 1.108580
Train - Epoch 213, Batch: 0, Loss: 1.114834
Train - Epoch 214, Batch: 0, Loss: 1.117573
Train - Epoch 215, Batch: 0, Loss: 1.112344
Train - Epoch 216, Batch: 0, Loss: 1.109275
Train - Epoch 217, Batch: 0, Loss: 1.114175
Train - Epoch 218, Batch: 0, Loss: 1.105615
Train - Epoch 219, Batch: 0, Loss: 1.097017
Train - Epoch 220, Batch: 0, Loss: 1.103251
Train - Epoch 221, Batch: 0, Loss: 1.114172
Train - Epoch 222, Batch: 0, Loss: 1.107254
Train - Epoch 223, Batch: 0, Loss: 1.110333
Train - Epoch 224, Batch: 0, Loss: 1.096242
Train - Epoch 225, Batch: 0, Loss: 1.117160
Train - Epoch 226, Batch: 0, Loss: 1.104653
Train - Epoch 227, Batch: 0, Loss: 1.110681
Train - Epoch 228, Batch: 0, Loss: 1.089410
Train - Epoch 229, Batch: 0, Loss: 1.104919
Train - Epoch 230, Batch: 0, Loss: 1.099379
Train - Epoch 231, Batch: 0, Loss: 1.105671
Train - Epoch 232, Batch: 0, Loss: 1.099089
Train - Epoch 233, Batch: 0, Loss: 1.081402
Train - Epoch 234, Batch: 0, Loss: 1.097878
Train - Epoch 235, Batch: 0, Loss: 1.103964
Train - Epoch 236, Batch: 0, Loss: 1.105490
Train - Epoch 237, Batch: 0, Loss: 1.115707
Train - Epoch 238, Batch: 0, Loss: 1.097185
Train - Epoch 239, Batch: 0, Loss: 1.100706
Train - Epoch 240, Batch: 0, Loss: 1.080978
Train - Epoch 241, Batch: 0, Loss: 1.115002
Train - Epoch 242, Batch: 0, Loss: 1.109989
Train - Epoch 243, Batch: 0, Loss: 1.097803
Train - Epoch 244, Batch: 0, Loss: 1.101873
Train - Epoch 245, Batch: 0, Loss: 1.098996
Train - Epoch 246, Batch: 0, Loss: 1.092535
Train - Epoch 247, Batch: 0, Loss: 1.090077
Train - Epoch 248, Batch: 0, Loss: 1.097203
Train - Epoch 249, Batch: 0, Loss: 1.091990
Train - Epoch 250, Batch: 0, Loss: 1.087031
Train - Epoch 251, Batch: 0, Loss: 1.090154
Train - Epoch 252, Batch: 0, Loss: 1.088283
Train - Epoch 253, Batch: 0, Loss: 1.100634
Train - Epoch 254, Batch: 0, Loss: 1.083755
Train - Epoch 255, Batch: 0, Loss: 1.092225
Train - Epoch 256, Batch: 0, Loss: 1.094738
Train - Epoch 257, Batch: 0, Loss: 1.095815
Train - Epoch 258, Batch: 0, Loss: 1.095576
Train - Epoch 259, Batch: 0, Loss: 1.083533
Train - Epoch 260, Batch: 0, Loss: 1.085600
Train - Epoch 261, Batch: 0, Loss: 1.076566
Train - Epoch 262, Batch: 0, Loss: 1.092660
Train - Epoch 263, Batch: 0, Loss: 1.084898
Train - Epoch 264, Batch: 0, Loss: 1.089780
Train - Epoch 265, Batch: 0, Loss: 1.081012
Train - Epoch 266, Batch: 0, Loss: 1.088434
Train - Epoch 267, Batch: 0, Loss: 1.081883
Train - Epoch 268, Batch: 0, Loss: 1.088437
Train - Epoch 269, Batch: 0, Loss: 1.088746
Train - Epoch 270, Batch: 0, Loss: 1.082158
Train - Epoch 271, Batch: 0, Loss: 1.093885
Train - Epoch 272, Batch: 0, Loss: 1.093919
Train - Epoch 273, Batch: 0, Loss: 1.085951
Train - Epoch 274, Batch: 0, Loss: 1.080535
Train - Epoch 275, Batch: 0, Loss: 1.080717
Train - Epoch 276, Batch: 0, Loss: 1.090031
Train - Epoch 277, Batch: 0, Loss: 1.082028
Train - Epoch 278, Batch: 0, Loss: 1.087484
Train - Epoch 279, Batch: 0, Loss: 1.077922
Train - Epoch 280, Batch: 0, Loss: 1.087632
Train - Epoch 281, Batch: 0, Loss: 1.089617
Train - Epoch 282, Batch: 0, Loss: 1.079646
Train - Epoch 283, Batch: 0, Loss: 1.084514
Train - Epoch 284, Batch: 0, Loss: 1.093775
Train - Epoch 285, Batch: 0, Loss: 1.076489
Train - Epoch 286, Batch: 0, Loss: 1.082794
Train - Epoch 287, Batch: 0, Loss: 1.076164
Train - Epoch 288, Batch: 0, Loss: 1.078780
Train - Epoch 289, Batch: 0, Loss: 1.076963
Train - Epoch 290, Batch: 0, Loss: 1.077439
Train - Epoch 291, Batch: 0, Loss: 1.092234
Train - Epoch 292, Batch: 0, Loss: 1.078316
Train - Epoch 293, Batch: 0, Loss: 1.068147
Train - Epoch 294, Batch: 0, Loss: 1.082998
Train - Epoch 295, Batch: 0, Loss: 1.077547
Train - Epoch 296, Batch: 0, Loss: 1.082706
Train - Epoch 297, Batch: 0, Loss: 1.078470
Train - Epoch 298, Batch: 0, Loss: 1.080983
Train - Epoch 299, Batch: 0, Loss: 1.064014
Train - Epoch 300, Batch: 0, Loss: 1.074650
Train - Epoch 301, Batch: 0, Loss: 1.080903
Train - Epoch 302, Batch: 0, Loss: 1.074381
Train - Epoch 303, Batch: 0, Loss: 1.087714
Train - Epoch 304, Batch: 0, Loss: 1.064700
Train - Epoch 305, Batch: 0, Loss: 1.089038
Train - Epoch 306, Batch: 0, Loss: 1.089384
Train - Epoch 307, Batch: 0, Loss: 1.063247
Train - Epoch 308, Batch: 0, Loss: 1.062591
Train - Epoch 309, Batch: 0, Loss: 1.070060
Train - Epoch 310, Batch: 0, Loss: 1.067018
Train - Epoch 311, Batch: 0, Loss: 1.066698
Train - Epoch 312, Batch: 0, Loss: 1.091446
Train - Epoch 313, Batch: 0, Loss: 1.080186
Train - Epoch 314, Batch: 0, Loss: 1.066205
Train - Epoch 315, Batch: 0, Loss: 1.073847
Train - Epoch 316, Batch: 0, Loss: 1.082503
Train - Epoch 317, Batch: 0, Loss: 1.067583
Train - Epoch 318, Batch: 0, Loss: 1.075610
Train - Epoch 319, Batch: 0, Loss: 1.068181
Train - Epoch 320, Batch: 0, Loss: 1.084872
Train - Epoch 321, Batch: 0, Loss: 1.069776
Train - Epoch 322, Batch: 0, Loss: 1.078692
Train - Epoch 323, Batch: 0, Loss: 1.074379
Train - Epoch 324, Batch: 0, Loss: 1.058000
Train - Epoch 325, Batch: 0, Loss: 1.073096
Train - Epoch 326, Batch: 0, Loss: 1.071751
Train - Epoch 327, Batch: 0, Loss: 1.077951
Train - Epoch 328, Batch: 0, Loss: 1.048171
Train - Epoch 329, Batch: 0, Loss: 1.076921
Train - Epoch 330, Batch: 0, Loss: 1.057866
Train - Epoch 331, Batch: 0, Loss: 1.079801
Train - Epoch 332, Batch: 0, Loss: 1.076607
Train - Epoch 333, Batch: 0, Loss: 1.066028
Train - Epoch 334, Batch: 0, Loss: 1.061980
Train - Epoch 335, Batch: 0, Loss: 1.059180
Train - Epoch 336, Batch: 0, Loss: 1.067449
Train - Epoch 337, Batch: 0, Loss: 1.073919
Train - Epoch 338, Batch: 0, Loss: 1.070637
Train - Epoch 339, Batch: 0, Loss: 1.060661
Train - Epoch 340, Batch: 0, Loss: 1.070250
Train - Epoch 341, Batch: 0, Loss: 1.066257
Train - Epoch 342, Batch: 0, Loss: 1.058432
Train - Epoch 343, Batch: 0, Loss: 1.058768
Train - Epoch 344, Batch: 0, Loss: 1.077070
Train - Epoch 345, Batch: 0, Loss: 1.075464
Train - Epoch 346, Batch: 0, Loss: 1.070086
Train - Epoch 347, Batch: 0, Loss: 1.059971
Train - Epoch 348, Batch: 0, Loss: 1.062951
Train - Epoch 349, Batch: 0, Loss: 1.071097
Train - Epoch 350, Batch: 0, Loss: 1.076524
Train - Epoch 351, Batch: 0, Loss: 1.058475
Train - Epoch 352, Batch: 0, Loss: 1.068136
Train - Epoch 353, Batch: 0, Loss: 1.060788
Train - Epoch 354, Batch: 0, Loss: 1.045202
Train - Epoch 355, Batch: 0, Loss: 1.063274
Train - Epoch 356, Batch: 0, Loss: 1.062340
Train - Epoch 357, Batch: 0, Loss: 1.060634
Train - Epoch 358, Batch: 0, Loss: 1.071268
Train - Epoch 359, Batch: 0, Loss: 1.060055
Train - Epoch 360, Batch: 0, Loss: 1.064225
Train - Epoch 361, Batch: 0, Loss: 1.065541
Train - Epoch 362, Batch: 0, Loss: 1.075799
Train - Epoch 363, Batch: 0, Loss: 1.053126
Train - Epoch 364, Batch: 0, Loss: 1.043730
Train - Epoch 365, Batch: 0, Loss: 1.059722
Train - Epoch 366, Batch: 0, Loss: 1.049159
Train - Epoch 367, Batch: 0, Loss: 1.045213
Train - Epoch 368, Batch: 0, Loss: 1.060663
Train - Epoch 369, Batch: 0, Loss: 1.057022
Train - Epoch 370, Batch: 0, Loss: 1.069269
Train - Epoch 371, Batch: 0, Loss: 1.051958
Train - Epoch 372, Batch: 0, Loss: 1.059405
Train - Epoch 373, Batch: 0, Loss: 1.048118/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 374, Batch: 0, Loss: 1.068322
Train - Epoch 375, Batch: 0, Loss: 1.066388
Train - Epoch 376, Batch: 0, Loss: 1.045517
Train - Epoch 377, Batch: 0, Loss: 1.067128
Train - Epoch 378, Batch: 0, Loss: 1.064753
Train - Epoch 379, Batch: 0, Loss: 1.058165
Train - Epoch 380, Batch: 0, Loss: 1.041739
Train - Epoch 381, Batch: 0, Loss: 1.054509
Train - Epoch 382, Batch: 0, Loss: 1.058350
Train - Epoch 383, Batch: 0, Loss: 1.058186
Train - Epoch 384, Batch: 0, Loss: 1.051590
Train - Epoch 385, Batch: 0, Loss: 1.062252
Train - Epoch 386, Batch: 0, Loss: 1.077848
Train - Epoch 387, Batch: 0, Loss: 1.050021
Train - Epoch 388, Batch: 0, Loss: 1.061600
Train - Epoch 389, Batch: 0, Loss: 1.058966
Train - Epoch 390, Batch: 0, Loss: 1.056461
Train - Epoch 391, Batch: 0, Loss: 1.059907
Train - Epoch 392, Batch: 0, Loss: 1.059226
Train - Epoch 393, Batch: 0, Loss: 1.061169
Train - Epoch 394, Batch: 0, Loss: 1.069728
Train - Epoch 395, Batch: 0, Loss: 1.054672
Train - Epoch 396, Batch: 0, Loss: 1.051633
Train - Epoch 397, Batch: 0, Loss: 1.047057
Train - Epoch 398, Batch: 0, Loss: 1.068941
Train - Epoch 399, Batch: 0, Loss: 1.040756
Train - Epoch 400, Batch: 0, Loss: 1.056569
Train - Epoch 401, Batch: 0, Loss: 1.058014
Train - Epoch 402, Batch: 0, Loss: 1.053949
Train - Epoch 403, Batch: 0, Loss: 1.061074
Train - Epoch 404, Batch: 0, Loss: 1.052836
Train - Epoch 405, Batch: 0, Loss: 1.052955
Train - Epoch 406, Batch: 0, Loss: 1.051774
Train - Epoch 407, Batch: 0, Loss: 1.056906
Train - Epoch 408, Batch: 0, Loss: 1.049236
Train - Epoch 409, Batch: 0, Loss: 1.051929
Train - Epoch 410, Batch: 0, Loss: 1.045237
Train - Epoch 411, Batch: 0, Loss: 1.040425
Train - Epoch 412, Batch: 0, Loss: 1.060280
Train - Epoch 413, Batch: 0, Loss: 1.069470
Train - Epoch 414, Batch: 0, Loss: 1.044111
Train - Epoch 415, Batch: 0, Loss: 1.045991
Train - Epoch 416, Batch: 0, Loss: 1.067103
Train - Epoch 417, Batch: 0, Loss: 1.052344
Train - Epoch 418, Batch: 0, Loss: 1.054827
Train - Epoch 419, Batch: 0, Loss: 1.051497
Train - Epoch 420, Batch: 0, Loss: 1.038723
Train - Epoch 421, Batch: 0, Loss: 1.042852
Train - Epoch 422, Batch: 0, Loss: 1.054849
Train - Epoch 423, Batch: 0, Loss: 1.064044
Train - Epoch 424, Batch: 0, Loss: 1.041286
Train - Epoch 425, Batch: 0, Loss: 1.045445
Train - Epoch 426, Batch: 0, Loss: 1.053184
Train - Epoch 427, Batch: 0, Loss: 1.049342
Train - Epoch 428, Batch: 0, Loss: 1.045275
Train - Epoch 429, Batch: 0, Loss: 1.060954
Train - Epoch 430, Batch: 0, Loss: 1.041975
Train - Epoch 431, Batch: 0, Loss: 1.042884
Train - Epoch 432, Batch: 0, Loss: 1.042326
Train - Epoch 433, Batch: 0, Loss: 1.044057
Train - Epoch 434, Batch: 0, Loss: 1.051630
Train - Epoch 435, Batch: 0, Loss: 1.054961
Train - Epoch 436, Batch: 0, Loss: 1.049063
Train - Epoch 437, Batch: 0, Loss: 1.050403
Train - Epoch 438, Batch: 0, Loss: 1.032858
Train - Epoch 439, Batch: 0, Loss: 1.057252
Train - Epoch 440, Batch: 0, Loss: 1.044381
Train - Epoch 441, Batch: 0, Loss: 1.049463
Train - Epoch 442, Batch: 0, Loss: 1.048022
Train - Epoch 443, Batch: 0, Loss: 1.046176
Train - Epoch 444, Batch: 0, Loss: 1.051098
Train - Epoch 445, Batch: 0, Loss: 1.041637
Train - Epoch 446, Batch: 0, Loss: 1.036348
Train - Epoch 447, Batch: 0, Loss: 1.052602
Train - Epoch 448, Batch: 0, Loss: 1.046454
Train - Epoch 449, Batch: 0, Loss: 1.045644
Train - Epoch 450, Batch: 0, Loss: 1.039521
Train - Epoch 451, Batch: 0, Loss: 1.059718
Train - Epoch 452, Batch: 0, Loss: 1.046332
Train - Epoch 453, Batch: 0, Loss: 1.053223
Train - Epoch 454, Batch: 0, Loss: 1.026388
Train - Epoch 455, Batch: 0, Loss: 1.055672
Train - Epoch 456, Batch: 0, Loss: 1.048354
Train - Epoch 457, Batch: 0, Loss: 1.057074
Train - Epoch 458, Batch: 0, Loss: 1.043105
Train - Epoch 459, Batch: 0, Loss: 1.047711
Train - Epoch 460, Batch: 0, Loss: 1.042450
Train - Epoch 461, Batch: 0, Loss: 1.044472
Train - Epoch 462, Batch: 0, Loss: 1.034300
Train - Epoch 463, Batch: 0, Loss: 1.049069
Train - Epoch 464, Batch: 0, Loss: 1.040017
Train - Epoch 465, Batch: 0, Loss: 1.052794
Train - Epoch 466, Batch: 0, Loss: 1.026748
Train - Epoch 467, Batch: 0, Loss: 1.040237
Train - Epoch 468, Batch: 0, Loss: 1.036537
Train - Epoch 469, Batch: 0, Loss: 1.029739
Train - Epoch 470, Batch: 0, Loss: 1.047335
Train - Epoch 471, Batch: 0, Loss: 1.043561
Train - Epoch 472, Batch: 0, Loss: 1.047256
Train - Epoch 473, Batch: 0, Loss: 1.055249
Train - Epoch 474, Batch: 0, Loss: 1.046548
Train - Epoch 475, Batch: 0, Loss: 1.035103
Train - Epoch 476, Batch: 0, Loss: 1.044212
Train - Epoch 477, Batch: 0, Loss: 1.050102
Train - Epoch 478, Batch: 0, Loss: 1.038816
Train - Epoch 479, Batch: 0, Loss: 1.031276
Train - Epoch 480, Batch: 0, Loss: 1.038345
Train - Epoch 481, Batch: 0, Loss: 1.057069
Train - Epoch 482, Batch: 0, Loss: 1.033392
Train - Epoch 483, Batch: 0, Loss: 1.030231
Train - Epoch 484, Batch: 0, Loss: 1.038813
Train - Epoch 485, Batch: 0, Loss: 1.040560
Train - Epoch 486, Batch: 0, Loss: 1.049794
Train - Epoch 487, Batch: 0, Loss: 1.041632
Train - Epoch 488, Batch: 0, Loss: 1.027612
Train - Epoch 489, Batch: 0, Loss: 1.021770
Train - Epoch 490, Batch: 0, Loss: 1.039423
Train - Epoch 491, Batch: 0, Loss: 1.028878
Train - Epoch 492, Batch: 0, Loss: 1.034434
Train - Epoch 493, Batch: 0, Loss: 1.018953
Train - Epoch 494, Batch: 0, Loss: 1.056728
Train - Epoch 495, Batch: 0, Loss: 1.040795
Train - Epoch 496, Batch: 0, Loss: 1.032434
Train - Epoch 497, Batch: 0, Loss: 1.040918
Train - Epoch 498, Batch: 0, Loss: 1.036106
Train - Epoch 499, Batch: 0, Loss: 1.041732
training_time:: 107.86938810348511
training time full:: 107.86945796012878
provenance prepare time:: 4.76837158203125e-07
here
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632900
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 5
max_epoch:: 500
batch_size:: 10000
epoch:: 0
tensor([    0, 32776, 32777,    12, 32780,    15,    16, 32792, 32800,    38,
           42, 32817,    50,    49,    52, 32818,    55, 32825, 32830, 32836,
        32837,    70, 32840, 32841,    78,    79,    91, 32860,    95,    96,
        32866,   100,   103, 32872,   106, 32876, 32880,   116, 32892,   126,
          128,   132,   135, 32904,   141,   145, 32913,   147,   149, 32920])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
epoch:: 60
epoch:: 61
epoch:: 62
epoch:: 63
epoch:: 64
epoch:: 65
epoch:: 66
epoch:: 67
epoch:: 68
epoch:: 69
epoch:: 70
epoch:: 71
epoch:: 72
epoch:: 73
epoch:: 74
epoch:: 75
epoch:: 76
epoch:: 77
epoch:: 78
epoch:: 79
epoch:: 80
epoch:: 81
epoch:: 82
epoch:: 83
epoch:: 84
epoch:: 85
epoch:: 86
epoch:: 87
epoch:: 88
epoch:: 89
epoch:: 90
epoch:: 91
epoch:: 92
epoch:: 93
epoch:: 94
epoch:: 95
epoch:: 96
epoch:: 97
epoch:: 98
epoch:: 99
epoch:: 100
epoch:: 101
epoch:: 102
epoch:: 103
epoch:: 104
epoch:: 105
epoch:: 106
epoch:: 107
epoch:: 108
epoch:: 109
epoch:: 110
epoch:: 111
epoch:: 112
epoch:: 113
epoch:: 114
epoch:: 115
epoch:: 116
epoch:: 117
epoch:: 118
epoch:: 119
epoch:: 120
epoch:: 121
epoch:: 122
epoch:: 123
epoch:: 124
epoch:: 125
epoch:: 126
epoch:: 127
epoch:: 128
epoch:: 129
epoch:: 130
epoch:: 131
epoch:: 132
epoch:: 133
epoch:: 134
epoch:: 135
epoch:: 136
epoch:: 137
epoch:: 138
epoch:: 139
epoch:: 140
epoch:: 141
epoch:: 142
epoch:: 143
epoch:: 144
epoch:: 145
epoch:: 146
epoch:: 147
epoch:: 148
epoch:: 149
epoch:: 150
epoch:: 151
epoch:: 152
epoch:: 153
epoch:: 154
epoch:: 155
epoch:: 156
epoch:: 157
epoch:: 158
epoch:: 159
epoch:: 160
epoch:: 161
epoch:: 162
epoch:: 163
epoch:: 164
epoch:: 165
epoch:: 166
epoch:: 167
epoch:: 168
epoch:: 169
epoch:: 170
epoch:: 171
epoch:: 172
epoch:: 173
epoch:: 174
epoch:: 175
epoch:: 176
epoch:: 177
epoch:: 178
epoch:: 179
epoch:: 180
epoch:: 181
epoch:: 182
epoch:: 183
epoch:: 184
epoch:: 185
epoch:: 186
epoch:: 187
epoch:: 188
epoch:: 189
epoch:: 190
epoch:: 191
epoch:: 192
epoch:: 193
epoch:: 194
epoch:: 195
epoch:: 196
epoch:: 197
epoch:: 198
epoch:: 199
epoch:: 200
epoch:: 201
epoch:: 202
epoch:: 203
epoch:: 204
epoch:: 205
epoch:: 206
epoch:: 207
epoch:: 208
epoch:: 209
epoch:: 210
epoch:: 211
epoch:: 212
epoch:: 213
epoch:: 214
epoch:: 215
epoch:: 216
epoch:: 217
epoch:: 218
epoch:: 219
epoch:: 220
epoch:: 221
epoch:: 222
epoch:: 223
epoch:: 224
epoch:: 225
epoch:: 226
epoch:: 227
epoch:: 228
epoch:: 229
epoch:: 230
epoch:: 231
epoch:: 232
epoch:: 233
epoch:: 234
epoch:: 235
epoch:: 236
epoch:: 237
epoch:: 238
epoch:: 239
epoch:: 240
epoch:: 241
epoch:: 242
epoch:: 243
epoch:: 244
epoch:: 245
epoch:: 246
epoch:: 247
epoch:: 248
epoch:: 249
epoch:: 250
epoch:: 251
epoch:: 252
epoch:: 253
epoch:: 254
epoch:: 255
epoch:: 256
epoch:: 257
epoch:: 258
epoch:: 259
epoch:: 260
epoch:: 261
epoch:: 262
epoch:: 263
epoch:: 264
epoch:: 265
epoch:: 266
epoch:: 267
epoch:: 268
epoch:: 269
epoch:: 270
epoch:: 271
epoch:: 272
epoch:: 273
epoch:: 274
epoch:: 275
epoch:: 276
epoch:: 277
epoch:: 278
epoch:: 279
epoch:: 280
epoch:: 281
epoch:: 282
epoch:: 283
epoch:: 284
epoch:: 285
epoch:: 286
epoch:: 287
epoch:: 288
epoch:: 289
epoch:: 290
epoch:: 291
epoch:: 292
epoch:: 293
epoch:: 294
epoch:: 295
epoch:: 296
epoch:: 297
epoch:: 298
epoch:: 299
epoch:: 300
epoch:: 301
epoch:: 302
epoch:: 303
epoch:: 304
epoch:: 305
epoch:: 306
epoch:: 307
epoch:: 308
epoch:: 309
epoch:: 310
epoch:: 311
epoch:: 312
epoch:: 313
epoch:: 314
epoch:: 315
epoch:: 316
epoch:: 317
epoch:: 318
epoch:: 319
epoch:: 320
epoch:: 321
epoch:: 322
epoch:: 323
epoch:: 324
epoch:: 325
epoch:: 326
epoch:: 327
epoch:: 328
epoch:: 329
epoch:: 330
epoch:: 331
epoch:: 332
epoch:: 333
epoch:: 334
epoch:: 335
epoch:: 336
epoch:: 337
epoch:: 338
epoch:: 339
epoch:: 340
epoch:: 341
epoch:: 342
epoch:: 343
epoch:: 344
epoch:: 345
epoch:: 346
epoch:: 347
epoch:: 348
epoch:: 349
epoch:: 350
epoch:: 351
epoch:: 352
epoch:: 353
epoch:: 354
epoch:: 355
epoch:: 356
epoch:: 357
epoch:: 358
epoch:: 359
epoch:: 360
epoch:: 361
epoch:: 362
epoch:: 363
epoch:: 364
epoch:: 365
epoch:: 366
epoch:: 367
epoch:: 368
epoch:: 369
epoch:: 370
epoch:: 371
epoch:: 372
epoch:: 373
epoch:: 374
epoch:: 375
epoch:: 376
epoch:: 377
epoch:: 378
epoch:: 379
epoch:: 380
epoch:: 381
epoch:: 382
epoch:: 383
epoch:: 384
epoch:: 385
epoch:: 386
epoch:: 387
epoch:: 388
epoch:: 389
epoch:: 390
epoch:: 391
epoch:: 392
epoch:: 393
epoch:: 394
epoch:: 395
epoch:: 396
epoch:: 397
epoch:: 398
epoch:: 399
epoch:: 400
epoch:: 401
epoch:: 402
epoch:: 403
epoch:: 404
epoch:: 405
epoch:: 406
epoch:: 407
epoch:: 408
epoch:: 409
epoch:: 410
epoch:: 411
epoch:: 412
epoch:: 413
epoch:: 414
epoch:: 415
epoch:: 416
epoch:: 417
epoch:: 418
epoch:: 419
epoch:: 420
epoch:: 421
epoch:: 422
epoch:: 423
epoch:: 424
epoch:: 425
epoch:: 426
epoch:: 427
epoch:: 428
epoch:: 429
epoch:: 430
epoch:: 431
epoch:: 432
epoch:: 433
epoch:: 434
epoch:: 435
epoch:: 436
epoch:: 437
epoch:: 438
epoch:: 439
epoch:: 440
epoch:: 441
epoch:: 442
epoch:: 443
epoch:: 444
epoch:: 445
epoch:: 446
epoch:: 447
epoch:: 448
epoch:: 449
epoch:: 450
epoch:: 451
epoch:: 452
epoch:: 453
epoch:: 454
epoch:: 455
epoch:: 456
epoch:: 457
epoch:: 458
epoch:: 459
epoch:: 460
epoch:: 461
epoch:: 462
epoch:: 463
epoch:: 464
epoch:: 465
epoch:: 466
epoch:: 467
epoch:: 468
epoch:: 469
epoch:: 470
epoch:: 471
epoch:: 472
epoch:: 473
epoch:: 474
epoch:: 475
epoch:: 476
epoch:: 477
epoch:: 478
epoch:: 479
epoch:: 480
epoch:: 481
epoch:: 482
epoch:: 483
epoch:: 484
epoch:: 485
epoch:: 486
epoch:: 487
epoch:: 488
epoch:: 489
epoch:: 490
epoch:: 491
epoch:: 492
epoch:: 493
epoch:: 494
epoch:: 495
epoch:: 496
epoch:: 497
epoch:: 498
epoch:: 499
training time is 80.2515959739685
overhead:: 0
overhead2:: 1.822845458984375
overhead3:: 0
time_baseline:: 80.25162816047668
curr_diff: 0 tensor(0.0194, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0194, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632700
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 5
max_epoch:: 500
overhead:: 0
overhead2:: 0.05516791343688965
overhead3:: 0.24035429954528809
overhead4:: 10.047526836395264
overhead5:: 0
memory usage:: 5632258048
time_provenance:: 16.8887882232666
curr_diff: 0 tensor(0.0047, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0047, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0229, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0229, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632700
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 5
max_epoch:: 500
overhead:: 0
overhead2:: 0.05792880058288574
overhead3:: 0.25436973571777344
overhead4:: 10.417784929275513
overhead5:: 0
memory usage:: 5636374528
time_provenance:: 17.381412267684937
curr_diff: 0 tensor(0.0047, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0047, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0230, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0230, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632400
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 5
max_epoch:: 500
overhead:: 0
overhead2:: 0.058374881744384766
overhead3:: 0.25932884216308594
overhead4:: 10.221292972564697
overhead5:: 0
memory usage:: 5627068416
time_provenance:: 17.19347882270813
curr_diff: 0 tensor(0.0047, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0047, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0229, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0229, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632400
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 5
max_epoch:: 500
overhead:: 0
overhead2:: 0.10651159286499023
overhead3:: 0.43918848037719727
overhead4:: 18.40609049797058
overhead5:: 0
memory usage:: 5640753152
time_provenance:: 27.300580501556396
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0197, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0197, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632700
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 5
max_epoch:: 500
overhead:: 0
overhead2:: 0.10672903060913086
overhead3:: 0.4421811103820801
overhead4:: 18.40667700767517
overhead5:: 0
memory usage:: 5628043264
time_provenance:: 27.330538749694824
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0197, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0197, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632700
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 5
max_epoch:: 500
overhead:: 0
overhead2:: 0.1081397533416748
overhead3:: 0.45143914222717285
overhead4:: 18.286437273025513
overhead5:: 0
memory usage:: 5623697408
time_provenance:: 27.244168996810913
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0197, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0197, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632700
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 5
max_epoch:: 500
overhead:: 0
overhead2:: 0.2667369842529297
overhead3:: 1.025132417678833
overhead4:: 44.0661678314209
overhead5:: 0
memory usage:: 5621985280
time_provenance:: 58.98817443847656
curr_diff: 0 tensor(6.3877e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.3877e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0194, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0194, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632700
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 5
max_epoch:: 500
overhead:: 0
overhead2:: 0.27228331565856934
overhead3:: 1.0080909729003906
overhead4:: 42.28921699523926
overhead5:: 0
memory usage:: 5637611520
time_provenance:: 57.18598699569702
curr_diff: 0 tensor(6.4015e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.4015e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0194, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0194, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632700
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 5
max_epoch:: 500
overhead:: 0
overhead2:: 0.2651944160461426
overhead3:: 1.014862060546875
overhead4:: 43.16994547843933
overhead5:: 0
memory usage:: 5623451648
time_provenance:: 58.080241441726685
curr_diff: 0 tensor(6.3972e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.3972e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0194, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0194, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632700
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 1 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 5
max_epoch:: 500
overhead:: 0
overhead2:: 0.5328013896942139
overhead3:: 2.0615530014038086
overhead4:: 80.63978934288025
overhead5:: 0
memory usage:: 5620289536
time_provenance:: 97.55250430107117
curr_diff: 0 tensor(5.0102e-13, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.0102e-13, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0194, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0194, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632700
repetition 2
python3 benchmark_exp_lr.py 0.001 10000 500 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05] Logistic_regression cifar10_2 2 0.0001 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 2.307674
Train - Epoch 1, Batch: 0, Loss: 2.245895
Train - Epoch 2, Batch: 0, Loss: 2.190746
Train - Epoch 3, Batch: 0, Loss: 2.139429
Train - Epoch 4, Batch: 0, Loss: 2.091797
Train - Epoch 5, Batch: 0, Loss: 2.053742
Train - Epoch 6, Batch: 0, Loss: 2.006796
Train - Epoch 7, Batch: 0, Loss: 1.969822
Train - Epoch 8, Batch: 0, Loss: 1.930913
Train - Epoch 9, Batch: 0, Loss: 1.900212
Train - Epoch 10, Batch: 0, Loss: 1.865876
Train - Epoch 11, Batch: 0, Loss: 1.833903
Train - Epoch 12, Batch: 0, Loss: 1.815182
Train - Epoch 13, Batch: 0, Loss: 1.790402
Train - Epoch 14, Batch: 0, Loss: 1.764666
Train - Epoch 15, Batch: 0, Loss: 1.737560
Train - Epoch 16, Batch: 0, Loss: 1.710151
Train - Epoch 17, Batch: 0, Loss: 1.696038
Train - Epoch 18, Batch: 0, Loss: 1.671145
Train - Epoch 19, Batch: 0, Loss: 1.658062
Train - Epoch 20, Batch: 0, Loss: 1.644634
Train - Epoch 21, Batch: 0, Loss: 1.622098
Train - Epoch 22, Batch: 0, Loss: 1.607063
Train - Epoch 23, Batch: 0, Loss: 1.586274
Train - Epoch 24, Batch: 0, Loss: 1.582963
Train - Epoch 25, Batch: 0, Loss: 1.568415
Train - Epoch 26, Batch: 0, Loss: 1.559432
Train - Epoch 27, Batch: 0, Loss: 1.546407
Train - Epoch 28, Batch: 0, Loss: 1.525710
Train - Epoch 29, Batch: 0, Loss: 1.526019
Train - Epoch 30, Batch: 0, Loss: 1.499669
Train - Epoch 31, Batch: 0, Loss: 1.497323
Train - Epoch 32, Batch: 0, Loss: 1.481461
Train - Epoch 33, Batch: 0, Loss: 1.482977
Train - Epoch 34, Batch: 0, Loss: 1.455622
Train - Epoch 35, Batch: 0, Loss: 1.462765
Train - Epoch 36, Batch: 0, Loss: 1.447612
Train - Epoch 37, Batch: 0, Loss: 1.453658
Train - Epoch 38, Batch: 0, Loss: 1.429143
Train - Epoch 39, Batch: 0, Loss: 1.423313
Train - Epoch 40, Batch: 0, Loss: 1.421277
Train - Epoch 41, Batch: 0, Loss: 1.417722
Train - Epoch 42, Batch: 0, Loss: 1.411537
Train - Epoch 43, Batch: 0, Loss: 1.395089
Train - Epoch 44, Batch: 0, Loss: 1.399940
Train - Epoch 45, Batch: 0, Loss: 1.390597
Train - Epoch 46, Batch: 0, Loss: 1.379205
Train - Epoch 47, Batch: 0, Loss: 1.379975
Train - Epoch 48, Batch: 0, Loss: 1.370333
Train - Epoch 49, Batch: 0, Loss: 1.363714
Train - Epoch 50, Batch: 0, Loss: 1.351719
Train - Epoch 51, Batch: 0, Loss: 1.356672
Train - Epoch 52, Batch: 0, Loss: 1.358294
Train - Epoch 53, Batch: 0, Loss: 1.359635
Train - Epoch 54, Batch: 0, Loss: 1.343204
Train - Epoch 55, Batch: 0, Loss: 1.335151
Train - Epoch 56, Batch: 0, Loss: 1.337192
Train - Epoch 57, Batch: 0, Loss: 1.332945
Train - Epoch 58, Batch: 0, Loss: 1.311502
Train - Epoch 59, Batch: 0, Loss: 1.319100
Train - Epoch 60, Batch: 0, Loss: 1.319558
Train - Epoch 61, Batch: 0, Loss: 1.319495
Train - Epoch 62, Batch: 0, Loss: 1.309113
Train - Epoch 63, Batch: 0, Loss: 1.313701
Train - Epoch 64, Batch: 0, Loss: 1.304256
Train - Epoch 65, Batch: 0, Loss: 1.301390
Train - Epoch 66, Batch: 0, Loss: 1.293251
Train - Epoch 67, Batch: 0, Loss: 1.295673
Train - Epoch 68, Batch: 0, Loss: 1.283015
Train - Epoch 69, Batch: 0, Loss: 1.293969
Train - Epoch 70, Batch: 0, Loss: 1.264787
Train - Epoch 71, Batch: 0, Loss: 1.279097
Train - Epoch 72, Batch: 0, Loss: 1.271222
Train - Epoch 73, Batch: 0, Loss: 1.279518
Train - Epoch 74, Batch: 0, Loss: 1.287865
Train - Epoch 75, Batch: 0, Loss: 1.259725
Train - Epoch 76, Batch: 0, Loss: 1.260162
Train - Epoch 77, Batch: 0, Loss: 1.254788
Train - Epoch 78, Batch: 0, Loss: 1.267469
Train - Epoch 79, Batch: 0, Loss: 1.256176
Train - Epoch 80, Batch: 0, Loss: 1.250905
Train - Epoch 81, Batch: 0, Loss: 1.264993
Train - Epoch 82, Batch: 0, Loss: 1.258897
Train - Epoch 83, Batch: 0, Loss: 1.246728
Train - Epoch 84, Batch: 0, Loss: 1.251880
Train - Epoch 85, Batch: 0, Loss: 1.251012
Train - Epoch 86, Batch: 0, Loss: 1.249990
Train - Epoch 87, Batch: 0, Loss: 1.239275
Train - Epoch 88, Batch: 0, Loss: 1.241124
Train - Epoch 89, Batch: 0, Loss: 1.239583
Train - Epoch 90, Batch: 0, Loss: 1.222386
Train - Epoch 91, Batch: 0, Loss: 1.233100
Train - Epoch 92, Batch: 0, Loss: 1.209999
Train - Epoch 93, Batch: 0, Loss: 1.233688
Train - Epoch 94, Batch: 0, Loss: 1.219819
Train - Epoch 95, Batch: 0, Loss: 1.248167
Train - Epoch 96, Batch: 0, Loss: 1.217575
Train - Epoch 97, Batch: 0, Loss: 1.234162
Train - Epoch 98, Batch: 0, Loss: 1.202905
Train - Epoch 99, Batch: 0, Loss: 1.206950
Train - Epoch 100, Batch: 0, Loss: 1.221986
Train - Epoch 101, Batch: 0, Loss: 1.205759
Train - Epoch 102, Batch: 0, Loss: 1.232548
Train - Epoch 103, Batch: 0, Loss: 1.202252
Train - Epoch 104, Batch: 0, Loss: 1.210270
Train - Epoch 105, Batch: 0, Loss: 1.203654
Train - Epoch 106, Batch: 0, Loss: 1.206701
Train - Epoch 107, Batch: 0, Loss: 1.193632
Train - Epoch 108, Batch: 0, Loss: 1.202153
Train - Epoch 109, Batch: 0, Loss: 1.216021
Train - Epoch 110, Batch: 0, Loss: 1.190906
Train - Epoch 111, Batch: 0, Loss: 1.195217
Train - Epoch 112, Batch: 0, Loss: 1.195679
Train - Epoch 113, Batch: 0, Loss: 1.195492
Train - Epoch 114, Batch: 0, Loss: 1.183150
Train - Epoch 115, Batch: 0, Loss: 1.193270
Train - Epoch 116, Batch: 0, Loss: 1.188683
Train - Epoch 117, Batch: 0, Loss: 1.181174
Train - Epoch 118, Batch: 0, Loss: 1.174923
Train - Epoch 119, Batch: 0, Loss: 1.182128
Train - Epoch 120, Batch: 0, Loss: 1.178628
Train - Epoch 121, Batch: 0, Loss: 1.180068
Train - Epoch 122, Batch: 0, Loss: 1.177440
Train - Epoch 123, Batch: 0, Loss: 1.184574
Train - Epoch 124, Batch: 0, Loss: 1.169116
Train - Epoch 125, Batch: 0, Loss: 1.176461
Train - Epoch 126, Batch: 0, Loss: 1.173160
Train - Epoch 127, Batch: 0, Loss: 1.185420
Train - Epoch 128, Batch: 0, Loss: 1.170003
Train - Epoch 129, Batch: 0, Loss: 1.172505
Train - Epoch 130, Batch: 0, Loss: 1.167320
Train - Epoch 131, Batch: 0, Loss: 1.162251
Train - Epoch 132, Batch: 0, Loss: 1.159868
Train - Epoch 133, Batch: 0, Loss: 1.165749
Train - Epoch 134, Batch: 0, Loss: 1.186745
Train - Epoch 135, Batch: 0, Loss: 1.155636
Train - Epoch 136, Batch: 0, Loss: 1.170997
Train - Epoch 137, Batch: 0, Loss: 1.165077
Train - Epoch 138, Batch: 0, Loss: 1.167074
Train - Epoch 139, Batch: 0, Loss: 1.158317
Train - Epoch 140, Batch: 0, Loss: 1.156479
Train - Epoch 141, Batch: 0, Loss: 1.174302
Train - Epoch 142, Batch: 0, Loss: 1.164090
Train - Epoch 143, Batch: 0, Loss: 1.163012
Train - Epoch 144, Batch: 0, Loss: 1.164920
Train - Epoch 145, Batch: 0, Loss: 1.170094
Train - Epoch 146, Batch: 0, Loss: 1.151024
Train - Epoch 147, Batch: 0, Loss: 1.158065
Train - Epoch 148, Batch: 0, Loss: 1.153814
Train - Epoch 149, Batch: 0, Loss: 1.155924
Train - Epoch 150, Batch: 0, Loss: 1.160804
Train - Epoch 151, Batch: 0, Loss: 1.144907
Train - Epoch 152, Batch: 0, Loss: 1.141659
Train - Epoch 153, Batch: 0, Loss: 1.156131
Train - Epoch 154, Batch: 0, Loss: 1.163183
Train - Epoch 155, Batch: 0, Loss: 1.151172
Train - Epoch 156, Batch: 0, Loss: 1.148680
Train - Epoch 157, Batch: 0, Loss: 1.152582
Train - Epoch 158, Batch: 0, Loss: 1.155929
Train - Epoch 159, Batch: 0, Loss: 1.139101
Train - Epoch 160, Batch: 0, Loss: 1.144505
Train - Epoch 161, Batch: 0, Loss: 1.137134
Train - Epoch 162, Batch: 0, Loss: 1.141738
Train - Epoch 163, Batch: 0, Loss: 1.144391
Train - Epoch 164, Batch: 0, Loss: 1.146458
Train - Epoch 165, Batch: 0, Loss: 1.134984
Train - Epoch 166, Batch: 0, Loss: 1.129417
Train - Epoch 167, Batch: 0, Loss: 1.133581
Train - Epoch 168, Batch: 0, Loss: 1.128529
Train - Epoch 169, Batch: 0, Loss: 1.123022
Train - Epoch 170, Batch: 0, Loss: 1.138605
Train - Epoch 171, Batch: 0, Loss: 1.137307
Train - Epoch 172, Batch: 0, Loss: 1.136517
Train - Epoch 173, Batch: 0, Loss: 1.123708
Train - Epoch 174, Batch: 0, Loss: 1.124226
Train - Epoch 175, Batch: 0, Loss: 1.138763
Train - Epoch 176, Batch: 0, Loss: 1.144869
Train - Epoch 177, Batch: 0, Loss: 1.123438
Train - Epoch 178, Batch: 0, Loss: 1.126678
Train - Epoch 179, Batch: 0, Loss: 1.131044
Train - Epoch 180, Batch: 0, Loss: 1.129542
Train - Epoch 181, Batch: 0, Loss: 1.132024
Train - Epoch 182, Batch: 0, Loss: 1.115218
Train - Epoch 183, Batch: 0, Loss: 1.122299
Train - Epoch 184, Batch: 0, Loss: 1.138837
Train - Epoch 185, Batch: 0, Loss: 1.138576
Train - Epoch 186, Batch: 0, Loss: 1.129804
Train - Epoch 187, Batch: 0, Loss: 1.120426
Train - Epoch 188, Batch: 0, Loss: 1.129872
Train - Epoch 189, Batch: 0, Loss: 1.127261
Train - Epoch 190, Batch: 0, Loss: 1.123131
Train - Epoch 191, Batch: 0, Loss: 1.129008
Train - Epoch 192, Batch: 0, Loss: 1.120111
Train - Epoch 193, Batch: 0, Loss: 1.127056
Train - Epoch 194, Batch: 0, Loss: 1.128541
Train - Epoch 195, Batch: 0, Loss: 1.129557
Train - Epoch 196, Batch: 0, Loss: 1.112809
Train - Epoch 197, Batch: 0, Loss: 1.121495
Train - Epoch 198, Batch: 0, Loss: 1.117978
Train - Epoch 199, Batch: 0, Loss: 1.118705
Train - Epoch 200, Batch: 0, Loss: 1.111599
Train - Epoch 201, Batch: 0, Loss: 1.123635
Train - Epoch 202, Batch: 0, Loss: 1.108886
Train - Epoch 203, Batch: 0, Loss: 1.100099
Train - Epoch 204, Batch: 0, Loss: 1.115656
Train - Epoch 205, Batch: 0, Loss: 1.101875
Train - Epoch 206, Batch: 0, Loss: 1.113140
Train - Epoch 207, Batch: 0, Loss: 1.115739
Train - Epoch 208, Batch: 0, Loss: 1.111158
Train - Epoch 209, Batch: 0, Loss: 1.123161
Train - Epoch 210, Batch: 0, Loss: 1.120094
Train - Epoch 211, Batch: 0, Loss: 1.126982
Train - Epoch 212, Batch: 0, Loss: 1.109518
Train - Epoch 213, Batch: 0, Loss: 1.106614
Train - Epoch 214, Batch: 0, Loss: 1.119004
Train - Epoch 215, Batch: 0, Loss: 1.111210
Train - Epoch 216, Batch: 0, Loss: 1.116605
Train - Epoch 217, Batch: 0, Loss: 1.103919
Train - Epoch 218, Batch: 0, Loss: 1.103879
Train - Epoch 219, Batch: 0, Loss: 1.108335
Train - Epoch 220, Batch: 0, Loss: 1.120224
Train - Epoch 221, Batch: 0, Loss: 1.105466
Train - Epoch 222, Batch: 0, Loss: 1.110490
Train - Epoch 223, Batch: 0, Loss: 1.104718
Train - Epoch 224, Batch: 0, Loss: 1.103963
Train - Epoch 225, Batch: 0, Loss: 1.096731
Train - Epoch 226, Batch: 0, Loss: 1.114544
Train - Epoch 227, Batch: 0, Loss: 1.102871
Train - Epoch 228, Batch: 0, Loss: 1.099601
Train - Epoch 229, Batch: 0, Loss: 1.083807
Train - Epoch 230, Batch: 0, Loss: 1.107367
Train - Epoch 231, Batch: 0, Loss: 1.095177
Train - Epoch 232, Batch: 0, Loss: 1.105247
Train - Epoch 233, Batch: 0, Loss: 1.095420
Train - Epoch 234, Batch: 0, Loss: 1.102547
Train - Epoch 235, Batch: 0, Loss: 1.107845
Train - Epoch 236, Batch: 0, Loss: 1.112099
Train - Epoch 237, Batch: 0, Loss: 1.111059
Train - Epoch 238, Batch: 0, Loss: 1.106467
Train - Epoch 239, Batch: 0, Loss: 1.097669
Train - Epoch 240, Batch: 0, Loss: 1.093500
Train - Epoch 241, Batch: 0, Loss: 1.103039
Train - Epoch 242, Batch: 0, Loss: 1.114455
Train - Epoch 243, Batch: 0, Loss: 1.088726
Train - Epoch 244, Batch: 0, Loss: 1.101130
Train - Epoch 245, Batch: 0, Loss: 1.087818
Train - Epoch 246, Batch: 0, Loss: 1.079875
Train - Epoch 247, Batch: 0, Loss: 1.086813
Train - Epoch 248, Batch: 0, Loss: 1.090320
Train - Epoch 249, Batch: 0, Loss: 1.095298
Train - Epoch 250, Batch: 0, Loss: 1.103659
Train - Epoch 251, Batch: 0, Loss: 1.106910
Train - Epoch 252, Batch: 0, Loss: 1.073844
Train - Epoch 253, Batch: 0, Loss: 1.089361
Train - Epoch 254, Batch: 0, Loss: 1.088616
Train - Epoch 255, Batch: 0, Loss: 1.097256
Train - Epoch 256, Batch: 0, Loss: 1.099306
Train - Epoch 257, Batch: 0, Loss: 1.086872
Train - Epoch 258, Batch: 0, Loss: 1.091392
Train - Epoch 259, Batch: 0, Loss: 1.091257
Train - Epoch 260, Batch: 0, Loss: 1.089894
Train - Epoch 261, Batch: 0, Loss: 1.091603
Train - Epoch 262, Batch: 0, Loss: 1.101529
Train - Epoch 263, Batch: 0, Loss: 1.080767
Train - Epoch 264, Batch: 0, Loss: 1.083028
Train - Epoch 265, Batch: 0, Loss: 1.084773
Train - Epoch 266, Batch: 0, Loss: 1.095756
Train - Epoch 267, Batch: 0, Loss: 1.103499
Train - Epoch 268, Batch: 0, Loss: 1.088877
Train - Epoch 269, Batch: 0, Loss: 1.090878
Train - Epoch 270, Batch: 0, Loss: 1.094594
Train - Epoch 271, Batch: 0, Loss: 1.071806
Train - Epoch 272, Batch: 0, Loss: 1.091283
Train - Epoch 273, Batch: 0, Loss: 1.088846
Train - Epoch 274, Batch: 0, Loss: 1.071362
Train - Epoch 275, Batch: 0, Loss: 1.070122
Train - Epoch 276, Batch: 0, Loss: 1.080561
Train - Epoch 277, Batch: 0, Loss: 1.086978
Train - Epoch 278, Batch: 0, Loss: 1.078607
Train - Epoch 279, Batch: 0, Loss: 1.092015
Train - Epoch 280, Batch: 0, Loss: 1.083666
Train - Epoch 281, Batch: 0, Loss: 1.087453
Train - Epoch 282, Batch: 0, Loss: 1.088383
Train - Epoch 283, Batch: 0, Loss: 1.084745
Train - Epoch 284, Batch: 0, Loss: 1.084802
Train - Epoch 285, Batch: 0, Loss: 1.078990
Train - Epoch 286, Batch: 0, Loss: 1.082656
Train - Epoch 287, Batch: 0, Loss: 1.083549
Train - Epoch 288, Batch: 0, Loss: 1.081827
Train - Epoch 289, Batch: 0, Loss: 1.079198
Train - Epoch 290, Batch: 0, Loss: 1.077042
Train - Epoch 291, Batch: 0, Loss: 1.073469
Train - Epoch 292, Batch: 0, Loss: 1.075219
Train - Epoch 293, Batch: 0, Loss: 1.082746
Train - Epoch 294, Batch: 0, Loss: 1.080192
Train - Epoch 295, Batch: 0, Loss: 1.085330
Train - Epoch 296, Batch: 0, Loss: 1.080112
Train - Epoch 297, Batch: 0, Loss: 1.066994
Train - Epoch 298, Batch: 0, Loss: 1.067647
Train - Epoch 299, Batch: 0, Loss: 1.082267
Train - Epoch 300, Batch: 0, Loss: 1.086386
Train - Epoch 301, Batch: 0, Loss: 1.079108
Train - Epoch 302, Batch: 0, Loss: 1.075131
Train - Epoch 303, Batch: 0, Loss: 1.078084
Train - Epoch 304, Batch: 0, Loss: 1.088407
Train - Epoch 305, Batch: 0, Loss: 1.068769
Train - Epoch 306, Batch: 0, Loss: 1.078815
Train - Epoch 307, Batch: 0, Loss: 1.055770
Train - Epoch 308, Batch: 0, Loss: 1.082985
Train - Epoch 309, Batch: 0, Loss: 1.070747
Train - Epoch 310, Batch: 0, Loss: 1.076061
Train - Epoch 311, Batch: 0, Loss: 1.073993
Train - Epoch 312, Batch: 0, Loss: 1.099017
Train - Epoch 313, Batch: 0, Loss: 1.081242
Train - Epoch 314, Batch: 0, Loss: 1.084713
Train - Epoch 315, Batch: 0, Loss: 1.062615
Train - Epoch 316, Batch: 0, Loss: 1.078129
Train - Epoch 317, Batch: 0, Loss: 1.060370
Train - Epoch 318, Batch: 0, Loss: 1.078426
Train - Epoch 319, Batch: 0, Loss: 1.083897
Train - Epoch 320, Batch: 0, Loss: 1.077569
Train - Epoch 321, Batch: 0, Loss: 1.078970
Train - Epoch 322, Batch: 0, Loss: 1.070704
Train - Epoch 323, Batch: 0, Loss: 1.074470
Train - Epoch 324, Batch: 0, Loss: 1.074337
Train - Epoch 325, Batch: 0, Loss: 1.076399
Train - Epoch 326, Batch: 0, Loss: 1.067572
Train - Epoch 327, Batch: 0, Loss: 1.073475
Train - Epoch 328, Batch: 0, Loss: 1.053591
Train - Epoch 329, Batch: 0, Loss: 1.065024
Train - Epoch 330, Batch: 0, Loss: 1.076244
Train - Epoch 331, Batch: 0, Loss: 1.070556
Train - Epoch 332, Batch: 0, Loss: 1.068220
Train - Epoch 333, Batch: 0, Loss: 1.059446
Train - Epoch 334, Batch: 0, Loss: 1.044652
Train - Epoch 335, Batch: 0, Loss: 1.061702
Train - Epoch 336, Batch: 0, Loss: 1.065662
Train - Epoch 337, Batch: 0, Loss: 1.078398
Train - Epoch 338, Batch: 0, Loss: 1.062920
Train - Epoch 339, Batch: 0, Loss: 1.072145
Train - Epoch 340, Batch: 0, Loss: 1.071935
Train - Epoch 341, Batch: 0, Loss: 1.060977
Train - Epoch 342, Batch: 0, Loss: 1.076988
Train - Epoch 343, Batch: 0, Loss: 1.054610
Train - Epoch 344, Batch: 0, Loss: 1.064741
Train - Epoch 345, Batch: 0, Loss: 1.058466
Train - Epoch 346, Batch: 0, Loss: 1.070048
Train - Epoch 347, Batch: 0, Loss: 1.070358
Train - Epoch 348, Batch: 0, Loss: 1.073535
Train - Epoch 349, Batch: 0, Loss: 1.067996
Train - Epoch 350, Batch: 0, Loss: 1.056445
Train - Epoch 351, Batch: 0, Loss: 1.061194
Train - Epoch 352, Batch: 0, Loss: 1.078992
Train - Epoch 353, Batch: 0, Loss: 1.074951
Train - Epoch 354, Batch: 0, Loss: 1.072823
Train - Epoch 355, Batch: 0, Loss: 1.075049
Train - Epoch 356, Batch: 0, Loss: 1.076479
Train - Epoch 357, Batch: 0, Loss: 1.063823
Train - Epoch 358, Batch: 0, Loss: 1.079206
Train - Epoch 359, Batch: 0, Loss: 1.052376
Train - Epoch 360, Batch: 0, Loss: 1.057290
Train - Epoch 361, Batch: 0, Loss: 1.050937
Train - Epoch 362, Batch: 0, Loss: 1.068455
Train - Epoch 363, Batch: 0, Loss: 1.071867
Train - Epoch 364, Batch: 0, Loss: 1.050573
Train - Epoch 365, Batch: 0, Loss: 1.074215
Train - Epoch 366, Batch: 0, Loss: 1.053373
Train - Epoch 367, Batch: 0, Loss: 1.047065
Train - Epoch 368, Batch: 0, Loss: 1.058651
Train - Epoch 369, Batch: 0, Loss: 1.061222
Train - Epoch 370, Batch: 0, Loss: 1.054878
Train - Epoch 371, Batch: 0, Loss: 1.053349
Train - Epoch 372, Batch: 0, Loss: 1.071522
Train - Epoch 373, Batch: 0, Loss: 1.055065/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 374, Batch: 0, Loss: 1.076082
Train - Epoch 375, Batch: 0, Loss: 1.058279
Train - Epoch 376, Batch: 0, Loss: 1.053168
Train - Epoch 377, Batch: 0, Loss: 1.067777
Train - Epoch 378, Batch: 0, Loss: 1.074104
Train - Epoch 379, Batch: 0, Loss: 1.058856
Train - Epoch 380, Batch: 0, Loss: 1.057588
Train - Epoch 381, Batch: 0, Loss: 1.056274
Train - Epoch 382, Batch: 0, Loss: 1.060712
Train - Epoch 383, Batch: 0, Loss: 1.065330
Train - Epoch 384, Batch: 0, Loss: 1.046783
Train - Epoch 385, Batch: 0, Loss: 1.067893
Train - Epoch 386, Batch: 0, Loss: 1.068092
Train - Epoch 387, Batch: 0, Loss: 1.055693
Train - Epoch 388, Batch: 0, Loss: 1.050587
Train - Epoch 389, Batch: 0, Loss: 1.061198
Train - Epoch 390, Batch: 0, Loss: 1.053860
Train - Epoch 391, Batch: 0, Loss: 1.045510
Train - Epoch 392, Batch: 0, Loss: 1.061278
Train - Epoch 393, Batch: 0, Loss: 1.056523
Train - Epoch 394, Batch: 0, Loss: 1.079098
Train - Epoch 395, Batch: 0, Loss: 1.056922
Train - Epoch 396, Batch: 0, Loss: 1.053039
Train - Epoch 397, Batch: 0, Loss: 1.044851
Train - Epoch 398, Batch: 0, Loss: 1.059070
Train - Epoch 399, Batch: 0, Loss: 1.050549
Train - Epoch 400, Batch: 0, Loss: 1.057741
Train - Epoch 401, Batch: 0, Loss: 1.063339
Train - Epoch 402, Batch: 0, Loss: 1.068889
Train - Epoch 403, Batch: 0, Loss: 1.055879
Train - Epoch 404, Batch: 0, Loss: 1.056110
Train - Epoch 405, Batch: 0, Loss: 1.061276
Train - Epoch 406, Batch: 0, Loss: 1.064693
Train - Epoch 407, Batch: 0, Loss: 1.029165
Train - Epoch 408, Batch: 0, Loss: 1.058078
Train - Epoch 409, Batch: 0, Loss: 1.051369
Train - Epoch 410, Batch: 0, Loss: 1.055231
Train - Epoch 411, Batch: 0, Loss: 1.060214
Train - Epoch 412, Batch: 0, Loss: 1.056178
Train - Epoch 413, Batch: 0, Loss: 1.045090
Train - Epoch 414, Batch: 0, Loss: 1.040339
Train - Epoch 415, Batch: 0, Loss: 1.062946
Train - Epoch 416, Batch: 0, Loss: 1.054337
Train - Epoch 417, Batch: 0, Loss: 1.055979
Train - Epoch 418, Batch: 0, Loss: 1.054260
Train - Epoch 419, Batch: 0, Loss: 1.044134
Train - Epoch 420, Batch: 0, Loss: 1.043229
Train - Epoch 421, Batch: 0, Loss: 1.040742
Train - Epoch 422, Batch: 0, Loss: 1.047260
Train - Epoch 423, Batch: 0, Loss: 1.063410
Train - Epoch 424, Batch: 0, Loss: 1.059494
Train - Epoch 425, Batch: 0, Loss: 1.051333
Train - Epoch 426, Batch: 0, Loss: 1.044008
Train - Epoch 427, Batch: 0, Loss: 1.063308
Train - Epoch 428, Batch: 0, Loss: 1.055569
Train - Epoch 429, Batch: 0, Loss: 1.039395
Train - Epoch 430, Batch: 0, Loss: 1.053122
Train - Epoch 431, Batch: 0, Loss: 1.039062
Train - Epoch 432, Batch: 0, Loss: 1.025010
Train - Epoch 433, Batch: 0, Loss: 1.055568
Train - Epoch 434, Batch: 0, Loss: 1.060146
Train - Epoch 435, Batch: 0, Loss: 1.031659
Train - Epoch 436, Batch: 0, Loss: 1.039709
Train - Epoch 437, Batch: 0, Loss: 1.055550
Train - Epoch 438, Batch: 0, Loss: 1.038407
Train - Epoch 439, Batch: 0, Loss: 1.061492
Train - Epoch 440, Batch: 0, Loss: 1.046847
Train - Epoch 441, Batch: 0, Loss: 1.050939
Train - Epoch 442, Batch: 0, Loss: 1.045644
Train - Epoch 443, Batch: 0, Loss: 1.059805
Train - Epoch 444, Batch: 0, Loss: 1.047985
Train - Epoch 445, Batch: 0, Loss: 1.049597
Train - Epoch 446, Batch: 0, Loss: 1.034034
Train - Epoch 447, Batch: 0, Loss: 1.042572
Train - Epoch 448, Batch: 0, Loss: 1.047095
Train - Epoch 449, Batch: 0, Loss: 1.040180
Train - Epoch 450, Batch: 0, Loss: 1.035973
Train - Epoch 451, Batch: 0, Loss: 1.039360
Train - Epoch 452, Batch: 0, Loss: 1.024382
Train - Epoch 453, Batch: 0, Loss: 1.044810
Train - Epoch 454, Batch: 0, Loss: 1.046742
Train - Epoch 455, Batch: 0, Loss: 1.036512
Train - Epoch 456, Batch: 0, Loss: 1.035139
Train - Epoch 457, Batch: 0, Loss: 1.033147
Train - Epoch 458, Batch: 0, Loss: 1.040297
Train - Epoch 459, Batch: 0, Loss: 1.053983
Train - Epoch 460, Batch: 0, Loss: 1.039465
Train - Epoch 461, Batch: 0, Loss: 1.044390
Train - Epoch 462, Batch: 0, Loss: 1.029612
Train - Epoch 463, Batch: 0, Loss: 1.034729
Train - Epoch 464, Batch: 0, Loss: 1.046679
Train - Epoch 465, Batch: 0, Loss: 1.067283
Train - Epoch 466, Batch: 0, Loss: 1.034729
Train - Epoch 467, Batch: 0, Loss: 1.045427
Train - Epoch 468, Batch: 0, Loss: 1.047179
Train - Epoch 469, Batch: 0, Loss: 1.051794
Train - Epoch 470, Batch: 0, Loss: 1.031093
Train - Epoch 471, Batch: 0, Loss: 1.044475
Train - Epoch 472, Batch: 0, Loss: 1.045346
Train - Epoch 473, Batch: 0, Loss: 1.038000
Train - Epoch 474, Batch: 0, Loss: 1.032149
Train - Epoch 475, Batch: 0, Loss: 1.042706
Train - Epoch 476, Batch: 0, Loss: 1.051036
Train - Epoch 477, Batch: 0, Loss: 1.042237
Train - Epoch 478, Batch: 0, Loss: 1.055549
Train - Epoch 479, Batch: 0, Loss: 1.029202
Train - Epoch 480, Batch: 0, Loss: 1.068573
Train - Epoch 481, Batch: 0, Loss: 1.048727
Train - Epoch 482, Batch: 0, Loss: 1.043215
Train - Epoch 483, Batch: 0, Loss: 1.049193
Train - Epoch 484, Batch: 0, Loss: 1.037910
Train - Epoch 485, Batch: 0, Loss: 1.035594
Train - Epoch 486, Batch: 0, Loss: 1.034557
Train - Epoch 487, Batch: 0, Loss: 1.022612
Train - Epoch 488, Batch: 0, Loss: 1.036903
Train - Epoch 489, Batch: 0, Loss: 1.031201
Train - Epoch 490, Batch: 0, Loss: 1.040809
Train - Epoch 491, Batch: 0, Loss: 1.040785
Train - Epoch 492, Batch: 0, Loss: 1.027140
Train - Epoch 493, Batch: 0, Loss: 1.033539
Train - Epoch 494, Batch: 0, Loss: 1.043234
Train - Epoch 495, Batch: 0, Loss: 1.040791
Train - Epoch 496, Batch: 0, Loss: 1.042538
Train - Epoch 497, Batch: 0, Loss: 1.040292
Train - Epoch 498, Batch: 0, Loss: 1.023363
Train - Epoch 499, Batch: 0, Loss: 1.027676
training_time:: 107.40925431251526
training time full:: 107.40932369232178
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633200
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 5
max_epoch:: 500
batch_size:: 10000
epoch:: 0
tensor([    1,     6, 32775, 32776, 32781, 32783,    31,    33, 32802,    43,
        32811,    45,    46, 32815, 32820,    53,    54, 32823,    55, 32826,
        32829, 32831, 32834,    75, 32844, 32856,    88,    94, 32864,    98,
          104, 32874,   107, 32876,   109,   112,   113,   118, 32886, 32893,
        32896,   131,   136, 32908,   144, 32913,   158,   159, 32927,   164])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
epoch:: 60
epoch:: 61
epoch:: 62
epoch:: 63
epoch:: 64
epoch:: 65
epoch:: 66
epoch:: 67
epoch:: 68
epoch:: 69
epoch:: 70
epoch:: 71
epoch:: 72
epoch:: 73
epoch:: 74
epoch:: 75
epoch:: 76
epoch:: 77
epoch:: 78
epoch:: 79
epoch:: 80
epoch:: 81
epoch:: 82
epoch:: 83
epoch:: 84
epoch:: 85
epoch:: 86
epoch:: 87
epoch:: 88
epoch:: 89
epoch:: 90
epoch:: 91
epoch:: 92
epoch:: 93
epoch:: 94
epoch:: 95
epoch:: 96
epoch:: 97
epoch:: 98
epoch:: 99
epoch:: 100
epoch:: 101
epoch:: 102
epoch:: 103
epoch:: 104
epoch:: 105
epoch:: 106
epoch:: 107
epoch:: 108
epoch:: 109
epoch:: 110
epoch:: 111
epoch:: 112
epoch:: 113
epoch:: 114
epoch:: 115
epoch:: 116
epoch:: 117
epoch:: 118
epoch:: 119
epoch:: 120
epoch:: 121
epoch:: 122
epoch:: 123
epoch:: 124
epoch:: 125
epoch:: 126
epoch:: 127
epoch:: 128
epoch:: 129
epoch:: 130
epoch:: 131
epoch:: 132
epoch:: 133
epoch:: 134
epoch:: 135
epoch:: 136
epoch:: 137
epoch:: 138
epoch:: 139
epoch:: 140
epoch:: 141
epoch:: 142
epoch:: 143
epoch:: 144
epoch:: 145
epoch:: 146
epoch:: 147
epoch:: 148
epoch:: 149
epoch:: 150
epoch:: 151
epoch:: 152
epoch:: 153
epoch:: 154
epoch:: 155
epoch:: 156
epoch:: 157
epoch:: 158
epoch:: 159
epoch:: 160
epoch:: 161
epoch:: 162
epoch:: 163
epoch:: 164
epoch:: 165
epoch:: 166
epoch:: 167
epoch:: 168
epoch:: 169
epoch:: 170
epoch:: 171
epoch:: 172
epoch:: 173
epoch:: 174
epoch:: 175
epoch:: 176
epoch:: 177
epoch:: 178
epoch:: 179
epoch:: 180
epoch:: 181
epoch:: 182
epoch:: 183
epoch:: 184
epoch:: 185
epoch:: 186
epoch:: 187
epoch:: 188
epoch:: 189
epoch:: 190
epoch:: 191
epoch:: 192
epoch:: 193
epoch:: 194
epoch:: 195
epoch:: 196
epoch:: 197
epoch:: 198
epoch:: 199
epoch:: 200
epoch:: 201
epoch:: 202
epoch:: 203
epoch:: 204
epoch:: 205
epoch:: 206
epoch:: 207
epoch:: 208
epoch:: 209
epoch:: 210
epoch:: 211
epoch:: 212
epoch:: 213
epoch:: 214
epoch:: 215
epoch:: 216
epoch:: 217
epoch:: 218
epoch:: 219
epoch:: 220
epoch:: 221
epoch:: 222
epoch:: 223
epoch:: 224
epoch:: 225
epoch:: 226
epoch:: 227
epoch:: 228
epoch:: 229
epoch:: 230
epoch:: 231
epoch:: 232
epoch:: 233
epoch:: 234
epoch:: 235
epoch:: 236
epoch:: 237
epoch:: 238
epoch:: 239
epoch:: 240
epoch:: 241
epoch:: 242
epoch:: 243
epoch:: 244
epoch:: 245
epoch:: 246
epoch:: 247
epoch:: 248
epoch:: 249
epoch:: 250
epoch:: 251
epoch:: 252
epoch:: 253
epoch:: 254
epoch:: 255
epoch:: 256
epoch:: 257
epoch:: 258
epoch:: 259
epoch:: 260
epoch:: 261
epoch:: 262
epoch:: 263
epoch:: 264
epoch:: 265
epoch:: 266
epoch:: 267
epoch:: 268
epoch:: 269
epoch:: 270
epoch:: 271
epoch:: 272
epoch:: 273
epoch:: 274
epoch:: 275
epoch:: 276
epoch:: 277
epoch:: 278
epoch:: 279
epoch:: 280
epoch:: 281
epoch:: 282
epoch:: 283
epoch:: 284
epoch:: 285
epoch:: 286
epoch:: 287
epoch:: 288
epoch:: 289
epoch:: 290
epoch:: 291
epoch:: 292
epoch:: 293
epoch:: 294
epoch:: 295
epoch:: 296
epoch:: 297
epoch:: 298
epoch:: 299
epoch:: 300
epoch:: 301
epoch:: 302
epoch:: 303
epoch:: 304
epoch:: 305
epoch:: 306
epoch:: 307
epoch:: 308
epoch:: 309
epoch:: 310
epoch:: 311
epoch:: 312
epoch:: 313
epoch:: 314
epoch:: 315
epoch:: 316
epoch:: 317
epoch:: 318
epoch:: 319
epoch:: 320
epoch:: 321
epoch:: 322
epoch:: 323
epoch:: 324
epoch:: 325
epoch:: 326
epoch:: 327
epoch:: 328
epoch:: 329
epoch:: 330
epoch:: 331
epoch:: 332
epoch:: 333
epoch:: 334
epoch:: 335
epoch:: 336
epoch:: 337
epoch:: 338
epoch:: 339
epoch:: 340
epoch:: 341
epoch:: 342
epoch:: 343
epoch:: 344
epoch:: 345
epoch:: 346
epoch:: 347
epoch:: 348
epoch:: 349
epoch:: 350
epoch:: 351
epoch:: 352
epoch:: 353
epoch:: 354
epoch:: 355
epoch:: 356
epoch:: 357
epoch:: 358
epoch:: 359
epoch:: 360
epoch:: 361
epoch:: 362
epoch:: 363
epoch:: 364
epoch:: 365
epoch:: 366
epoch:: 367
epoch:: 368
epoch:: 369
epoch:: 370
epoch:: 371
epoch:: 372
epoch:: 373
epoch:: 374
epoch:: 375
epoch:: 376
epoch:: 377
epoch:: 378
epoch:: 379
epoch:: 380
epoch:: 381
epoch:: 382
epoch:: 383
epoch:: 384
epoch:: 385
epoch:: 386
epoch:: 387
epoch:: 388
epoch:: 389
epoch:: 390
epoch:: 391
epoch:: 392
epoch:: 393
epoch:: 394
epoch:: 395
epoch:: 396
epoch:: 397
epoch:: 398
epoch:: 399
epoch:: 400
epoch:: 401
epoch:: 402
epoch:: 403
epoch:: 404
epoch:: 405
epoch:: 406
epoch:: 407
epoch:: 408
epoch:: 409
epoch:: 410
epoch:: 411
epoch:: 412
epoch:: 413
epoch:: 414
epoch:: 415
epoch:: 416
epoch:: 417
epoch:: 418
epoch:: 419
epoch:: 420
epoch:: 421
epoch:: 422
epoch:: 423
epoch:: 424
epoch:: 425
epoch:: 426
epoch:: 427
epoch:: 428
epoch:: 429
epoch:: 430
epoch:: 431
epoch:: 432
epoch:: 433
epoch:: 434
epoch:: 435
epoch:: 436
epoch:: 437
epoch:: 438
epoch:: 439
epoch:: 440
epoch:: 441
epoch:: 442
epoch:: 443
epoch:: 444
epoch:: 445
epoch:: 446
epoch:: 447
epoch:: 448
epoch:: 449
epoch:: 450
epoch:: 451
epoch:: 452
epoch:: 453
epoch:: 454
epoch:: 455
epoch:: 456
epoch:: 457
epoch:: 458
epoch:: 459
epoch:: 460
epoch:: 461
epoch:: 462
epoch:: 463
epoch:: 464
epoch:: 465
epoch:: 466
epoch:: 467
epoch:: 468
epoch:: 469
epoch:: 470
epoch:: 471
epoch:: 472
epoch:: 473
epoch:: 474
epoch:: 475
epoch:: 476
epoch:: 477
epoch:: 478
epoch:: 479
epoch:: 480
epoch:: 481
epoch:: 482
epoch:: 483
epoch:: 484
epoch:: 485
epoch:: 486
epoch:: 487
epoch:: 488
epoch:: 489
epoch:: 490
epoch:: 491
epoch:: 492
epoch:: 493
epoch:: 494
epoch:: 495
epoch:: 496
epoch:: 497
epoch:: 498
epoch:: 499
training time is 80.49069237709045
overhead:: 0
overhead2:: 1.7627239227294922
overhead3:: 0
time_baseline:: 80.49072551727295
curr_diff: 0 tensor(0.0194, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0194, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633000
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 5
max_epoch:: 500
overhead:: 0
overhead2:: 0.05484724044799805
overhead3:: 0.25022411346435547
overhead4:: 9.828736305236816
overhead5:: 0
memory usage:: 5641830400
time_provenance:: 16.690679788589478
curr_diff: 0 tensor(0.0048, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0048, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0230, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0230, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633000
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 5
max_epoch:: 500
overhead:: 0
overhead2:: 0.057187795639038086
overhead3:: 0.25832629203796387
overhead4:: 10.179782152175903
overhead5:: 0
memory usage:: 5625126912
time_provenance:: 17.10717487335205
curr_diff: 0 tensor(0.0048, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0048, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0230, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0230, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633000
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 5
max_epoch:: 500
overhead:: 0
overhead2:: 0.06023406982421875
overhead3:: 0.2646164894104004
overhead4:: 10.611648797988892
overhead5:: 0
memory usage:: 5633466368
time_provenance:: 17.58702826499939
curr_diff: 0 tensor(0.0048, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0048, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0230, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0230, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633100
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 5
max_epoch:: 500
overhead:: 0
overhead2:: 0.10762310028076172
overhead3:: 0.44280505180358887
overhead4:: 18.370192766189575
overhead5:: 0
memory usage:: 5643255808
time_provenance:: 27.24094033241272
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0197, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0197, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633000
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 5
max_epoch:: 500
overhead:: 0
overhead2:: 0.11045575141906738
overhead3:: 0.44756364822387695
overhead4:: 18.213520765304565
overhead5:: 0
memory usage:: 5626212352
time_provenance:: 27.1019389629364
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0197, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0197, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633000
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 5
max_epoch:: 500
overhead:: 0
overhead2:: 0.11119699478149414
overhead3:: 0.4560558795928955
overhead4:: 18.24110698699951
overhead5:: 0
memory usage:: 5643341824
time_provenance:: 27.18762731552124
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0197, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0197, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633100
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 5
max_epoch:: 500
overhead:: 0
overhead2:: 0.2546529769897461
overhead3:: 1.0162405967712402
overhead4:: 44.038655519485474
overhead5:: 0
memory usage:: 5647106048
time_provenance:: 58.956953287124634
curr_diff: 0 tensor(6.5730e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.5730e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0194, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0194, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633000
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 5
max_epoch:: 500
overhead:: 0
overhead2:: 0.25075817108154297
overhead3:: 1.0313966274261475
overhead4:: 44.112857818603516
overhead5:: 0
memory usage:: 5643591680
time_provenance:: 59.02747988700867
curr_diff: 0 tensor(6.5705e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.5705e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0194, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0194, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633000
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 5
max_epoch:: 500
overhead:: 0
overhead2:: 0.2581493854522705
overhead3:: 1.0361804962158203
overhead4:: 43.46548628807068
overhead5:: 0
memory usage:: 5632933888
time_provenance:: 58.402992725372314
curr_diff: 0 tensor(6.5684e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.5684e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0194, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0194, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633000
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 2 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 5
max_epoch:: 500
overhead:: 0
overhead2:: 0.5105743408203125
overhead3:: 1.9969902038574219
overhead4:: 80.23355317115784
overhead5:: 0
memory usage:: 5625139200
time_provenance:: 96.56291818618774
curr_diff: 0 tensor(5.0110e-13, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.0110e-13, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0194, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0194, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633000
repetition 3
python3 benchmark_exp_lr.py 0.001 10000 500 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05] Logistic_regression cifar10_2 3 0.0001 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 2.307095
Train - Epoch 1, Batch: 0, Loss: 2.244537
Train - Epoch 2, Batch: 0, Loss: 2.190640
Train - Epoch 3, Batch: 0, Loss: 2.138980
Train - Epoch 4, Batch: 0, Loss: 2.094062
Train - Epoch 5, Batch: 0, Loss: 2.048775
Train - Epoch 6, Batch: 0, Loss: 2.006208
Train - Epoch 7, Batch: 0, Loss: 1.971374
Train - Epoch 8, Batch: 0, Loss: 1.934496
Train - Epoch 9, Batch: 0, Loss: 1.901895
Train - Epoch 10, Batch: 0, Loss: 1.871296
Train - Epoch 11, Batch: 0, Loss: 1.841678
Train - Epoch 12, Batch: 0, Loss: 1.814382
Train - Epoch 13, Batch: 0, Loss: 1.788489
Train - Epoch 14, Batch: 0, Loss: 1.754856
Train - Epoch 15, Batch: 0, Loss: 1.737817
Train - Epoch 16, Batch: 0, Loss: 1.719297
Train - Epoch 17, Batch: 0, Loss: 1.691651
Train - Epoch 18, Batch: 0, Loss: 1.675008
Train - Epoch 19, Batch: 0, Loss: 1.662299
Train - Epoch 20, Batch: 0, Loss: 1.644568
Train - Epoch 21, Batch: 0, Loss: 1.621596
Train - Epoch 22, Batch: 0, Loss: 1.617373
Train - Epoch 23, Batch: 0, Loss: 1.591715
Train - Epoch 24, Batch: 0, Loss: 1.580221
Train - Epoch 25, Batch: 0, Loss: 1.569732
Train - Epoch 26, Batch: 0, Loss: 1.545657
Train - Epoch 27, Batch: 0, Loss: 1.546400
Train - Epoch 28, Batch: 0, Loss: 1.531085
Train - Epoch 29, Batch: 0, Loss: 1.527248
Train - Epoch 30, Batch: 0, Loss: 1.508855
Train - Epoch 31, Batch: 0, Loss: 1.501292
Train - Epoch 32, Batch: 0, Loss: 1.489354
Train - Epoch 33, Batch: 0, Loss: 1.479187
Train - Epoch 34, Batch: 0, Loss: 1.484463
Train - Epoch 35, Batch: 0, Loss: 1.458324
Train - Epoch 36, Batch: 0, Loss: 1.460120
Train - Epoch 37, Batch: 0, Loss: 1.443415
Train - Epoch 38, Batch: 0, Loss: 1.440347
Train - Epoch 39, Batch: 0, Loss: 1.434317
Train - Epoch 40, Batch: 0, Loss: 1.419173
Train - Epoch 41, Batch: 0, Loss: 1.418484
Train - Epoch 42, Batch: 0, Loss: 1.401742
Train - Epoch 43, Batch: 0, Loss: 1.408835
Train - Epoch 44, Batch: 0, Loss: 1.406510
Train - Epoch 45, Batch: 0, Loss: 1.384763
Train - Epoch 46, Batch: 0, Loss: 1.381748
Train - Epoch 47, Batch: 0, Loss: 1.372087
Train - Epoch 48, Batch: 0, Loss: 1.376237
Train - Epoch 49, Batch: 0, Loss: 1.365120
Train - Epoch 50, Batch: 0, Loss: 1.357300
Train - Epoch 51, Batch: 0, Loss: 1.356352
Train - Epoch 52, Batch: 0, Loss: 1.350787
Train - Epoch 53, Batch: 0, Loss: 1.369336
Train - Epoch 54, Batch: 0, Loss: 1.339023
Train - Epoch 55, Batch: 0, Loss: 1.344624
Train - Epoch 56, Batch: 0, Loss: 1.336484
Train - Epoch 57, Batch: 0, Loss: 1.325155
Train - Epoch 58, Batch: 0, Loss: 1.323320
Train - Epoch 59, Batch: 0, Loss: 1.318244
Train - Epoch 60, Batch: 0, Loss: 1.313545
Train - Epoch 61, Batch: 0, Loss: 1.309532
Train - Epoch 62, Batch: 0, Loss: 1.315264
Train - Epoch 63, Batch: 0, Loss: 1.305681
Train - Epoch 64, Batch: 0, Loss: 1.302010
Train - Epoch 65, Batch: 0, Loss: 1.307682
Train - Epoch 66, Batch: 0, Loss: 1.290923
Train - Epoch 67, Batch: 0, Loss: 1.292606
Train - Epoch 68, Batch: 0, Loss: 1.286267
Train - Epoch 69, Batch: 0, Loss: 1.289835
Train - Epoch 70, Batch: 0, Loss: 1.289488
Train - Epoch 71, Batch: 0, Loss: 1.270542
Train - Epoch 72, Batch: 0, Loss: 1.268522
Train - Epoch 73, Batch: 0, Loss: 1.260842
Train - Epoch 74, Batch: 0, Loss: 1.269292
Train - Epoch 75, Batch: 0, Loss: 1.274140
Train - Epoch 76, Batch: 0, Loss: 1.272741
Train - Epoch 77, Batch: 0, Loss: 1.266637
Train - Epoch 78, Batch: 0, Loss: 1.266472
Train - Epoch 79, Batch: 0, Loss: 1.270869
Train - Epoch 80, Batch: 0, Loss: 1.251820
Train - Epoch 81, Batch: 0, Loss: 1.244097
Train - Epoch 82, Batch: 0, Loss: 1.253046
Train - Epoch 83, Batch: 0, Loss: 1.243906
Train - Epoch 84, Batch: 0, Loss: 1.244689
Train - Epoch 85, Batch: 0, Loss: 1.251095
Train - Epoch 86, Batch: 0, Loss: 1.256017
Train - Epoch 87, Batch: 0, Loss: 1.246728
Train - Epoch 88, Batch: 0, Loss: 1.229098
Train - Epoch 89, Batch: 0, Loss: 1.238640
Train - Epoch 90, Batch: 0, Loss: 1.230632
Train - Epoch 91, Batch: 0, Loss: 1.229907
Train - Epoch 92, Batch: 0, Loss: 1.240738
Train - Epoch 93, Batch: 0, Loss: 1.218761
Train - Epoch 94, Batch: 0, Loss: 1.224667
Train - Epoch 95, Batch: 0, Loss: 1.241073
Train - Epoch 96, Batch: 0, Loss: 1.232865
Train - Epoch 97, Batch: 0, Loss: 1.221049
Train - Epoch 98, Batch: 0, Loss: 1.223833
Train - Epoch 99, Batch: 0, Loss: 1.226893
Train - Epoch 100, Batch: 0, Loss: 1.208101
Train - Epoch 101, Batch: 0, Loss: 1.200299
Train - Epoch 102, Batch: 0, Loss: 1.215178
Train - Epoch 103, Batch: 0, Loss: 1.212086
Train - Epoch 104, Batch: 0, Loss: 1.202889
Train - Epoch 105, Batch: 0, Loss: 1.196263
Train - Epoch 106, Batch: 0, Loss: 1.214170
Train - Epoch 107, Batch: 0, Loss: 1.206524
Train - Epoch 108, Batch: 0, Loss: 1.203694
Train - Epoch 109, Batch: 0, Loss: 1.188942
Train - Epoch 110, Batch: 0, Loss: 1.198673
Train - Epoch 111, Batch: 0, Loss: 1.198782
Train - Epoch 112, Batch: 0, Loss: 1.187363
Train - Epoch 113, Batch: 0, Loss: 1.187896
Train - Epoch 114, Batch: 0, Loss: 1.193652
Train - Epoch 115, Batch: 0, Loss: 1.196812
Train - Epoch 116, Batch: 0, Loss: 1.182810
Train - Epoch 117, Batch: 0, Loss: 1.193286
Train - Epoch 118, Batch: 0, Loss: 1.188026
Train - Epoch 119, Batch: 0, Loss: 1.171160
Train - Epoch 120, Batch: 0, Loss: 1.187642
Train - Epoch 121, Batch: 0, Loss: 1.192263
Train - Epoch 122, Batch: 0, Loss: 1.196384
Train - Epoch 123, Batch: 0, Loss: 1.189970
Train - Epoch 124, Batch: 0, Loss: 1.173386
Train - Epoch 125, Batch: 0, Loss: 1.186951
Train - Epoch 126, Batch: 0, Loss: 1.185614
Train - Epoch 127, Batch: 0, Loss: 1.196487
Train - Epoch 128, Batch: 0, Loss: 1.186516
Train - Epoch 129, Batch: 0, Loss: 1.158785
Train - Epoch 130, Batch: 0, Loss: 1.162488
Train - Epoch 131, Batch: 0, Loss: 1.165888
Train - Epoch 132, Batch: 0, Loss: 1.178334
Train - Epoch 133, Batch: 0, Loss: 1.165964
Train - Epoch 134, Batch: 0, Loss: 1.158149
Train - Epoch 135, Batch: 0, Loss: 1.169484
Train - Epoch 136, Batch: 0, Loss: 1.161003
Train - Epoch 137, Batch: 0, Loss: 1.173231
Train - Epoch 138, Batch: 0, Loss: 1.160171
Train - Epoch 139, Batch: 0, Loss: 1.154317
Train - Epoch 140, Batch: 0, Loss: 1.169548
Train - Epoch 141, Batch: 0, Loss: 1.174521
Train - Epoch 142, Batch: 0, Loss: 1.153225
Train - Epoch 143, Batch: 0, Loss: 1.162899
Train - Epoch 144, Batch: 0, Loss: 1.154924
Train - Epoch 145, Batch: 0, Loss: 1.164835
Train - Epoch 146, Batch: 0, Loss: 1.162948
Train - Epoch 147, Batch: 0, Loss: 1.153245
Train - Epoch 148, Batch: 0, Loss: 1.163167
Train - Epoch 149, Batch: 0, Loss: 1.155425
Train - Epoch 150, Batch: 0, Loss: 1.152817
Train - Epoch 151, Batch: 0, Loss: 1.158352
Train - Epoch 152, Batch: 0, Loss: 1.142381
Train - Epoch 153, Batch: 0, Loss: 1.144146
Train - Epoch 154, Batch: 0, Loss: 1.156775
Train - Epoch 155, Batch: 0, Loss: 1.155029
Train - Epoch 156, Batch: 0, Loss: 1.154331
Train - Epoch 157, Batch: 0, Loss: 1.148737
Train - Epoch 158, Batch: 0, Loss: 1.140994
Train - Epoch 159, Batch: 0, Loss: 1.144301
Train - Epoch 160, Batch: 0, Loss: 1.143556
Train - Epoch 161, Batch: 0, Loss: 1.138695
Train - Epoch 162, Batch: 0, Loss: 1.139802
Train - Epoch 163, Batch: 0, Loss: 1.144337
Train - Epoch 164, Batch: 0, Loss: 1.155461
Train - Epoch 165, Batch: 0, Loss: 1.145201
Train - Epoch 166, Batch: 0, Loss: 1.149533
Train - Epoch 167, Batch: 0, Loss: 1.144711
Train - Epoch 168, Batch: 0, Loss: 1.145475
Train - Epoch 169, Batch: 0, Loss: 1.136448
Train - Epoch 170, Batch: 0, Loss: 1.147902
Train - Epoch 171, Batch: 0, Loss: 1.139476
Train - Epoch 172, Batch: 0, Loss: 1.130414
Train - Epoch 173, Batch: 0, Loss: 1.143539
Train - Epoch 174, Batch: 0, Loss: 1.132700
Train - Epoch 175, Batch: 0, Loss: 1.133742
Train - Epoch 176, Batch: 0, Loss: 1.126469
Train - Epoch 177, Batch: 0, Loss: 1.118314
Train - Epoch 178, Batch: 0, Loss: 1.127171
Train - Epoch 179, Batch: 0, Loss: 1.122739
Train - Epoch 180, Batch: 0, Loss: 1.131686
Train - Epoch 181, Batch: 0, Loss: 1.113246
Train - Epoch 182, Batch: 0, Loss: 1.118863
Train - Epoch 183, Batch: 0, Loss: 1.133597
Train - Epoch 184, Batch: 0, Loss: 1.140152
Train - Epoch 185, Batch: 0, Loss: 1.110804
Train - Epoch 186, Batch: 0, Loss: 1.123432
Train - Epoch 187, Batch: 0, Loss: 1.126146
Train - Epoch 188, Batch: 0, Loss: 1.116192
Train - Epoch 189, Batch: 0, Loss: 1.134088
Train - Epoch 190, Batch: 0, Loss: 1.123959
Train - Epoch 191, Batch: 0, Loss: 1.123863
Train - Epoch 192, Batch: 0, Loss: 1.125714
Train - Epoch 193, Batch: 0, Loss: 1.127217
Train - Epoch 194, Batch: 0, Loss: 1.126055
Train - Epoch 195, Batch: 0, Loss: 1.126829
Train - Epoch 196, Batch: 0, Loss: 1.125217
Train - Epoch 197, Batch: 0, Loss: 1.113499
Train - Epoch 198, Batch: 0, Loss: 1.124396
Train - Epoch 199, Batch: 0, Loss: 1.119020
Train - Epoch 200, Batch: 0, Loss: 1.113694
Train - Epoch 201, Batch: 0, Loss: 1.111805
Train - Epoch 202, Batch: 0, Loss: 1.117294
Train - Epoch 203, Batch: 0, Loss: 1.130675
Train - Epoch 204, Batch: 0, Loss: 1.126342
Train - Epoch 205, Batch: 0, Loss: 1.108378
Train - Epoch 206, Batch: 0, Loss: 1.130058
Train - Epoch 207, Batch: 0, Loss: 1.117419
Train - Epoch 208, Batch: 0, Loss: 1.118703
Train - Epoch 209, Batch: 0, Loss: 1.111548
Train - Epoch 210, Batch: 0, Loss: 1.115211
Train - Epoch 211, Batch: 0, Loss: 1.108358
Train - Epoch 212, Batch: 0, Loss: 1.123563
Train - Epoch 213, Batch: 0, Loss: 1.109601
Train - Epoch 214, Batch: 0, Loss: 1.111564
Train - Epoch 215, Batch: 0, Loss: 1.123424
Train - Epoch 216, Batch: 0, Loss: 1.111707
Train - Epoch 217, Batch: 0, Loss: 1.105750
Train - Epoch 218, Batch: 0, Loss: 1.113714
Train - Epoch 219, Batch: 0, Loss: 1.114563
Train - Epoch 220, Batch: 0, Loss: 1.111387
Train - Epoch 221, Batch: 0, Loss: 1.105782
Train - Epoch 222, Batch: 0, Loss: 1.096250
Train - Epoch 223, Batch: 0, Loss: 1.109402
Train - Epoch 224, Batch: 0, Loss: 1.101645
Train - Epoch 225, Batch: 0, Loss: 1.110156
Train - Epoch 226, Batch: 0, Loss: 1.109875
Train - Epoch 227, Batch: 0, Loss: 1.111099
Train - Epoch 228, Batch: 0, Loss: 1.098716
Train - Epoch 229, Batch: 0, Loss: 1.111696
Train - Epoch 230, Batch: 0, Loss: 1.091646
Train - Epoch 231, Batch: 0, Loss: 1.090680
Train - Epoch 232, Batch: 0, Loss: 1.096874
Train - Epoch 233, Batch: 0, Loss: 1.097769
Train - Epoch 234, Batch: 0, Loss: 1.090550
Train - Epoch 235, Batch: 0, Loss: 1.098449
Train - Epoch 236, Batch: 0, Loss: 1.098888
Train - Epoch 237, Batch: 0, Loss: 1.106216
Train - Epoch 238, Batch: 0, Loss: 1.098014
Train - Epoch 239, Batch: 0, Loss: 1.097400
Train - Epoch 240, Batch: 0, Loss: 1.111691
Train - Epoch 241, Batch: 0, Loss: 1.100148
Train - Epoch 242, Batch: 0, Loss: 1.094474
Train - Epoch 243, Batch: 0, Loss: 1.116171
Train - Epoch 244, Batch: 0, Loss: 1.096637
Train - Epoch 245, Batch: 0, Loss: 1.086962
Train - Epoch 246, Batch: 0, Loss: 1.094067
Train - Epoch 247, Batch: 0, Loss: 1.090502
Train - Epoch 248, Batch: 0, Loss: 1.089549
Train - Epoch 249, Batch: 0, Loss: 1.084030
Train - Epoch 250, Batch: 0, Loss: 1.085184
Train - Epoch 251, Batch: 0, Loss: 1.106973
Train - Epoch 252, Batch: 0, Loss: 1.086219
Train - Epoch 253, Batch: 0, Loss: 1.100816
Train - Epoch 254, Batch: 0, Loss: 1.092922
Train - Epoch 255, Batch: 0, Loss: 1.107472
Train - Epoch 256, Batch: 0, Loss: 1.085919
Train - Epoch 257, Batch: 0, Loss: 1.085342
Train - Epoch 258, Batch: 0, Loss: 1.081345
Train - Epoch 259, Batch: 0, Loss: 1.082757
Train - Epoch 260, Batch: 0, Loss: 1.093599
Train - Epoch 261, Batch: 0, Loss: 1.085709
Train - Epoch 262, Batch: 0, Loss: 1.088771
Train - Epoch 263, Batch: 0, Loss: 1.084935
Train - Epoch 264, Batch: 0, Loss: 1.072635
Train - Epoch 265, Batch: 0, Loss: 1.082185
Train - Epoch 266, Batch: 0, Loss: 1.084203
Train - Epoch 267, Batch: 0, Loss: 1.091112
Train - Epoch 268, Batch: 0, Loss: 1.087589
Train - Epoch 269, Batch: 0, Loss: 1.080204
Train - Epoch 270, Batch: 0, Loss: 1.074564
Train - Epoch 271, Batch: 0, Loss: 1.087783
Train - Epoch 272, Batch: 0, Loss: 1.089173
Train - Epoch 273, Batch: 0, Loss: 1.092627
Train - Epoch 274, Batch: 0, Loss: 1.094252
Train - Epoch 275, Batch: 0, Loss: 1.080061
Train - Epoch 276, Batch: 0, Loss: 1.079749
Train - Epoch 277, Batch: 0, Loss: 1.081219
Train - Epoch 278, Batch: 0, Loss: 1.066346
Train - Epoch 279, Batch: 0, Loss: 1.092141
Train - Epoch 280, Batch: 0, Loss: 1.081213
Train - Epoch 281, Batch: 0, Loss: 1.091715
Train - Epoch 282, Batch: 0, Loss: 1.087747
Train - Epoch 283, Batch: 0, Loss: 1.104585
Train - Epoch 284, Batch: 0, Loss: 1.085472
Train - Epoch 285, Batch: 0, Loss: 1.071918
Train - Epoch 286, Batch: 0, Loss: 1.091350
Train - Epoch 287, Batch: 0, Loss: 1.089549
Train - Epoch 288, Batch: 0, Loss: 1.074426
Train - Epoch 289, Batch: 0, Loss: 1.087098
Train - Epoch 290, Batch: 0, Loss: 1.078590
Train - Epoch 291, Batch: 0, Loss: 1.083303
Train - Epoch 292, Batch: 0, Loss: 1.061692
Train - Epoch 293, Batch: 0, Loss: 1.083275
Train - Epoch 294, Batch: 0, Loss: 1.075150
Train - Epoch 295, Batch: 0, Loss: 1.078653
Train - Epoch 296, Batch: 0, Loss: 1.079803
Train - Epoch 297, Batch: 0, Loss: 1.076043
Train - Epoch 298, Batch: 0, Loss: 1.068281
Train - Epoch 299, Batch: 0, Loss: 1.072781
Train - Epoch 300, Batch: 0, Loss: 1.077278
Train - Epoch 301, Batch: 0, Loss: 1.092115
Train - Epoch 302, Batch: 0, Loss: 1.080202
Train - Epoch 303, Batch: 0, Loss: 1.077880
Train - Epoch 304, Batch: 0, Loss: 1.081867
Train - Epoch 305, Batch: 0, Loss: 1.067270
Train - Epoch 306, Batch: 0, Loss: 1.072090
Train - Epoch 307, Batch: 0, Loss: 1.081689
Train - Epoch 308, Batch: 0, Loss: 1.079623
Train - Epoch 309, Batch: 0, Loss: 1.062740
Train - Epoch 310, Batch: 0, Loss: 1.075518
Train - Epoch 311, Batch: 0, Loss: 1.062671
Train - Epoch 312, Batch: 0, Loss: 1.063559
Train - Epoch 313, Batch: 0, Loss: 1.067307
Train - Epoch 314, Batch: 0, Loss: 1.080634
Train - Epoch 315, Batch: 0, Loss: 1.081216
Train - Epoch 316, Batch: 0, Loss: 1.071428
Train - Epoch 317, Batch: 0, Loss: 1.076724
Train - Epoch 318, Batch: 0, Loss: 1.085150
Train - Epoch 319, Batch: 0, Loss: 1.078027
Train - Epoch 320, Batch: 0, Loss: 1.066488
Train - Epoch 321, Batch: 0, Loss: 1.083829
Train - Epoch 322, Batch: 0, Loss: 1.058174
Train - Epoch 323, Batch: 0, Loss: 1.072702
Train - Epoch 324, Batch: 0, Loss: 1.072312
Train - Epoch 325, Batch: 0, Loss: 1.074517
Train - Epoch 326, Batch: 0, Loss: 1.054346
Train - Epoch 327, Batch: 0, Loss: 1.064166
Train - Epoch 328, Batch: 0, Loss: 1.068146
Train - Epoch 329, Batch: 0, Loss: 1.062581
Train - Epoch 330, Batch: 0, Loss: 1.069660
Train - Epoch 331, Batch: 0, Loss: 1.081422
Train - Epoch 332, Batch: 0, Loss: 1.064880
Train - Epoch 333, Batch: 0, Loss: 1.081750
Train - Epoch 334, Batch: 0, Loss: 1.087998
Train - Epoch 335, Batch: 0, Loss: 1.087334
Train - Epoch 336, Batch: 0, Loss: 1.083464
Train - Epoch 337, Batch: 0, Loss: 1.070542
Train - Epoch 338, Batch: 0, Loss: 1.072101
Train - Epoch 339, Batch: 0, Loss: 1.056751
Train - Epoch 340, Batch: 0, Loss: 1.080594
Train - Epoch 341, Batch: 0, Loss: 1.068493
Train - Epoch 342, Batch: 0, Loss: 1.059194
Train - Epoch 343, Batch: 0, Loss: 1.061225
Train - Epoch 344, Batch: 0, Loss: 1.059454
Train - Epoch 345, Batch: 0, Loss: 1.071642
Train - Epoch 346, Batch: 0, Loss: 1.072100
Train - Epoch 347, Batch: 0, Loss: 1.063394
Train - Epoch 348, Batch: 0, Loss: 1.072851
Train - Epoch 349, Batch: 0, Loss: 1.070500
Train - Epoch 350, Batch: 0, Loss: 1.061328
Train - Epoch 351, Batch: 0, Loss: 1.070542
Train - Epoch 352, Batch: 0, Loss: 1.055426
Train - Epoch 353, Batch: 0, Loss: 1.062144
Train - Epoch 354, Batch: 0, Loss: 1.067730
Train - Epoch 355, Batch: 0, Loss: 1.059105
Train - Epoch 356, Batch: 0, Loss: 1.048198
Train - Epoch 357, Batch: 0, Loss: 1.062523
Train - Epoch 358, Batch: 0, Loss: 1.068805
Train - Epoch 359, Batch: 0, Loss: 1.068068
Train - Epoch 360, Batch: 0, Loss: 1.061682
Train - Epoch 361, Batch: 0, Loss: 1.056367
Train - Epoch 362, Batch: 0, Loss: 1.070001
Train - Epoch 363, Batch: 0, Loss: 1.063033
Train - Epoch 364, Batch: 0, Loss: 1.078852
Train - Epoch 365, Batch: 0, Loss: 1.061769
Train - Epoch 366, Batch: 0, Loss: 1.067760
Train - Epoch 367, Batch: 0, Loss: 1.069073
Train - Epoch 368, Batch: 0, Loss: 1.061674
Train - Epoch 369, Batch: 0, Loss: 1.074431
Train - Epoch 370, Batch: 0, Loss: 1.074231
Train - Epoch 371, Batch: 0, Loss: 1.060219
Train - Epoch 372, Batch: 0, Loss: 1.058250
Train - Epoch 373, Batch: 0, Loss: 1.060847/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 374, Batch: 0, Loss: 1.058200
Train - Epoch 375, Batch: 0, Loss: 1.056823
Train - Epoch 376, Batch: 0, Loss: 1.063552
Train - Epoch 377, Batch: 0, Loss: 1.068103
Train - Epoch 378, Batch: 0, Loss: 1.061803
Train - Epoch 379, Batch: 0, Loss: 1.045782
Train - Epoch 380, Batch: 0, Loss: 1.041580
Train - Epoch 381, Batch: 0, Loss: 1.057559
Train - Epoch 382, Batch: 0, Loss: 1.057402
Train - Epoch 383, Batch: 0, Loss: 1.058394
Train - Epoch 384, Batch: 0, Loss: 1.061250
Train - Epoch 385, Batch: 0, Loss: 1.055072
Train - Epoch 386, Batch: 0, Loss: 1.058537
Train - Epoch 387, Batch: 0, Loss: 1.061305
Train - Epoch 388, Batch: 0, Loss: 1.054422
Train - Epoch 389, Batch: 0, Loss: 1.071654
Train - Epoch 390, Batch: 0, Loss: 1.051337
Train - Epoch 391, Batch: 0, Loss: 1.059408
Train - Epoch 392, Batch: 0, Loss: 1.058302
Train - Epoch 393, Batch: 0, Loss: 1.058272
Train - Epoch 394, Batch: 0, Loss: 1.064122
Train - Epoch 395, Batch: 0, Loss: 1.063150
Train - Epoch 396, Batch: 0, Loss: 1.047756
Train - Epoch 397, Batch: 0, Loss: 1.073099
Train - Epoch 398, Batch: 0, Loss: 1.051755
Train - Epoch 399, Batch: 0, Loss: 1.067814
Train - Epoch 400, Batch: 0, Loss: 1.066754
Train - Epoch 401, Batch: 0, Loss: 1.042965
Train - Epoch 402, Batch: 0, Loss: 1.043881
Train - Epoch 403, Batch: 0, Loss: 1.063839
Train - Epoch 404, Batch: 0, Loss: 1.072343
Train - Epoch 405, Batch: 0, Loss: 1.047247
Train - Epoch 406, Batch: 0, Loss: 1.053464
Train - Epoch 407, Batch: 0, Loss: 1.055511
Train - Epoch 408, Batch: 0, Loss: 1.052113
Train - Epoch 409, Batch: 0, Loss: 1.055524
Train - Epoch 410, Batch: 0, Loss: 1.039649
Train - Epoch 411, Batch: 0, Loss: 1.062475
Train - Epoch 412, Batch: 0, Loss: 1.050866
Train - Epoch 413, Batch: 0, Loss: 1.051138
Train - Epoch 414, Batch: 0, Loss: 1.058607
Train - Epoch 415, Batch: 0, Loss: 1.043650
Train - Epoch 416, Batch: 0, Loss: 1.043530
Train - Epoch 417, Batch: 0, Loss: 1.049746
Train - Epoch 418, Batch: 0, Loss: 1.051227
Train - Epoch 419, Batch: 0, Loss: 1.040467
Train - Epoch 420, Batch: 0, Loss: 1.049108
Train - Epoch 421, Batch: 0, Loss: 1.054615
Train - Epoch 422, Batch: 0, Loss: 1.056312
Train - Epoch 423, Batch: 0, Loss: 1.041938
Train - Epoch 424, Batch: 0, Loss: 1.047288
Train - Epoch 425, Batch: 0, Loss: 1.047213
Train - Epoch 426, Batch: 0, Loss: 1.033293
Train - Epoch 427, Batch: 0, Loss: 1.042029
Train - Epoch 428, Batch: 0, Loss: 1.044108
Train - Epoch 429, Batch: 0, Loss: 1.048907
Train - Epoch 430, Batch: 0, Loss: 1.052405
Train - Epoch 431, Batch: 0, Loss: 1.053875
Train - Epoch 432, Batch: 0, Loss: 1.062277
Train - Epoch 433, Batch: 0, Loss: 1.056368
Train - Epoch 434, Batch: 0, Loss: 1.046551
Train - Epoch 435, Batch: 0, Loss: 1.063060
Train - Epoch 436, Batch: 0, Loss: 1.039042
Train - Epoch 437, Batch: 0, Loss: 1.051142
Train - Epoch 438, Batch: 0, Loss: 1.050694
Train - Epoch 439, Batch: 0, Loss: 1.037430
Train - Epoch 440, Batch: 0, Loss: 1.034508
Train - Epoch 441, Batch: 0, Loss: 1.057721
Train - Epoch 442, Batch: 0, Loss: 1.072319
Train - Epoch 443, Batch: 0, Loss: 1.033545
Train - Epoch 444, Batch: 0, Loss: 1.058112
Train - Epoch 445, Batch: 0, Loss: 1.042088
Train - Epoch 446, Batch: 0, Loss: 1.040856
Train - Epoch 447, Batch: 0, Loss: 1.046875
Train - Epoch 448, Batch: 0, Loss: 1.057429
Train - Epoch 449, Batch: 0, Loss: 1.037505
Train - Epoch 450, Batch: 0, Loss: 1.036911
Train - Epoch 451, Batch: 0, Loss: 1.049689
Train - Epoch 452, Batch: 0, Loss: 1.047049
Train - Epoch 453, Batch: 0, Loss: 1.035845
Train - Epoch 454, Batch: 0, Loss: 1.054571
Train - Epoch 455, Batch: 0, Loss: 1.048137
Train - Epoch 456, Batch: 0, Loss: 1.042798
Train - Epoch 457, Batch: 0, Loss: 1.057173
Train - Epoch 458, Batch: 0, Loss: 1.044253
Train - Epoch 459, Batch: 0, Loss: 1.046460
Train - Epoch 460, Batch: 0, Loss: 1.049414
Train - Epoch 461, Batch: 0, Loss: 1.048450
Train - Epoch 462, Batch: 0, Loss: 1.045446
Train - Epoch 463, Batch: 0, Loss: 1.037359
Train - Epoch 464, Batch: 0, Loss: 1.041467
Train - Epoch 465, Batch: 0, Loss: 1.047739
Train - Epoch 466, Batch: 0, Loss: 1.051268
Train - Epoch 467, Batch: 0, Loss: 1.048823
Train - Epoch 468, Batch: 0, Loss: 1.059833
Train - Epoch 469, Batch: 0, Loss: 1.050610
Train - Epoch 470, Batch: 0, Loss: 1.036166
Train - Epoch 471, Batch: 0, Loss: 1.041306
Train - Epoch 472, Batch: 0, Loss: 1.041558
Train - Epoch 473, Batch: 0, Loss: 1.030919
Train - Epoch 474, Batch: 0, Loss: 1.041327
Train - Epoch 475, Batch: 0, Loss: 1.044265
Train - Epoch 476, Batch: 0, Loss: 1.045976
Train - Epoch 477, Batch: 0, Loss: 1.038213
Train - Epoch 478, Batch: 0, Loss: 1.025300
Train - Epoch 479, Batch: 0, Loss: 1.033085
Train - Epoch 480, Batch: 0, Loss: 1.035281
Train - Epoch 481, Batch: 0, Loss: 1.038105
Train - Epoch 482, Batch: 0, Loss: 1.060342
Train - Epoch 483, Batch: 0, Loss: 1.038029
Train - Epoch 484, Batch: 0, Loss: 1.036698
Train - Epoch 485, Batch: 0, Loss: 1.037944
Train - Epoch 486, Batch: 0, Loss: 1.032524
Train - Epoch 487, Batch: 0, Loss: 1.032867
Train - Epoch 488, Batch: 0, Loss: 1.021531
Train - Epoch 489, Batch: 0, Loss: 1.036558
Train - Epoch 490, Batch: 0, Loss: 1.041615
Train - Epoch 491, Batch: 0, Loss: 1.046451
Train - Epoch 492, Batch: 0, Loss: 1.036662
Train - Epoch 493, Batch: 0, Loss: 1.043979
Train - Epoch 494, Batch: 0, Loss: 1.042509
Train - Epoch 495, Batch: 0, Loss: 1.049723
Train - Epoch 496, Batch: 0, Loss: 1.034447
Train - Epoch 497, Batch: 0, Loss: 1.040596
Train - Epoch 498, Batch: 0, Loss: 1.040355
Train - Epoch 499, Batch: 0, Loss: 1.049684
training_time:: 107.41867637634277
training time full:: 107.41874647140503
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632500
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 5
max_epoch:: 500
batch_size:: 10000
epoch:: 0
tensor([32768,     4, 32774,    10, 32780,    13,    15, 32784,    20,    28,
        32798,    31,    45, 32816,    50, 32820,    54, 32831,    65, 32834,
        32836,    69,    75, 32847,    81,    82, 32849,    84,    85, 32853,
        32855, 32861, 32864,    97, 32865, 32866, 32869,   110,   111, 32881,
        32882, 32883,   118,   127,   129, 32899, 32901,   136, 32909, 32914])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
epoch:: 60
epoch:: 61
epoch:: 62
epoch:: 63
epoch:: 64
epoch:: 65
epoch:: 66
epoch:: 67
epoch:: 68
epoch:: 69
epoch:: 70
epoch:: 71
epoch:: 72
epoch:: 73
epoch:: 74
epoch:: 75
epoch:: 76
epoch:: 77
epoch:: 78
epoch:: 79
epoch:: 80
epoch:: 81
epoch:: 82
epoch:: 83
epoch:: 84
epoch:: 85
epoch:: 86
epoch:: 87
epoch:: 88
epoch:: 89
epoch:: 90
epoch:: 91
epoch:: 92
epoch:: 93
epoch:: 94
epoch:: 95
epoch:: 96
epoch:: 97
epoch:: 98
epoch:: 99
epoch:: 100
epoch:: 101
epoch:: 102
epoch:: 103
epoch:: 104
epoch:: 105
epoch:: 106
epoch:: 107
epoch:: 108
epoch:: 109
epoch:: 110
epoch:: 111
epoch:: 112
epoch:: 113
epoch:: 114
epoch:: 115
epoch:: 116
epoch:: 117
epoch:: 118
epoch:: 119
epoch:: 120
epoch:: 121
epoch:: 122
epoch:: 123
epoch:: 124
epoch:: 125
epoch:: 126
epoch:: 127
epoch:: 128
epoch:: 129
epoch:: 130
epoch:: 131
epoch:: 132
epoch:: 133
epoch:: 134
epoch:: 135
epoch:: 136
epoch:: 137
epoch:: 138
epoch:: 139
epoch:: 140
epoch:: 141
epoch:: 142
epoch:: 143
epoch:: 144
epoch:: 145
epoch:: 146
epoch:: 147
epoch:: 148
epoch:: 149
epoch:: 150
epoch:: 151
epoch:: 152
epoch:: 153
epoch:: 154
epoch:: 155
epoch:: 156
epoch:: 157
epoch:: 158
epoch:: 159
epoch:: 160
epoch:: 161
epoch:: 162
epoch:: 163
epoch:: 164
epoch:: 165
epoch:: 166
epoch:: 167
epoch:: 168
epoch:: 169
epoch:: 170
epoch:: 171
epoch:: 172
epoch:: 173
epoch:: 174
epoch:: 175
epoch:: 176
epoch:: 177
epoch:: 178
epoch:: 179
epoch:: 180
epoch:: 181
epoch:: 182
epoch:: 183
epoch:: 184
epoch:: 185
epoch:: 186
epoch:: 187
epoch:: 188
epoch:: 189
epoch:: 190
epoch:: 191
epoch:: 192
epoch:: 193
epoch:: 194
epoch:: 195
epoch:: 196
epoch:: 197
epoch:: 198
epoch:: 199
epoch:: 200
epoch:: 201
epoch:: 202
epoch:: 203
epoch:: 204
epoch:: 205
epoch:: 206
epoch:: 207
epoch:: 208
epoch:: 209
epoch:: 210
epoch:: 211
epoch:: 212
epoch:: 213
epoch:: 214
epoch:: 215
epoch:: 216
epoch:: 217
epoch:: 218
epoch:: 219
epoch:: 220
epoch:: 221
epoch:: 222
epoch:: 223
epoch:: 224
epoch:: 225
epoch:: 226
epoch:: 227
epoch:: 228
epoch:: 229
epoch:: 230
epoch:: 231
epoch:: 232
epoch:: 233
epoch:: 234
epoch:: 235
epoch:: 236
epoch:: 237
epoch:: 238
epoch:: 239
epoch:: 240
epoch:: 241
epoch:: 242
epoch:: 243
epoch:: 244
epoch:: 245
epoch:: 246
epoch:: 247
epoch:: 248
epoch:: 249
epoch:: 250
epoch:: 251
epoch:: 252
epoch:: 253
epoch:: 254
epoch:: 255
epoch:: 256
epoch:: 257
epoch:: 258
epoch:: 259
epoch:: 260
epoch:: 261
epoch:: 262
epoch:: 263
epoch:: 264
epoch:: 265
epoch:: 266
epoch:: 267
epoch:: 268
epoch:: 269
epoch:: 270
epoch:: 271
epoch:: 272
epoch:: 273
epoch:: 274
epoch:: 275
epoch:: 276
epoch:: 277
epoch:: 278
epoch:: 279
epoch:: 280
epoch:: 281
epoch:: 282
epoch:: 283
epoch:: 284
epoch:: 285
epoch:: 286
epoch:: 287
epoch:: 288
epoch:: 289
epoch:: 290
epoch:: 291
epoch:: 292
epoch:: 293
epoch:: 294
epoch:: 295
epoch:: 296
epoch:: 297
epoch:: 298
epoch:: 299
epoch:: 300
epoch:: 301
epoch:: 302
epoch:: 303
epoch:: 304
epoch:: 305
epoch:: 306
epoch:: 307
epoch:: 308
epoch:: 309
epoch:: 310
epoch:: 311
epoch:: 312
epoch:: 313
epoch:: 314
epoch:: 315
epoch:: 316
epoch:: 317
epoch:: 318
epoch:: 319
epoch:: 320
epoch:: 321
epoch:: 322
epoch:: 323
epoch:: 324
epoch:: 325
epoch:: 326
epoch:: 327
epoch:: 328
epoch:: 329
epoch:: 330
epoch:: 331
epoch:: 332
epoch:: 333
epoch:: 334
epoch:: 335
epoch:: 336
epoch:: 337
epoch:: 338
epoch:: 339
epoch:: 340
epoch:: 341
epoch:: 342
epoch:: 343
epoch:: 344
epoch:: 345
epoch:: 346
epoch:: 347
epoch:: 348
epoch:: 349
epoch:: 350
epoch:: 351
epoch:: 352
epoch:: 353
epoch:: 354
epoch:: 355
epoch:: 356
epoch:: 357
epoch:: 358
epoch:: 359
epoch:: 360
epoch:: 361
epoch:: 362
epoch:: 363
epoch:: 364
epoch:: 365
epoch:: 366
epoch:: 367
epoch:: 368
epoch:: 369
epoch:: 370
epoch:: 371
epoch:: 372
epoch:: 373
epoch:: 374
epoch:: 375
epoch:: 376
epoch:: 377
epoch:: 378
epoch:: 379
epoch:: 380
epoch:: 381
epoch:: 382
epoch:: 383
epoch:: 384
epoch:: 385
epoch:: 386
epoch:: 387
epoch:: 388
epoch:: 389
epoch:: 390
epoch:: 391
epoch:: 392
epoch:: 393
epoch:: 394
epoch:: 395
epoch:: 396
epoch:: 397
epoch:: 398
epoch:: 399
epoch:: 400
epoch:: 401
epoch:: 402
epoch:: 403
epoch:: 404
epoch:: 405
epoch:: 406
epoch:: 407
epoch:: 408
epoch:: 409
epoch:: 410
epoch:: 411
epoch:: 412
epoch:: 413
epoch:: 414
epoch:: 415
epoch:: 416
epoch:: 417
epoch:: 418
epoch:: 419
epoch:: 420
epoch:: 421
epoch:: 422
epoch:: 423
epoch:: 424
epoch:: 425
epoch:: 426
epoch:: 427
epoch:: 428
epoch:: 429
epoch:: 430
epoch:: 431
epoch:: 432
epoch:: 433
epoch:: 434
epoch:: 435
epoch:: 436
epoch:: 437
epoch:: 438
epoch:: 439
epoch:: 440
epoch:: 441
epoch:: 442
epoch:: 443
epoch:: 444
epoch:: 445
epoch:: 446
epoch:: 447
epoch:: 448
epoch:: 449
epoch:: 450
epoch:: 451
epoch:: 452
epoch:: 453
epoch:: 454
epoch:: 455
epoch:: 456
epoch:: 457
epoch:: 458
epoch:: 459
epoch:: 460
epoch:: 461
epoch:: 462
epoch:: 463
epoch:: 464
epoch:: 465
epoch:: 466
epoch:: 467
epoch:: 468
epoch:: 469
epoch:: 470
epoch:: 471
epoch:: 472
epoch:: 473
epoch:: 474
epoch:: 475
epoch:: 476
epoch:: 477
epoch:: 478
epoch:: 479
epoch:: 480
epoch:: 481
epoch:: 482
epoch:: 483
epoch:: 484
epoch:: 485
epoch:: 486
epoch:: 487
epoch:: 488
epoch:: 489
epoch:: 490
epoch:: 491
epoch:: 492
epoch:: 493
epoch:: 494
epoch:: 495
epoch:: 496
epoch:: 497
epoch:: 498
epoch:: 499
training time is 80.35869979858398
overhead:: 0
overhead2:: 1.8258476257324219
overhead3:: 0
time_baseline:: 80.35873055458069
curr_diff: 0 tensor(0.0193, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0193, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632600
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 5
max_epoch:: 500
overhead:: 0
overhead2:: 0.05694103240966797
overhead3:: 0.2545173168182373
overhead4:: 9.653741359710693
overhead5:: 0
memory usage:: 5622734848
time_provenance:: 16.661805152893066
curr_diff: 0 tensor(0.0048, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0048, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0229, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0229, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632800
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 5
max_epoch:: 500
overhead:: 0
overhead2:: 0.05451631546020508
overhead3:: 0.25382518768310547
overhead4:: 10.01332139968872
overhead5:: 0
memory usage:: 5627441152
time_provenance:: 16.916765928268433
curr_diff: 0 tensor(0.0048, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0048, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0229, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0229, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632800
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 5
max_epoch:: 500
overhead:: 0
overhead2:: 0.057546377182006836
overhead3:: 0.26241564750671387
overhead4:: 10.35400915145874
overhead5:: 0
memory usage:: 5625012224
time_provenance:: 17.325016736984253
curr_diff: 0 tensor(0.0048, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0048, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0229, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0229, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632800
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 5
max_epoch:: 500
overhead:: 0
overhead2:: 0.10412812232971191
overhead3:: 0.4430382251739502
overhead4:: 18.103752374649048
overhead5:: 0
memory usage:: 5621858304
time_provenance:: 26.999149799346924
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0196, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0196, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632600
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 5
max_epoch:: 500
overhead:: 0
overhead2:: 0.11340022087097168
overhead3:: 0.44969892501831055
overhead4:: 18.56483745574951
overhead5:: 0
memory usage:: 5647384576
time_provenance:: 27.441858768463135
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0196, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0196, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632600
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 5
max_epoch:: 500
overhead:: 0
overhead2:: 0.11304664611816406
overhead3:: 0.45435166358947754
overhead4:: 18.526807069778442
overhead5:: 0
memory usage:: 5626626048
time_provenance:: 27.466702222824097
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0196, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0196, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632600
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 5
max_epoch:: 500
overhead:: 0
overhead2:: 0.2642018795013428
overhead3:: 1.0226705074310303
overhead4:: 44.121981382369995
overhead5:: 0
memory usage:: 5636915200
time_provenance:: 59.05220365524292
curr_diff: 0 tensor(7.3757e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.3757e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0193, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0193, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632600
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 5
max_epoch:: 500
overhead:: 0
overhead2:: 0.25605058670043945
overhead3:: 1.0262978076934814
overhead4:: 44.04563307762146
overhead5:: 0
memory usage:: 5626003456
time_provenance:: 58.93777656555176
curr_diff: 0 tensor(7.3905e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.3905e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0193, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0193, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632600
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 5
max_epoch:: 500
overhead:: 0
overhead2:: 0.2552030086517334
overhead3:: 1.0294296741485596
overhead4:: 42.793816328048706
overhead5:: 0
memory usage:: 5626081280
time_provenance:: 57.68992638587952
curr_diff: 0 tensor(7.3942e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(7.3942e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0193, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0193, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632600
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 3 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 5
max_epoch:: 500
overhead:: 0
overhead2:: 0.5308518409729004
overhead3:: 2.0783703327178955
overhead4:: 80.8655571937561
overhead5:: 0
memory usage:: 5618073600
time_provenance:: 97.51623892784119
curr_diff: 0 tensor(4.9982e-13, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.9982e-13, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0193, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0193, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632600
repetition 4
python3 benchmark_exp_lr.py 0.001 10000 500 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05] Logistic_regression cifar10_2 4 0.0001 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 2.314299
Train - Epoch 1, Batch: 0, Loss: 2.249046
Train - Epoch 2, Batch: 0, Loss: 2.193517
Train - Epoch 3, Batch: 0, Loss: 2.144263
Train - Epoch 4, Batch: 0, Loss: 2.094143
Train - Epoch 5, Batch: 0, Loss: 2.051309
Train - Epoch 6, Batch: 0, Loss: 2.011110
Train - Epoch 7, Batch: 0, Loss: 1.972564
Train - Epoch 8, Batch: 0, Loss: 1.935547
Train - Epoch 9, Batch: 0, Loss: 1.899362
Train - Epoch 10, Batch: 0, Loss: 1.862706
Train - Epoch 11, Batch: 0, Loss: 1.839619
Train - Epoch 12, Batch: 0, Loss: 1.807166
Train - Epoch 13, Batch: 0, Loss: 1.786668
Train - Epoch 14, Batch: 0, Loss: 1.766933
Train - Epoch 15, Batch: 0, Loss: 1.737640
Train - Epoch 16, Batch: 0, Loss: 1.711244
Train - Epoch 17, Batch: 0, Loss: 1.699320
Train - Epoch 18, Batch: 0, Loss: 1.681322
Train - Epoch 19, Batch: 0, Loss: 1.658654
Train - Epoch 20, Batch: 0, Loss: 1.647064
Train - Epoch 21, Batch: 0, Loss: 1.622821
Train - Epoch 22, Batch: 0, Loss: 1.607009
Train - Epoch 23, Batch: 0, Loss: 1.592695
Train - Epoch 24, Batch: 0, Loss: 1.577539
Train - Epoch 25, Batch: 0, Loss: 1.572644
Train - Epoch 26, Batch: 0, Loss: 1.552754
Train - Epoch 27, Batch: 0, Loss: 1.539262
Train - Epoch 28, Batch: 0, Loss: 1.524978
Train - Epoch 29, Batch: 0, Loss: 1.521762
Train - Epoch 30, Batch: 0, Loss: 1.502662
Train - Epoch 31, Batch: 0, Loss: 1.497316
Train - Epoch 32, Batch: 0, Loss: 1.478666
Train - Epoch 33, Batch: 0, Loss: 1.483587
Train - Epoch 34, Batch: 0, Loss: 1.466579
Train - Epoch 35, Batch: 0, Loss: 1.462287
Train - Epoch 36, Batch: 0, Loss: 1.445626
Train - Epoch 37, Batch: 0, Loss: 1.439881
Train - Epoch 38, Batch: 0, Loss: 1.441495
Train - Epoch 39, Batch: 0, Loss: 1.425003
Train - Epoch 40, Batch: 0, Loss: 1.420730
Train - Epoch 41, Batch: 0, Loss: 1.416508
Train - Epoch 42, Batch: 0, Loss: 1.400876
Train - Epoch 43, Batch: 0, Loss: 1.404611
Train - Epoch 44, Batch: 0, Loss: 1.398029
Train - Epoch 45, Batch: 0, Loss: 1.387642
Train - Epoch 46, Batch: 0, Loss: 1.387168
Train - Epoch 47, Batch: 0, Loss: 1.381377
Train - Epoch 48, Batch: 0, Loss: 1.377469
Train - Epoch 49, Batch: 0, Loss: 1.378237
Train - Epoch 50, Batch: 0, Loss: 1.352031
Train - Epoch 51, Batch: 0, Loss: 1.350552
Train - Epoch 52, Batch: 0, Loss: 1.348055
Train - Epoch 53, Batch: 0, Loss: 1.346752
Train - Epoch 54, Batch: 0, Loss: 1.346493
Train - Epoch 55, Batch: 0, Loss: 1.340926
Train - Epoch 56, Batch: 0, Loss: 1.347755
Train - Epoch 57, Batch: 0, Loss: 1.331436
Train - Epoch 58, Batch: 0, Loss: 1.324403
Train - Epoch 59, Batch: 0, Loss: 1.313461
Train - Epoch 60, Batch: 0, Loss: 1.320166
Train - Epoch 61, Batch: 0, Loss: 1.317697
Train - Epoch 62, Batch: 0, Loss: 1.307640
Train - Epoch 63, Batch: 0, Loss: 1.309258
Train - Epoch 64, Batch: 0, Loss: 1.303769
Train - Epoch 65, Batch: 0, Loss: 1.294803
Train - Epoch 66, Batch: 0, Loss: 1.304073
Train - Epoch 67, Batch: 0, Loss: 1.289585
Train - Epoch 68, Batch: 0, Loss: 1.280819
Train - Epoch 69, Batch: 0, Loss: 1.268578
Train - Epoch 70, Batch: 0, Loss: 1.276691
Train - Epoch 71, Batch: 0, Loss: 1.291434
Train - Epoch 72, Batch: 0, Loss: 1.272264
Train - Epoch 73, Batch: 0, Loss: 1.277216
Train - Epoch 74, Batch: 0, Loss: 1.263033
Train - Epoch 75, Batch: 0, Loss: 1.271541
Train - Epoch 76, Batch: 0, Loss: 1.262540
Train - Epoch 77, Batch: 0, Loss: 1.275613
Train - Epoch 78, Batch: 0, Loss: 1.262163
Train - Epoch 79, Batch: 0, Loss: 1.254023
Train - Epoch 80, Batch: 0, Loss: 1.255366
Train - Epoch 81, Batch: 0, Loss: 1.244417
Train - Epoch 82, Batch: 0, Loss: 1.247270
Train - Epoch 83, Batch: 0, Loss: 1.247699
Train - Epoch 84, Batch: 0, Loss: 1.247671
Train - Epoch 85, Batch: 0, Loss: 1.255683
Train - Epoch 86, Batch: 0, Loss: 1.239043
Train - Epoch 87, Batch: 0, Loss: 1.246776
Train - Epoch 88, Batch: 0, Loss: 1.244750
Train - Epoch 89, Batch: 0, Loss: 1.237884
Train - Epoch 90, Batch: 0, Loss: 1.217335
Train - Epoch 91, Batch: 0, Loss: 1.231315
Train - Epoch 92, Batch: 0, Loss: 1.218767
Train - Epoch 93, Batch: 0, Loss: 1.230282
Train - Epoch 94, Batch: 0, Loss: 1.212579
Train - Epoch 95, Batch: 0, Loss: 1.229159
Train - Epoch 96, Batch: 0, Loss: 1.221098
Train - Epoch 97, Batch: 0, Loss: 1.213858
Train - Epoch 98, Batch: 0, Loss: 1.217620
Train - Epoch 99, Batch: 0, Loss: 1.218058
Train - Epoch 100, Batch: 0, Loss: 1.216430
Train - Epoch 101, Batch: 0, Loss: 1.200334
Train - Epoch 102, Batch: 0, Loss: 1.204300
Train - Epoch 103, Batch: 0, Loss: 1.207302
Train - Epoch 104, Batch: 0, Loss: 1.212597
Train - Epoch 105, Batch: 0, Loss: 1.223662
Train - Epoch 106, Batch: 0, Loss: 1.203347
Train - Epoch 107, Batch: 0, Loss: 1.208114
Train - Epoch 108, Batch: 0, Loss: 1.208156
Train - Epoch 109, Batch: 0, Loss: 1.196256
Train - Epoch 110, Batch: 0, Loss: 1.195002
Train - Epoch 111, Batch: 0, Loss: 1.190982
Train - Epoch 112, Batch: 0, Loss: 1.197809
Train - Epoch 113, Batch: 0, Loss: 1.182412
Train - Epoch 114, Batch: 0, Loss: 1.190762
Train - Epoch 115, Batch: 0, Loss: 1.197775
Train - Epoch 116, Batch: 0, Loss: 1.185729
Train - Epoch 117, Batch: 0, Loss: 1.190311
Train - Epoch 118, Batch: 0, Loss: 1.186814
Train - Epoch 119, Batch: 0, Loss: 1.192129
Train - Epoch 120, Batch: 0, Loss: 1.177657
Train - Epoch 121, Batch: 0, Loss: 1.188493
Train - Epoch 122, Batch: 0, Loss: 1.176323
Train - Epoch 123, Batch: 0, Loss: 1.190869
Train - Epoch 124, Batch: 0, Loss: 1.177916
Train - Epoch 125, Batch: 0, Loss: 1.175380
Train - Epoch 126, Batch: 0, Loss: 1.173410
Train - Epoch 127, Batch: 0, Loss: 1.160562
Train - Epoch 128, Batch: 0, Loss: 1.177986
Train - Epoch 129, Batch: 0, Loss: 1.178653
Train - Epoch 130, Batch: 0, Loss: 1.171909
Train - Epoch 131, Batch: 0, Loss: 1.169697
Train - Epoch 132, Batch: 0, Loss: 1.177958
Train - Epoch 133, Batch: 0, Loss: 1.180621
Train - Epoch 134, Batch: 0, Loss: 1.173131
Train - Epoch 135, Batch: 0, Loss: 1.172353
Train - Epoch 136, Batch: 0, Loss: 1.156095
Train - Epoch 137, Batch: 0, Loss: 1.160376
Train - Epoch 138, Batch: 0, Loss: 1.151048
Train - Epoch 139, Batch: 0, Loss: 1.174448
Train - Epoch 140, Batch: 0, Loss: 1.164883
Train - Epoch 141, Batch: 0, Loss: 1.167629
Train - Epoch 142, Batch: 0, Loss: 1.163966
Train - Epoch 143, Batch: 0, Loss: 1.166589
Train - Epoch 144, Batch: 0, Loss: 1.157821
Train - Epoch 145, Batch: 0, Loss: 1.155904
Train - Epoch 146, Batch: 0, Loss: 1.159099
Train - Epoch 147, Batch: 0, Loss: 1.156089
Train - Epoch 148, Batch: 0, Loss: 1.159506
Train - Epoch 149, Batch: 0, Loss: 1.155397
Train - Epoch 150, Batch: 0, Loss: 1.143235
Train - Epoch 151, Batch: 0, Loss: 1.136859
Train - Epoch 152, Batch: 0, Loss: 1.150914
Train - Epoch 153, Batch: 0, Loss: 1.147715
Train - Epoch 154, Batch: 0, Loss: 1.145206
Train - Epoch 155, Batch: 0, Loss: 1.143428
Train - Epoch 156, Batch: 0, Loss: 1.140924
Train - Epoch 157, Batch: 0, Loss: 1.146170
Train - Epoch 158, Batch: 0, Loss: 1.144939
Train - Epoch 159, Batch: 0, Loss: 1.146322
Train - Epoch 160, Batch: 0, Loss: 1.147141
Train - Epoch 161, Batch: 0, Loss: 1.146420
Train - Epoch 162, Batch: 0, Loss: 1.159941
Train - Epoch 163, Batch: 0, Loss: 1.129209
Train - Epoch 164, Batch: 0, Loss: 1.137843
Train - Epoch 165, Batch: 0, Loss: 1.129904
Train - Epoch 166, Batch: 0, Loss: 1.137459
Train - Epoch 167, Batch: 0, Loss: 1.134293
Train - Epoch 168, Batch: 0, Loss: 1.144413
Train - Epoch 169, Batch: 0, Loss: 1.135552
Train - Epoch 170, Batch: 0, Loss: 1.129085
Train - Epoch 171, Batch: 0, Loss: 1.136330
Train - Epoch 172, Batch: 0, Loss: 1.138491
Train - Epoch 173, Batch: 0, Loss: 1.136657
Train - Epoch 174, Batch: 0, Loss: 1.128600
Train - Epoch 175, Batch: 0, Loss: 1.132062
Train - Epoch 176, Batch: 0, Loss: 1.139256
Train - Epoch 177, Batch: 0, Loss: 1.139434
Train - Epoch 178, Batch: 0, Loss: 1.132562
Train - Epoch 179, Batch: 0, Loss: 1.122026
Train - Epoch 180, Batch: 0, Loss: 1.134434
Train - Epoch 181, Batch: 0, Loss: 1.120394
Train - Epoch 182, Batch: 0, Loss: 1.133355
Train - Epoch 183, Batch: 0, Loss: 1.137133
Train - Epoch 184, Batch: 0, Loss: 1.128225
Train - Epoch 185, Batch: 0, Loss: 1.102282
Train - Epoch 186, Batch: 0, Loss: 1.133830
Train - Epoch 187, Batch: 0, Loss: 1.130045
Train - Epoch 188, Batch: 0, Loss: 1.140001
Train - Epoch 189, Batch: 0, Loss: 1.118956
Train - Epoch 190, Batch: 0, Loss: 1.126196
Train - Epoch 191, Batch: 0, Loss: 1.129243
Train - Epoch 192, Batch: 0, Loss: 1.127094
Train - Epoch 193, Batch: 0, Loss: 1.119195
Train - Epoch 194, Batch: 0, Loss: 1.111025
Train - Epoch 195, Batch: 0, Loss: 1.118437
Train - Epoch 196, Batch: 0, Loss: 1.141578
Train - Epoch 197, Batch: 0, Loss: 1.120040
Train - Epoch 198, Batch: 0, Loss: 1.115992
Train - Epoch 199, Batch: 0, Loss: 1.115464
Train - Epoch 200, Batch: 0, Loss: 1.122211
Train - Epoch 201, Batch: 0, Loss: 1.117071
Train - Epoch 202, Batch: 0, Loss: 1.118916
Train - Epoch 203, Batch: 0, Loss: 1.098326
Train - Epoch 204, Batch: 0, Loss: 1.124093
Train - Epoch 205, Batch: 0, Loss: 1.117734
Train - Epoch 206, Batch: 0, Loss: 1.116059
Train - Epoch 207, Batch: 0, Loss: 1.109955
Train - Epoch 208, Batch: 0, Loss: 1.127348
Train - Epoch 209, Batch: 0, Loss: 1.108205
Train - Epoch 210, Batch: 0, Loss: 1.100919
Train - Epoch 211, Batch: 0, Loss: 1.114234
Train - Epoch 212, Batch: 0, Loss: 1.129808
Train - Epoch 213, Batch: 0, Loss: 1.120460
Train - Epoch 214, Batch: 0, Loss: 1.117034
Train - Epoch 215, Batch: 0, Loss: 1.103092
Train - Epoch 216, Batch: 0, Loss: 1.113325
Train - Epoch 217, Batch: 0, Loss: 1.101927
Train - Epoch 218, Batch: 0, Loss: 1.114093
Train - Epoch 219, Batch: 0, Loss: 1.108282
Train - Epoch 220, Batch: 0, Loss: 1.121529
Train - Epoch 221, Batch: 0, Loss: 1.109248
Train - Epoch 222, Batch: 0, Loss: 1.098592
Train - Epoch 223, Batch: 0, Loss: 1.111109
Train - Epoch 224, Batch: 0, Loss: 1.106198
Train - Epoch 225, Batch: 0, Loss: 1.109360
Train - Epoch 226, Batch: 0, Loss: 1.098975
Train - Epoch 227, Batch: 0, Loss: 1.099879
Train - Epoch 228, Batch: 0, Loss: 1.114212
Train - Epoch 229, Batch: 0, Loss: 1.109288
Train - Epoch 230, Batch: 0, Loss: 1.095064
Train - Epoch 231, Batch: 0, Loss: 1.095021
Train - Epoch 232, Batch: 0, Loss: 1.101926
Train - Epoch 233, Batch: 0, Loss: 1.107483
Train - Epoch 234, Batch: 0, Loss: 1.111487
Train - Epoch 235, Batch: 0, Loss: 1.109824
Train - Epoch 236, Batch: 0, Loss: 1.106233
Train - Epoch 237, Batch: 0, Loss: 1.103539
Train - Epoch 238, Batch: 0, Loss: 1.099250
Train - Epoch 239, Batch: 0, Loss: 1.102473
Train - Epoch 240, Batch: 0, Loss: 1.098587
Train - Epoch 241, Batch: 0, Loss: 1.093744
Train - Epoch 242, Batch: 0, Loss: 1.109327
Train - Epoch 243, Batch: 0, Loss: 1.082061
Train - Epoch 244, Batch: 0, Loss: 1.093632
Train - Epoch 245, Batch: 0, Loss: 1.096877
Train - Epoch 246, Batch: 0, Loss: 1.101634
Train - Epoch 247, Batch: 0, Loss: 1.093918
Train - Epoch 248, Batch: 0, Loss: 1.097726
Train - Epoch 249, Batch: 0, Loss: 1.078112
Train - Epoch 250, Batch: 0, Loss: 1.084126
Train - Epoch 251, Batch: 0, Loss: 1.103407
Train - Epoch 252, Batch: 0, Loss: 1.084839
Train - Epoch 253, Batch: 0, Loss: 1.088532
Train - Epoch 254, Batch: 0, Loss: 1.106027
Train - Epoch 255, Batch: 0, Loss: 1.097068
Train - Epoch 256, Batch: 0, Loss: 1.084805
Train - Epoch 257, Batch: 0, Loss: 1.094886
Train - Epoch 258, Batch: 0, Loss: 1.108945
Train - Epoch 259, Batch: 0, Loss: 1.089445
Train - Epoch 260, Batch: 0, Loss: 1.093638
Train - Epoch 261, Batch: 0, Loss: 1.085733
Train - Epoch 262, Batch: 0, Loss: 1.095963
Train - Epoch 263, Batch: 0, Loss: 1.086612
Train - Epoch 264, Batch: 0, Loss: 1.084489
Train - Epoch 265, Batch: 0, Loss: 1.089306
Train - Epoch 266, Batch: 0, Loss: 1.084311
Train - Epoch 267, Batch: 0, Loss: 1.088624
Train - Epoch 268, Batch: 0, Loss: 1.093512
Train - Epoch 269, Batch: 0, Loss: 1.096635
Train - Epoch 270, Batch: 0, Loss: 1.096121
Train - Epoch 271, Batch: 0, Loss: 1.092049
Train - Epoch 272, Batch: 0, Loss: 1.092642
Train - Epoch 273, Batch: 0, Loss: 1.093571
Train - Epoch 274, Batch: 0, Loss: 1.096214
Train - Epoch 275, Batch: 0, Loss: 1.074542
Train - Epoch 276, Batch: 0, Loss: 1.090789
Train - Epoch 277, Batch: 0, Loss: 1.064061
Train - Epoch 278, Batch: 0, Loss: 1.067548
Train - Epoch 279, Batch: 0, Loss: 1.092784
Train - Epoch 280, Batch: 0, Loss: 1.085108
Train - Epoch 281, Batch: 0, Loss: 1.090129
Train - Epoch 282, Batch: 0, Loss: 1.084649
Train - Epoch 283, Batch: 0, Loss: 1.083239
Train - Epoch 284, Batch: 0, Loss: 1.077288
Train - Epoch 285, Batch: 0, Loss: 1.067318
Train - Epoch 286, Batch: 0, Loss: 1.083653
Train - Epoch 287, Batch: 0, Loss: 1.065153
Train - Epoch 288, Batch: 0, Loss: 1.084078
Train - Epoch 289, Batch: 0, Loss: 1.080105
Train - Epoch 290, Batch: 0, Loss: 1.082600
Train - Epoch 291, Batch: 0, Loss: 1.077979
Train - Epoch 292, Batch: 0, Loss: 1.069620
Train - Epoch 293, Batch: 0, Loss: 1.080110
Train - Epoch 294, Batch: 0, Loss: 1.081113
Train - Epoch 295, Batch: 0, Loss: 1.090085
Train - Epoch 296, Batch: 0, Loss: 1.076400
Train - Epoch 297, Batch: 0, Loss: 1.057493
Train - Epoch 298, Batch: 0, Loss: 1.079114
Train - Epoch 299, Batch: 0, Loss: 1.069605
Train - Epoch 300, Batch: 0, Loss: 1.080440
Train - Epoch 301, Batch: 0, Loss: 1.099999
Train - Epoch 302, Batch: 0, Loss: 1.077448
Train - Epoch 303, Batch: 0, Loss: 1.078573
Train - Epoch 304, Batch: 0, Loss: 1.079201
Train - Epoch 305, Batch: 0, Loss: 1.074816
Train - Epoch 306, Batch: 0, Loss: 1.067407
Train - Epoch 307, Batch: 0, Loss: 1.084413
Train - Epoch 308, Batch: 0, Loss: 1.077722
Train - Epoch 309, Batch: 0, Loss: 1.087204
Train - Epoch 310, Batch: 0, Loss: 1.075726
Train - Epoch 311, Batch: 0, Loss: 1.077094
Train - Epoch 312, Batch: 0, Loss: 1.079418
Train - Epoch 313, Batch: 0, Loss: 1.072344
Train - Epoch 314, Batch: 0, Loss: 1.060923
Train - Epoch 315, Batch: 0, Loss: 1.071756
Train - Epoch 316, Batch: 0, Loss: 1.058900
Train - Epoch 317, Batch: 0, Loss: 1.079506
Train - Epoch 318, Batch: 0, Loss: 1.068037
Train - Epoch 319, Batch: 0, Loss: 1.079024
Train - Epoch 320, Batch: 0, Loss: 1.064902
Train - Epoch 321, Batch: 0, Loss: 1.078306
Train - Epoch 322, Batch: 0, Loss: 1.079678
Train - Epoch 323, Batch: 0, Loss: 1.079625
Train - Epoch 324, Batch: 0, Loss: 1.057740
Train - Epoch 325, Batch: 0, Loss: 1.073226
Train - Epoch 326, Batch: 0, Loss: 1.064566
Train - Epoch 327, Batch: 0, Loss: 1.066868
Train - Epoch 328, Batch: 0, Loss: 1.068605
Train - Epoch 329, Batch: 0, Loss: 1.059938
Train - Epoch 330, Batch: 0, Loss: 1.075439
Train - Epoch 331, Batch: 0, Loss: 1.078686
Train - Epoch 332, Batch: 0, Loss: 1.072533
Train - Epoch 333, Batch: 0, Loss: 1.072723
Train - Epoch 334, Batch: 0, Loss: 1.069138
Train - Epoch 335, Batch: 0, Loss: 1.072268
Train - Epoch 336, Batch: 0, Loss: 1.072151
Train - Epoch 337, Batch: 0, Loss: 1.072532
Train - Epoch 338, Batch: 0, Loss: 1.076674
Train - Epoch 339, Batch: 0, Loss: 1.072920
Train - Epoch 340, Batch: 0, Loss: 1.074119
Train - Epoch 341, Batch: 0, Loss: 1.063753
Train - Epoch 342, Batch: 0, Loss: 1.069635
Train - Epoch 343, Batch: 0, Loss: 1.069313
Train - Epoch 344, Batch: 0, Loss: 1.072833
Train - Epoch 345, Batch: 0, Loss: 1.066236
Train - Epoch 346, Batch: 0, Loss: 1.065598
Train - Epoch 347, Batch: 0, Loss: 1.074662
Train - Epoch 348, Batch: 0, Loss: 1.090608
Train - Epoch 349, Batch: 0, Loss: 1.080421
Train - Epoch 350, Batch: 0, Loss: 1.065173
Train - Epoch 351, Batch: 0, Loss: 1.056930
Train - Epoch 352, Batch: 0, Loss: 1.058975
Train - Epoch 353, Batch: 0, Loss: 1.053281
Train - Epoch 354, Batch: 0, Loss: 1.073281
Train - Epoch 355, Batch: 0, Loss: 1.062738
Train - Epoch 356, Batch: 0, Loss: 1.067854
Train - Epoch 357, Batch: 0, Loss: 1.056786
Train - Epoch 358, Batch: 0, Loss: 1.064137
Train - Epoch 359, Batch: 0, Loss: 1.059993
Train - Epoch 360, Batch: 0, Loss: 1.058552
Train - Epoch 361, Batch: 0, Loss: 1.065330
Train - Epoch 362, Batch: 0, Loss: 1.060229
Train - Epoch 363, Batch: 0, Loss: 1.058390
Train - Epoch 364, Batch: 0, Loss: 1.046865
Train - Epoch 365, Batch: 0, Loss: 1.065727
Train - Epoch 366, Batch: 0, Loss: 1.065650
Train - Epoch 367, Batch: 0, Loss: 1.057943
Train - Epoch 368, Batch: 0, Loss: 1.060048
Train - Epoch 369, Batch: 0, Loss: 1.069273
Train - Epoch 370, Batch: 0, Loss: 1.061054
Train - Epoch 371, Batch: 0, Loss: 1.057007
Train - Epoch 372, Batch: 0, Loss: 1.081309
Train - Epoch 373, Batch: 0, Loss: 1.059503/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 374, Batch: 0, Loss: 1.052887
Train - Epoch 375, Batch: 0, Loss: 1.056427
Train - Epoch 376, Batch: 0, Loss: 1.063509
Train - Epoch 377, Batch: 0, Loss: 1.065913
Train - Epoch 378, Batch: 0, Loss: 1.054442
Train - Epoch 379, Batch: 0, Loss: 1.062820
Train - Epoch 380, Batch: 0, Loss: 1.064411
Train - Epoch 381, Batch: 0, Loss: 1.058735
Train - Epoch 382, Batch: 0, Loss: 1.059312
Train - Epoch 383, Batch: 0, Loss: 1.054595
Train - Epoch 384, Batch: 0, Loss: 1.054710
Train - Epoch 385, Batch: 0, Loss: 1.060494
Train - Epoch 386, Batch: 0, Loss: 1.063725
Train - Epoch 387, Batch: 0, Loss: 1.063424
Train - Epoch 388, Batch: 0, Loss: 1.061533
Train - Epoch 389, Batch: 0, Loss: 1.052157
Train - Epoch 390, Batch: 0, Loss: 1.066282
Train - Epoch 391, Batch: 0, Loss: 1.048270
Train - Epoch 392, Batch: 0, Loss: 1.051807
Train - Epoch 393, Batch: 0, Loss: 1.058057
Train - Epoch 394, Batch: 0, Loss: 1.054258
Train - Epoch 395, Batch: 0, Loss: 1.040593
Train - Epoch 396, Batch: 0, Loss: 1.055563
Train - Epoch 397, Batch: 0, Loss: 1.042012
Train - Epoch 398, Batch: 0, Loss: 1.054715
Train - Epoch 399, Batch: 0, Loss: 1.061637
Train - Epoch 400, Batch: 0, Loss: 1.053260
Train - Epoch 401, Batch: 0, Loss: 1.049213
Train - Epoch 402, Batch: 0, Loss: 1.056764
Train - Epoch 403, Batch: 0, Loss: 1.052827
Train - Epoch 404, Batch: 0, Loss: 1.064300
Train - Epoch 405, Batch: 0, Loss: 1.058258
Train - Epoch 406, Batch: 0, Loss: 1.061474
Train - Epoch 407, Batch: 0, Loss: 1.064164
Train - Epoch 408, Batch: 0, Loss: 1.057319
Train - Epoch 409, Batch: 0, Loss: 1.065838
Train - Epoch 410, Batch: 0, Loss: 1.077335
Train - Epoch 411, Batch: 0, Loss: 1.050288
Train - Epoch 412, Batch: 0, Loss: 1.048558
Train - Epoch 413, Batch: 0, Loss: 1.055603
Train - Epoch 414, Batch: 0, Loss: 1.050498
Train - Epoch 415, Batch: 0, Loss: 1.058027
Train - Epoch 416, Batch: 0, Loss: 1.061107
Train - Epoch 417, Batch: 0, Loss: 1.058333
Train - Epoch 418, Batch: 0, Loss: 1.042363
Train - Epoch 419, Batch: 0, Loss: 1.047900
Train - Epoch 420, Batch: 0, Loss: 1.052395
Train - Epoch 421, Batch: 0, Loss: 1.039565
Train - Epoch 422, Batch: 0, Loss: 1.048280
Train - Epoch 423, Batch: 0, Loss: 1.076205
Train - Epoch 424, Batch: 0, Loss: 1.050396
Train - Epoch 425, Batch: 0, Loss: 1.054488
Train - Epoch 426, Batch: 0, Loss: 1.051796
Train - Epoch 427, Batch: 0, Loss: 1.049561
Train - Epoch 428, Batch: 0, Loss: 1.058955
Train - Epoch 429, Batch: 0, Loss: 1.060616
Train - Epoch 430, Batch: 0, Loss: 1.066450
Train - Epoch 431, Batch: 0, Loss: 1.042571
Train - Epoch 432, Batch: 0, Loss: 1.056333
Train - Epoch 433, Batch: 0, Loss: 1.050338
Train - Epoch 434, Batch: 0, Loss: 1.057130
Train - Epoch 435, Batch: 0, Loss: 1.034707
Train - Epoch 436, Batch: 0, Loss: 1.046633
Train - Epoch 437, Batch: 0, Loss: 1.055675
Train - Epoch 438, Batch: 0, Loss: 1.042017
Train - Epoch 439, Batch: 0, Loss: 1.057646
Train - Epoch 440, Batch: 0, Loss: 1.055394
Train - Epoch 441, Batch: 0, Loss: 1.045965
Train - Epoch 442, Batch: 0, Loss: 1.026860
Train - Epoch 443, Batch: 0, Loss: 1.053839
Train - Epoch 444, Batch: 0, Loss: 1.043950
Train - Epoch 445, Batch: 0, Loss: 1.050431
Train - Epoch 446, Batch: 0, Loss: 1.041708
Train - Epoch 447, Batch: 0, Loss: 1.033064
Train - Epoch 448, Batch: 0, Loss: 1.026641
Train - Epoch 449, Batch: 0, Loss: 1.039146
Train - Epoch 450, Batch: 0, Loss: 1.053475
Train - Epoch 451, Batch: 0, Loss: 1.039172
Train - Epoch 452, Batch: 0, Loss: 1.057118
Train - Epoch 453, Batch: 0, Loss: 1.039333
Train - Epoch 454, Batch: 0, Loss: 1.036528
Train - Epoch 455, Batch: 0, Loss: 1.044952
Train - Epoch 456, Batch: 0, Loss: 1.056023
Train - Epoch 457, Batch: 0, Loss: 1.056507
Train - Epoch 458, Batch: 0, Loss: 1.047490
Train - Epoch 459, Batch: 0, Loss: 1.042289
Train - Epoch 460, Batch: 0, Loss: 1.051574
Train - Epoch 461, Batch: 0, Loss: 1.046918
Train - Epoch 462, Batch: 0, Loss: 1.053724
Train - Epoch 463, Batch: 0, Loss: 1.040547
Train - Epoch 464, Batch: 0, Loss: 1.042220
Train - Epoch 465, Batch: 0, Loss: 1.043260
Train - Epoch 466, Batch: 0, Loss: 1.057738
Train - Epoch 467, Batch: 0, Loss: 1.065304
Train - Epoch 468, Batch: 0, Loss: 1.032135
Train - Epoch 469, Batch: 0, Loss: 1.054453
Train - Epoch 470, Batch: 0, Loss: 1.038299
Train - Epoch 471, Batch: 0, Loss: 1.044494
Train - Epoch 472, Batch: 0, Loss: 1.025568
Train - Epoch 473, Batch: 0, Loss: 1.037683
Train - Epoch 474, Batch: 0, Loss: 1.040343
Train - Epoch 475, Batch: 0, Loss: 1.052280
Train - Epoch 476, Batch: 0, Loss: 1.040356
Train - Epoch 477, Batch: 0, Loss: 1.044501
Train - Epoch 478, Batch: 0, Loss: 1.048044
Train - Epoch 479, Batch: 0, Loss: 1.060901
Train - Epoch 480, Batch: 0, Loss: 1.046316
Train - Epoch 481, Batch: 0, Loss: 1.057778
Train - Epoch 482, Batch: 0, Loss: 1.039763
Train - Epoch 483, Batch: 0, Loss: 1.040767
Train - Epoch 484, Batch: 0, Loss: 1.041647
Train - Epoch 485, Batch: 0, Loss: 1.060297
Train - Epoch 486, Batch: 0, Loss: 1.039109
Train - Epoch 487, Batch: 0, Loss: 1.041692
Train - Epoch 488, Batch: 0, Loss: 1.039002
Train - Epoch 489, Batch: 0, Loss: 1.047530
Train - Epoch 490, Batch: 0, Loss: 1.035912
Train - Epoch 491, Batch: 0, Loss: 1.046625
Train - Epoch 492, Batch: 0, Loss: 1.040358
Train - Epoch 493, Batch: 0, Loss: 1.038287
Train - Epoch 494, Batch: 0, Loss: 1.023987
Train - Epoch 495, Batch: 0, Loss: 1.034826
Train - Epoch 496, Batch: 0, Loss: 1.028253
Train - Epoch 497, Batch: 0, Loss: 1.045625
Train - Epoch 498, Batch: 0, Loss: 1.033463
Train - Epoch 499, Batch: 0, Loss: 1.049848
training_time:: 106.87428212165833
training time full:: 106.874351978302
provenance prepare time:: 4.76837158203125e-07
here
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632400
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 5
max_epoch:: 500
batch_size:: 10000
epoch:: 0
tensor([36014, 37325,  3378,  7705, 43323,  4669,   853, 49635, 21735,  7046,
        39942,  2175, 15571, 31141,  7027,  2128, 18176, 48801, 49801, 27558,
        44810,  6823, 27326,  7182, 17458,  3524,  8631,   244,  8326, 37809,
        32265, 20555, 48873, 31202, 39495,  8728, 16806, 41631, 40670, 46916,
        33178, 38623,  6722, 19937, 13953, 34869, 41336,  9084, 26464, 46786])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
epoch:: 60
epoch:: 61
epoch:: 62
epoch:: 63
epoch:: 64
epoch:: 65
epoch:: 66
epoch:: 67
epoch:: 68
epoch:: 69
epoch:: 70
epoch:: 71
epoch:: 72
epoch:: 73
epoch:: 74
epoch:: 75
epoch:: 76
epoch:: 77
epoch:: 78
epoch:: 79
epoch:: 80
epoch:: 81
epoch:: 82
epoch:: 83
epoch:: 84
epoch:: 85
epoch:: 86
epoch:: 87
epoch:: 88
epoch:: 89
epoch:: 90
epoch:: 91
epoch:: 92
epoch:: 93
epoch:: 94
epoch:: 95
epoch:: 96
epoch:: 97
epoch:: 98
epoch:: 99
epoch:: 100
epoch:: 101
epoch:: 102
epoch:: 103
epoch:: 104
epoch:: 105
epoch:: 106
epoch:: 107
epoch:: 108
epoch:: 109
epoch:: 110
epoch:: 111
epoch:: 112
epoch:: 113
epoch:: 114
epoch:: 115
epoch:: 116
epoch:: 117
epoch:: 118
epoch:: 119
epoch:: 120
epoch:: 121
epoch:: 122
epoch:: 123
epoch:: 124
epoch:: 125
epoch:: 126
epoch:: 127
epoch:: 128
epoch:: 129
epoch:: 130
epoch:: 131
epoch:: 132
epoch:: 133
epoch:: 134
epoch:: 135
epoch:: 136
epoch:: 137
epoch:: 138
epoch:: 139
epoch:: 140
epoch:: 141
epoch:: 142
epoch:: 143
epoch:: 144
epoch:: 145
epoch:: 146
epoch:: 147
epoch:: 148
epoch:: 149
epoch:: 150
epoch:: 151
epoch:: 152
epoch:: 153
epoch:: 154
epoch:: 155
epoch:: 156
epoch:: 157
epoch:: 158
epoch:: 159
epoch:: 160
epoch:: 161
epoch:: 162
epoch:: 163
epoch:: 164
epoch:: 165
epoch:: 166
epoch:: 167
epoch:: 168
epoch:: 169
epoch:: 170
epoch:: 171
epoch:: 172
epoch:: 173
epoch:: 174
epoch:: 175
epoch:: 176
epoch:: 177
epoch:: 178
epoch:: 179
epoch:: 180
epoch:: 181
epoch:: 182
epoch:: 183
epoch:: 184
epoch:: 185
epoch:: 186
epoch:: 187
epoch:: 188
epoch:: 189
epoch:: 190
epoch:: 191
epoch:: 192
epoch:: 193
epoch:: 194
epoch:: 195
epoch:: 196
epoch:: 197
epoch:: 198
epoch:: 199
epoch:: 200
epoch:: 201
epoch:: 202
epoch:: 203
epoch:: 204
epoch:: 205
epoch:: 206
epoch:: 207
epoch:: 208
epoch:: 209
epoch:: 210
epoch:: 211
epoch:: 212
epoch:: 213
epoch:: 214
epoch:: 215
epoch:: 216
epoch:: 217
epoch:: 218
epoch:: 219
epoch:: 220
epoch:: 221
epoch:: 222
epoch:: 223
epoch:: 224
epoch:: 225
epoch:: 226
epoch:: 227
epoch:: 228
epoch:: 229
epoch:: 230
epoch:: 231
epoch:: 232
epoch:: 233
epoch:: 234
epoch:: 235
epoch:: 236
epoch:: 237
epoch:: 238
epoch:: 239
epoch:: 240
epoch:: 241
epoch:: 242
epoch:: 243
epoch:: 244
epoch:: 245
epoch:: 246
epoch:: 247
epoch:: 248
epoch:: 249
epoch:: 250
epoch:: 251
epoch:: 252
epoch:: 253
epoch:: 254
epoch:: 255
epoch:: 256
epoch:: 257
epoch:: 258
epoch:: 259
epoch:: 260
epoch:: 261
epoch:: 262
epoch:: 263
epoch:: 264
epoch:: 265
epoch:: 266
epoch:: 267
epoch:: 268
epoch:: 269
epoch:: 270
epoch:: 271
epoch:: 272
epoch:: 273
epoch:: 274
epoch:: 275
epoch:: 276
epoch:: 277
epoch:: 278
epoch:: 279
epoch:: 280
epoch:: 281
epoch:: 282
epoch:: 283
epoch:: 284
epoch:: 285
epoch:: 286
epoch:: 287
epoch:: 288
epoch:: 289
epoch:: 290
epoch:: 291
epoch:: 292
epoch:: 293
epoch:: 294
epoch:: 295
epoch:: 296
epoch:: 297
epoch:: 298
epoch:: 299
epoch:: 300
epoch:: 301
epoch:: 302
epoch:: 303
epoch:: 304
epoch:: 305
epoch:: 306
epoch:: 307
epoch:: 308
epoch:: 309
epoch:: 310
epoch:: 311
epoch:: 312
epoch:: 313
epoch:: 314
epoch:: 315
epoch:: 316
epoch:: 317
epoch:: 318
epoch:: 319
epoch:: 320
epoch:: 321
epoch:: 322
epoch:: 323
epoch:: 324
epoch:: 325
epoch:: 326
epoch:: 327
epoch:: 328
epoch:: 329
epoch:: 330
epoch:: 331
epoch:: 332
epoch:: 333
epoch:: 334
epoch:: 335
epoch:: 336
epoch:: 337
epoch:: 338
epoch:: 339
epoch:: 340
epoch:: 341
epoch:: 342
epoch:: 343
epoch:: 344
epoch:: 345
epoch:: 346
epoch:: 347
epoch:: 348
epoch:: 349
epoch:: 350
epoch:: 351
epoch:: 352
epoch:: 353
epoch:: 354
epoch:: 355
epoch:: 356
epoch:: 357
epoch:: 358
epoch:: 359
epoch:: 360
epoch:: 361
epoch:: 362
epoch:: 363
epoch:: 364
epoch:: 365
epoch:: 366
epoch:: 367
epoch:: 368
epoch:: 369
epoch:: 370
epoch:: 371
epoch:: 372
epoch:: 373
epoch:: 374
epoch:: 375
epoch:: 376
epoch:: 377
epoch:: 378
epoch:: 379
epoch:: 380
epoch:: 381
epoch:: 382
epoch:: 383
epoch:: 384
epoch:: 385
epoch:: 386
epoch:: 387
epoch:: 388
epoch:: 389
epoch:: 390
epoch:: 391
epoch:: 392
epoch:: 393
epoch:: 394
epoch:: 395
epoch:: 396
epoch:: 397
epoch:: 398
epoch:: 399
epoch:: 400
epoch:: 401
epoch:: 402
epoch:: 403
epoch:: 404
epoch:: 405
epoch:: 406
epoch:: 407
epoch:: 408
epoch:: 409
epoch:: 410
epoch:: 411
epoch:: 412
epoch:: 413
epoch:: 414
epoch:: 415
epoch:: 416
epoch:: 417
epoch:: 418
epoch:: 419
epoch:: 420
epoch:: 421
epoch:: 422
epoch:: 423
epoch:: 424
epoch:: 425
epoch:: 426
epoch:: 427
epoch:: 428
epoch:: 429
epoch:: 430
epoch:: 431
epoch:: 432
epoch:: 433
epoch:: 434
epoch:: 435
epoch:: 436
epoch:: 437
epoch:: 438
epoch:: 439
epoch:: 440
epoch:: 441
epoch:: 442
epoch:: 443
epoch:: 444
epoch:: 445
epoch:: 446
epoch:: 447
epoch:: 448
epoch:: 449
epoch:: 450
epoch:: 451
epoch:: 452
epoch:: 453
epoch:: 454
epoch:: 455
epoch:: 456
epoch:: 457
epoch:: 458
epoch:: 459
epoch:: 460
epoch:: 461
epoch:: 462
epoch:: 463
epoch:: 464
epoch:: 465
epoch:: 466
epoch:: 467
epoch:: 468
epoch:: 469
epoch:: 470
epoch:: 471
epoch:: 472
epoch:: 473
epoch:: 474
epoch:: 475
epoch:: 476
epoch:: 477
epoch:: 478
epoch:: 479
epoch:: 480
epoch:: 481
epoch:: 482
epoch:: 483
epoch:: 484
epoch:: 485
epoch:: 486
epoch:: 487
epoch:: 488
epoch:: 489
epoch:: 490
epoch:: 491
epoch:: 492
epoch:: 493
epoch:: 494
epoch:: 495
epoch:: 496
epoch:: 497
epoch:: 498
epoch:: 499
training time is 79.92016315460205
overhead:: 0
overhead2:: 1.8026893138885498
overhead3:: 0
time_baseline:: 79.92019557952881
curr_diff: 0 tensor(0.0192, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0192, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632500
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 5
max_epoch:: 500
overhead:: 0
overhead2:: 0.056078195571899414
overhead3:: 0.24951648712158203
overhead4:: 9.700740814208984
overhead5:: 0
memory usage:: 5629870080
time_provenance:: 16.608362674713135
curr_diff: 0 tensor(0.0047, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0047, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0228, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0228, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632500
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 5
max_epoch:: 500
overhead:: 0
overhead2:: 0.05988335609436035
overhead3:: 0.258455753326416
overhead4:: 10.282683372497559
overhead5:: 0
memory usage:: 5623574528
time_provenance:: 17.253618478775024
curr_diff: 0 tensor(0.0047, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0047, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0228, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0228, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632200
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 5
max_epoch:: 500
overhead:: 0
overhead2:: 0.058157920837402344
overhead3:: 0.2656106948852539
overhead4:: 10.418268918991089
overhead5:: 0
memory usage:: 5640335360
time_provenance:: 17.414018630981445
curr_diff: 0 tensor(0.0047, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0047, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0228, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0228, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632500
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 5
max_epoch:: 500
overhead:: 0
overhead2:: 0.1115102767944336
overhead3:: 0.43415021896362305
overhead4:: 18.066461086273193
overhead5:: 0
memory usage:: 5654917120
time_provenance:: 26.94078755378723
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0195, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0195, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632500
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 5
max_epoch:: 500
overhead:: 0
overhead2:: 0.11620688438415527
overhead3:: 0.45099711418151855
overhead4:: 18.491905450820923
overhead5:: 0
memory usage:: 5625823232
time_provenance:: 27.441343069076538
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0195, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0195, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632500
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 5
max_epoch:: 500
overhead:: 0
overhead2:: 0.12239670753479004
overhead3:: 0.437427282333374
overhead4:: 18.502782583236694
overhead5:: 0
memory usage:: 5642932224
time_provenance:: 27.43006992340088
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0195, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0195, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632500
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 5
max_epoch:: 500
overhead:: 0
overhead2:: 0.26161813735961914
overhead3:: 1.0350978374481201
overhead4:: 43.200026512145996
overhead5:: 0
memory usage:: 5627387904
time_provenance:: 58.150612354278564
curr_diff: 0 tensor(6.2579e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.2579e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0192, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0192, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632600
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 5
max_epoch:: 500
overhead:: 0
overhead2:: 0.264984130859375
overhead3:: 1.0186691284179688
overhead4:: 44.19837784767151
overhead5:: 0
memory usage:: 5629878272
time_provenance:: 59.14084815979004
curr_diff: 0 tensor(6.2821e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.2821e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0192, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0192, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632600
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 5
max_epoch:: 500
overhead:: 0
overhead2:: 0.27228426933288574
overhead3:: 1.0231714248657227
overhead4:: 43.90879774093628
overhead5:: 0
memory usage:: 5627936768
time_provenance:: 58.85059976577759
curr_diff: 0 tensor(6.2843e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.2843e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0192, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0192, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632600
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 4 0.0001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 5
max_epoch:: 500
overhead:: 0
overhead2:: 0.5320761203765869
overhead3:: 2.090589761734009
overhead4:: 79.70125770568848
overhead5:: 0
memory usage:: 5623693312
time_provenance:: 96.56329107284546
curr_diff: 0 tensor(5.0089e-13, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.0089e-13, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0192, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0192, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632500
deletion rate:: 0.0005
python3 generate_rand_ids 0.0005  cifar10_2 0
tensor([43333, 33741,  2445,  4877, 26512, 47253,  8856, 15130, 39900, 36896,
        41572, 32998, 40745, 10409, 39406,  8239, 22513, 22193, 46964, 35638,
        12278,   312, 39992,  3131, 33087])
batch size:: 10000
repetition 0
python3 benchmark_exp_lr.py 0.001 10000 500 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05] Logistic_regression cifar10_2 0 0.0001 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 2.309371
Train - Epoch 1, Batch: 0, Loss: 2.246312
Train - Epoch 2, Batch: 0, Loss: 2.192096
Train - Epoch 3, Batch: 0, Loss: 2.141943
Train - Epoch 4, Batch: 0, Loss: 2.092603
Train - Epoch 5, Batch: 0, Loss: 2.049536
Train - Epoch 6, Batch: 0, Loss: 2.002623
Train - Epoch 7, Batch: 0, Loss: 1.969358
Train - Epoch 8, Batch: 0, Loss: 1.931202
Train - Epoch 9, Batch: 0, Loss: 1.895165
Train - Epoch 10, Batch: 0, Loss: 1.871203
Train - Epoch 11, Batch: 0, Loss: 1.837996
Train - Epoch 12, Batch: 0, Loss: 1.808218
Train - Epoch 13, Batch: 0, Loss: 1.787114
Train - Epoch 14, Batch: 0, Loss: 1.757104
Train - Epoch 15, Batch: 0, Loss: 1.737119
Train - Epoch 16, Batch: 0, Loss: 1.721235
Train - Epoch 17, Batch: 0, Loss: 1.693274
Train - Epoch 18, Batch: 0, Loss: 1.674404
Train - Epoch 19, Batch: 0, Loss: 1.661903
Train - Epoch 20, Batch: 0, Loss: 1.635566
Train - Epoch 21, Batch: 0, Loss: 1.622299
Train - Epoch 22, Batch: 0, Loss: 1.605412
Train - Epoch 23, Batch: 0, Loss: 1.606046
Train - Epoch 24, Batch: 0, Loss: 1.574317
Train - Epoch 25, Batch: 0, Loss: 1.563697
Train - Epoch 26, Batch: 0, Loss: 1.544547
Train - Epoch 27, Batch: 0, Loss: 1.545964
Train - Epoch 28, Batch: 0, Loss: 1.525853
Train - Epoch 29, Batch: 0, Loss: 1.513333
Train - Epoch 30, Batch: 0, Loss: 1.505101
Train - Epoch 31, Batch: 0, Loss: 1.500076
Train - Epoch 32, Batch: 0, Loss: 1.475586
Train - Epoch 33, Batch: 0, Loss: 1.472719
Train - Epoch 34, Batch: 0, Loss: 1.476750
Train - Epoch 35, Batch: 0, Loss: 1.462950
Train - Epoch 36, Batch: 0, Loss: 1.445874
Train - Epoch 37, Batch: 0, Loss: 1.451112
Train - Epoch 38, Batch: 0, Loss: 1.434402
Train - Epoch 39, Batch: 0, Loss: 1.430153
Train - Epoch 40, Batch: 0, Loss: 1.429772
Train - Epoch 41, Batch: 0, Loss: 1.413609
Train - Epoch 42, Batch: 0, Loss: 1.406032
Train - Epoch 43, Batch: 0, Loss: 1.403312
Train - Epoch 44, Batch: 0, Loss: 1.398819
Train - Epoch 45, Batch: 0, Loss: 1.379163
Train - Epoch 46, Batch: 0, Loss: 1.381408
Train - Epoch 47, Batch: 0, Loss: 1.370207
Train - Epoch 48, Batch: 0, Loss: 1.372521
Train - Epoch 49, Batch: 0, Loss: 1.365539
Train - Epoch 50, Batch: 0, Loss: 1.360815
Train - Epoch 51, Batch: 0, Loss: 1.360129
Train - Epoch 52, Batch: 0, Loss: 1.361602
Train - Epoch 53, Batch: 0, Loss: 1.341269
Train - Epoch 54, Batch: 0, Loss: 1.336622
Train - Epoch 55, Batch: 0, Loss: 1.333594
Train - Epoch 56, Batch: 0, Loss: 1.338989
Train - Epoch 57, Batch: 0, Loss: 1.325074
Train - Epoch 58, Batch: 0, Loss: 1.316094
Train - Epoch 59, Batch: 0, Loss: 1.306397
Train - Epoch 60, Batch: 0, Loss: 1.319763
Train - Epoch 61, Batch: 0, Loss: 1.302638
Train - Epoch 62, Batch: 0, Loss: 1.310331
Train - Epoch 63, Batch: 0, Loss: 1.299206
Train - Epoch 64, Batch: 0, Loss: 1.296842
Train - Epoch 65, Batch: 0, Loss: 1.298296
Train - Epoch 66, Batch: 0, Loss: 1.296099
Train - Epoch 67, Batch: 0, Loss: 1.297999
Train - Epoch 68, Batch: 0, Loss: 1.300272
Train - Epoch 69, Batch: 0, Loss: 1.291010
Train - Epoch 70, Batch: 0, Loss: 1.282469
Train - Epoch 71, Batch: 0, Loss: 1.284862
Train - Epoch 72, Batch: 0, Loss: 1.277771
Train - Epoch 73, Batch: 0, Loss: 1.279602
Train - Epoch 74, Batch: 0, Loss: 1.279124
Train - Epoch 75, Batch: 0, Loss: 1.270474
Train - Epoch 76, Batch: 0, Loss: 1.264653
Train - Epoch 77, Batch: 0, Loss: 1.270009
Train - Epoch 78, Batch: 0, Loss: 1.257448
Train - Epoch 79, Batch: 0, Loss: 1.270340
Train - Epoch 80, Batch: 0, Loss: 1.259298
Train - Epoch 81, Batch: 0, Loss: 1.248635
Train - Epoch 82, Batch: 0, Loss: 1.242188
Train - Epoch 83, Batch: 0, Loss: 1.255446
Train - Epoch 84, Batch: 0, Loss: 1.257202
Train - Epoch 85, Batch: 0, Loss: 1.240578
Train - Epoch 86, Batch: 0, Loss: 1.243782
Train - Epoch 87, Batch: 0, Loss: 1.237601
Train - Epoch 88, Batch: 0, Loss: 1.234772
Train - Epoch 89, Batch: 0, Loss: 1.231607
Train - Epoch 90, Batch: 0, Loss: 1.226969
Train - Epoch 91, Batch: 0, Loss: 1.229656
Train - Epoch 92, Batch: 0, Loss: 1.220263
Train - Epoch 93, Batch: 0, Loss: 1.221736
Train - Epoch 94, Batch: 0, Loss: 1.229850
Train - Epoch 95, Batch: 0, Loss: 1.222315
Train - Epoch 96, Batch: 0, Loss: 1.218517
Train - Epoch 97, Batch: 0, Loss: 1.220261
Train - Epoch 98, Batch: 0, Loss: 1.218017
Train - Epoch 99, Batch: 0, Loss: 1.219376
Train - Epoch 100, Batch: 0, Loss: 1.223726
Train - Epoch 101, Batch: 0, Loss: 1.216187
Train - Epoch 102, Batch: 0, Loss: 1.223083
Train - Epoch 103, Batch: 0, Loss: 1.202521
Train - Epoch 104, Batch: 0, Loss: 1.206352
Train - Epoch 105, Batch: 0, Loss: 1.207313
Train - Epoch 106, Batch: 0, Loss: 1.205107
Train - Epoch 107, Batch: 0, Loss: 1.198099
Train - Epoch 108, Batch: 0, Loss: 1.194560
Train - Epoch 109, Batch: 0, Loss: 1.207722
Train - Epoch 110, Batch: 0, Loss: 1.206695
Train - Epoch 111, Batch: 0, Loss: 1.200100
Train - Epoch 112, Batch: 0, Loss: 1.186643
Train - Epoch 113, Batch: 0, Loss: 1.181777
Train - Epoch 114, Batch: 0, Loss: 1.191816
Train - Epoch 115, Batch: 0, Loss: 1.200585
Train - Epoch 116, Batch: 0, Loss: 1.187534
Train - Epoch 117, Batch: 0, Loss: 1.188404
Train - Epoch 118, Batch: 0, Loss: 1.173902
Train - Epoch 119, Batch: 0, Loss: 1.179109
Train - Epoch 120, Batch: 0, Loss: 1.182709
Train - Epoch 121, Batch: 0, Loss: 1.161702
Train - Epoch 122, Batch: 0, Loss: 1.182446
Train - Epoch 123, Batch: 0, Loss: 1.180631
Train - Epoch 124, Batch: 0, Loss: 1.186929
Train - Epoch 125, Batch: 0, Loss: 1.177654
Train - Epoch 126, Batch: 0, Loss: 1.184285
Train - Epoch 127, Batch: 0, Loss: 1.189606
Train - Epoch 128, Batch: 0, Loss: 1.176278
Train - Epoch 129, Batch: 0, Loss: 1.185312
Train - Epoch 130, Batch: 0, Loss: 1.178445
Train - Epoch 131, Batch: 0, Loss: 1.176006
Train - Epoch 132, Batch: 0, Loss: 1.165700
Train - Epoch 133, Batch: 0, Loss: 1.167725
Train - Epoch 134, Batch: 0, Loss: 1.174353
Train - Epoch 135, Batch: 0, Loss: 1.179958
Train - Epoch 136, Batch: 0, Loss: 1.170287
Train - Epoch 137, Batch: 0, Loss: 1.168104
Train - Epoch 138, Batch: 0, Loss: 1.161448
Train - Epoch 139, Batch: 0, Loss: 1.148595
Train - Epoch 140, Batch: 0, Loss: 1.164278
Train - Epoch 141, Batch: 0, Loss: 1.169554
Train - Epoch 142, Batch: 0, Loss: 1.144872
Train - Epoch 143, Batch: 0, Loss: 1.156997
Train - Epoch 144, Batch: 0, Loss: 1.169205
Train - Epoch 145, Batch: 0, Loss: 1.161273
Train - Epoch 146, Batch: 0, Loss: 1.161680
Train - Epoch 147, Batch: 0, Loss: 1.143124
Train - Epoch 148, Batch: 0, Loss: 1.153056
Train - Epoch 149, Batch: 0, Loss: 1.142372
Train - Epoch 150, Batch: 0, Loss: 1.156835
Train - Epoch 151, Batch: 0, Loss: 1.156587
Train - Epoch 152, Batch: 0, Loss: 1.159341
Train - Epoch 153, Batch: 0, Loss: 1.157869
Train - Epoch 154, Batch: 0, Loss: 1.154700
Train - Epoch 155, Batch: 0, Loss: 1.146255
Train - Epoch 156, Batch: 0, Loss: 1.148144
Train - Epoch 157, Batch: 0, Loss: 1.145713
Train - Epoch 158, Batch: 0, Loss: 1.151716
Train - Epoch 159, Batch: 0, Loss: 1.158872
Train - Epoch 160, Batch: 0, Loss: 1.146715
Train - Epoch 161, Batch: 0, Loss: 1.125918
Train - Epoch 162, Batch: 0, Loss: 1.149025
Train - Epoch 163, Batch: 0, Loss: 1.142822
Train - Epoch 164, Batch: 0, Loss: 1.154211
Train - Epoch 165, Batch: 0, Loss: 1.133749
Train - Epoch 166, Batch: 0, Loss: 1.144853
Train - Epoch 167, Batch: 0, Loss: 1.140402
Train - Epoch 168, Batch: 0, Loss: 1.124852
Train - Epoch 169, Batch: 0, Loss: 1.133555
Train - Epoch 170, Batch: 0, Loss: 1.140844
Train - Epoch 171, Batch: 0, Loss: 1.141152
Train - Epoch 172, Batch: 0, Loss: 1.135970
Train - Epoch 173, Batch: 0, Loss: 1.133623
Train - Epoch 174, Batch: 0, Loss: 1.133934
Train - Epoch 175, Batch: 0, Loss: 1.129409
Train - Epoch 176, Batch: 0, Loss: 1.138915
Train - Epoch 177, Batch: 0, Loss: 1.131054
Train - Epoch 178, Batch: 0, Loss: 1.128520
Train - Epoch 179, Batch: 0, Loss: 1.130989
Train - Epoch 180, Batch: 0, Loss: 1.132996
Train - Epoch 181, Batch: 0, Loss: 1.141689
Train - Epoch 182, Batch: 0, Loss: 1.130330
Train - Epoch 183, Batch: 0, Loss: 1.125852
Train - Epoch 184, Batch: 0, Loss: 1.103644
Train - Epoch 185, Batch: 0, Loss: 1.129211
Train - Epoch 186, Batch: 0, Loss: 1.142493
Train - Epoch 187, Batch: 0, Loss: 1.123352
Train - Epoch 188, Batch: 0, Loss: 1.126931
Train - Epoch 189, Batch: 0, Loss: 1.147918
Train - Epoch 190, Batch: 0, Loss: 1.130719
Train - Epoch 191, Batch: 0, Loss: 1.126896
Train - Epoch 192, Batch: 0, Loss: 1.117401
Train - Epoch 193, Batch: 0, Loss: 1.130978
Train - Epoch 194, Batch: 0, Loss: 1.114250
Train - Epoch 195, Batch: 0, Loss: 1.115789
Train - Epoch 196, Batch: 0, Loss: 1.117894
Train - Epoch 197, Batch: 0, Loss: 1.106596
Train - Epoch 198, Batch: 0, Loss: 1.122807
Train - Epoch 199, Batch: 0, Loss: 1.117017
Train - Epoch 200, Batch: 0, Loss: 1.126645
Train - Epoch 201, Batch: 0, Loss: 1.131053
Train - Epoch 202, Batch: 0, Loss: 1.125372
Train - Epoch 203, Batch: 0, Loss: 1.115370
Train - Epoch 204, Batch: 0, Loss: 1.132385
Train - Epoch 205, Batch: 0, Loss: 1.115490
Train - Epoch 206, Batch: 0, Loss: 1.108922
Train - Epoch 207, Batch: 0, Loss: 1.121018
Train - Epoch 208, Batch: 0, Loss: 1.109253
Train - Epoch 209, Batch: 0, Loss: 1.102511
Train - Epoch 210, Batch: 0, Loss: 1.109961
Train - Epoch 211, Batch: 0, Loss: 1.121615
Train - Epoch 212, Batch: 0, Loss: 1.103271
Train - Epoch 213, Batch: 0, Loss: 1.090542
Train - Epoch 214, Batch: 0, Loss: 1.109415
Train - Epoch 215, Batch: 0, Loss: 1.107103
Train - Epoch 216, Batch: 0, Loss: 1.109445
Train - Epoch 217, Batch: 0, Loss: 1.123690
Train - Epoch 218, Batch: 0, Loss: 1.108636
Train - Epoch 219, Batch: 0, Loss: 1.098450
Train - Epoch 220, Batch: 0, Loss: 1.114662
Train - Epoch 221, Batch: 0, Loss: 1.106524
Train - Epoch 222, Batch: 0, Loss: 1.132529
Train - Epoch 223, Batch: 0, Loss: 1.106850
Train - Epoch 224, Batch: 0, Loss: 1.108710
Train - Epoch 225, Batch: 0, Loss: 1.108471
Train - Epoch 226, Batch: 0, Loss: 1.101758
Train - Epoch 227, Batch: 0, Loss: 1.092343
Train - Epoch 228, Batch: 0, Loss: 1.113318
Train - Epoch 229, Batch: 0, Loss: 1.083754
Train - Epoch 230, Batch: 0, Loss: 1.107228
Train - Epoch 231, Batch: 0, Loss: 1.099284
Train - Epoch 232, Batch: 0, Loss: 1.114591
Train - Epoch 233, Batch: 0, Loss: 1.099813
Train - Epoch 234, Batch: 0, Loss: 1.108607
Train - Epoch 235, Batch: 0, Loss: 1.103425
Train - Epoch 236, Batch: 0, Loss: 1.111779
Train - Epoch 237, Batch: 0, Loss: 1.095069
Train - Epoch 238, Batch: 0, Loss: 1.107820
Train - Epoch 239, Batch: 0, Loss: 1.089662
Train - Epoch 240, Batch: 0, Loss: 1.107196
Train - Epoch 241, Batch: 0, Loss: 1.099397
Train - Epoch 242, Batch: 0, Loss: 1.100290
Train - Epoch 243, Batch: 0, Loss: 1.088636
Train - Epoch 244, Batch: 0, Loss: 1.100183
Train - Epoch 245, Batch: 0, Loss: 1.092722
Train - Epoch 246, Batch: 0, Loss: 1.096761
Train - Epoch 247, Batch: 0, Loss: 1.091665
Train - Epoch 248, Batch: 0, Loss: 1.085265
Train - Epoch 249, Batch: 0, Loss: 1.094154
Train - Epoch 250, Batch: 0, Loss: 1.091723
Train - Epoch 251, Batch: 0, Loss: 1.083047
Train - Epoch 252, Batch: 0, Loss: 1.092214
Train - Epoch 253, Batch: 0, Loss: 1.083424
Train - Epoch 254, Batch: 0, Loss: 1.083486
Train - Epoch 255, Batch: 0, Loss: 1.102127
Train - Epoch 256, Batch: 0, Loss: 1.083761
Train - Epoch 257, Batch: 0, Loss: 1.098171
Train - Epoch 258, Batch: 0, Loss: 1.082275
Train - Epoch 259, Batch: 0, Loss: 1.092268
Train - Epoch 260, Batch: 0, Loss: 1.096142
Train - Epoch 261, Batch: 0, Loss: 1.087957
Train - Epoch 262, Batch: 0, Loss: 1.098333
Train - Epoch 263, Batch: 0, Loss: 1.107745
Train - Epoch 264, Batch: 0, Loss: 1.078298
Train - Epoch 265, Batch: 0, Loss: 1.090741
Train - Epoch 266, Batch: 0, Loss: 1.080237
Train - Epoch 267, Batch: 0, Loss: 1.102491
Train - Epoch 268, Batch: 0, Loss: 1.085649
Train - Epoch 269, Batch: 0, Loss: 1.080264
Train - Epoch 270, Batch: 0, Loss: 1.096185
Train - Epoch 271, Batch: 0, Loss: 1.086232
Train - Epoch 272, Batch: 0, Loss: 1.089929
Train - Epoch 273, Batch: 0, Loss: 1.083740
Train - Epoch 274, Batch: 0, Loss: 1.086128
Train - Epoch 275, Batch: 0, Loss: 1.083067
Train - Epoch 276, Batch: 0, Loss: 1.089619
Train - Epoch 277, Batch: 0, Loss: 1.094390
Train - Epoch 278, Batch: 0, Loss: 1.087744
Train - Epoch 279, Batch: 0, Loss: 1.089933
Train - Epoch 280, Batch: 0, Loss: 1.110298
Train - Epoch 281, Batch: 0, Loss: 1.085517
Train - Epoch 282, Batch: 0, Loss: 1.075681
Train - Epoch 283, Batch: 0, Loss: 1.083767
Train - Epoch 284, Batch: 0, Loss: 1.086723
Train - Epoch 285, Batch: 0, Loss: 1.080595
Train - Epoch 286, Batch: 0, Loss: 1.083998
Train - Epoch 287, Batch: 0, Loss: 1.091685
Train - Epoch 288, Batch: 0, Loss: 1.071832
Train - Epoch 289, Batch: 0, Loss: 1.069121
Train - Epoch 290, Batch: 0, Loss: 1.088752
Train - Epoch 291, Batch: 0, Loss: 1.075748
Train - Epoch 292, Batch: 0, Loss: 1.084364
Train - Epoch 293, Batch: 0, Loss: 1.105442
Train - Epoch 294, Batch: 0, Loss: 1.094096
Train - Epoch 295, Batch: 0, Loss: 1.094064
Train - Epoch 296, Batch: 0, Loss: 1.078278
Train - Epoch 297, Batch: 0, Loss: 1.098508
Train - Epoch 298, Batch: 0, Loss: 1.073183
Train - Epoch 299, Batch: 0, Loss: 1.075109
Train - Epoch 300, Batch: 0, Loss: 1.081476
Train - Epoch 301, Batch: 0, Loss: 1.077649
Train - Epoch 302, Batch: 0, Loss: 1.081332
Train - Epoch 303, Batch: 0, Loss: 1.079081
Train - Epoch 304, Batch: 0, Loss: 1.085593
Train - Epoch 305, Batch: 0, Loss: 1.071472
Train - Epoch 306, Batch: 0, Loss: 1.081527
Train - Epoch 307, Batch: 0, Loss: 1.067996
Train - Epoch 308, Batch: 0, Loss: 1.077204
Train - Epoch 309, Batch: 0, Loss: 1.084670
Train - Epoch 310, Batch: 0, Loss: 1.068437
Train - Epoch 311, Batch: 0, Loss: 1.077658
Train - Epoch 312, Batch: 0, Loss: 1.070274
Train - Epoch 313, Batch: 0, Loss: 1.077629
Train - Epoch 314, Batch: 0, Loss: 1.084713
Train - Epoch 315, Batch: 0, Loss: 1.059389
Train - Epoch 316, Batch: 0, Loss: 1.066656
Train - Epoch 317, Batch: 0, Loss: 1.080470
Train - Epoch 318, Batch: 0, Loss: 1.084016
Train - Epoch 319, Batch: 0, Loss: 1.078747
Train - Epoch 320, Batch: 0, Loss: 1.075547
Train - Epoch 321, Batch: 0, Loss: 1.077155
Train - Epoch 322, Batch: 0, Loss: 1.060861
Train - Epoch 323, Batch: 0, Loss: 1.050607
Train - Epoch 324, Batch: 0, Loss: 1.059389
Train - Epoch 325, Batch: 0, Loss: 1.059782
Train - Epoch 326, Batch: 0, Loss: 1.056227
Train - Epoch 327, Batch: 0, Loss: 1.067870
Train - Epoch 328, Batch: 0, Loss: 1.067826
Train - Epoch 329, Batch: 0, Loss: 1.077924
Train - Epoch 330, Batch: 0, Loss: 1.086837
Train - Epoch 331, Batch: 0, Loss: 1.056257
Train - Epoch 332, Batch: 0, Loss: 1.075435
Train - Epoch 333, Batch: 0, Loss: 1.082379
Train - Epoch 334, Batch: 0, Loss: 1.054822
Train - Epoch 335, Batch: 0, Loss: 1.064297
Train - Epoch 336, Batch: 0, Loss: 1.079359
Train - Epoch 337, Batch: 0, Loss: 1.058827
Train - Epoch 338, Batch: 0, Loss: 1.073449
Train - Epoch 339, Batch: 0, Loss: 1.063769
Train - Epoch 340, Batch: 0, Loss: 1.063553
Train - Epoch 341, Batch: 0, Loss: 1.057261
Train - Epoch 342, Batch: 0, Loss: 1.064106
Train - Epoch 343, Batch: 0, Loss: 1.058710
Train - Epoch 344, Batch: 0, Loss: 1.071271
Train - Epoch 345, Batch: 0, Loss: 1.057813
Train - Epoch 346, Batch: 0, Loss: 1.067202
Train - Epoch 347, Batch: 0, Loss: 1.062859
Train - Epoch 348, Batch: 0, Loss: 1.062095
Train - Epoch 349, Batch: 0, Loss: 1.049697
Train - Epoch 350, Batch: 0, Loss: 1.067237
Train - Epoch 351, Batch: 0, Loss: 1.068173
Train - Epoch 352, Batch: 0, Loss: 1.070245
Train - Epoch 353, Batch: 0, Loss: 1.066341
Train - Epoch 354, Batch: 0, Loss: 1.070372
Train - Epoch 355, Batch: 0, Loss: 1.075622
Train - Epoch 356, Batch: 0, Loss: 1.073485
Train - Epoch 357, Batch: 0, Loss: 1.060444
Train - Epoch 358, Batch: 0, Loss: 1.047844
Train - Epoch 359, Batch: 0, Loss: 1.060768
Train - Epoch 360, Batch: 0, Loss: 1.071170
Train - Epoch 361, Batch: 0, Loss: 1.046300
Train - Epoch 362, Batch: 0, Loss: 1.062648
Train - Epoch 363, Batch: 0, Loss: 1.055446
Train - Epoch 364, Batch: 0, Loss: 1.055342
Train - Epoch 365, Batch: 0, Loss: 1.054146
Train - Epoch 366, Batch: 0, Loss: 1.045523
Train - Epoch 367, Batch: 0, Loss: 1.064250
Train - Epoch 368, Batch: 0, Loss: 1.046613
Train - Epoch 369, Batch: 0, Loss: 1.071317
Train - Epoch 370, Batch: 0, Loss: 1.060024
Train - Epoch 371, Batch: 0, Loss: 1.062905
Train - Epoch 372, Batch: 0, Loss: 1.055150
Train - Epoch 373, Batch: 0, Loss: 1.051073/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 374, Batch: 0, Loss: 1.046086
Train - Epoch 375, Batch: 0, Loss: 1.053704
Train - Epoch 376, Batch: 0, Loss: 1.051627
Train - Epoch 377, Batch: 0, Loss: 1.060306
Train - Epoch 378, Batch: 0, Loss: 1.060287
Train - Epoch 379, Batch: 0, Loss: 1.041313
Train - Epoch 380, Batch: 0, Loss: 1.051820
Train - Epoch 381, Batch: 0, Loss: 1.054348
Train - Epoch 382, Batch: 0, Loss: 1.054375
Train - Epoch 383, Batch: 0, Loss: 1.065002
Train - Epoch 384, Batch: 0, Loss: 1.048940
Train - Epoch 385, Batch: 0, Loss: 1.052520
Train - Epoch 386, Batch: 0, Loss: 1.069015
Train - Epoch 387, Batch: 0, Loss: 1.033624
Train - Epoch 388, Batch: 0, Loss: 1.053091
Train - Epoch 389, Batch: 0, Loss: 1.061025
Train - Epoch 390, Batch: 0, Loss: 1.057456
Train - Epoch 391, Batch: 0, Loss: 1.048625
Train - Epoch 392, Batch: 0, Loss: 1.050452
Train - Epoch 393, Batch: 0, Loss: 1.072115
Train - Epoch 394, Batch: 0, Loss: 1.065536
Train - Epoch 395, Batch: 0, Loss: 1.058982
Train - Epoch 396, Batch: 0, Loss: 1.051877
Train - Epoch 397, Batch: 0, Loss: 1.067015
Train - Epoch 398, Batch: 0, Loss: 1.063315
Train - Epoch 399, Batch: 0, Loss: 1.061592
Train - Epoch 400, Batch: 0, Loss: 1.050441
Train - Epoch 401, Batch: 0, Loss: 1.051924
Train - Epoch 402, Batch: 0, Loss: 1.065554
Train - Epoch 403, Batch: 0, Loss: 1.055421
Train - Epoch 404, Batch: 0, Loss: 1.047429
Train - Epoch 405, Batch: 0, Loss: 1.043716
Train - Epoch 406, Batch: 0, Loss: 1.056780
Train - Epoch 407, Batch: 0, Loss: 1.059141
Train - Epoch 408, Batch: 0, Loss: 1.038789
Train - Epoch 409, Batch: 0, Loss: 1.055366
Train - Epoch 410, Batch: 0, Loss: 1.066335
Train - Epoch 411, Batch: 0, Loss: 1.050882
Train - Epoch 412, Batch: 0, Loss: 1.039045
Train - Epoch 413, Batch: 0, Loss: 1.036412
Train - Epoch 414, Batch: 0, Loss: 1.048175
Train - Epoch 415, Batch: 0, Loss: 1.052376
Train - Epoch 416, Batch: 0, Loss: 1.054200
Train - Epoch 417, Batch: 0, Loss: 1.059258
Train - Epoch 418, Batch: 0, Loss: 1.064547
Train - Epoch 419, Batch: 0, Loss: 1.055982
Train - Epoch 420, Batch: 0, Loss: 1.060616
Train - Epoch 421, Batch: 0, Loss: 1.054375
Train - Epoch 422, Batch: 0, Loss: 1.058906
Train - Epoch 423, Batch: 0, Loss: 1.054266
Train - Epoch 424, Batch: 0, Loss: 1.051036
Train - Epoch 425, Batch: 0, Loss: 1.057344
Train - Epoch 426, Batch: 0, Loss: 1.037440
Train - Epoch 427, Batch: 0, Loss: 1.035790
Train - Epoch 428, Batch: 0, Loss: 1.061623
Train - Epoch 429, Batch: 0, Loss: 1.060736
Train - Epoch 430, Batch: 0, Loss: 1.048372
Train - Epoch 431, Batch: 0, Loss: 1.051905
Train - Epoch 432, Batch: 0, Loss: 1.046751
Train - Epoch 433, Batch: 0, Loss: 1.040179
Train - Epoch 434, Batch: 0, Loss: 1.038175
Train - Epoch 435, Batch: 0, Loss: 1.046313
Train - Epoch 436, Batch: 0, Loss: 1.035230
Train - Epoch 437, Batch: 0, Loss: 1.051935
Train - Epoch 438, Batch: 0, Loss: 1.054715
Train - Epoch 439, Batch: 0, Loss: 1.055442
Train - Epoch 440, Batch: 0, Loss: 1.051855
Train - Epoch 441, Batch: 0, Loss: 1.052838
Train - Epoch 442, Batch: 0, Loss: 1.046892
Train - Epoch 443, Batch: 0, Loss: 1.047267
Train - Epoch 444, Batch: 0, Loss: 1.033540
Train - Epoch 445, Batch: 0, Loss: 1.029762
Train - Epoch 446, Batch: 0, Loss: 1.038640
Train - Epoch 447, Batch: 0, Loss: 1.039555
Train - Epoch 448, Batch: 0, Loss: 1.040949
Train - Epoch 449, Batch: 0, Loss: 1.047891
Train - Epoch 450, Batch: 0, Loss: 1.055807
Train - Epoch 451, Batch: 0, Loss: 1.032549
Train - Epoch 452, Batch: 0, Loss: 1.053235
Train - Epoch 453, Batch: 0, Loss: 1.045042
Train - Epoch 454, Batch: 0, Loss: 1.040606
Train - Epoch 455, Batch: 0, Loss: 1.046515
Train - Epoch 456, Batch: 0, Loss: 1.044273
Train - Epoch 457, Batch: 0, Loss: 1.038958
Train - Epoch 458, Batch: 0, Loss: 1.062344
Train - Epoch 459, Batch: 0, Loss: 1.052299
Train - Epoch 460, Batch: 0, Loss: 1.055876
Train - Epoch 461, Batch: 0, Loss: 1.040236
Train - Epoch 462, Batch: 0, Loss: 1.033487
Train - Epoch 463, Batch: 0, Loss: 1.053675
Train - Epoch 464, Batch: 0, Loss: 1.052239
Train - Epoch 465, Batch: 0, Loss: 1.044224
Train - Epoch 466, Batch: 0, Loss: 1.037919
Train - Epoch 467, Batch: 0, Loss: 1.050398
Train - Epoch 468, Batch: 0, Loss: 1.048627
Train - Epoch 469, Batch: 0, Loss: 1.049152
Train - Epoch 470, Batch: 0, Loss: 1.058480
Train - Epoch 471, Batch: 0, Loss: 1.042155
Train - Epoch 472, Batch: 0, Loss: 1.048995
Train - Epoch 473, Batch: 0, Loss: 1.046439
Train - Epoch 474, Batch: 0, Loss: 1.032224
Train - Epoch 475, Batch: 0, Loss: 1.048022
Train - Epoch 476, Batch: 0, Loss: 1.036158
Train - Epoch 477, Batch: 0, Loss: 1.043853
Train - Epoch 478, Batch: 0, Loss: 1.032384
Train - Epoch 479, Batch: 0, Loss: 1.048609
Train - Epoch 480, Batch: 0, Loss: 1.046019
Train - Epoch 481, Batch: 0, Loss: 1.050912
Train - Epoch 482, Batch: 0, Loss: 1.049375
Train - Epoch 483, Batch: 0, Loss: 1.033208
Train - Epoch 484, Batch: 0, Loss: 1.050525
Train - Epoch 485, Batch: 0, Loss: 1.035410
Train - Epoch 486, Batch: 0, Loss: 1.046263
Train - Epoch 487, Batch: 0, Loss: 1.024934
Train - Epoch 488, Batch: 0, Loss: 1.036169
Train - Epoch 489, Batch: 0, Loss: 1.052887
Train - Epoch 490, Batch: 0, Loss: 1.056647
Train - Epoch 491, Batch: 0, Loss: 1.017953
Train - Epoch 492, Batch: 0, Loss: 1.053092
Train - Epoch 493, Batch: 0, Loss: 1.055257
Train - Epoch 494, Batch: 0, Loss: 1.037097
Train - Epoch 495, Batch: 0, Loss: 1.046097
Train - Epoch 496, Batch: 0, Loss: 1.056061
Train - Epoch 497, Batch: 0, Loss: 1.028843
Train - Epoch 498, Batch: 0, Loss: 1.020767
Train - Epoch 499, Batch: 0, Loss: 1.037156
training_time:: 106.0243661403656
training time full:: 106.02444005012512
provenance prepare time:: 0.0
here
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632300
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 25
max_epoch:: 500
batch_size:: 10000
epoch:: 0
tensor([    3, 32773,     7,    13,    17,    22,    24,    25,    32, 32801,
        32802,    35, 32803,    36, 32806, 32810, 32812, 32815,    49,    53,
           54, 32823,    61,    67, 32835,    68,    71, 32840, 32841, 32842,
        32843, 32848,    81, 32850, 32852,    85,    86,    91, 32860, 32862,
           95,    98,    99,   100,   102,   104,   105,   107, 32876,   117])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
epoch:: 60
epoch:: 61
epoch:: 62
epoch:: 63
epoch:: 64
epoch:: 65
epoch:: 66
epoch:: 67
epoch:: 68
epoch:: 69
epoch:: 70
epoch:: 71
epoch:: 72
epoch:: 73
epoch:: 74
epoch:: 75
epoch:: 76
epoch:: 77
epoch:: 78
epoch:: 79
epoch:: 80
epoch:: 81
epoch:: 82
epoch:: 83
epoch:: 84
epoch:: 85
epoch:: 86
epoch:: 87
epoch:: 88
epoch:: 89
epoch:: 90
epoch:: 91
epoch:: 92
epoch:: 93
epoch:: 94
epoch:: 95
epoch:: 96
epoch:: 97
epoch:: 98
epoch:: 99
epoch:: 100
epoch:: 101
epoch:: 102
epoch:: 103
epoch:: 104
epoch:: 105
epoch:: 106
epoch:: 107
epoch:: 108
epoch:: 109
epoch:: 110
epoch:: 111
epoch:: 112
epoch:: 113
epoch:: 114
epoch:: 115
epoch:: 116
epoch:: 117
epoch:: 118
epoch:: 119
epoch:: 120
epoch:: 121
epoch:: 122
epoch:: 123
epoch:: 124
epoch:: 125
epoch:: 126
epoch:: 127
epoch:: 128
epoch:: 129
epoch:: 130
epoch:: 131
epoch:: 132
epoch:: 133
epoch:: 134
epoch:: 135
epoch:: 136
epoch:: 137
epoch:: 138
epoch:: 139
epoch:: 140
epoch:: 141
epoch:: 142
epoch:: 143
epoch:: 144
epoch:: 145
epoch:: 146
epoch:: 147
epoch:: 148
epoch:: 149
epoch:: 150
epoch:: 151
epoch:: 152
epoch:: 153
epoch:: 154
epoch:: 155
epoch:: 156
epoch:: 157
epoch:: 158
epoch:: 159
epoch:: 160
epoch:: 161
epoch:: 162
epoch:: 163
epoch:: 164
epoch:: 165
epoch:: 166
epoch:: 167
epoch:: 168
epoch:: 169
epoch:: 170
epoch:: 171
epoch:: 172
epoch:: 173
epoch:: 174
epoch:: 175
epoch:: 176
epoch:: 177
epoch:: 178
epoch:: 179
epoch:: 180
epoch:: 181
epoch:: 182
epoch:: 183
epoch:: 184
epoch:: 185
epoch:: 186
epoch:: 187
epoch:: 188
epoch:: 189
epoch:: 190
epoch:: 191
epoch:: 192
epoch:: 193
epoch:: 194
epoch:: 195
epoch:: 196
epoch:: 197
epoch:: 198
epoch:: 199
epoch:: 200
epoch:: 201
epoch:: 202
epoch:: 203
epoch:: 204
epoch:: 205
epoch:: 206
epoch:: 207
epoch:: 208
epoch:: 209
epoch:: 210
epoch:: 211
epoch:: 212
epoch:: 213
epoch:: 214
epoch:: 215
epoch:: 216
epoch:: 217
epoch:: 218
epoch:: 219
epoch:: 220
epoch:: 221
epoch:: 222
epoch:: 223
epoch:: 224
epoch:: 225
epoch:: 226
epoch:: 227
epoch:: 228
epoch:: 229
epoch:: 230
epoch:: 231
epoch:: 232
epoch:: 233
epoch:: 234
epoch:: 235
epoch:: 236
epoch:: 237
epoch:: 238
epoch:: 239
epoch:: 240
epoch:: 241
epoch:: 242
epoch:: 243
epoch:: 244
epoch:: 245
epoch:: 246
epoch:: 247
epoch:: 248
epoch:: 249
epoch:: 250
epoch:: 251
epoch:: 252
epoch:: 253
epoch:: 254
epoch:: 255
epoch:: 256
epoch:: 257
epoch:: 258
epoch:: 259
epoch:: 260
epoch:: 261
epoch:: 262
epoch:: 263
epoch:: 264
epoch:: 265
epoch:: 266
epoch:: 267
epoch:: 268
epoch:: 269
epoch:: 270
epoch:: 271
epoch:: 272
epoch:: 273
epoch:: 274
epoch:: 275
epoch:: 276
epoch:: 277
epoch:: 278
epoch:: 279
epoch:: 280
epoch:: 281
epoch:: 282
epoch:: 283
epoch:: 284
epoch:: 285
epoch:: 286
epoch:: 287
epoch:: 288
epoch:: 289
epoch:: 290
epoch:: 291
epoch:: 292
epoch:: 293
epoch:: 294
epoch:: 295
epoch:: 296
epoch:: 297
epoch:: 298
epoch:: 299
epoch:: 300
epoch:: 301
epoch:: 302
epoch:: 303
epoch:: 304
epoch:: 305
epoch:: 306
epoch:: 307
epoch:: 308
epoch:: 309
epoch:: 310
epoch:: 311
epoch:: 312
epoch:: 313
epoch:: 314
epoch:: 315
epoch:: 316
epoch:: 317
epoch:: 318
epoch:: 319
epoch:: 320
epoch:: 321
epoch:: 322
epoch:: 323
epoch:: 324
epoch:: 325
epoch:: 326
epoch:: 327
epoch:: 328
epoch:: 329
epoch:: 330
epoch:: 331
epoch:: 332
epoch:: 333
epoch:: 334
epoch:: 335
epoch:: 336
epoch:: 337
epoch:: 338
epoch:: 339
epoch:: 340
epoch:: 341
epoch:: 342
epoch:: 343
epoch:: 344
epoch:: 345
epoch:: 346
epoch:: 347
epoch:: 348
epoch:: 349
epoch:: 350
epoch:: 351
epoch:: 352
epoch:: 353
epoch:: 354
epoch:: 355
epoch:: 356
epoch:: 357
epoch:: 358
epoch:: 359
epoch:: 360
epoch:: 361
epoch:: 362
epoch:: 363
epoch:: 364
epoch:: 365
epoch:: 366
epoch:: 367
epoch:: 368
epoch:: 369
epoch:: 370
epoch:: 371
epoch:: 372
epoch:: 373
epoch:: 374
epoch:: 375
epoch:: 376
epoch:: 377
epoch:: 378
epoch:: 379
epoch:: 380
epoch:: 381
epoch:: 382
epoch:: 383
epoch:: 384
epoch:: 385
epoch:: 386
epoch:: 387
epoch:: 388
epoch:: 389
epoch:: 390
epoch:: 391
epoch:: 392
epoch:: 393
epoch:: 394
epoch:: 395
epoch:: 396
epoch:: 397
epoch:: 398
epoch:: 399
epoch:: 400
epoch:: 401
epoch:: 402
epoch:: 403
epoch:: 404
epoch:: 405
epoch:: 406
epoch:: 407
epoch:: 408
epoch:: 409
epoch:: 410
epoch:: 411
epoch:: 412
epoch:: 413
epoch:: 414
epoch:: 415
epoch:: 416
epoch:: 417
epoch:: 418
epoch:: 419
epoch:: 420
epoch:: 421
epoch:: 422
epoch:: 423
epoch:: 424
epoch:: 425
epoch:: 426
epoch:: 427
epoch:: 428
epoch:: 429
epoch:: 430
epoch:: 431
epoch:: 432
epoch:: 433
epoch:: 434
epoch:: 435
epoch:: 436
epoch:: 437
epoch:: 438
epoch:: 439
epoch:: 440
epoch:: 441
epoch:: 442
epoch:: 443
epoch:: 444
epoch:: 445
epoch:: 446
epoch:: 447
epoch:: 448
epoch:: 449
epoch:: 450
epoch:: 451
epoch:: 452
epoch:: 453
epoch:: 454
epoch:: 455
epoch:: 456
epoch:: 457
epoch:: 458
epoch:: 459
epoch:: 460
epoch:: 461
epoch:: 462
epoch:: 463
epoch:: 464
epoch:: 465
epoch:: 466
epoch:: 467
epoch:: 468
epoch:: 469
epoch:: 470
epoch:: 471
epoch:: 472
epoch:: 473
epoch:: 474
epoch:: 475
epoch:: 476
epoch:: 477
epoch:: 478
epoch:: 479
epoch:: 480
epoch:: 481
epoch:: 482
epoch:: 483
epoch:: 484
epoch:: 485
epoch:: 486
epoch:: 487
epoch:: 488
epoch:: 489
epoch:: 490
epoch:: 491
epoch:: 492
epoch:: 493
epoch:: 494
epoch:: 495
epoch:: 496
epoch:: 497
epoch:: 498
epoch:: 499
training time is 80.75043058395386
overhead:: 0
overhead2:: 2.6294033527374268
overhead3:: 0
time_baseline:: 80.7504608631134
curr_diff: 0 tensor(0.0382, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0382, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632600
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 25
max_epoch:: 500
overhead:: 0
overhead2:: 0.08085823059082031
overhead3:: 0.2522773742675781
overhead4:: 10.25579571723938
overhead5:: 0
memory usage:: 5644025856
time_provenance:: 17.700625896453857
curr_diff: 0 tensor(0.0094, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0094, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0454, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0454, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633000
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 25
max_epoch:: 500
overhead:: 0
overhead2:: 0.08017730712890625
overhead3:: 0.25545454025268555
overhead4:: 10.544252157211304
overhead5:: 0
memory usage:: 5638934528
time_provenance:: 18.052776336669922
curr_diff: 0 tensor(0.0094, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0094, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0454, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0454, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633000
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 25
max_epoch:: 500
overhead:: 0
overhead2:: 0.08328509330749512
overhead3:: 0.26642727851867676
overhead4:: 10.360027074813843
overhead5:: 0
memory usage:: 5676773376
time_provenance:: 17.93441104888916
curr_diff: 0 tensor(0.0094, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0094, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0454, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0454, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633000
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 25
max_epoch:: 500
overhead:: 0
overhead2:: 0.1526341438293457
overhead3:: 0.44078874588012695
overhead4:: 18.756380081176758
overhead5:: 0
memory usage:: 5634326528
time_provenance:: 28.159067153930664
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0388, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0388, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632700
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 25
max_epoch:: 500
overhead:: 0
overhead2:: 0.1566600799560547
overhead3:: 0.4525790214538574
overhead4:: 18.94004726409912
overhead5:: 0
memory usage:: 5657489408
time_provenance:: 28.402998685836792
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0389, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0389, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633000
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 25
max_epoch:: 500
overhead:: 0
overhead2:: 0.15728259086608887
overhead3:: 0.4561734199523926
overhead4:: 18.559900999069214
overhead5:: 0
memory usage:: 5687574528
time_provenance:: 28.043710470199585
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0388, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0388, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632700
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 25
max_epoch:: 500
overhead:: 0
overhead2:: 0.3973243236541748
overhead3:: 1.0348258018493652
overhead4:: 44.37285399436951
overhead5:: 0
memory usage:: 5628301312
time_provenance:: 59.722792625427246
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0383, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0383, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632700
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 25
max_epoch:: 500
overhead:: 0
overhead2:: 0.3920423984527588
overhead3:: 1.0304021835327148
overhead4:: 44.71514320373535
overhead5:: 0
memory usage:: 5626306560
time_provenance:: 60.07177925109863
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0383, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0383, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632700
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 25
max_epoch:: 500
overhead:: 0
overhead2:: 0.38504481315612793
overhead3:: 1.0452003479003906
overhead4:: 44.04828381538391
overhead5:: 0
memory usage:: 5627424768
time_provenance:: 59.44781231880188
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0383, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0383, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632700
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 0 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 25
max_epoch:: 500
overhead:: 0
overhead2:: 0.7940597534179688
overhead3:: 2.083509683609009
overhead4:: 82.33002972602844
overhead5:: 0
memory usage:: 5620621312
time_provenance:: 105.57520818710327
curr_diff: 0 tensor(5.0034e-13, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.0034e-13, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0382, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0382, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632600
repetition 1
python3 benchmark_exp_lr.py 0.001 10000 500 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05] Logistic_regression cifar10_2 1 0.0001 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 2.309326
Train - Epoch 1, Batch: 0, Loss: 2.246768
Train - Epoch 2, Batch: 0, Loss: 2.190865
Train - Epoch 3, Batch: 0, Loss: 2.140212
Train - Epoch 4, Batch: 0, Loss: 2.093947
Train - Epoch 5, Batch: 0, Loss: 2.049543
Train - Epoch 6, Batch: 0, Loss: 2.008568
Train - Epoch 7, Batch: 0, Loss: 1.971519
Train - Epoch 8, Batch: 0, Loss: 1.931544
Train - Epoch 9, Batch: 0, Loss: 1.900813
Train - Epoch 10, Batch: 0, Loss: 1.869952
Train - Epoch 11, Batch: 0, Loss: 1.844200
Train - Epoch 12, Batch: 0, Loss: 1.811591
Train - Epoch 13, Batch: 0, Loss: 1.787702
Train - Epoch 14, Batch: 0, Loss: 1.755584
Train - Epoch 15, Batch: 0, Loss: 1.730328
Train - Epoch 16, Batch: 0, Loss: 1.721618
Train - Epoch 17, Batch: 0, Loss: 1.690473
Train - Epoch 18, Batch: 0, Loss: 1.681341
Train - Epoch 19, Batch: 0, Loss: 1.658314
Train - Epoch 20, Batch: 0, Loss: 1.638624
Train - Epoch 21, Batch: 0, Loss: 1.628201
Train - Epoch 22, Batch: 0, Loss: 1.607571
Train - Epoch 23, Batch: 0, Loss: 1.596759
Train - Epoch 24, Batch: 0, Loss: 1.580552
Train - Epoch 25, Batch: 0, Loss: 1.563208
Train - Epoch 26, Batch: 0, Loss: 1.551778
Train - Epoch 27, Batch: 0, Loss: 1.547978
Train - Epoch 28, Batch: 0, Loss: 1.526792
Train - Epoch 29, Batch: 0, Loss: 1.518996
Train - Epoch 30, Batch: 0, Loss: 1.504315
Train - Epoch 31, Batch: 0, Loss: 1.495922
Train - Epoch 32, Batch: 0, Loss: 1.492006
Train - Epoch 33, Batch: 0, Loss: 1.479440
Train - Epoch 34, Batch: 0, Loss: 1.477544
Train - Epoch 35, Batch: 0, Loss: 1.466188
Train - Epoch 36, Batch: 0, Loss: 1.440665
Train - Epoch 37, Batch: 0, Loss: 1.446991
Train - Epoch 38, Batch: 0, Loss: 1.435222
Train - Epoch 39, Batch: 0, Loss: 1.430405
Train - Epoch 40, Batch: 0, Loss: 1.421077
Train - Epoch 41, Batch: 0, Loss: 1.413354
Train - Epoch 42, Batch: 0, Loss: 1.418490
Train - Epoch 43, Batch: 0, Loss: 1.404183
Train - Epoch 44, Batch: 0, Loss: 1.392018
Train - Epoch 45, Batch: 0, Loss: 1.385122
Train - Epoch 46, Batch: 0, Loss: 1.382291
Train - Epoch 47, Batch: 0, Loss: 1.385945
Train - Epoch 48, Batch: 0, Loss: 1.369966
Train - Epoch 49, Batch: 0, Loss: 1.360693
Train - Epoch 50, Batch: 0, Loss: 1.352240
Train - Epoch 51, Batch: 0, Loss: 1.360729
Train - Epoch 52, Batch: 0, Loss: 1.354148
Train - Epoch 53, Batch: 0, Loss: 1.345328
Train - Epoch 54, Batch: 0, Loss: 1.332679
Train - Epoch 55, Batch: 0, Loss: 1.336915
Train - Epoch 56, Batch: 0, Loss: 1.333423
Train - Epoch 57, Batch: 0, Loss: 1.334849
Train - Epoch 58, Batch: 0, Loss: 1.330055
Train - Epoch 59, Batch: 0, Loss: 1.321517
Train - Epoch 60, Batch: 0, Loss: 1.310240
Train - Epoch 61, Batch: 0, Loss: 1.316434
Train - Epoch 62, Batch: 0, Loss: 1.314187
Train - Epoch 63, Batch: 0, Loss: 1.304393
Train - Epoch 64, Batch: 0, Loss: 1.300128
Train - Epoch 65, Batch: 0, Loss: 1.297407
Train - Epoch 66, Batch: 0, Loss: 1.298401
Train - Epoch 67, Batch: 0, Loss: 1.288451
Train - Epoch 68, Batch: 0, Loss: 1.291714
Train - Epoch 69, Batch: 0, Loss: 1.292727
Train - Epoch 70, Batch: 0, Loss: 1.280850
Train - Epoch 71, Batch: 0, Loss: 1.270354
Train - Epoch 72, Batch: 0, Loss: 1.277329
Train - Epoch 73, Batch: 0, Loss: 1.274736
Train - Epoch 74, Batch: 0, Loss: 1.263524
Train - Epoch 75, Batch: 0, Loss: 1.263588
Train - Epoch 76, Batch: 0, Loss: 1.266911
Train - Epoch 77, Batch: 0, Loss: 1.264038
Train - Epoch 78, Batch: 0, Loss: 1.267627
Train - Epoch 79, Batch: 0, Loss: 1.260935
Train - Epoch 80, Batch: 0, Loss: 1.250919
Train - Epoch 81, Batch: 0, Loss: 1.264830
Train - Epoch 82, Batch: 0, Loss: 1.248737
Train - Epoch 83, Batch: 0, Loss: 1.245501
Train - Epoch 84, Batch: 0, Loss: 1.233956
Train - Epoch 85, Batch: 0, Loss: 1.247645
Train - Epoch 86, Batch: 0, Loss: 1.253629
Train - Epoch 87, Batch: 0, Loss: 1.245661
Train - Epoch 88, Batch: 0, Loss: 1.231352
Train - Epoch 89, Batch: 0, Loss: 1.233330
Train - Epoch 90, Batch: 0, Loss: 1.233002
Train - Epoch 91, Batch: 0, Loss: 1.234780
Train - Epoch 92, Batch: 0, Loss: 1.231921
Train - Epoch 93, Batch: 0, Loss: 1.230426
Train - Epoch 94, Batch: 0, Loss: 1.233112
Train - Epoch 95, Batch: 0, Loss: 1.228345
Train - Epoch 96, Batch: 0, Loss: 1.213812
Train - Epoch 97, Batch: 0, Loss: 1.217966
Train - Epoch 98, Batch: 0, Loss: 1.219752
Train - Epoch 99, Batch: 0, Loss: 1.208908
Train - Epoch 100, Batch: 0, Loss: 1.209283
Train - Epoch 101, Batch: 0, Loss: 1.216704
Train - Epoch 102, Batch: 0, Loss: 1.213718
Train - Epoch 103, Batch: 0, Loss: 1.216525
Train - Epoch 104, Batch: 0, Loss: 1.222047
Train - Epoch 105, Batch: 0, Loss: 1.209480
Train - Epoch 106, Batch: 0, Loss: 1.211470
Train - Epoch 107, Batch: 0, Loss: 1.207205
Train - Epoch 108, Batch: 0, Loss: 1.207187
Train - Epoch 109, Batch: 0, Loss: 1.188774
Train - Epoch 110, Batch: 0, Loss: 1.194327
Train - Epoch 111, Batch: 0, Loss: 1.195700
Train - Epoch 112, Batch: 0, Loss: 1.204109
Train - Epoch 113, Batch: 0, Loss: 1.188030
Train - Epoch 114, Batch: 0, Loss: 1.204816
Train - Epoch 115, Batch: 0, Loss: 1.191438
Train - Epoch 116, Batch: 0, Loss: 1.176717
Train - Epoch 117, Batch: 0, Loss: 1.186776
Train - Epoch 118, Batch: 0, Loss: 1.188411
Train - Epoch 119, Batch: 0, Loss: 1.187889
Train - Epoch 120, Batch: 0, Loss: 1.182277
Train - Epoch 121, Batch: 0, Loss: 1.185896
Train - Epoch 122, Batch: 0, Loss: 1.188833
Train - Epoch 123, Batch: 0, Loss: 1.195657
Train - Epoch 124, Batch: 0, Loss: 1.171736
Train - Epoch 125, Batch: 0, Loss: 1.173662
Train - Epoch 126, Batch: 0, Loss: 1.199324
Train - Epoch 127, Batch: 0, Loss: 1.176896
Train - Epoch 128, Batch: 0, Loss: 1.175185
Train - Epoch 129, Batch: 0, Loss: 1.161100
Train - Epoch 130, Batch: 0, Loss: 1.171739
Train - Epoch 131, Batch: 0, Loss: 1.162707
Train - Epoch 132, Batch: 0, Loss: 1.164129
Train - Epoch 133, Batch: 0, Loss: 1.170330
Train - Epoch 134, Batch: 0, Loss: 1.165665
Train - Epoch 135, Batch: 0, Loss: 1.167001
Train - Epoch 136, Batch: 0, Loss: 1.172236
Train - Epoch 137, Batch: 0, Loss: 1.159161
Train - Epoch 138, Batch: 0, Loss: 1.161435
Train - Epoch 139, Batch: 0, Loss: 1.178215
Train - Epoch 140, Batch: 0, Loss: 1.161642
Train - Epoch 141, Batch: 0, Loss: 1.162801
Train - Epoch 142, Batch: 0, Loss: 1.175707
Train - Epoch 143, Batch: 0, Loss: 1.165158
Train - Epoch 144, Batch: 0, Loss: 1.155853
Train - Epoch 145, Batch: 0, Loss: 1.154594
Train - Epoch 146, Batch: 0, Loss: 1.166732
Train - Epoch 147, Batch: 0, Loss: 1.158217
Train - Epoch 148, Batch: 0, Loss: 1.164568
Train - Epoch 149, Batch: 0, Loss: 1.167191
Train - Epoch 150, Batch: 0, Loss: 1.152379
Train - Epoch 151, Batch: 0, Loss: 1.158557
Train - Epoch 152, Batch: 0, Loss: 1.138327
Train - Epoch 153, Batch: 0, Loss: 1.154155
Train - Epoch 154, Batch: 0, Loss: 1.154489
Train - Epoch 155, Batch: 0, Loss: 1.154444
Train - Epoch 156, Batch: 0, Loss: 1.139403
Train - Epoch 157, Batch: 0, Loss: 1.164406
Train - Epoch 158, Batch: 0, Loss: 1.143834
Train - Epoch 159, Batch: 0, Loss: 1.161743
Train - Epoch 160, Batch: 0, Loss: 1.148408
Train - Epoch 161, Batch: 0, Loss: 1.144039
Train - Epoch 162, Batch: 0, Loss: 1.152839
Train - Epoch 163, Batch: 0, Loss: 1.152505
Train - Epoch 164, Batch: 0, Loss: 1.142916
Train - Epoch 165, Batch: 0, Loss: 1.145723
Train - Epoch 166, Batch: 0, Loss: 1.151645
Train - Epoch 167, Batch: 0, Loss: 1.132898
Train - Epoch 168, Batch: 0, Loss: 1.141635
Train - Epoch 169, Batch: 0, Loss: 1.127334
Train - Epoch 170, Batch: 0, Loss: 1.144738
Train - Epoch 171, Batch: 0, Loss: 1.130962
Train - Epoch 172, Batch: 0, Loss: 1.135960
Train - Epoch 173, Batch: 0, Loss: 1.129132
Train - Epoch 174, Batch: 0, Loss: 1.132510
Train - Epoch 175, Batch: 0, Loss: 1.143774
Train - Epoch 176, Batch: 0, Loss: 1.135564
Train - Epoch 177, Batch: 0, Loss: 1.143742
Train - Epoch 178, Batch: 0, Loss: 1.144578
Train - Epoch 179, Batch: 0, Loss: 1.132156
Train - Epoch 180, Batch: 0, Loss: 1.131334
Train - Epoch 181, Batch: 0, Loss: 1.126941
Train - Epoch 182, Batch: 0, Loss: 1.124027
Train - Epoch 183, Batch: 0, Loss: 1.129177
Train - Epoch 184, Batch: 0, Loss: 1.137285
Train - Epoch 185, Batch: 0, Loss: 1.127112
Train - Epoch 186, Batch: 0, Loss: 1.151338
Train - Epoch 187, Batch: 0, Loss: 1.121757
Train - Epoch 188, Batch: 0, Loss: 1.128367
Train - Epoch 189, Batch: 0, Loss: 1.126499
Train - Epoch 190, Batch: 0, Loss: 1.131521
Train - Epoch 191, Batch: 0, Loss: 1.132437
Train - Epoch 192, Batch: 0, Loss: 1.130678
Train - Epoch 193, Batch: 0, Loss: 1.113966
Train - Epoch 194, Batch: 0, Loss: 1.130034
Train - Epoch 195, Batch: 0, Loss: 1.121386
Train - Epoch 196, Batch: 0, Loss: 1.126077
Train - Epoch 197, Batch: 0, Loss: 1.124413
Train - Epoch 198, Batch: 0, Loss: 1.113941
Train - Epoch 199, Batch: 0, Loss: 1.107492
Train - Epoch 200, Batch: 0, Loss: 1.118973
Train - Epoch 201, Batch: 0, Loss: 1.110119
Train - Epoch 202, Batch: 0, Loss: 1.118172
Train - Epoch 203, Batch: 0, Loss: 1.118015
Train - Epoch 204, Batch: 0, Loss: 1.123853
Train - Epoch 205, Batch: 0, Loss: 1.124766
Train - Epoch 206, Batch: 0, Loss: 1.132841
Train - Epoch 207, Batch: 0, Loss: 1.103701
Train - Epoch 208, Batch: 0, Loss: 1.109159
Train - Epoch 209, Batch: 0, Loss: 1.098289
Train - Epoch 210, Batch: 0, Loss: 1.110648
Train - Epoch 211, Batch: 0, Loss: 1.115499
Train - Epoch 212, Batch: 0, Loss: 1.108606
Train - Epoch 213, Batch: 0, Loss: 1.114304
Train - Epoch 214, Batch: 0, Loss: 1.117674
Train - Epoch 215, Batch: 0, Loss: 1.112047
Train - Epoch 216, Batch: 0, Loss: 1.109064
Train - Epoch 217, Batch: 0, Loss: 1.114085
Train - Epoch 218, Batch: 0, Loss: 1.105435
Train - Epoch 219, Batch: 0, Loss: 1.097120
Train - Epoch 220, Batch: 0, Loss: 1.103763
Train - Epoch 221, Batch: 0, Loss: 1.114047
Train - Epoch 222, Batch: 0, Loss: 1.107136
Train - Epoch 223, Batch: 0, Loss: 1.110143
Train - Epoch 224, Batch: 0, Loss: 1.096067
Train - Epoch 225, Batch: 0, Loss: 1.117386
Train - Epoch 226, Batch: 0, Loss: 1.104996
Train - Epoch 227, Batch: 0, Loss: 1.110447
Train - Epoch 228, Batch: 0, Loss: 1.089678
Train - Epoch 229, Batch: 0, Loss: 1.105351
Train - Epoch 230, Batch: 0, Loss: 1.099523
Train - Epoch 231, Batch: 0, Loss: 1.105564
Train - Epoch 232, Batch: 0, Loss: 1.098724
Train - Epoch 233, Batch: 0, Loss: 1.081551
Train - Epoch 234, Batch: 0, Loss: 1.097812
Train - Epoch 235, Batch: 0, Loss: 1.104033
Train - Epoch 236, Batch: 0, Loss: 1.105006
Train - Epoch 237, Batch: 0, Loss: 1.115980
Train - Epoch 238, Batch: 0, Loss: 1.097389
Train - Epoch 239, Batch: 0, Loss: 1.100754
Train - Epoch 240, Batch: 0, Loss: 1.080593
Train - Epoch 241, Batch: 0, Loss: 1.115000
Train - Epoch 242, Batch: 0, Loss: 1.110018
Train - Epoch 243, Batch: 0, Loss: 1.097533
Train - Epoch 244, Batch: 0, Loss: 1.102338
Train - Epoch 245, Batch: 0, Loss: 1.098897
Train - Epoch 246, Batch: 0, Loss: 1.092626
Train - Epoch 247, Batch: 0, Loss: 1.090039
Train - Epoch 248, Batch: 0, Loss: 1.097174
Train - Epoch 249, Batch: 0, Loss: 1.091930
Train - Epoch 250, Batch: 0, Loss: 1.087175
Train - Epoch 251, Batch: 0, Loss: 1.089904
Train - Epoch 252, Batch: 0, Loss: 1.088498
Train - Epoch 253, Batch: 0, Loss: 1.100255
Train - Epoch 254, Batch: 0, Loss: 1.083665
Train - Epoch 255, Batch: 0, Loss: 1.092003
Train - Epoch 256, Batch: 0, Loss: 1.094828
Train - Epoch 257, Batch: 0, Loss: 1.095954
Train - Epoch 258, Batch: 0, Loss: 1.095544
Train - Epoch 259, Batch: 0, Loss: 1.083437
Train - Epoch 260, Batch: 0, Loss: 1.085583
Train - Epoch 261, Batch: 0, Loss: 1.076616
Train - Epoch 262, Batch: 0, Loss: 1.092504
Train - Epoch 263, Batch: 0, Loss: 1.084819
Train - Epoch 264, Batch: 0, Loss: 1.090193
Train - Epoch 265, Batch: 0, Loss: 1.080845
Train - Epoch 266, Batch: 0, Loss: 1.088407
Train - Epoch 267, Batch: 0, Loss: 1.081830
Train - Epoch 268, Batch: 0, Loss: 1.088763
Train - Epoch 269, Batch: 0, Loss: 1.088969
Train - Epoch 270, Batch: 0, Loss: 1.082364
Train - Epoch 271, Batch: 0, Loss: 1.093750
Train - Epoch 272, Batch: 0, Loss: 1.094069
Train - Epoch 273, Batch: 0, Loss: 1.086073
Train - Epoch 274, Batch: 0, Loss: 1.080948
Train - Epoch 275, Batch: 0, Loss: 1.081132
Train - Epoch 276, Batch: 0, Loss: 1.090241
Train - Epoch 277, Batch: 0, Loss: 1.082132
Train - Epoch 278, Batch: 0, Loss: 1.087401
Train - Epoch 279, Batch: 0, Loss: 1.077908
Train - Epoch 280, Batch: 0, Loss: 1.087558
Train - Epoch 281, Batch: 0, Loss: 1.089227
Train - Epoch 282, Batch: 0, Loss: 1.079767
Train - Epoch 283, Batch: 0, Loss: 1.084490
Train - Epoch 284, Batch: 0, Loss: 1.093397
Train - Epoch 285, Batch: 0, Loss: 1.076534
Train - Epoch 286, Batch: 0, Loss: 1.082761
Train - Epoch 287, Batch: 0, Loss: 1.076021
Train - Epoch 288, Batch: 0, Loss: 1.078899
Train - Epoch 289, Batch: 0, Loss: 1.076893
Train - Epoch 290, Batch: 0, Loss: 1.077761
Train - Epoch 291, Batch: 0, Loss: 1.092545
Train - Epoch 292, Batch: 0, Loss: 1.078796
Train - Epoch 293, Batch: 0, Loss: 1.068098
Train - Epoch 294, Batch: 0, Loss: 1.082917
Train - Epoch 295, Batch: 0, Loss: 1.077756
Train - Epoch 296, Batch: 0, Loss: 1.083116
Train - Epoch 297, Batch: 0, Loss: 1.078353
Train - Epoch 298, Batch: 0, Loss: 1.081049
Train - Epoch 299, Batch: 0, Loss: 1.063878
Train - Epoch 300, Batch: 0, Loss: 1.074413
Train - Epoch 301, Batch: 0, Loss: 1.080748
Train - Epoch 302, Batch: 0, Loss: 1.074575
Train - Epoch 303, Batch: 0, Loss: 1.087969
Train - Epoch 304, Batch: 0, Loss: 1.065010
Train - Epoch 305, Batch: 0, Loss: 1.089295
Train - Epoch 306, Batch: 0, Loss: 1.089762
Train - Epoch 307, Batch: 0, Loss: 1.063209
Train - Epoch 308, Batch: 0, Loss: 1.062984
Train - Epoch 309, Batch: 0, Loss: 1.070271
Train - Epoch 310, Batch: 0, Loss: 1.067158
Train - Epoch 311, Batch: 0, Loss: 1.066773
Train - Epoch 312, Batch: 0, Loss: 1.091193
Train - Epoch 313, Batch: 0, Loss: 1.080359
Train - Epoch 314, Batch: 0, Loss: 1.066148
Train - Epoch 315, Batch: 0, Loss: 1.073975
Train - Epoch 316, Batch: 0, Loss: 1.081947
Train - Epoch 317, Batch: 0, Loss: 1.067744
Train - Epoch 318, Batch: 0, Loss: 1.075578
Train - Epoch 319, Batch: 0, Loss: 1.068363
Train - Epoch 320, Batch: 0, Loss: 1.084802
Train - Epoch 321, Batch: 0, Loss: 1.069776
Train - Epoch 322, Batch: 0, Loss: 1.079262
Train - Epoch 323, Batch: 0, Loss: 1.074566
Train - Epoch 324, Batch: 0, Loss: 1.058516
Train - Epoch 325, Batch: 0, Loss: 1.073156
Train - Epoch 326, Batch: 0, Loss: 1.072219
Train - Epoch 327, Batch: 0, Loss: 1.078564
Train - Epoch 328, Batch: 0, Loss: 1.048546
Train - Epoch 329, Batch: 0, Loss: 1.076880
Train - Epoch 330, Batch: 0, Loss: 1.057923
Train - Epoch 331, Batch: 0, Loss: 1.079987
Train - Epoch 332, Batch: 0, Loss: 1.076612
Train - Epoch 333, Batch: 0, Loss: 1.065844
Train - Epoch 334, Batch: 0, Loss: 1.061592
Train - Epoch 335, Batch: 0, Loss: 1.058795
Train - Epoch 336, Batch: 0, Loss: 1.067608
Train - Epoch 337, Batch: 0, Loss: 1.073878
Train - Epoch 338, Batch: 0, Loss: 1.071099
Train - Epoch 339, Batch: 0, Loss: 1.060888
Train - Epoch 340, Batch: 0, Loss: 1.070207
Train - Epoch 341, Batch: 0, Loss: 1.066250
Train - Epoch 342, Batch: 0, Loss: 1.058524
Train - Epoch 343, Batch: 0, Loss: 1.058689
Train - Epoch 344, Batch: 0, Loss: 1.076953
Train - Epoch 345, Batch: 0, Loss: 1.075326
Train - Epoch 346, Batch: 0, Loss: 1.070179
Train - Epoch 347, Batch: 0, Loss: 1.060181
Train - Epoch 348, Batch: 0, Loss: 1.063006
Train - Epoch 349, Batch: 0, Loss: 1.071165
Train - Epoch 350, Batch: 0, Loss: 1.076410
Train - Epoch 351, Batch: 0, Loss: 1.058468
Train - Epoch 352, Batch: 0, Loss: 1.068330
Train - Epoch 353, Batch: 0, Loss: 1.060903
Train - Epoch 354, Batch: 0, Loss: 1.045399
Train - Epoch 355, Batch: 0, Loss: 1.063232
Train - Epoch 356, Batch: 0, Loss: 1.062559
Train - Epoch 357, Batch: 0, Loss: 1.060608
Train - Epoch 358, Batch: 0, Loss: 1.071287
Train - Epoch 359, Batch: 0, Loss: 1.059705
Train - Epoch 360, Batch: 0, Loss: 1.064091
Train - Epoch 361, Batch: 0, Loss: 1.065417
Train - Epoch 362, Batch: 0, Loss: 1.076219
Train - Epoch 363, Batch: 0, Loss: 1.053354
Train - Epoch 364, Batch: 0, Loss: 1.043970
Train - Epoch 365, Batch: 0, Loss: 1.060250
Train - Epoch 366, Batch: 0, Loss: 1.049615
Train - Epoch 367, Batch: 0, Loss: 1.045339
Train - Epoch 368, Batch: 0, Loss: 1.060577
Train - Epoch 369, Batch: 0, Loss: 1.057354
Train - Epoch 370, Batch: 0, Loss: 1.069331
Train - Epoch 371, Batch: 0, Loss: 1.051936
Train - Epoch 372, Batch: 0, Loss: 1.059099
Train - Epoch 373, Batch: 0, Loss: 1.048594/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 374, Batch: 0, Loss: 1.068193
Train - Epoch 375, Batch: 0, Loss: 1.066673
Train - Epoch 376, Batch: 0, Loss: 1.045984
Train - Epoch 377, Batch: 0, Loss: 1.067545
Train - Epoch 378, Batch: 0, Loss: 1.064881
Train - Epoch 379, Batch: 0, Loss: 1.058108
Train - Epoch 380, Batch: 0, Loss: 1.041549
Train - Epoch 381, Batch: 0, Loss: 1.054801
Train - Epoch 382, Batch: 0, Loss: 1.058378
Train - Epoch 383, Batch: 0, Loss: 1.058079
Train - Epoch 384, Batch: 0, Loss: 1.051820
Train - Epoch 385, Batch: 0, Loss: 1.062304
Train - Epoch 386, Batch: 0, Loss: 1.078019
Train - Epoch 387, Batch: 0, Loss: 1.049795
Train - Epoch 388, Batch: 0, Loss: 1.062117
Train - Epoch 389, Batch: 0, Loss: 1.059108
Train - Epoch 390, Batch: 0, Loss: 1.056355
Train - Epoch 391, Batch: 0, Loss: 1.060014
Train - Epoch 392, Batch: 0, Loss: 1.059410
Train - Epoch 393, Batch: 0, Loss: 1.060996
Train - Epoch 394, Batch: 0, Loss: 1.069924
Train - Epoch 395, Batch: 0, Loss: 1.054993
Train - Epoch 396, Batch: 0, Loss: 1.051638
Train - Epoch 397, Batch: 0, Loss: 1.047057
Train - Epoch 398, Batch: 0, Loss: 1.068994
Train - Epoch 399, Batch: 0, Loss: 1.040897
Train - Epoch 400, Batch: 0, Loss: 1.056815
Train - Epoch 401, Batch: 0, Loss: 1.058464
Train - Epoch 402, Batch: 0, Loss: 1.054114
Train - Epoch 403, Batch: 0, Loss: 1.060532
Train - Epoch 404, Batch: 0, Loss: 1.052652
Train - Epoch 405, Batch: 0, Loss: 1.053153
Train - Epoch 406, Batch: 0, Loss: 1.051824
Train - Epoch 407, Batch: 0, Loss: 1.056822
Train - Epoch 408, Batch: 0, Loss: 1.049289
Train - Epoch 409, Batch: 0, Loss: 1.051922
Train - Epoch 410, Batch: 0, Loss: 1.044974
Train - Epoch 411, Batch: 0, Loss: 1.040561
Train - Epoch 412, Batch: 0, Loss: 1.060096
Train - Epoch 413, Batch: 0, Loss: 1.069752
Train - Epoch 414, Batch: 0, Loss: 1.044078
Train - Epoch 415, Batch: 0, Loss: 1.046162
Train - Epoch 416, Batch: 0, Loss: 1.067387
Train - Epoch 417, Batch: 0, Loss: 1.052409
Train - Epoch 418, Batch: 0, Loss: 1.054944
Train - Epoch 419, Batch: 0, Loss: 1.051258
Train - Epoch 420, Batch: 0, Loss: 1.038739
Train - Epoch 421, Batch: 0, Loss: 1.042552
Train - Epoch 422, Batch: 0, Loss: 1.055363
Train - Epoch 423, Batch: 0, Loss: 1.064232
Train - Epoch 424, Batch: 0, Loss: 1.041684
Train - Epoch 425, Batch: 0, Loss: 1.045577
Train - Epoch 426, Batch: 0, Loss: 1.053408
Train - Epoch 427, Batch: 0, Loss: 1.049255
Train - Epoch 428, Batch: 0, Loss: 1.045261
Train - Epoch 429, Batch: 0, Loss: 1.060943
Train - Epoch 430, Batch: 0, Loss: 1.041995
Train - Epoch 431, Batch: 0, Loss: 1.043152
Train - Epoch 432, Batch: 0, Loss: 1.042124
Train - Epoch 433, Batch: 0, Loss: 1.044132
Train - Epoch 434, Batch: 0, Loss: 1.052004
Train - Epoch 435, Batch: 0, Loss: 1.055049
Train - Epoch 436, Batch: 0, Loss: 1.049154
Train - Epoch 437, Batch: 0, Loss: 1.050574
Train - Epoch 438, Batch: 0, Loss: 1.032781
Train - Epoch 439, Batch: 0, Loss: 1.057410
Train - Epoch 440, Batch: 0, Loss: 1.044616
Train - Epoch 441, Batch: 0, Loss: 1.049879
Train - Epoch 442, Batch: 0, Loss: 1.048031
Train - Epoch 443, Batch: 0, Loss: 1.046254
Train - Epoch 444, Batch: 0, Loss: 1.051681
Train - Epoch 445, Batch: 0, Loss: 1.042060
Train - Epoch 446, Batch: 0, Loss: 1.036185
Train - Epoch 447, Batch: 0, Loss: 1.052684
Train - Epoch 448, Batch: 0, Loss: 1.046713
Train - Epoch 449, Batch: 0, Loss: 1.045698
Train - Epoch 450, Batch: 0, Loss: 1.039766
Train - Epoch 451, Batch: 0, Loss: 1.059818
Train - Epoch 452, Batch: 0, Loss: 1.046407
Train - Epoch 453, Batch: 0, Loss: 1.052928
Train - Epoch 454, Batch: 0, Loss: 1.026671
Train - Epoch 455, Batch: 0, Loss: 1.055927
Train - Epoch 456, Batch: 0, Loss: 1.048965
Train - Epoch 457, Batch: 0, Loss: 1.057077
Train - Epoch 458, Batch: 0, Loss: 1.043429
Train - Epoch 459, Batch: 0, Loss: 1.047397
Train - Epoch 460, Batch: 0, Loss: 1.042786
Train - Epoch 461, Batch: 0, Loss: 1.044574
Train - Epoch 462, Batch: 0, Loss: 1.034304
Train - Epoch 463, Batch: 0, Loss: 1.049048
Train - Epoch 464, Batch: 0, Loss: 1.040173
Train - Epoch 465, Batch: 0, Loss: 1.053067
Train - Epoch 466, Batch: 0, Loss: 1.026668
Train - Epoch 467, Batch: 0, Loss: 1.040333
Train - Epoch 468, Batch: 0, Loss: 1.036647
Train - Epoch 469, Batch: 0, Loss: 1.029990
Train - Epoch 470, Batch: 0, Loss: 1.047463
Train - Epoch 471, Batch: 0, Loss: 1.043685
Train - Epoch 472, Batch: 0, Loss: 1.047783
Train - Epoch 473, Batch: 0, Loss: 1.055539
Train - Epoch 474, Batch: 0, Loss: 1.046463
Train - Epoch 475, Batch: 0, Loss: 1.035394
Train - Epoch 476, Batch: 0, Loss: 1.043971
Train - Epoch 477, Batch: 0, Loss: 1.050115
Train - Epoch 478, Batch: 0, Loss: 1.038901
Train - Epoch 479, Batch: 0, Loss: 1.031358
Train - Epoch 480, Batch: 0, Loss: 1.038193
Train - Epoch 481, Batch: 0, Loss: 1.056888
Train - Epoch 482, Batch: 0, Loss: 1.032877
Train - Epoch 483, Batch: 0, Loss: 1.030488
Train - Epoch 484, Batch: 0, Loss: 1.039007
Train - Epoch 485, Batch: 0, Loss: 1.040628
Train - Epoch 486, Batch: 0, Loss: 1.049838
Train - Epoch 487, Batch: 0, Loss: 1.041593
Train - Epoch 488, Batch: 0, Loss: 1.027397
Train - Epoch 489, Batch: 0, Loss: 1.021892
Train - Epoch 490, Batch: 0, Loss: 1.039273
Train - Epoch 491, Batch: 0, Loss: 1.028838
Train - Epoch 492, Batch: 0, Loss: 1.034517
Train - Epoch 493, Batch: 0, Loss: 1.018976
Train - Epoch 494, Batch: 0, Loss: 1.056724
Train - Epoch 495, Batch: 0, Loss: 1.041110
Train - Epoch 496, Batch: 0, Loss: 1.032866
Train - Epoch 497, Batch: 0, Loss: 1.040826
Train - Epoch 498, Batch: 0, Loss: 1.035819
Train - Epoch 499, Batch: 0, Loss: 1.041930
training_time:: 106.68437385559082
training time full:: 106.68444442749023
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.631300
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 25
max_epoch:: 500
batch_size:: 10000
epoch:: 0
tensor([    0, 32776, 32777,    12, 32780,    15,    16, 32792, 32800,    38,
           42, 32817,    50,    49,    52, 32818,    55, 32825, 32830, 32836,
        32837,    70, 32840, 32841,    78,    79,    91, 32860,    95,    96,
        32866,   100,   103, 32872,   106, 32876, 32880,   116, 32892,   126,
          128,   132,   135, 32904,   141,   145, 32913,   147,   149, 32920])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
epoch:: 60
epoch:: 61
epoch:: 62
epoch:: 63
epoch:: 64
epoch:: 65
epoch:: 66
epoch:: 67
epoch:: 68
epoch:: 69
epoch:: 70
epoch:: 71
epoch:: 72
epoch:: 73
epoch:: 74
epoch:: 75
epoch:: 76
epoch:: 77
epoch:: 78
epoch:: 79
epoch:: 80
epoch:: 81
epoch:: 82
epoch:: 83
epoch:: 84
epoch:: 85
epoch:: 86
epoch:: 87
epoch:: 88
epoch:: 89
epoch:: 90
epoch:: 91
epoch:: 92
epoch:: 93
epoch:: 94
epoch:: 95
epoch:: 96
epoch:: 97
epoch:: 98
epoch:: 99
epoch:: 100
epoch:: 101
epoch:: 102
epoch:: 103
epoch:: 104
epoch:: 105
epoch:: 106
epoch:: 107
epoch:: 108
epoch:: 109
epoch:: 110
epoch:: 111
epoch:: 112
epoch:: 113
epoch:: 114
epoch:: 115
epoch:: 116
epoch:: 117
epoch:: 118
epoch:: 119
epoch:: 120
epoch:: 121
epoch:: 122
epoch:: 123
epoch:: 124
epoch:: 125
epoch:: 126
epoch:: 127
epoch:: 128
epoch:: 129
epoch:: 130
epoch:: 131
epoch:: 132
epoch:: 133
epoch:: 134
epoch:: 135
epoch:: 136
epoch:: 137
epoch:: 138
epoch:: 139
epoch:: 140
epoch:: 141
epoch:: 142
epoch:: 143
epoch:: 144
epoch:: 145
epoch:: 146
epoch:: 147
epoch:: 148
epoch:: 149
epoch:: 150
epoch:: 151
epoch:: 152
epoch:: 153
epoch:: 154
epoch:: 155
epoch:: 156
epoch:: 157
epoch:: 158
epoch:: 159
epoch:: 160
epoch:: 161
epoch:: 162
epoch:: 163
epoch:: 164
epoch:: 165
epoch:: 166
epoch:: 167
epoch:: 168
epoch:: 169
epoch:: 170
epoch:: 171
epoch:: 172
epoch:: 173
epoch:: 174
epoch:: 175
epoch:: 176
epoch:: 177
epoch:: 178
epoch:: 179
epoch:: 180
epoch:: 181
epoch:: 182
epoch:: 183
epoch:: 184
epoch:: 185
epoch:: 186
epoch:: 187
epoch:: 188
epoch:: 189
epoch:: 190
epoch:: 191
epoch:: 192
epoch:: 193
epoch:: 194
epoch:: 195
epoch:: 196
epoch:: 197
epoch:: 198
epoch:: 199
epoch:: 200
epoch:: 201
epoch:: 202
epoch:: 203
epoch:: 204
epoch:: 205
epoch:: 206
epoch:: 207
epoch:: 208
epoch:: 209
epoch:: 210
epoch:: 211
epoch:: 212
epoch:: 213
epoch:: 214
epoch:: 215
epoch:: 216
epoch:: 217
epoch:: 218
epoch:: 219
epoch:: 220
epoch:: 221
epoch:: 222
epoch:: 223
epoch:: 224
epoch:: 225
epoch:: 226
epoch:: 227
epoch:: 228
epoch:: 229
epoch:: 230
epoch:: 231
epoch:: 232
epoch:: 233
epoch:: 234
epoch:: 235
epoch:: 236
epoch:: 237
epoch:: 238
epoch:: 239
epoch:: 240
epoch:: 241
epoch:: 242
epoch:: 243
epoch:: 244
epoch:: 245
epoch:: 246
epoch:: 247
epoch:: 248
epoch:: 249
epoch:: 250
epoch:: 251
epoch:: 252
epoch:: 253
epoch:: 254
epoch:: 255
epoch:: 256
epoch:: 257
epoch:: 258
epoch:: 259
epoch:: 260
epoch:: 261
epoch:: 262
epoch:: 263
epoch:: 264
epoch:: 265
epoch:: 266
epoch:: 267
epoch:: 268
epoch:: 269
epoch:: 270
epoch:: 271
epoch:: 272
epoch:: 273
epoch:: 274
epoch:: 275
epoch:: 276
epoch:: 277
epoch:: 278
epoch:: 279
epoch:: 280
epoch:: 281
epoch:: 282
epoch:: 283
epoch:: 284
epoch:: 285
epoch:: 286
epoch:: 287
epoch:: 288
epoch:: 289
epoch:: 290
epoch:: 291
epoch:: 292
epoch:: 293
epoch:: 294
epoch:: 295
epoch:: 296
epoch:: 297
epoch:: 298
epoch:: 299
epoch:: 300
epoch:: 301
epoch:: 302
epoch:: 303
epoch:: 304
epoch:: 305
epoch:: 306
epoch:: 307
epoch:: 308
epoch:: 309
epoch:: 310
epoch:: 311
epoch:: 312
epoch:: 313
epoch:: 314
epoch:: 315
epoch:: 316
epoch:: 317
epoch:: 318
epoch:: 319
epoch:: 320
epoch:: 321
epoch:: 322
epoch:: 323
epoch:: 324
epoch:: 325
epoch:: 326
epoch:: 327
epoch:: 328
epoch:: 329
epoch:: 330
epoch:: 331
epoch:: 332
epoch:: 333
epoch:: 334
epoch:: 335
epoch:: 336
epoch:: 337
epoch:: 338
epoch:: 339
epoch:: 340
epoch:: 341
epoch:: 342
epoch:: 343
epoch:: 344
epoch:: 345
epoch:: 346
epoch:: 347
epoch:: 348
epoch:: 349
epoch:: 350
epoch:: 351
epoch:: 352
epoch:: 353
epoch:: 354
epoch:: 355
epoch:: 356
epoch:: 357
epoch:: 358
epoch:: 359
epoch:: 360
epoch:: 361
epoch:: 362
epoch:: 363
epoch:: 364
epoch:: 365
epoch:: 366
epoch:: 367
epoch:: 368
epoch:: 369
epoch:: 370
epoch:: 371
epoch:: 372
epoch:: 373
epoch:: 374
epoch:: 375
epoch:: 376
epoch:: 377
epoch:: 378
epoch:: 379
epoch:: 380
epoch:: 381
epoch:: 382
epoch:: 383
epoch:: 384
epoch:: 385
epoch:: 386
epoch:: 387
epoch:: 388
epoch:: 389
epoch:: 390
epoch:: 391
epoch:: 392
epoch:: 393
epoch:: 394
epoch:: 395
epoch:: 396
epoch:: 397
epoch:: 398
epoch:: 399
epoch:: 400
epoch:: 401
epoch:: 402
epoch:: 403
epoch:: 404
epoch:: 405
epoch:: 406
epoch:: 407
epoch:: 408
epoch:: 409
epoch:: 410
epoch:: 411
epoch:: 412
epoch:: 413
epoch:: 414
epoch:: 415
epoch:: 416
epoch:: 417
epoch:: 418
epoch:: 419
epoch:: 420
epoch:: 421
epoch:: 422
epoch:: 423
epoch:: 424
epoch:: 425
epoch:: 426
epoch:: 427
epoch:: 428
epoch:: 429
epoch:: 430
epoch:: 431
epoch:: 432
epoch:: 433
epoch:: 434
epoch:: 435
epoch:: 436
epoch:: 437
epoch:: 438
epoch:: 439
epoch:: 440
epoch:: 441
epoch:: 442
epoch:: 443
epoch:: 444
epoch:: 445
epoch:: 446
epoch:: 447
epoch:: 448
epoch:: 449
epoch:: 450
epoch:: 451
epoch:: 452
epoch:: 453
epoch:: 454
epoch:: 455
epoch:: 456
epoch:: 457
epoch:: 458
epoch:: 459
epoch:: 460
epoch:: 461
epoch:: 462
epoch:: 463
epoch:: 464
epoch:: 465
epoch:: 466
epoch:: 467
epoch:: 468
epoch:: 469
epoch:: 470
epoch:: 471
epoch:: 472
epoch:: 473
epoch:: 474
epoch:: 475
epoch:: 476
epoch:: 477
epoch:: 478
epoch:: 479
epoch:: 480
epoch:: 481
epoch:: 482
epoch:: 483
epoch:: 484
epoch:: 485
epoch:: 486
epoch:: 487
epoch:: 488
epoch:: 489
epoch:: 490
epoch:: 491
epoch:: 492
epoch:: 493
epoch:: 494
epoch:: 495
epoch:: 496
epoch:: 497
epoch:: 498
epoch:: 499
training time is 80.95558524131775
overhead:: 0
overhead2:: 2.6242196559906006
overhead3:: 0
time_baseline:: 80.9556176662445
curr_diff: 0 tensor(0.0383, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0383, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.631200
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 25
max_epoch:: 500
overhead:: 0
overhead2:: 0.0782923698425293
overhead3:: 0.2503776550292969
overhead4:: 10.316564559936523
overhead5:: 0
memory usage:: 5634244608
time_provenance:: 17.775142908096313
curr_diff: 0 tensor(0.0094, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0094, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0455, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0455, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.631200
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 25
max_epoch:: 500
overhead:: 0
overhead2:: 0.08358597755432129
overhead3:: 0.2613236904144287
overhead4:: 10.765792608261108
overhead5:: 0
memory usage:: 5636460544
time_provenance:: 18.328556060791016
curr_diff: 0 tensor(0.0094, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0094, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0454, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0454, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.631600
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 25
max_epoch:: 500
overhead:: 0
overhead2:: 0.08405733108520508
overhead3:: 0.2640700340270996
overhead4:: 10.460088968276978
overhead5:: 0
memory usage:: 5631234048
time_provenance:: 17.999841451644897
curr_diff: 0 tensor(0.0094, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0094, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0455, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0455, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.631200
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 25
max_epoch:: 500
overhead:: 0
overhead2:: 0.15898776054382324
overhead3:: 0.44869565963745117
overhead4:: 18.631869316101074
overhead5:: 0
memory usage:: 5646532608
time_provenance:: 28.05755352973938
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0389, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0389, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.631100
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 25
max_epoch:: 500
overhead:: 0
overhead2:: 0.16169476509094238
overhead3:: 0.4573194980621338
overhead4:: 19.016016006469727
overhead5:: 0
memory usage:: 5634166784
time_provenance:: 28.47678852081299
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0389, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0389, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.631100
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 25
max_epoch:: 500
overhead:: 0
overhead2:: 0.15711236000061035
overhead3:: 0.44935107231140137
overhead4:: 18.54879879951477
overhead5:: 0
memory usage:: 5637570560
time_provenance:: 27.992573022842407
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0389, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0389, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.631100
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 25
max_epoch:: 500
overhead:: 0
overhead2:: 0.41653966903686523
overhead3:: 0.9668092727661133
overhead4:: 44.106529235839844
overhead5:: 0
memory usage:: 5624623104
time_provenance:: 59.34940767288208
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0383, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0383, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.631200
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 25
max_epoch:: 500
overhead:: 0
overhead2:: 0.3911929130554199
overhead3:: 1.0071003437042236
overhead4:: 43.83881998062134
overhead5:: 0
memory usage:: 5648928768
time_provenance:: 59.14978504180908
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0383, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0383, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.631200
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 25
max_epoch:: 500
overhead:: 0
overhead2:: 0.3930823802947998
overhead3:: 1.0341320037841797
overhead4:: 44.87853240966797
overhead5:: 0
memory usage:: 5635805184
time_provenance:: 60.27682018280029
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0383, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0383, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.631200
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 1 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 25
max_epoch:: 500
overhead:: 0
overhead2:: 0.7709786891937256
overhead3:: 2.058580160140991
overhead4:: 82.654860496521
overhead5:: 0
memory usage:: 5633015808
time_provenance:: 105.9421923160553
curr_diff: 0 tensor(5.0084e-13, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.0084e-13, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0383, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0383, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.631200
repetition 2
python3 benchmark_exp_lr.py 0.001 10000 500 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05] Logistic_regression cifar10_2 2 0.0001 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 2.314905
Train - Epoch 1, Batch: 0, Loss: 2.250150
Train - Epoch 2, Batch: 0, Loss: 2.193846
Train - Epoch 3, Batch: 0, Loss: 2.143409
Train - Epoch 4, Batch: 0, Loss: 2.094094
Train - Epoch 5, Batch: 0, Loss: 2.056040
Train - Epoch 6, Batch: 0, Loss: 2.009134
Train - Epoch 7, Batch: 0, Loss: 1.971661
Train - Epoch 8, Batch: 0, Loss: 1.932926
Train - Epoch 9, Batch: 0, Loss: 1.902030
Train - Epoch 10, Batch: 0, Loss: 1.868218
Train - Epoch 11, Batch: 0, Loss: 1.836439
Train - Epoch 12, Batch: 0, Loss: 1.815971
Train - Epoch 13, Batch: 0, Loss: 1.791637
Train - Epoch 14, Batch: 0, Loss: 1.766197
Train - Epoch 15, Batch: 0, Loss: 1.739191
Train - Epoch 16, Batch: 0, Loss: 1.711103
Train - Epoch 17, Batch: 0, Loss: 1.697618
Train - Epoch 18, Batch: 0, Loss: 1.672380
Train - Epoch 19, Batch: 0, Loss: 1.659631
Train - Epoch 20, Batch: 0, Loss: 1.645360
Train - Epoch 21, Batch: 0, Loss: 1.623497
Train - Epoch 22, Batch: 0, Loss: 1.608162
Train - Epoch 23, Batch: 0, Loss: 1.587470
Train - Epoch 24, Batch: 0, Loss: 1.583222
Train - Epoch 25, Batch: 0, Loss: 1.568648
Train - Epoch 26, Batch: 0, Loss: 1.560417
Train - Epoch 27, Batch: 0, Loss: 1.548245
Train - Epoch 28, Batch: 0, Loss: 1.525801
Train - Epoch 29, Batch: 0, Loss: 1.526297
Train - Epoch 30, Batch: 0, Loss: 1.500606
Train - Epoch 31, Batch: 0, Loss: 1.498691
Train - Epoch 32, Batch: 0, Loss: 1.482470
Train - Epoch 33, Batch: 0, Loss: 1.484019
Train - Epoch 34, Batch: 0, Loss: 1.456271
Train - Epoch 35, Batch: 0, Loss: 1.463455
Train - Epoch 36, Batch: 0, Loss: 1.448126
Train - Epoch 37, Batch: 0, Loss: 1.453608
Train - Epoch 38, Batch: 0, Loss: 1.429501
Train - Epoch 39, Batch: 0, Loss: 1.423692
Train - Epoch 40, Batch: 0, Loss: 1.421626
Train - Epoch 41, Batch: 0, Loss: 1.418751
Train - Epoch 42, Batch: 0, Loss: 1.412160
Train - Epoch 43, Batch: 0, Loss: 1.395739
Train - Epoch 44, Batch: 0, Loss: 1.400122
Train - Epoch 45, Batch: 0, Loss: 1.391486
Train - Epoch 46, Batch: 0, Loss: 1.379405
Train - Epoch 47, Batch: 0, Loss: 1.380227
Train - Epoch 48, Batch: 0, Loss: 1.371144
Train - Epoch 49, Batch: 0, Loss: 1.363509
Train - Epoch 50, Batch: 0, Loss: 1.352969
Train - Epoch 51, Batch: 0, Loss: 1.357210
Train - Epoch 52, Batch: 0, Loss: 1.358767
Train - Epoch 53, Batch: 0, Loss: 1.359618
Train - Epoch 54, Batch: 0, Loss: 1.342789
Train - Epoch 55, Batch: 0, Loss: 1.335244
Train - Epoch 56, Batch: 0, Loss: 1.337674
Train - Epoch 57, Batch: 0, Loss: 1.333552
Train - Epoch 58, Batch: 0, Loss: 1.312114
Train - Epoch 59, Batch: 0, Loss: 1.318934
Train - Epoch 60, Batch: 0, Loss: 1.319700
Train - Epoch 61, Batch: 0, Loss: 1.320406
Train - Epoch 62, Batch: 0, Loss: 1.309626
Train - Epoch 63, Batch: 0, Loss: 1.314091
Train - Epoch 64, Batch: 0, Loss: 1.303878
Train - Epoch 65, Batch: 0, Loss: 1.301377
Train - Epoch 66, Batch: 0, Loss: 1.293386
Train - Epoch 67, Batch: 0, Loss: 1.295450
Train - Epoch 68, Batch: 0, Loss: 1.283127
Train - Epoch 69, Batch: 0, Loss: 1.293815
Train - Epoch 70, Batch: 0, Loss: 1.265450
Train - Epoch 71, Batch: 0, Loss: 1.279478
Train - Epoch 72, Batch: 0, Loss: 1.271422
Train - Epoch 73, Batch: 0, Loss: 1.280391
Train - Epoch 74, Batch: 0, Loss: 1.288446
Train - Epoch 75, Batch: 0, Loss: 1.259927
Train - Epoch 76, Batch: 0, Loss: 1.260310
Train - Epoch 77, Batch: 0, Loss: 1.254893
Train - Epoch 78, Batch: 0, Loss: 1.267539
Train - Epoch 79, Batch: 0, Loss: 1.256555
Train - Epoch 80, Batch: 0, Loss: 1.250935
Train - Epoch 81, Batch: 0, Loss: 1.265664
Train - Epoch 82, Batch: 0, Loss: 1.258948
Train - Epoch 83, Batch: 0, Loss: 1.247320
Train - Epoch 84, Batch: 0, Loss: 1.252237
Train - Epoch 85, Batch: 0, Loss: 1.251517
Train - Epoch 86, Batch: 0, Loss: 1.250447
Train - Epoch 87, Batch: 0, Loss: 1.239084
Train - Epoch 88, Batch: 0, Loss: 1.241316
Train - Epoch 89, Batch: 0, Loss: 1.239452
Train - Epoch 90, Batch: 0, Loss: 1.222198
Train - Epoch 91, Batch: 0, Loss: 1.233934
Train - Epoch 92, Batch: 0, Loss: 1.209790
Train - Epoch 93, Batch: 0, Loss: 1.233962
Train - Epoch 94, Batch: 0, Loss: 1.219601
Train - Epoch 95, Batch: 0, Loss: 1.248171
Train - Epoch 96, Batch: 0, Loss: 1.217117
Train - Epoch 97, Batch: 0, Loss: 1.234863
Train - Epoch 98, Batch: 0, Loss: 1.203302
Train - Epoch 99, Batch: 0, Loss: 1.207423
Train - Epoch 100, Batch: 0, Loss: 1.222710
Train - Epoch 101, Batch: 0, Loss: 1.205640
Train - Epoch 102, Batch: 0, Loss: 1.232562
Train - Epoch 103, Batch: 0, Loss: 1.202337
Train - Epoch 104, Batch: 0, Loss: 1.210079
Train - Epoch 105, Batch: 0, Loss: 1.203830
Train - Epoch 106, Batch: 0, Loss: 1.206939
Train - Epoch 107, Batch: 0, Loss: 1.193732
Train - Epoch 108, Batch: 0, Loss: 1.202286
Train - Epoch 109, Batch: 0, Loss: 1.216336
Train - Epoch 110, Batch: 0, Loss: 1.191060
Train - Epoch 111, Batch: 0, Loss: 1.195160
Train - Epoch 112, Batch: 0, Loss: 1.195964
Train - Epoch 113, Batch: 0, Loss: 1.195312
Train - Epoch 114, Batch: 0, Loss: 1.183605
Train - Epoch 115, Batch: 0, Loss: 1.193437
Train - Epoch 116, Batch: 0, Loss: 1.188999
Train - Epoch 117, Batch: 0, Loss: 1.181542
Train - Epoch 118, Batch: 0, Loss: 1.175840
Train - Epoch 119, Batch: 0, Loss: 1.182523
Train - Epoch 120, Batch: 0, Loss: 1.178728
Train - Epoch 121, Batch: 0, Loss: 1.179862
Train - Epoch 122, Batch: 0, Loss: 1.177440
Train - Epoch 123, Batch: 0, Loss: 1.184548
Train - Epoch 124, Batch: 0, Loss: 1.168470
Train - Epoch 125, Batch: 0, Loss: 1.176886
Train - Epoch 126, Batch: 0, Loss: 1.173619
Train - Epoch 127, Batch: 0, Loss: 1.185537
Train - Epoch 128, Batch: 0, Loss: 1.170365
Train - Epoch 129, Batch: 0, Loss: 1.172293
Train - Epoch 130, Batch: 0, Loss: 1.167192
Train - Epoch 131, Batch: 0, Loss: 1.162985
Train - Epoch 132, Batch: 0, Loss: 1.159688
Train - Epoch 133, Batch: 0, Loss: 1.165859
Train - Epoch 134, Batch: 0, Loss: 1.186383
Train - Epoch 135, Batch: 0, Loss: 1.155951
Train - Epoch 136, Batch: 0, Loss: 1.171216
Train - Epoch 137, Batch: 0, Loss: 1.164846
Train - Epoch 138, Batch: 0, Loss: 1.167188
Train - Epoch 139, Batch: 0, Loss: 1.158649
Train - Epoch 140, Batch: 0, Loss: 1.156939
Train - Epoch 141, Batch: 0, Loss: 1.174437
Train - Epoch 142, Batch: 0, Loss: 1.164997
Train - Epoch 143, Batch: 0, Loss: 1.162939
Train - Epoch 144, Batch: 0, Loss: 1.164929
Train - Epoch 145, Batch: 0, Loss: 1.170613
Train - Epoch 146, Batch: 0, Loss: 1.151020
Train - Epoch 147, Batch: 0, Loss: 1.158381
Train - Epoch 148, Batch: 0, Loss: 1.153562
Train - Epoch 149, Batch: 0, Loss: 1.155927
Train - Epoch 150, Batch: 0, Loss: 1.160667
Train - Epoch 151, Batch: 0, Loss: 1.144704
Train - Epoch 152, Batch: 0, Loss: 1.141427
Train - Epoch 153, Batch: 0, Loss: 1.155758
Train - Epoch 154, Batch: 0, Loss: 1.163367
Train - Epoch 155, Batch: 0, Loss: 1.151451
Train - Epoch 156, Batch: 0, Loss: 1.148257
Train - Epoch 157, Batch: 0, Loss: 1.152273
Train - Epoch 158, Batch: 0, Loss: 1.156323
Train - Epoch 159, Batch: 0, Loss: 1.139575
Train - Epoch 160, Batch: 0, Loss: 1.144989
Train - Epoch 161, Batch: 0, Loss: 1.137485
Train - Epoch 162, Batch: 0, Loss: 1.142153
Train - Epoch 163, Batch: 0, Loss: 1.144230
Train - Epoch 164, Batch: 0, Loss: 1.146778
Train - Epoch 165, Batch: 0, Loss: 1.135147
Train - Epoch 166, Batch: 0, Loss: 1.129598
Train - Epoch 167, Batch: 0, Loss: 1.133717
Train - Epoch 168, Batch: 0, Loss: 1.128722
Train - Epoch 169, Batch: 0, Loss: 1.123275
Train - Epoch 170, Batch: 0, Loss: 1.138597
Train - Epoch 171, Batch: 0, Loss: 1.137622
Train - Epoch 172, Batch: 0, Loss: 1.136688
Train - Epoch 173, Batch: 0, Loss: 1.123740
Train - Epoch 174, Batch: 0, Loss: 1.124482
Train - Epoch 175, Batch: 0, Loss: 1.138994
Train - Epoch 176, Batch: 0, Loss: 1.145615
Train - Epoch 177, Batch: 0, Loss: 1.123316
Train - Epoch 178, Batch: 0, Loss: 1.127264
Train - Epoch 179, Batch: 0, Loss: 1.131287
Train - Epoch 180, Batch: 0, Loss: 1.129265
Train - Epoch 181, Batch: 0, Loss: 1.132173
Train - Epoch 182, Batch: 0, Loss: 1.115443
Train - Epoch 183, Batch: 0, Loss: 1.122364
Train - Epoch 184, Batch: 0, Loss: 1.138757
Train - Epoch 185, Batch: 0, Loss: 1.138297
Train - Epoch 186, Batch: 0, Loss: 1.130153
Train - Epoch 187, Batch: 0, Loss: 1.120735
Train - Epoch 188, Batch: 0, Loss: 1.129864
Train - Epoch 189, Batch: 0, Loss: 1.127466
Train - Epoch 190, Batch: 0, Loss: 1.123592
Train - Epoch 191, Batch: 0, Loss: 1.129405
Train - Epoch 192, Batch: 0, Loss: 1.119723
Train - Epoch 193, Batch: 0, Loss: 1.126626
Train - Epoch 194, Batch: 0, Loss: 1.129316
Train - Epoch 195, Batch: 0, Loss: 1.129653
Train - Epoch 196, Batch: 0, Loss: 1.112644
Train - Epoch 197, Batch: 0, Loss: 1.121484
Train - Epoch 198, Batch: 0, Loss: 1.118763
Train - Epoch 199, Batch: 0, Loss: 1.119112
Train - Epoch 200, Batch: 0, Loss: 1.111818
Train - Epoch 201, Batch: 0, Loss: 1.123839
Train - Epoch 202, Batch: 0, Loss: 1.109162
Train - Epoch 203, Batch: 0, Loss: 1.100087
Train - Epoch 204, Batch: 0, Loss: 1.115311
Train - Epoch 205, Batch: 0, Loss: 1.102138
Train - Epoch 206, Batch: 0, Loss: 1.113534
Train - Epoch 207, Batch: 0, Loss: 1.116209
Train - Epoch 208, Batch: 0, Loss: 1.111420
Train - Epoch 209, Batch: 0, Loss: 1.123693
Train - Epoch 210, Batch: 0, Loss: 1.120298
Train - Epoch 211, Batch: 0, Loss: 1.127722
Train - Epoch 212, Batch: 0, Loss: 1.109438
Train - Epoch 213, Batch: 0, Loss: 1.106923
Train - Epoch 214, Batch: 0, Loss: 1.119826
Train - Epoch 215, Batch: 0, Loss: 1.111322
Train - Epoch 216, Batch: 0, Loss: 1.116736
Train - Epoch 217, Batch: 0, Loss: 1.104429
Train - Epoch 218, Batch: 0, Loss: 1.104489
Train - Epoch 219, Batch: 0, Loss: 1.108708
Train - Epoch 220, Batch: 0, Loss: 1.120568
Train - Epoch 221, Batch: 0, Loss: 1.105745
Train - Epoch 222, Batch: 0, Loss: 1.110544
Train - Epoch 223, Batch: 0, Loss: 1.105472
Train - Epoch 224, Batch: 0, Loss: 1.104661
Train - Epoch 225, Batch: 0, Loss: 1.097021
Train - Epoch 226, Batch: 0, Loss: 1.114729
Train - Epoch 227, Batch: 0, Loss: 1.102514
Train - Epoch 228, Batch: 0, Loss: 1.099600
Train - Epoch 229, Batch: 0, Loss: 1.083921
Train - Epoch 230, Batch: 0, Loss: 1.107890
Train - Epoch 231, Batch: 0, Loss: 1.095132
Train - Epoch 232, Batch: 0, Loss: 1.105430
Train - Epoch 233, Batch: 0, Loss: 1.095978
Train - Epoch 234, Batch: 0, Loss: 1.103117
Train - Epoch 235, Batch: 0, Loss: 1.108398
Train - Epoch 236, Batch: 0, Loss: 1.112083
Train - Epoch 237, Batch: 0, Loss: 1.111512
Train - Epoch 238, Batch: 0, Loss: 1.106964
Train - Epoch 239, Batch: 0, Loss: 1.097626
Train - Epoch 240, Batch: 0, Loss: 1.093597
Train - Epoch 241, Batch: 0, Loss: 1.102828
Train - Epoch 242, Batch: 0, Loss: 1.114439
Train - Epoch 243, Batch: 0, Loss: 1.088994
Train - Epoch 244, Batch: 0, Loss: 1.100895
Train - Epoch 245, Batch: 0, Loss: 1.088330
Train - Epoch 246, Batch: 0, Loss: 1.080370
Train - Epoch 247, Batch: 0, Loss: 1.086572
Train - Epoch 248, Batch: 0, Loss: 1.090522
Train - Epoch 249, Batch: 0, Loss: 1.095134
Train - Epoch 250, Batch: 0, Loss: 1.103883
Train - Epoch 251, Batch: 0, Loss: 1.107612
Train - Epoch 252, Batch: 0, Loss: 1.074065
Train - Epoch 253, Batch: 0, Loss: 1.089449
Train - Epoch 254, Batch: 0, Loss: 1.088302
Train - Epoch 255, Batch: 0, Loss: 1.097411
Train - Epoch 256, Batch: 0, Loss: 1.099997
Train - Epoch 257, Batch: 0, Loss: 1.086751
Train - Epoch 258, Batch: 0, Loss: 1.091482
Train - Epoch 259, Batch: 0, Loss: 1.091635
Train - Epoch 260, Batch: 0, Loss: 1.089748
Train - Epoch 261, Batch: 0, Loss: 1.091842
Train - Epoch 262, Batch: 0, Loss: 1.101511
Train - Epoch 263, Batch: 0, Loss: 1.081403
Train - Epoch 264, Batch: 0, Loss: 1.083501
Train - Epoch 265, Batch: 0, Loss: 1.084953
Train - Epoch 266, Batch: 0, Loss: 1.095817
Train - Epoch 267, Batch: 0, Loss: 1.104164
Train - Epoch 268, Batch: 0, Loss: 1.089209
Train - Epoch 269, Batch: 0, Loss: 1.090850
Train - Epoch 270, Batch: 0, Loss: 1.094864
Train - Epoch 271, Batch: 0, Loss: 1.071934
Train - Epoch 272, Batch: 0, Loss: 1.091161
Train - Epoch 273, Batch: 0, Loss: 1.089000
Train - Epoch 274, Batch: 0, Loss: 1.071657
Train - Epoch 275, Batch: 0, Loss: 1.070574
Train - Epoch 276, Batch: 0, Loss: 1.080957
Train - Epoch 277, Batch: 0, Loss: 1.087184
Train - Epoch 278, Batch: 0, Loss: 1.078595
Train - Epoch 279, Batch: 0, Loss: 1.091936
Train - Epoch 280, Batch: 0, Loss: 1.083733
Train - Epoch 281, Batch: 0, Loss: 1.087508
Train - Epoch 282, Batch: 0, Loss: 1.088961
Train - Epoch 283, Batch: 0, Loss: 1.084727
Train - Epoch 284, Batch: 0, Loss: 1.085034
Train - Epoch 285, Batch: 0, Loss: 1.078973
Train - Epoch 286, Batch: 0, Loss: 1.082959
Train - Epoch 287, Batch: 0, Loss: 1.083999
Train - Epoch 288, Batch: 0, Loss: 1.082126
Train - Epoch 289, Batch: 0, Loss: 1.079103
Train - Epoch 290, Batch: 0, Loss: 1.077166
Train - Epoch 291, Batch: 0, Loss: 1.073911
Train - Epoch 292, Batch: 0, Loss: 1.075463
Train - Epoch 293, Batch: 0, Loss: 1.082967
Train - Epoch 294, Batch: 0, Loss: 1.080750
Train - Epoch 295, Batch: 0, Loss: 1.085332
Train - Epoch 296, Batch: 0, Loss: 1.080089
Train - Epoch 297, Batch: 0, Loss: 1.067165
Train - Epoch 298, Batch: 0, Loss: 1.067440
Train - Epoch 299, Batch: 0, Loss: 1.082094
Train - Epoch 300, Batch: 0, Loss: 1.086573
Train - Epoch 301, Batch: 0, Loss: 1.078909
Train - Epoch 302, Batch: 0, Loss: 1.074973
Train - Epoch 303, Batch: 0, Loss: 1.078148
Train - Epoch 304, Batch: 0, Loss: 1.088705
Train - Epoch 305, Batch: 0, Loss: 1.068635
Train - Epoch 306, Batch: 0, Loss: 1.079202
Train - Epoch 307, Batch: 0, Loss: 1.055757
Train - Epoch 308, Batch: 0, Loss: 1.083208
Train - Epoch 309, Batch: 0, Loss: 1.070855
Train - Epoch 310, Batch: 0, Loss: 1.076386
Train - Epoch 311, Batch: 0, Loss: 1.074153
Train - Epoch 312, Batch: 0, Loss: 1.099180
Train - Epoch 313, Batch: 0, Loss: 1.081771
Train - Epoch 314, Batch: 0, Loss: 1.084836
Train - Epoch 315, Batch: 0, Loss: 1.063367
Train - Epoch 316, Batch: 0, Loss: 1.078554
Train - Epoch 317, Batch: 0, Loss: 1.060811
Train - Epoch 318, Batch: 0, Loss: 1.078576
Train - Epoch 319, Batch: 0, Loss: 1.083866
Train - Epoch 320, Batch: 0, Loss: 1.077841
Train - Epoch 321, Batch: 0, Loss: 1.079170
Train - Epoch 322, Batch: 0, Loss: 1.071101
Train - Epoch 323, Batch: 0, Loss: 1.075277
Train - Epoch 324, Batch: 0, Loss: 1.074624
Train - Epoch 325, Batch: 0, Loss: 1.076185
Train - Epoch 326, Batch: 0, Loss: 1.067339
Train - Epoch 327, Batch: 0, Loss: 1.073793
Train - Epoch 328, Batch: 0, Loss: 1.053635
Train - Epoch 329, Batch: 0, Loss: 1.065226
Train - Epoch 330, Batch: 0, Loss: 1.076437
Train - Epoch 331, Batch: 0, Loss: 1.070698
Train - Epoch 332, Batch: 0, Loss: 1.068521
Train - Epoch 333, Batch: 0, Loss: 1.059984
Train - Epoch 334, Batch: 0, Loss: 1.044964
Train - Epoch 335, Batch: 0, Loss: 1.061510
Train - Epoch 336, Batch: 0, Loss: 1.065839
Train - Epoch 337, Batch: 0, Loss: 1.078757
Train - Epoch 338, Batch: 0, Loss: 1.062759
Train - Epoch 339, Batch: 0, Loss: 1.072752
Train - Epoch 340, Batch: 0, Loss: 1.072330
Train - Epoch 341, Batch: 0, Loss: 1.061198
Train - Epoch 342, Batch: 0, Loss: 1.077214
Train - Epoch 343, Batch: 0, Loss: 1.054661
Train - Epoch 344, Batch: 0, Loss: 1.064967
Train - Epoch 345, Batch: 0, Loss: 1.058542
Train - Epoch 346, Batch: 0, Loss: 1.069900
Train - Epoch 347, Batch: 0, Loss: 1.070719
Train - Epoch 348, Batch: 0, Loss: 1.073793
Train - Epoch 349, Batch: 0, Loss: 1.068090
Train - Epoch 350, Batch: 0, Loss: 1.056447
Train - Epoch 351, Batch: 0, Loss: 1.061670
Train - Epoch 352, Batch: 0, Loss: 1.079611
Train - Epoch 353, Batch: 0, Loss: 1.075265
Train - Epoch 354, Batch: 0, Loss: 1.073064
Train - Epoch 355, Batch: 0, Loss: 1.075247
Train - Epoch 356, Batch: 0, Loss: 1.077175
Train - Epoch 357, Batch: 0, Loss: 1.063897
Train - Epoch 358, Batch: 0, Loss: 1.079440
Train - Epoch 359, Batch: 0, Loss: 1.052737
Train - Epoch 360, Batch: 0, Loss: 1.057544
Train - Epoch 361, Batch: 0, Loss: 1.050968
Train - Epoch 362, Batch: 0, Loss: 1.068581
Train - Epoch 363, Batch: 0, Loss: 1.071877
Train - Epoch 364, Batch: 0, Loss: 1.050722
Train - Epoch 365, Batch: 0, Loss: 1.074427
Train - Epoch 366, Batch: 0, Loss: 1.053608
Train - Epoch 367, Batch: 0, Loss: 1.047697
Train - Epoch 368, Batch: 0, Loss: 1.058387
Train - Epoch 369, Batch: 0, Loss: 1.061469
Train - Epoch 370, Batch: 0, Loss: 1.054818
Train - Epoch 371, Batch: 0, Loss: 1.053424
Train - Epoch 372, Batch: 0, Loss: 1.071916
Train - Epoch 373, Batch: 0, Loss: 1.055196/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 374, Batch: 0, Loss: 1.076298
Train - Epoch 375, Batch: 0, Loss: 1.058310
Train - Epoch 376, Batch: 0, Loss: 1.053192
Train - Epoch 377, Batch: 0, Loss: 1.068341
Train - Epoch 378, Batch: 0, Loss: 1.074403
Train - Epoch 379, Batch: 0, Loss: 1.058961
Train - Epoch 380, Batch: 0, Loss: 1.057679
Train - Epoch 381, Batch: 0, Loss: 1.056112
Train - Epoch 382, Batch: 0, Loss: 1.060859
Train - Epoch 383, Batch: 0, Loss: 1.065604
Train - Epoch 384, Batch: 0, Loss: 1.047165
Train - Epoch 385, Batch: 0, Loss: 1.068066
Train - Epoch 386, Batch: 0, Loss: 1.068154
Train - Epoch 387, Batch: 0, Loss: 1.055533
Train - Epoch 388, Batch: 0, Loss: 1.050700
Train - Epoch 389, Batch: 0, Loss: 1.061505
Train - Epoch 390, Batch: 0, Loss: 1.053891
Train - Epoch 391, Batch: 0, Loss: 1.045496
Train - Epoch 392, Batch: 0, Loss: 1.061306
Train - Epoch 393, Batch: 0, Loss: 1.057360
Train - Epoch 394, Batch: 0, Loss: 1.079174
Train - Epoch 395, Batch: 0, Loss: 1.057237
Train - Epoch 396, Batch: 0, Loss: 1.053329
Train - Epoch 397, Batch: 0, Loss: 1.045141
Train - Epoch 398, Batch: 0, Loss: 1.059180
Train - Epoch 399, Batch: 0, Loss: 1.050883
Train - Epoch 400, Batch: 0, Loss: 1.057817
Train - Epoch 401, Batch: 0, Loss: 1.063285
Train - Epoch 402, Batch: 0, Loss: 1.069369
Train - Epoch 403, Batch: 0, Loss: 1.056140
Train - Epoch 404, Batch: 0, Loss: 1.056500
Train - Epoch 405, Batch: 0, Loss: 1.061413
Train - Epoch 406, Batch: 0, Loss: 1.064803
Train - Epoch 407, Batch: 0, Loss: 1.029042
Train - Epoch 408, Batch: 0, Loss: 1.058004
Train - Epoch 409, Batch: 0, Loss: 1.051493
Train - Epoch 410, Batch: 0, Loss: 1.055207
Train - Epoch 411, Batch: 0, Loss: 1.060312
Train - Epoch 412, Batch: 0, Loss: 1.056652
Train - Epoch 413, Batch: 0, Loss: 1.045181
Train - Epoch 414, Batch: 0, Loss: 1.040360
Train - Epoch 415, Batch: 0, Loss: 1.063255
Train - Epoch 416, Batch: 0, Loss: 1.054780
Train - Epoch 417, Batch: 0, Loss: 1.056337
Train - Epoch 418, Batch: 0, Loss: 1.054281
Train - Epoch 419, Batch: 0, Loss: 1.043963
Train - Epoch 420, Batch: 0, Loss: 1.043503
Train - Epoch 421, Batch: 0, Loss: 1.041047
Train - Epoch 422, Batch: 0, Loss: 1.047493
Train - Epoch 423, Batch: 0, Loss: 1.063657
Train - Epoch 424, Batch: 0, Loss: 1.059592
Train - Epoch 425, Batch: 0, Loss: 1.051852
Train - Epoch 426, Batch: 0, Loss: 1.044080
Train - Epoch 427, Batch: 0, Loss: 1.063542
Train - Epoch 428, Batch: 0, Loss: 1.055863
Train - Epoch 429, Batch: 0, Loss: 1.039267
Train - Epoch 430, Batch: 0, Loss: 1.053436
Train - Epoch 431, Batch: 0, Loss: 1.039373
Train - Epoch 432, Batch: 0, Loss: 1.024975
Train - Epoch 433, Batch: 0, Loss: 1.055618
Train - Epoch 434, Batch: 0, Loss: 1.060536
Train - Epoch 435, Batch: 0, Loss: 1.031574
Train - Epoch 436, Batch: 0, Loss: 1.039856
Train - Epoch 437, Batch: 0, Loss: 1.056144
Train - Epoch 438, Batch: 0, Loss: 1.038518
Train - Epoch 439, Batch: 0, Loss: 1.061549
Train - Epoch 440, Batch: 0, Loss: 1.047299
Train - Epoch 441, Batch: 0, Loss: 1.051016
Train - Epoch 442, Batch: 0, Loss: 1.045913
Train - Epoch 443, Batch: 0, Loss: 1.059957
Train - Epoch 444, Batch: 0, Loss: 1.047951
Train - Epoch 445, Batch: 0, Loss: 1.049756
Train - Epoch 446, Batch: 0, Loss: 1.034342
Train - Epoch 447, Batch: 0, Loss: 1.042888
Train - Epoch 448, Batch: 0, Loss: 1.047565
Train - Epoch 449, Batch: 0, Loss: 1.040431
Train - Epoch 450, Batch: 0, Loss: 1.035808
Train - Epoch 451, Batch: 0, Loss: 1.039568
Train - Epoch 452, Batch: 0, Loss: 1.024266
Train - Epoch 453, Batch: 0, Loss: 1.045337
Train - Epoch 454, Batch: 0, Loss: 1.046831
Train - Epoch 455, Batch: 0, Loss: 1.036859
Train - Epoch 456, Batch: 0, Loss: 1.035580
Train - Epoch 457, Batch: 0, Loss: 1.033422
Train - Epoch 458, Batch: 0, Loss: 1.040642
Train - Epoch 459, Batch: 0, Loss: 1.054337
Train - Epoch 460, Batch: 0, Loss: 1.039981
Train - Epoch 461, Batch: 0, Loss: 1.044544
Train - Epoch 462, Batch: 0, Loss: 1.029450
Train - Epoch 463, Batch: 0, Loss: 1.034880
Train - Epoch 464, Batch: 0, Loss: 1.046949
Train - Epoch 465, Batch: 0, Loss: 1.067510
Train - Epoch 466, Batch: 0, Loss: 1.034731
Train - Epoch 467, Batch: 0, Loss: 1.045314
Train - Epoch 468, Batch: 0, Loss: 1.047729
Train - Epoch 469, Batch: 0, Loss: 1.052211
Train - Epoch 470, Batch: 0, Loss: 1.031333
Train - Epoch 471, Batch: 0, Loss: 1.044764
Train - Epoch 472, Batch: 0, Loss: 1.045699
Train - Epoch 473, Batch: 0, Loss: 1.038227
Train - Epoch 474, Batch: 0, Loss: 1.032026
Train - Epoch 475, Batch: 0, Loss: 1.043310
Train - Epoch 476, Batch: 0, Loss: 1.051125
Train - Epoch 477, Batch: 0, Loss: 1.042694
Train - Epoch 478, Batch: 0, Loss: 1.055697
Train - Epoch 479, Batch: 0, Loss: 1.029598
Train - Epoch 480, Batch: 0, Loss: 1.068829
Train - Epoch 481, Batch: 0, Loss: 1.049377
Train - Epoch 482, Batch: 0, Loss: 1.043548
Train - Epoch 483, Batch: 0, Loss: 1.049286
Train - Epoch 484, Batch: 0, Loss: 1.038012
Train - Epoch 485, Batch: 0, Loss: 1.036072
Train - Epoch 486, Batch: 0, Loss: 1.034670
Train - Epoch 487, Batch: 0, Loss: 1.022873
Train - Epoch 488, Batch: 0, Loss: 1.037472
Train - Epoch 489, Batch: 0, Loss: 1.031446
Train - Epoch 490, Batch: 0, Loss: 1.040948
Train - Epoch 491, Batch: 0, Loss: 1.040859
Train - Epoch 492, Batch: 0, Loss: 1.027253
Train - Epoch 493, Batch: 0, Loss: 1.033730
Train - Epoch 494, Batch: 0, Loss: 1.043559
Train - Epoch 495, Batch: 0, Loss: 1.041024
Train - Epoch 496, Batch: 0, Loss: 1.042875
Train - Epoch 497, Batch: 0, Loss: 1.040514
Train - Epoch 498, Batch: 0, Loss: 1.023338
Train - Epoch 499, Batch: 0, Loss: 1.028264
training_time:: 106.84081363677979
training time full:: 106.8408830165863
provenance prepare time:: 0.0
here
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.631800
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 25
max_epoch:: 500
batch_size:: 10000
epoch:: 0
tensor([    1,     6, 32775, 32776, 32781, 32783,    31,    33, 32802,    43,
        32811,    45,    46, 32815, 32820,    53,    54, 32823,    55, 32826,
        32829, 32831, 32834,    75, 32844, 32856,    88,    94, 32864,    98,
          104, 32874,   107, 32876,   109,   112,   113,   118, 32886, 32893,
        32896,   131,   136, 32908,   144, 32913,   158,   159, 32927,   164])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
epoch:: 60
epoch:: 61
epoch:: 62
epoch:: 63
epoch:: 64
epoch:: 65
epoch:: 66
epoch:: 67
epoch:: 68
epoch:: 69
epoch:: 70
epoch:: 71
epoch:: 72
epoch:: 73
epoch:: 74
epoch:: 75
epoch:: 76
epoch:: 77
epoch:: 78
epoch:: 79
epoch:: 80
epoch:: 81
epoch:: 82
epoch:: 83
epoch:: 84
epoch:: 85
epoch:: 86
epoch:: 87
epoch:: 88
epoch:: 89
epoch:: 90
epoch:: 91
epoch:: 92
epoch:: 93
epoch:: 94
epoch:: 95
epoch:: 96
epoch:: 97
epoch:: 98
epoch:: 99
epoch:: 100
epoch:: 101
epoch:: 102
epoch:: 103
epoch:: 104
epoch:: 105
epoch:: 106
epoch:: 107
epoch:: 108
epoch:: 109
epoch:: 110
epoch:: 111
epoch:: 112
epoch:: 113
epoch:: 114
epoch:: 115
epoch:: 116
epoch:: 117
epoch:: 118
epoch:: 119
epoch:: 120
epoch:: 121
epoch:: 122
epoch:: 123
epoch:: 124
epoch:: 125
epoch:: 126
epoch:: 127
epoch:: 128
epoch:: 129
epoch:: 130
epoch:: 131
epoch:: 132
epoch:: 133
epoch:: 134
epoch:: 135
epoch:: 136
epoch:: 137
epoch:: 138
epoch:: 139
epoch:: 140
epoch:: 141
epoch:: 142
epoch:: 143
epoch:: 144
epoch:: 145
epoch:: 146
epoch:: 147
epoch:: 148
epoch:: 149
epoch:: 150
epoch:: 151
epoch:: 152
epoch:: 153
epoch:: 154
epoch:: 155
epoch:: 156
epoch:: 157
epoch:: 158
epoch:: 159
epoch:: 160
epoch:: 161
epoch:: 162
epoch:: 163
epoch:: 164
epoch:: 165
epoch:: 166
epoch:: 167
epoch:: 168
epoch:: 169
epoch:: 170
epoch:: 171
epoch:: 172
epoch:: 173
epoch:: 174
epoch:: 175
epoch:: 176
epoch:: 177
epoch:: 178
epoch:: 179
epoch:: 180
epoch:: 181
epoch:: 182
epoch:: 183
epoch:: 184
epoch:: 185
epoch:: 186
epoch:: 187
epoch:: 188
epoch:: 189
epoch:: 190
epoch:: 191
epoch:: 192
epoch:: 193
epoch:: 194
epoch:: 195
epoch:: 196
epoch:: 197
epoch:: 198
epoch:: 199
epoch:: 200
epoch:: 201
epoch:: 202
epoch:: 203
epoch:: 204
epoch:: 205
epoch:: 206
epoch:: 207
epoch:: 208
epoch:: 209
epoch:: 210
epoch:: 211
epoch:: 212
epoch:: 213
epoch:: 214
epoch:: 215
epoch:: 216
epoch:: 217
epoch:: 218
epoch:: 219
epoch:: 220
epoch:: 221
epoch:: 222
epoch:: 223
epoch:: 224
epoch:: 225
epoch:: 226
epoch:: 227
epoch:: 228
epoch:: 229
epoch:: 230
epoch:: 231
epoch:: 232
epoch:: 233
epoch:: 234
epoch:: 235
epoch:: 236
epoch:: 237
epoch:: 238
epoch:: 239
epoch:: 240
epoch:: 241
epoch:: 242
epoch:: 243
epoch:: 244
epoch:: 245
epoch:: 246
epoch:: 247
epoch:: 248
epoch:: 249
epoch:: 250
epoch:: 251
epoch:: 252
epoch:: 253
epoch:: 254
epoch:: 255
epoch:: 256
epoch:: 257
epoch:: 258
epoch:: 259
epoch:: 260
epoch:: 261
epoch:: 262
epoch:: 263
epoch:: 264
epoch:: 265
epoch:: 266
epoch:: 267
epoch:: 268
epoch:: 269
epoch:: 270
epoch:: 271
epoch:: 272
epoch:: 273
epoch:: 274
epoch:: 275
epoch:: 276
epoch:: 277
epoch:: 278
epoch:: 279
epoch:: 280
epoch:: 281
epoch:: 282
epoch:: 283
epoch:: 284
epoch:: 285
epoch:: 286
epoch:: 287
epoch:: 288
epoch:: 289
epoch:: 290
epoch:: 291
epoch:: 292
epoch:: 293
epoch:: 294
epoch:: 295
epoch:: 296
epoch:: 297
epoch:: 298
epoch:: 299
epoch:: 300
epoch:: 301
epoch:: 302
epoch:: 303
epoch:: 304
epoch:: 305
epoch:: 306
epoch:: 307
epoch:: 308
epoch:: 309
epoch:: 310
epoch:: 311
epoch:: 312
epoch:: 313
epoch:: 314
epoch:: 315
epoch:: 316
epoch:: 317
epoch:: 318
epoch:: 319
epoch:: 320
epoch:: 321
epoch:: 322
epoch:: 323
epoch:: 324
epoch:: 325
epoch:: 326
epoch:: 327
epoch:: 328
epoch:: 329
epoch:: 330
epoch:: 331
epoch:: 332
epoch:: 333
epoch:: 334
epoch:: 335
epoch:: 336
epoch:: 337
epoch:: 338
epoch:: 339
epoch:: 340
epoch:: 341
epoch:: 342
epoch:: 343
epoch:: 344
epoch:: 345
epoch:: 346
epoch:: 347
epoch:: 348
epoch:: 349
epoch:: 350
epoch:: 351
epoch:: 352
epoch:: 353
epoch:: 354
epoch:: 355
epoch:: 356
epoch:: 357
epoch:: 358
epoch:: 359
epoch:: 360
epoch:: 361
epoch:: 362
epoch:: 363
epoch:: 364
epoch:: 365
epoch:: 366
epoch:: 367
epoch:: 368
epoch:: 369
epoch:: 370
epoch:: 371
epoch:: 372
epoch:: 373
epoch:: 374
epoch:: 375
epoch:: 376
epoch:: 377
epoch:: 378
epoch:: 379
epoch:: 380
epoch:: 381
epoch:: 382
epoch:: 383
epoch:: 384
epoch:: 385
epoch:: 386
epoch:: 387
epoch:: 388
epoch:: 389
epoch:: 390
epoch:: 391
epoch:: 392
epoch:: 393
epoch:: 394
epoch:: 395
epoch:: 396
epoch:: 397
epoch:: 398
epoch:: 399
epoch:: 400
epoch:: 401
epoch:: 402
epoch:: 403
epoch:: 404
epoch:: 405
epoch:: 406
epoch:: 407
epoch:: 408
epoch:: 409
epoch:: 410
epoch:: 411
epoch:: 412
epoch:: 413
epoch:: 414
epoch:: 415
epoch:: 416
epoch:: 417
epoch:: 418
epoch:: 419
epoch:: 420
epoch:: 421
epoch:: 422
epoch:: 423
epoch:: 424
epoch:: 425
epoch:: 426
epoch:: 427
epoch:: 428
epoch:: 429
epoch:: 430
epoch:: 431
epoch:: 432
epoch:: 433
epoch:: 434
epoch:: 435
epoch:: 436
epoch:: 437
epoch:: 438
epoch:: 439
epoch:: 440
epoch:: 441
epoch:: 442
epoch:: 443
epoch:: 444
epoch:: 445
epoch:: 446
epoch:: 447
epoch:: 448
epoch:: 449
epoch:: 450
epoch:: 451
epoch:: 452
epoch:: 453
epoch:: 454
epoch:: 455
epoch:: 456
epoch:: 457
epoch:: 458
epoch:: 459
epoch:: 460
epoch:: 461
epoch:: 462
epoch:: 463
epoch:: 464
epoch:: 465
epoch:: 466
epoch:: 467
epoch:: 468
epoch:: 469
epoch:: 470
epoch:: 471
epoch:: 472
epoch:: 473
epoch:: 474
epoch:: 475
epoch:: 476
epoch:: 477
epoch:: 478
epoch:: 479
epoch:: 480
epoch:: 481
epoch:: 482
epoch:: 483
epoch:: 484
epoch:: 485
epoch:: 486
epoch:: 487
epoch:: 488
epoch:: 489
epoch:: 490
epoch:: 491
epoch:: 492
epoch:: 493
epoch:: 494
epoch:: 495
epoch:: 496
epoch:: 497
epoch:: 498
epoch:: 499
training time is 81.06235218048096
overhead:: 0
overhead2:: 2.679842472076416
overhead3:: 0
time_baseline:: 81.06238603591919
curr_diff: 0 tensor(0.0386, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0386, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.631600
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 25
max_epoch:: 500
overhead:: 0
overhead2:: 0.08186221122741699
overhead3:: 0.25264716148376465
overhead4:: 9.972233295440674
overhead5:: 0
memory usage:: 5627863040
time_provenance:: 17.434084177017212
curr_diff: 0 tensor(0.0095, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0095, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0458, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0458, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632000
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 25
max_epoch:: 500
overhead:: 0
overhead2:: 0.08399748802185059
overhead3:: 0.2623467445373535
overhead4:: 10.545963048934937
overhead5:: 0
memory usage:: 5628928000
time_provenance:: 18.062536239624023
curr_diff: 0 tensor(0.0095, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0095, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0458, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0458, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.631800
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 25
max_epoch:: 500
overhead:: 0
overhead2:: 0.08296465873718262
overhead3:: 0.26367616653442383
overhead4:: 10.80560564994812
overhead5:: 0
memory usage:: 5626494976
time_provenance:: 18.363093376159668
curr_diff: 0 tensor(0.0095, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0095, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0458, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0458, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632000
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 25
max_epoch:: 500
overhead:: 0
overhead2:: 0.15448665618896484
overhead3:: 0.44989490509033203
overhead4:: 18.282600164413452
overhead5:: 0
memory usage:: 5636079616
time_provenance:: 27.686319828033447
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0392, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0392, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.631600
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 25
max_epoch:: 500
overhead:: 0
overhead2:: 0.15443706512451172
overhead3:: 0.44556331634521484
overhead4:: 18.747158527374268
overhead5:: 0
memory usage:: 5624451072
time_provenance:: 28.180542707443237
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0392, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0392, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.631600
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 25
max_epoch:: 500
overhead:: 0
overhead2:: 0.1591968536376953
overhead3:: 0.4569885730743408
overhead4:: 18.413328409194946
overhead5:: 0
memory usage:: 5635768320
time_provenance:: 27.887960195541382
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0392, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0392, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.631600
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 25
max_epoch:: 500
overhead:: 0
overhead2:: 0.3738064765930176
overhead3:: 1.0216825008392334
overhead4:: 44.664149045944214
overhead5:: 0
memory usage:: 5644091392
time_provenance:: 59.97801494598389
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0386, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0386, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.631600
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 25
max_epoch:: 500
overhead:: 0
overhead2:: 0.39327049255371094
overhead3:: 1.0226590633392334
overhead4:: 44.901585817337036
overhead5:: 0
memory usage:: 5623250944
time_provenance:: 60.26547431945801
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0386, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0386, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.631600
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 25
max_epoch:: 500
overhead:: 0
overhead2:: 0.3906583786010742
overhead3:: 1.0519840717315674
overhead4:: 44.69678020477295
overhead5:: 0
memory usage:: 5634543616
time_provenance:: 60.10419201850891
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0386, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0386, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.631600
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 2 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 25
max_epoch:: 500
overhead:: 0
overhead2:: 0.7980911731719971
overhead3:: 2.0622398853302
overhead4:: 83.5612063407898
overhead5:: 0
memory usage:: 5630578688
time_provenance:: 106.84981942176819
curr_diff: 0 tensor(5.0005e-13, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.0005e-13, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0386, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0386, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.631600
repetition 3
python3 benchmark_exp_lr.py 0.001 10000 500 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05] Logistic_regression cifar10_2 3 0.0001 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 2.311922
Train - Epoch 1, Batch: 0, Loss: 2.248340
Train - Epoch 2, Batch: 0, Loss: 2.192598
Train - Epoch 3, Batch: 0, Loss: 2.142248
Train - Epoch 4, Batch: 0, Loss: 2.094844
Train - Epoch 5, Batch: 0, Loss: 2.050468
Train - Epoch 6, Batch: 0, Loss: 2.007619
Train - Epoch 7, Batch: 0, Loss: 1.972945
Train - Epoch 8, Batch: 0, Loss: 1.936084
Train - Epoch 9, Batch: 0, Loss: 1.902807
Train - Epoch 10, Batch: 0, Loss: 1.872449
Train - Epoch 11, Batch: 0, Loss: 1.842233
Train - Epoch 12, Batch: 0, Loss: 1.815004
Train - Epoch 13, Batch: 0, Loss: 1.789467
Train - Epoch 14, Batch: 0, Loss: 1.756105
Train - Epoch 15, Batch: 0, Loss: 1.737985
Train - Epoch 16, Batch: 0, Loss: 1.720508
Train - Epoch 17, Batch: 0, Loss: 1.692745
Train - Epoch 18, Batch: 0, Loss: 1.675041
Train - Epoch 19, Batch: 0, Loss: 1.663971
Train - Epoch 20, Batch: 0, Loss: 1.645627
Train - Epoch 21, Batch: 0, Loss: 1.621797
Train - Epoch 22, Batch: 0, Loss: 1.618129
Train - Epoch 23, Batch: 0, Loss: 1.591916
Train - Epoch 24, Batch: 0, Loss: 1.581829
Train - Epoch 25, Batch: 0, Loss: 1.570523
Train - Epoch 26, Batch: 0, Loss: 1.545007
Train - Epoch 27, Batch: 0, Loss: 1.547422
Train - Epoch 28, Batch: 0, Loss: 1.531944
Train - Epoch 29, Batch: 0, Loss: 1.528107
Train - Epoch 30, Batch: 0, Loss: 1.509518
Train - Epoch 31, Batch: 0, Loss: 1.502970
Train - Epoch 32, Batch: 0, Loss: 1.490451
Train - Epoch 33, Batch: 0, Loss: 1.480626
Train - Epoch 34, Batch: 0, Loss: 1.484716
Train - Epoch 35, Batch: 0, Loss: 1.458946
Train - Epoch 36, Batch: 0, Loss: 1.461764
Train - Epoch 37, Batch: 0, Loss: 1.444042
Train - Epoch 38, Batch: 0, Loss: 1.440691
Train - Epoch 39, Batch: 0, Loss: 1.435031
Train - Epoch 40, Batch: 0, Loss: 1.420385
Train - Epoch 41, Batch: 0, Loss: 1.419619
Train - Epoch 42, Batch: 0, Loss: 1.402365
Train - Epoch 43, Batch: 0, Loss: 1.409913
Train - Epoch 44, Batch: 0, Loss: 1.407591
Train - Epoch 45, Batch: 0, Loss: 1.385805
Train - Epoch 46, Batch: 0, Loss: 1.382041
Train - Epoch 47, Batch: 0, Loss: 1.372573
Train - Epoch 48, Batch: 0, Loss: 1.376806
Train - Epoch 49, Batch: 0, Loss: 1.365905
Train - Epoch 50, Batch: 0, Loss: 1.357783
Train - Epoch 51, Batch: 0, Loss: 1.357222
Train - Epoch 52, Batch: 0, Loss: 1.351140
Train - Epoch 53, Batch: 0, Loss: 1.370392
Train - Epoch 54, Batch: 0, Loss: 1.339565
Train - Epoch 55, Batch: 0, Loss: 1.345466
Train - Epoch 56, Batch: 0, Loss: 1.337133
Train - Epoch 57, Batch: 0, Loss: 1.325276
Train - Epoch 58, Batch: 0, Loss: 1.324764
Train - Epoch 59, Batch: 0, Loss: 1.319211
Train - Epoch 60, Batch: 0, Loss: 1.314509
Train - Epoch 61, Batch: 0, Loss: 1.310425
Train - Epoch 62, Batch: 0, Loss: 1.315992
Train - Epoch 63, Batch: 0, Loss: 1.306471
Train - Epoch 64, Batch: 0, Loss: 1.302459
Train - Epoch 65, Batch: 0, Loss: 1.307929
Train - Epoch 66, Batch: 0, Loss: 1.291394
Train - Epoch 67, Batch: 0, Loss: 1.293105
Train - Epoch 68, Batch: 0, Loss: 1.287427
Train - Epoch 69, Batch: 0, Loss: 1.290688
Train - Epoch 70, Batch: 0, Loss: 1.290473
Train - Epoch 71, Batch: 0, Loss: 1.271068
Train - Epoch 72, Batch: 0, Loss: 1.269586
Train - Epoch 73, Batch: 0, Loss: 1.261146
Train - Epoch 74, Batch: 0, Loss: 1.270113
Train - Epoch 75, Batch: 0, Loss: 1.274342
Train - Epoch 76, Batch: 0, Loss: 1.273236
Train - Epoch 77, Batch: 0, Loss: 1.266766
Train - Epoch 78, Batch: 0, Loss: 1.267188
Train - Epoch 79, Batch: 0, Loss: 1.270885
Train - Epoch 80, Batch: 0, Loss: 1.251971
Train - Epoch 81, Batch: 0, Loss: 1.244602
Train - Epoch 82, Batch: 0, Loss: 1.254080
Train - Epoch 83, Batch: 0, Loss: 1.244643
Train - Epoch 84, Batch: 0, Loss: 1.244292
Train - Epoch 85, Batch: 0, Loss: 1.251657
Train - Epoch 86, Batch: 0, Loss: 1.256548
Train - Epoch 87, Batch: 0, Loss: 1.246795
Train - Epoch 88, Batch: 0, Loss: 1.229611
Train - Epoch 89, Batch: 0, Loss: 1.239254
Train - Epoch 90, Batch: 0, Loss: 1.231118
Train - Epoch 91, Batch: 0, Loss: 1.230399
Train - Epoch 92, Batch: 0, Loss: 1.241203
Train - Epoch 93, Batch: 0, Loss: 1.218740
Train - Epoch 94, Batch: 0, Loss: 1.224979
Train - Epoch 95, Batch: 0, Loss: 1.241622
Train - Epoch 96, Batch: 0, Loss: 1.233712
Train - Epoch 97, Batch: 0, Loss: 1.220815
Train - Epoch 98, Batch: 0, Loss: 1.223849
Train - Epoch 99, Batch: 0, Loss: 1.227674
Train - Epoch 100, Batch: 0, Loss: 1.208468
Train - Epoch 101, Batch: 0, Loss: 1.200651
Train - Epoch 102, Batch: 0, Loss: 1.215770
Train - Epoch 103, Batch: 0, Loss: 1.212304
Train - Epoch 104, Batch: 0, Loss: 1.203292
Train - Epoch 105, Batch: 0, Loss: 1.196441
Train - Epoch 106, Batch: 0, Loss: 1.214264
Train - Epoch 107, Batch: 0, Loss: 1.207038
Train - Epoch 108, Batch: 0, Loss: 1.203749
Train - Epoch 109, Batch: 0, Loss: 1.189701
Train - Epoch 110, Batch: 0, Loss: 1.198630
Train - Epoch 111, Batch: 0, Loss: 1.198930
Train - Epoch 112, Batch: 0, Loss: 1.188010
Train - Epoch 113, Batch: 0, Loss: 1.188549
Train - Epoch 114, Batch: 0, Loss: 1.193790
Train - Epoch 115, Batch: 0, Loss: 1.197326
Train - Epoch 116, Batch: 0, Loss: 1.182646
Train - Epoch 117, Batch: 0, Loss: 1.193583
Train - Epoch 118, Batch: 0, Loss: 1.187594
Train - Epoch 119, Batch: 0, Loss: 1.171404
Train - Epoch 120, Batch: 0, Loss: 1.187801
Train - Epoch 121, Batch: 0, Loss: 1.192814
Train - Epoch 122, Batch: 0, Loss: 1.196963
Train - Epoch 123, Batch: 0, Loss: 1.189104
Train - Epoch 124, Batch: 0, Loss: 1.173066
Train - Epoch 125, Batch: 0, Loss: 1.187095
Train - Epoch 126, Batch: 0, Loss: 1.186209
Train - Epoch 127, Batch: 0, Loss: 1.196745
Train - Epoch 128, Batch: 0, Loss: 1.186592
Train - Epoch 129, Batch: 0, Loss: 1.159205
Train - Epoch 130, Batch: 0, Loss: 1.162745
Train - Epoch 131, Batch: 0, Loss: 1.166010
Train - Epoch 132, Batch: 0, Loss: 1.178335
Train - Epoch 133, Batch: 0, Loss: 1.166538
Train - Epoch 134, Batch: 0, Loss: 1.158699
Train - Epoch 135, Batch: 0, Loss: 1.169371
Train - Epoch 136, Batch: 0, Loss: 1.161353
Train - Epoch 137, Batch: 0, Loss: 1.173699
Train - Epoch 138, Batch: 0, Loss: 1.160874
Train - Epoch 139, Batch: 0, Loss: 1.154826
Train - Epoch 140, Batch: 0, Loss: 1.169875
Train - Epoch 141, Batch: 0, Loss: 1.174596
Train - Epoch 142, Batch: 0, Loss: 1.153694
Train - Epoch 143, Batch: 0, Loss: 1.163636
Train - Epoch 144, Batch: 0, Loss: 1.155555
Train - Epoch 145, Batch: 0, Loss: 1.164693
Train - Epoch 146, Batch: 0, Loss: 1.164008
Train - Epoch 147, Batch: 0, Loss: 1.153358
Train - Epoch 148, Batch: 0, Loss: 1.163680
Train - Epoch 149, Batch: 0, Loss: 1.155505
Train - Epoch 150, Batch: 0, Loss: 1.152538
Train - Epoch 151, Batch: 0, Loss: 1.158439
Train - Epoch 152, Batch: 0, Loss: 1.142795
Train - Epoch 153, Batch: 0, Loss: 1.144483
Train - Epoch 154, Batch: 0, Loss: 1.157073
Train - Epoch 155, Batch: 0, Loss: 1.155053
Train - Epoch 156, Batch: 0, Loss: 1.154508
Train - Epoch 157, Batch: 0, Loss: 1.148895
Train - Epoch 158, Batch: 0, Loss: 1.140842
Train - Epoch 159, Batch: 0, Loss: 1.144643
Train - Epoch 160, Batch: 0, Loss: 1.143652
Train - Epoch 161, Batch: 0, Loss: 1.139693
Train - Epoch 162, Batch: 0, Loss: 1.139597
Train - Epoch 163, Batch: 0, Loss: 1.144125
Train - Epoch 164, Batch: 0, Loss: 1.155785
Train - Epoch 165, Batch: 0, Loss: 1.145853
Train - Epoch 166, Batch: 0, Loss: 1.149541
Train - Epoch 167, Batch: 0, Loss: 1.144389
Train - Epoch 168, Batch: 0, Loss: 1.145637
Train - Epoch 169, Batch: 0, Loss: 1.136491
Train - Epoch 170, Batch: 0, Loss: 1.147576
Train - Epoch 171, Batch: 0, Loss: 1.139431
Train - Epoch 172, Batch: 0, Loss: 1.130466
Train - Epoch 173, Batch: 0, Loss: 1.143492
Train - Epoch 174, Batch: 0, Loss: 1.133083
Train - Epoch 175, Batch: 0, Loss: 1.133714
Train - Epoch 176, Batch: 0, Loss: 1.126791
Train - Epoch 177, Batch: 0, Loss: 1.118622
Train - Epoch 178, Batch: 0, Loss: 1.127215
Train - Epoch 179, Batch: 0, Loss: 1.123120
Train - Epoch 180, Batch: 0, Loss: 1.131998
Train - Epoch 181, Batch: 0, Loss: 1.113627
Train - Epoch 182, Batch: 0, Loss: 1.118632
Train - Epoch 183, Batch: 0, Loss: 1.134113
Train - Epoch 184, Batch: 0, Loss: 1.140381
Train - Epoch 185, Batch: 0, Loss: 1.110880
Train - Epoch 186, Batch: 0, Loss: 1.123900
Train - Epoch 187, Batch: 0, Loss: 1.125705
Train - Epoch 188, Batch: 0, Loss: 1.116033
Train - Epoch 189, Batch: 0, Loss: 1.134325
Train - Epoch 190, Batch: 0, Loss: 1.124015
Train - Epoch 191, Batch: 0, Loss: 1.123805
Train - Epoch 192, Batch: 0, Loss: 1.125780
Train - Epoch 193, Batch: 0, Loss: 1.126975
Train - Epoch 194, Batch: 0, Loss: 1.126229
Train - Epoch 195, Batch: 0, Loss: 1.126576
Train - Epoch 196, Batch: 0, Loss: 1.125171
Train - Epoch 197, Batch: 0, Loss: 1.114029
Train - Epoch 198, Batch: 0, Loss: 1.124276
Train - Epoch 199, Batch: 0, Loss: 1.119055
Train - Epoch 200, Batch: 0, Loss: 1.113737
Train - Epoch 201, Batch: 0, Loss: 1.111832
Train - Epoch 202, Batch: 0, Loss: 1.117352
Train - Epoch 203, Batch: 0, Loss: 1.130455
Train - Epoch 204, Batch: 0, Loss: 1.126107
Train - Epoch 205, Batch: 0, Loss: 1.108501
Train - Epoch 206, Batch: 0, Loss: 1.130096
Train - Epoch 207, Batch: 0, Loss: 1.117480
Train - Epoch 208, Batch: 0, Loss: 1.118991
Train - Epoch 209, Batch: 0, Loss: 1.111476
Train - Epoch 210, Batch: 0, Loss: 1.115382
Train - Epoch 211, Batch: 0, Loss: 1.108366
Train - Epoch 212, Batch: 0, Loss: 1.123812
Train - Epoch 213, Batch: 0, Loss: 1.109487
Train - Epoch 214, Batch: 0, Loss: 1.111834
Train - Epoch 215, Batch: 0, Loss: 1.123135
Train - Epoch 216, Batch: 0, Loss: 1.111462
Train - Epoch 217, Batch: 0, Loss: 1.106210
Train - Epoch 218, Batch: 0, Loss: 1.113536
Train - Epoch 219, Batch: 0, Loss: 1.114209
Train - Epoch 220, Batch: 0, Loss: 1.111489
Train - Epoch 221, Batch: 0, Loss: 1.105885
Train - Epoch 222, Batch: 0, Loss: 1.096535
Train - Epoch 223, Batch: 0, Loss: 1.109437
Train - Epoch 224, Batch: 0, Loss: 1.101252
Train - Epoch 225, Batch: 0, Loss: 1.110042
Train - Epoch 226, Batch: 0, Loss: 1.110006
Train - Epoch 227, Batch: 0, Loss: 1.111620
Train - Epoch 228, Batch: 0, Loss: 1.098594
Train - Epoch 229, Batch: 0, Loss: 1.111573
Train - Epoch 230, Batch: 0, Loss: 1.091669
Train - Epoch 231, Batch: 0, Loss: 1.090655
Train - Epoch 232, Batch: 0, Loss: 1.096636
Train - Epoch 233, Batch: 0, Loss: 1.097703
Train - Epoch 234, Batch: 0, Loss: 1.090268
Train - Epoch 235, Batch: 0, Loss: 1.098561
Train - Epoch 236, Batch: 0, Loss: 1.099348
Train - Epoch 237, Batch: 0, Loss: 1.106279
Train - Epoch 238, Batch: 0, Loss: 1.097777
Train - Epoch 239, Batch: 0, Loss: 1.097684
Train - Epoch 240, Batch: 0, Loss: 1.111493
Train - Epoch 241, Batch: 0, Loss: 1.099979
Train - Epoch 242, Batch: 0, Loss: 1.094524
Train - Epoch 243, Batch: 0, Loss: 1.116130
Train - Epoch 244, Batch: 0, Loss: 1.096371
Train - Epoch 245, Batch: 0, Loss: 1.086518
Train - Epoch 246, Batch: 0, Loss: 1.094034
Train - Epoch 247, Batch: 0, Loss: 1.090925
Train - Epoch 248, Batch: 0, Loss: 1.089944
Train - Epoch 249, Batch: 0, Loss: 1.083842
Train - Epoch 250, Batch: 0, Loss: 1.085289
Train - Epoch 251, Batch: 0, Loss: 1.106804
Train - Epoch 252, Batch: 0, Loss: 1.085950
Train - Epoch 253, Batch: 0, Loss: 1.100918
Train - Epoch 254, Batch: 0, Loss: 1.093063
Train - Epoch 255, Batch: 0, Loss: 1.107190
Train - Epoch 256, Batch: 0, Loss: 1.086204
Train - Epoch 257, Batch: 0, Loss: 1.085738
Train - Epoch 258, Batch: 0, Loss: 1.080935
Train - Epoch 259, Batch: 0, Loss: 1.082547
Train - Epoch 260, Batch: 0, Loss: 1.093812
Train - Epoch 261, Batch: 0, Loss: 1.085770
Train - Epoch 262, Batch: 0, Loss: 1.088876
Train - Epoch 263, Batch: 0, Loss: 1.084737
Train - Epoch 264, Batch: 0, Loss: 1.072337
Train - Epoch 265, Batch: 0, Loss: 1.082071
Train - Epoch 266, Batch: 0, Loss: 1.084052
Train - Epoch 267, Batch: 0, Loss: 1.091497
Train - Epoch 268, Batch: 0, Loss: 1.087393
Train - Epoch 269, Batch: 0, Loss: 1.080523
Train - Epoch 270, Batch: 0, Loss: 1.074941
Train - Epoch 271, Batch: 0, Loss: 1.087498
Train - Epoch 272, Batch: 0, Loss: 1.088705
Train - Epoch 273, Batch: 0, Loss: 1.092452
Train - Epoch 274, Batch: 0, Loss: 1.094054
Train - Epoch 275, Batch: 0, Loss: 1.079911
Train - Epoch 276, Batch: 0, Loss: 1.079699
Train - Epoch 277, Batch: 0, Loss: 1.080780
Train - Epoch 278, Batch: 0, Loss: 1.066801
Train - Epoch 279, Batch: 0, Loss: 1.092206
Train - Epoch 280, Batch: 0, Loss: 1.081555
Train - Epoch 281, Batch: 0, Loss: 1.091464
Train - Epoch 282, Batch: 0, Loss: 1.087441
Train - Epoch 283, Batch: 0, Loss: 1.104519
Train - Epoch 284, Batch: 0, Loss: 1.085421
Train - Epoch 285, Batch: 0, Loss: 1.072159
Train - Epoch 286, Batch: 0, Loss: 1.091272
Train - Epoch 287, Batch: 0, Loss: 1.089636
Train - Epoch 288, Batch: 0, Loss: 1.074429
Train - Epoch 289, Batch: 0, Loss: 1.086732
Train - Epoch 290, Batch: 0, Loss: 1.078656
Train - Epoch 291, Batch: 0, Loss: 1.083320
Train - Epoch 292, Batch: 0, Loss: 1.061505
Train - Epoch 293, Batch: 0, Loss: 1.083549
Train - Epoch 294, Batch: 0, Loss: 1.075534
Train - Epoch 295, Batch: 0, Loss: 1.078467
Train - Epoch 296, Batch: 0, Loss: 1.079393
Train - Epoch 297, Batch: 0, Loss: 1.076148
Train - Epoch 298, Batch: 0, Loss: 1.068453
Train - Epoch 299, Batch: 0, Loss: 1.072671
Train - Epoch 300, Batch: 0, Loss: 1.077426
Train - Epoch 301, Batch: 0, Loss: 1.092030
Train - Epoch 302, Batch: 0, Loss: 1.080455
Train - Epoch 303, Batch: 0, Loss: 1.077928
Train - Epoch 304, Batch: 0, Loss: 1.081836
Train - Epoch 305, Batch: 0, Loss: 1.067250
Train - Epoch 306, Batch: 0, Loss: 1.072569
Train - Epoch 307, Batch: 0, Loss: 1.081900
Train - Epoch 308, Batch: 0, Loss: 1.079770
Train - Epoch 309, Batch: 0, Loss: 1.062995
Train - Epoch 310, Batch: 0, Loss: 1.075559
Train - Epoch 311, Batch: 0, Loss: 1.062889
Train - Epoch 312, Batch: 0, Loss: 1.063360
Train - Epoch 313, Batch: 0, Loss: 1.067062
Train - Epoch 314, Batch: 0, Loss: 1.080888
Train - Epoch 315, Batch: 0, Loss: 1.081310
Train - Epoch 316, Batch: 0, Loss: 1.071864
Train - Epoch 317, Batch: 0, Loss: 1.075927
Train - Epoch 318, Batch: 0, Loss: 1.085002
Train - Epoch 319, Batch: 0, Loss: 1.078144
Train - Epoch 320, Batch: 0, Loss: 1.066487
Train - Epoch 321, Batch: 0, Loss: 1.083873
Train - Epoch 322, Batch: 0, Loss: 1.058344
Train - Epoch 323, Batch: 0, Loss: 1.072659
Train - Epoch 324, Batch: 0, Loss: 1.072097
Train - Epoch 325, Batch: 0, Loss: 1.074104
Train - Epoch 326, Batch: 0, Loss: 1.054252
Train - Epoch 327, Batch: 0, Loss: 1.064363
Train - Epoch 328, Batch: 0, Loss: 1.068145
Train - Epoch 329, Batch: 0, Loss: 1.062473
Train - Epoch 330, Batch: 0, Loss: 1.069812
Train - Epoch 331, Batch: 0, Loss: 1.081842
Train - Epoch 332, Batch: 0, Loss: 1.064963
Train - Epoch 333, Batch: 0, Loss: 1.081651
Train - Epoch 334, Batch: 0, Loss: 1.088221
Train - Epoch 335, Batch: 0, Loss: 1.087203
Train - Epoch 336, Batch: 0, Loss: 1.083416
Train - Epoch 337, Batch: 0, Loss: 1.070631
Train - Epoch 338, Batch: 0, Loss: 1.072279
Train - Epoch 339, Batch: 0, Loss: 1.056696
Train - Epoch 340, Batch: 0, Loss: 1.080344
Train - Epoch 341, Batch: 0, Loss: 1.068466
Train - Epoch 342, Batch: 0, Loss: 1.058627
Train - Epoch 343, Batch: 0, Loss: 1.061340
Train - Epoch 344, Batch: 0, Loss: 1.059822
Train - Epoch 345, Batch: 0, Loss: 1.071689
Train - Epoch 346, Batch: 0, Loss: 1.072234
Train - Epoch 347, Batch: 0, Loss: 1.063530
Train - Epoch 348, Batch: 0, Loss: 1.073232
Train - Epoch 349, Batch: 0, Loss: 1.070318
Train - Epoch 350, Batch: 0, Loss: 1.060930
Train - Epoch 351, Batch: 0, Loss: 1.070420
Train - Epoch 352, Batch: 0, Loss: 1.055604
Train - Epoch 353, Batch: 0, Loss: 1.061972
Train - Epoch 354, Batch: 0, Loss: 1.067887
Train - Epoch 355, Batch: 0, Loss: 1.059254
Train - Epoch 356, Batch: 0, Loss: 1.048126
Train - Epoch 357, Batch: 0, Loss: 1.062671
Train - Epoch 358, Batch: 0, Loss: 1.068751
Train - Epoch 359, Batch: 0, Loss: 1.067869
Train - Epoch 360, Batch: 0, Loss: 1.061107
Train - Epoch 361, Batch: 0, Loss: 1.056189
Train - Epoch 362, Batch: 0, Loss: 1.070069
Train - Epoch 363, Batch: 0, Loss: 1.062911
Train - Epoch 364, Batch: 0, Loss: 1.078789
Train - Epoch 365, Batch: 0, Loss: 1.061762
Train - Epoch 366, Batch: 0, Loss: 1.068037
Train - Epoch 367, Batch: 0, Loss: 1.069165
Train - Epoch 368, Batch: 0, Loss: 1.061357
Train - Epoch 369, Batch: 0, Loss: 1.074283
Train - Epoch 370, Batch: 0, Loss: 1.074579
Train - Epoch 371, Batch: 0, Loss: 1.060336
Train - Epoch 372, Batch: 0, Loss: 1.058049
Train - Epoch 373, Batch: 0, Loss: 1.060900/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 374, Batch: 0, Loss: 1.058064
Train - Epoch 375, Batch: 0, Loss: 1.056934
Train - Epoch 376, Batch: 0, Loss: 1.063450
Train - Epoch 377, Batch: 0, Loss: 1.067959
Train - Epoch 378, Batch: 0, Loss: 1.061912
Train - Epoch 379, Batch: 0, Loss: 1.045904
Train - Epoch 380, Batch: 0, Loss: 1.041588
Train - Epoch 381, Batch: 0, Loss: 1.057137
Train - Epoch 382, Batch: 0, Loss: 1.057531
Train - Epoch 383, Batch: 0, Loss: 1.058669
Train - Epoch 384, Batch: 0, Loss: 1.060991
Train - Epoch 385, Batch: 0, Loss: 1.054738
Train - Epoch 386, Batch: 0, Loss: 1.058066
Train - Epoch 387, Batch: 0, Loss: 1.062097
Train - Epoch 388, Batch: 0, Loss: 1.054521
Train - Epoch 389, Batch: 0, Loss: 1.071337
Train - Epoch 390, Batch: 0, Loss: 1.051412
Train - Epoch 391, Batch: 0, Loss: 1.059301
Train - Epoch 392, Batch: 0, Loss: 1.057784
Train - Epoch 393, Batch: 0, Loss: 1.058185
Train - Epoch 394, Batch: 0, Loss: 1.064291
Train - Epoch 395, Batch: 0, Loss: 1.063080
Train - Epoch 396, Batch: 0, Loss: 1.047794
Train - Epoch 397, Batch: 0, Loss: 1.073335
Train - Epoch 398, Batch: 0, Loss: 1.051871
Train - Epoch 399, Batch: 0, Loss: 1.068170
Train - Epoch 400, Batch: 0, Loss: 1.066364
Train - Epoch 401, Batch: 0, Loss: 1.042812
Train - Epoch 402, Batch: 0, Loss: 1.043704
Train - Epoch 403, Batch: 0, Loss: 1.063958
Train - Epoch 404, Batch: 0, Loss: 1.072348
Train - Epoch 405, Batch: 0, Loss: 1.047588
Train - Epoch 406, Batch: 0, Loss: 1.053203
Train - Epoch 407, Batch: 0, Loss: 1.055727
Train - Epoch 408, Batch: 0, Loss: 1.051815
Train - Epoch 409, Batch: 0, Loss: 1.055364
Train - Epoch 410, Batch: 0, Loss: 1.039757
Train - Epoch 411, Batch: 0, Loss: 1.062427
Train - Epoch 412, Batch: 0, Loss: 1.050612
Train - Epoch 413, Batch: 0, Loss: 1.051128
Train - Epoch 414, Batch: 0, Loss: 1.058895
Train - Epoch 415, Batch: 0, Loss: 1.043672
Train - Epoch 416, Batch: 0, Loss: 1.042864
Train - Epoch 417, Batch: 0, Loss: 1.049644
Train - Epoch 418, Batch: 0, Loss: 1.051218
Train - Epoch 419, Batch: 0, Loss: 1.040639
Train - Epoch 420, Batch: 0, Loss: 1.049141
Train - Epoch 421, Batch: 0, Loss: 1.054731
Train - Epoch 422, Batch: 0, Loss: 1.056356
Train - Epoch 423, Batch: 0, Loss: 1.041972
Train - Epoch 424, Batch: 0, Loss: 1.047340
Train - Epoch 425, Batch: 0, Loss: 1.047080
Train - Epoch 426, Batch: 0, Loss: 1.033460
Train - Epoch 427, Batch: 0, Loss: 1.042470
Train - Epoch 428, Batch: 0, Loss: 1.043823
Train - Epoch 429, Batch: 0, Loss: 1.049060
Train - Epoch 430, Batch: 0, Loss: 1.052325
Train - Epoch 431, Batch: 0, Loss: 1.053823
Train - Epoch 432, Batch: 0, Loss: 1.062083
Train - Epoch 433, Batch: 0, Loss: 1.056156
Train - Epoch 434, Batch: 0, Loss: 1.046549
Train - Epoch 435, Batch: 0, Loss: 1.062902
Train - Epoch 436, Batch: 0, Loss: 1.039406
Train - Epoch 437, Batch: 0, Loss: 1.051167
Train - Epoch 438, Batch: 0, Loss: 1.050756
Train - Epoch 439, Batch: 0, Loss: 1.037110
Train - Epoch 440, Batch: 0, Loss: 1.034627
Train - Epoch 441, Batch: 0, Loss: 1.057781
Train - Epoch 442, Batch: 0, Loss: 1.072520
Train - Epoch 443, Batch: 0, Loss: 1.033357
Train - Epoch 444, Batch: 0, Loss: 1.058316
Train - Epoch 445, Batch: 0, Loss: 1.042235
Train - Epoch 446, Batch: 0, Loss: 1.040713
Train - Epoch 447, Batch: 0, Loss: 1.046886
Train - Epoch 448, Batch: 0, Loss: 1.057941
Train - Epoch 449, Batch: 0, Loss: 1.037584
Train - Epoch 450, Batch: 0, Loss: 1.036719
Train - Epoch 451, Batch: 0, Loss: 1.049588
Train - Epoch 452, Batch: 0, Loss: 1.047206
Train - Epoch 453, Batch: 0, Loss: 1.036034
Train - Epoch 454, Batch: 0, Loss: 1.054393
Train - Epoch 455, Batch: 0, Loss: 1.048137
Train - Epoch 456, Batch: 0, Loss: 1.042689
Train - Epoch 457, Batch: 0, Loss: 1.057191
Train - Epoch 458, Batch: 0, Loss: 1.044375
Train - Epoch 459, Batch: 0, Loss: 1.046184
Train - Epoch 460, Batch: 0, Loss: 1.049554
Train - Epoch 461, Batch: 0, Loss: 1.048294
Train - Epoch 462, Batch: 0, Loss: 1.045611
Train - Epoch 463, Batch: 0, Loss: 1.037287
Train - Epoch 464, Batch: 0, Loss: 1.041398
Train - Epoch 465, Batch: 0, Loss: 1.047685
Train - Epoch 466, Batch: 0, Loss: 1.051340
Train - Epoch 467, Batch: 0, Loss: 1.048626
Train - Epoch 468, Batch: 0, Loss: 1.059860
Train - Epoch 469, Batch: 0, Loss: 1.050649
Train - Epoch 470, Batch: 0, Loss: 1.036457
Train - Epoch 471, Batch: 0, Loss: 1.041148
Train - Epoch 472, Batch: 0, Loss: 1.041110
Train - Epoch 473, Batch: 0, Loss: 1.030847
Train - Epoch 474, Batch: 0, Loss: 1.041460
Train - Epoch 475, Batch: 0, Loss: 1.044074
Train - Epoch 476, Batch: 0, Loss: 1.045517
Train - Epoch 477, Batch: 0, Loss: 1.038619
Train - Epoch 478, Batch: 0, Loss: 1.025337
Train - Epoch 479, Batch: 0, Loss: 1.032817
Train - Epoch 480, Batch: 0, Loss: 1.035537
Train - Epoch 481, Batch: 0, Loss: 1.037656
Train - Epoch 482, Batch: 0, Loss: 1.060068
Train - Epoch 483, Batch: 0, Loss: 1.038076
Train - Epoch 484, Batch: 0, Loss: 1.036349
Train - Epoch 485, Batch: 0, Loss: 1.037913
Train - Epoch 486, Batch: 0, Loss: 1.032686
Train - Epoch 487, Batch: 0, Loss: 1.032886
Train - Epoch 488, Batch: 0, Loss: 1.021189
Train - Epoch 489, Batch: 0, Loss: 1.036154
Train - Epoch 490, Batch: 0, Loss: 1.041622
Train - Epoch 491, Batch: 0, Loss: 1.046261
Train - Epoch 492, Batch: 0, Loss: 1.036724
Train - Epoch 493, Batch: 0, Loss: 1.043911
Train - Epoch 494, Batch: 0, Loss: 1.042620
Train - Epoch 495, Batch: 0, Loss: 1.049581
Train - Epoch 496, Batch: 0, Loss: 1.034296
Train - Epoch 497, Batch: 0, Loss: 1.040736
Train - Epoch 498, Batch: 0, Loss: 1.040361
Train - Epoch 499, Batch: 0, Loss: 1.049407
training_time:: 108.02537393569946
training time full:: 108.02544212341309
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633200
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 25
max_epoch:: 500
batch_size:: 10000
epoch:: 0
tensor([32768,     4, 32774,    10, 32780,    13,    15, 32784,    20,    28,
        32798,    31,    45, 32816,    50, 32820,    54, 32831,    65, 32834,
        32836,    69,    75, 32847,    81,    82, 32849,    84,    85, 32853,
        32855, 32861, 32864,    97, 32865, 32866, 32869,   110,   111, 32881,
        32882, 32883,   118,   127,   129, 32899, 32901,   136, 32909, 32914])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
epoch:: 60
epoch:: 61
epoch:: 62
epoch:: 63
epoch:: 64
epoch:: 65
epoch:: 66
epoch:: 67
epoch:: 68
epoch:: 69
epoch:: 70
epoch:: 71
epoch:: 72
epoch:: 73
epoch:: 74
epoch:: 75
epoch:: 76
epoch:: 77
epoch:: 78
epoch:: 79
epoch:: 80
epoch:: 81
epoch:: 82
epoch:: 83
epoch:: 84
epoch:: 85
epoch:: 86
epoch:: 87
epoch:: 88
epoch:: 89
epoch:: 90
epoch:: 91
epoch:: 92
epoch:: 93
epoch:: 94
epoch:: 95
epoch:: 96
epoch:: 97
epoch:: 98
epoch:: 99
epoch:: 100
epoch:: 101
epoch:: 102
epoch:: 103
epoch:: 104
epoch:: 105
epoch:: 106
epoch:: 107
epoch:: 108
epoch:: 109
epoch:: 110
epoch:: 111
epoch:: 112
epoch:: 113
epoch:: 114
epoch:: 115
epoch:: 116
epoch:: 117
epoch:: 118
epoch:: 119
epoch:: 120
epoch:: 121
epoch:: 122
epoch:: 123
epoch:: 124
epoch:: 125
epoch:: 126
epoch:: 127
epoch:: 128
epoch:: 129
epoch:: 130
epoch:: 131
epoch:: 132
epoch:: 133
epoch:: 134
epoch:: 135
epoch:: 136
epoch:: 137
epoch:: 138
epoch:: 139
epoch:: 140
epoch:: 141
epoch:: 142
epoch:: 143
epoch:: 144
epoch:: 145
epoch:: 146
epoch:: 147
epoch:: 148
epoch:: 149
epoch:: 150
epoch:: 151
epoch:: 152
epoch:: 153
epoch:: 154
epoch:: 155
epoch:: 156
epoch:: 157
epoch:: 158
epoch:: 159
epoch:: 160
epoch:: 161
epoch:: 162
epoch:: 163
epoch:: 164
epoch:: 165
epoch:: 166
epoch:: 167
epoch:: 168
epoch:: 169
epoch:: 170
epoch:: 171
epoch:: 172
epoch:: 173
epoch:: 174
epoch:: 175
epoch:: 176
epoch:: 177
epoch:: 178
epoch:: 179
epoch:: 180
epoch:: 181
epoch:: 182
epoch:: 183
epoch:: 184
epoch:: 185
epoch:: 186
epoch:: 187
epoch:: 188
epoch:: 189
epoch:: 190
epoch:: 191
epoch:: 192
epoch:: 193
epoch:: 194
epoch:: 195
epoch:: 196
epoch:: 197
epoch:: 198
epoch:: 199
epoch:: 200
epoch:: 201
epoch:: 202
epoch:: 203
epoch:: 204
epoch:: 205
epoch:: 206
epoch:: 207
epoch:: 208
epoch:: 209
epoch:: 210
epoch:: 211
epoch:: 212
epoch:: 213
epoch:: 214
epoch:: 215
epoch:: 216
epoch:: 217
epoch:: 218
epoch:: 219
epoch:: 220
epoch:: 221
epoch:: 222
epoch:: 223
epoch:: 224
epoch:: 225
epoch:: 226
epoch:: 227
epoch:: 228
epoch:: 229
epoch:: 230
epoch:: 231
epoch:: 232
epoch:: 233
epoch:: 234
epoch:: 235
epoch:: 236
epoch:: 237
epoch:: 238
epoch:: 239
epoch:: 240
epoch:: 241
epoch:: 242
epoch:: 243
epoch:: 244
epoch:: 245
epoch:: 246
epoch:: 247
epoch:: 248
epoch:: 249
epoch:: 250
epoch:: 251
epoch:: 252
epoch:: 253
epoch:: 254
epoch:: 255
epoch:: 256
epoch:: 257
epoch:: 258
epoch:: 259
epoch:: 260
epoch:: 261
epoch:: 262
epoch:: 263
epoch:: 264
epoch:: 265
epoch:: 266
epoch:: 267
epoch:: 268
epoch:: 269
epoch:: 270
epoch:: 271
epoch:: 272
epoch:: 273
epoch:: 274
epoch:: 275
epoch:: 276
epoch:: 277
epoch:: 278
epoch:: 279
epoch:: 280
epoch:: 281
epoch:: 282
epoch:: 283
epoch:: 284
epoch:: 285
epoch:: 286
epoch:: 287
epoch:: 288
epoch:: 289
epoch:: 290
epoch:: 291
epoch:: 292
epoch:: 293
epoch:: 294
epoch:: 295
epoch:: 296
epoch:: 297
epoch:: 298
epoch:: 299
epoch:: 300
epoch:: 301
epoch:: 302
epoch:: 303
epoch:: 304
epoch:: 305
epoch:: 306
epoch:: 307
epoch:: 308
epoch:: 309
epoch:: 310
epoch:: 311
epoch:: 312
epoch:: 313
epoch:: 314
epoch:: 315
epoch:: 316
epoch:: 317
epoch:: 318
epoch:: 319
epoch:: 320
epoch:: 321
epoch:: 322
epoch:: 323
epoch:: 324
epoch:: 325
epoch:: 326
epoch:: 327
epoch:: 328
epoch:: 329
epoch:: 330
epoch:: 331
epoch:: 332
epoch:: 333
epoch:: 334
epoch:: 335
epoch:: 336
epoch:: 337
epoch:: 338
epoch:: 339
epoch:: 340
epoch:: 341
epoch:: 342
epoch:: 343
epoch:: 344
epoch:: 345
epoch:: 346
epoch:: 347
epoch:: 348
epoch:: 349
epoch:: 350
epoch:: 351
epoch:: 352
epoch:: 353
epoch:: 354
epoch:: 355
epoch:: 356
epoch:: 357
epoch:: 358
epoch:: 359
epoch:: 360
epoch:: 361
epoch:: 362
epoch:: 363
epoch:: 364
epoch:: 365
epoch:: 366
epoch:: 367
epoch:: 368
epoch:: 369
epoch:: 370
epoch:: 371
epoch:: 372
epoch:: 373
epoch:: 374
epoch:: 375
epoch:: 376
epoch:: 377
epoch:: 378
epoch:: 379
epoch:: 380
epoch:: 381
epoch:: 382
epoch:: 383
epoch:: 384
epoch:: 385
epoch:: 386
epoch:: 387
epoch:: 388
epoch:: 389
epoch:: 390
epoch:: 391
epoch:: 392
epoch:: 393
epoch:: 394
epoch:: 395
epoch:: 396
epoch:: 397
epoch:: 398
epoch:: 399
epoch:: 400
epoch:: 401
epoch:: 402
epoch:: 403
epoch:: 404
epoch:: 405
epoch:: 406
epoch:: 407
epoch:: 408
epoch:: 409
epoch:: 410
epoch:: 411
epoch:: 412
epoch:: 413
epoch:: 414
epoch:: 415
epoch:: 416
epoch:: 417
epoch:: 418
epoch:: 419
epoch:: 420
epoch:: 421
epoch:: 422
epoch:: 423
epoch:: 424
epoch:: 425
epoch:: 426
epoch:: 427
epoch:: 428
epoch:: 429
epoch:: 430
epoch:: 431
epoch:: 432
epoch:: 433
epoch:: 434
epoch:: 435
epoch:: 436
epoch:: 437
epoch:: 438
epoch:: 439
epoch:: 440
epoch:: 441
epoch:: 442
epoch:: 443
epoch:: 444
epoch:: 445
epoch:: 446
epoch:: 447
epoch:: 448
epoch:: 449
epoch:: 450
epoch:: 451
epoch:: 452
epoch:: 453
epoch:: 454
epoch:: 455
epoch:: 456
epoch:: 457
epoch:: 458
epoch:: 459
epoch:: 460
epoch:: 461
epoch:: 462
epoch:: 463
epoch:: 464
epoch:: 465
epoch:: 466
epoch:: 467
epoch:: 468
epoch:: 469
epoch:: 470
epoch:: 471
epoch:: 472
epoch:: 473
epoch:: 474
epoch:: 475
epoch:: 476
epoch:: 477
epoch:: 478
epoch:: 479
epoch:: 480
epoch:: 481
epoch:: 482
epoch:: 483
epoch:: 484
epoch:: 485
epoch:: 486
epoch:: 487
epoch:: 488
epoch:: 489
epoch:: 490
epoch:: 491
epoch:: 492
epoch:: 493
epoch:: 494
epoch:: 495
epoch:: 496
epoch:: 497
epoch:: 498
epoch:: 499
training time is 80.82718110084534
overhead:: 0
overhead2:: 2.6534178256988525
overhead3:: 0
time_baseline:: 80.82721376419067
curr_diff: 0 tensor(0.0383, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0383, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633500
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 25
max_epoch:: 500
overhead:: 0
overhead2:: 0.08167409896850586
overhead3:: 0.254009485244751
overhead4:: 10.394173622131348
overhead5:: 0
memory usage:: 5634392064
time_provenance:: 17.912861347198486
curr_diff: 0 tensor(0.0094, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0094, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0455, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0455, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633600
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 25
max_epoch:: 500
overhead:: 0
overhead2:: 0.08490753173828125
overhead3:: 0.2609381675720215
overhead4:: 10.39934492111206
overhead5:: 0
memory usage:: 5624016896
time_provenance:: 17.942304372787476
curr_diff: 0 tensor(0.0094, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0094, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0455, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0455, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633600
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 25
max_epoch:: 500
overhead:: 0
overhead2:: 0.08488917350769043
overhead3:: 0.2668910026550293
overhead4:: 10.917984962463379
overhead5:: 0
memory usage:: 5630529536
time_provenance:: 18.503469705581665
curr_diff: 0 tensor(0.0094, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0094, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0455, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0455, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633500
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 25
max_epoch:: 500
overhead:: 0
overhead2:: 0.1550137996673584
overhead3:: 0.4452700614929199
overhead4:: 18.682607889175415
overhead5:: 0
memory usage:: 5639462912
time_provenance:: 28.078357934951782
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0389, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0389, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633500
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 25
max_epoch:: 500
overhead:: 0
overhead2:: 0.15529704093933105
overhead3:: 0.450439453125
overhead4:: 18.09941792488098
overhead5:: 0
memory usage:: 5647478784
time_provenance:: 27.54264187812805
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0389, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0389, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633500
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 25
max_epoch:: 500
overhead:: 0
overhead2:: 0.1601581573486328
overhead3:: 0.4343564510345459
overhead4:: 18.179662942886353
overhead5:: 0
memory usage:: 5633724416
time_provenance:: 27.571448802947998
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0389, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0389, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633500
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 25
max_epoch:: 500
overhead:: 0
overhead2:: 0.39012932777404785
overhead3:: 1.0307683944702148
overhead4:: 44.066237926483154
overhead5:: 0
memory usage:: 5702651904
time_provenance:: 59.416807651519775
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0384, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0384, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633500
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 25
max_epoch:: 500
overhead:: 0
overhead2:: 0.3754444122314453
overhead3:: 1.0156059265136719
overhead4:: 44.10719132423401
overhead5:: 0
memory usage:: 5634752512
time_provenance:: 59.431941986083984
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0384, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0384, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633500
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 25
max_epoch:: 500
overhead:: 0
overhead2:: 0.3988986015319824
overhead3:: 1.0489130020141602
overhead4:: 44.93276619911194
overhead5:: 0
memory usage:: 5630418944
time_provenance:: 60.35615110397339
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0384, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0384, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633500
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 3 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 25
max_epoch:: 500
overhead:: 0
overhead2:: 0.7935853004455566
overhead3:: 2.048858404159546
overhead4:: 82.51998329162598
overhead5:: 0
memory usage:: 5623083008
time_provenance:: 105.77578377723694
curr_diff: 0 tensor(4.9975e-13, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.9975e-13, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0383, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0383, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633500
repetition 4
python3 benchmark_exp_lr.py 0.001 10000 500 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05] Logistic_regression cifar10_2 4 0.0001 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 2.313263
Train - Epoch 1, Batch: 0, Loss: 2.247222
Train - Epoch 2, Batch: 0, Loss: 2.191105
Train - Epoch 3, Batch: 0, Loss: 2.141780
Train - Epoch 4, Batch: 0, Loss: 2.092470
Train - Epoch 5, Batch: 0, Loss: 2.048736
Train - Epoch 6, Batch: 0, Loss: 2.007829
Train - Epoch 7, Batch: 0, Loss: 1.970151
Train - Epoch 8, Batch: 0, Loss: 1.932120
Train - Epoch 9, Batch: 0, Loss: 1.897442
Train - Epoch 10, Batch: 0, Loss: 1.860464
Train - Epoch 11, Batch: 0, Loss: 1.836956
Train - Epoch 12, Batch: 0, Loss: 1.804623
Train - Epoch 13, Batch: 0, Loss: 1.784451
Train - Epoch 14, Batch: 0, Loss: 1.763557
Train - Epoch 15, Batch: 0, Loss: 1.733708
Train - Epoch 16, Batch: 0, Loss: 1.708263
Train - Epoch 17, Batch: 0, Loss: 1.696571
Train - Epoch 18, Batch: 0, Loss: 1.678419
Train - Epoch 19, Batch: 0, Loss: 1.656112
Train - Epoch 20, Batch: 0, Loss: 1.645029
Train - Epoch 21, Batch: 0, Loss: 1.619977
Train - Epoch 22, Batch: 0, Loss: 1.604644
Train - Epoch 23, Batch: 0, Loss: 1.590425
Train - Epoch 24, Batch: 0, Loss: 1.575034
Train - Epoch 25, Batch: 0, Loss: 1.571383
Train - Epoch 26, Batch: 0, Loss: 1.551457
Train - Epoch 27, Batch: 0, Loss: 1.536907
Train - Epoch 28, Batch: 0, Loss: 1.524315
Train - Epoch 29, Batch: 0, Loss: 1.520182
Train - Epoch 30, Batch: 0, Loss: 1.501039
Train - Epoch 31, Batch: 0, Loss: 1.495830
Train - Epoch 32, Batch: 0, Loss: 1.476802
Train - Epoch 33, Batch: 0, Loss: 1.482144
Train - Epoch 34, Batch: 0, Loss: 1.464636
Train - Epoch 35, Batch: 0, Loss: 1.460493
Train - Epoch 36, Batch: 0, Loss: 1.444589
Train - Epoch 37, Batch: 0, Loss: 1.437978
Train - Epoch 38, Batch: 0, Loss: 1.439976
Train - Epoch 39, Batch: 0, Loss: 1.423073
Train - Epoch 40, Batch: 0, Loss: 1.418954
Train - Epoch 41, Batch: 0, Loss: 1.415430
Train - Epoch 42, Batch: 0, Loss: 1.399772
Train - Epoch 43, Batch: 0, Loss: 1.403374
Train - Epoch 44, Batch: 0, Loss: 1.397041
Train - Epoch 45, Batch: 0, Loss: 1.386699
Train - Epoch 46, Batch: 0, Loss: 1.386699
Train - Epoch 47, Batch: 0, Loss: 1.380023
Train - Epoch 48, Batch: 0, Loss: 1.376802
Train - Epoch 49, Batch: 0, Loss: 1.377155
Train - Epoch 50, Batch: 0, Loss: 1.351321
Train - Epoch 51, Batch: 0, Loss: 1.349233
Train - Epoch 52, Batch: 0, Loss: 1.347717
Train - Epoch 53, Batch: 0, Loss: 1.345944
Train - Epoch 54, Batch: 0, Loss: 1.345082
Train - Epoch 55, Batch: 0, Loss: 1.339956
Train - Epoch 56, Batch: 0, Loss: 1.347336
Train - Epoch 57, Batch: 0, Loss: 1.330149
Train - Epoch 58, Batch: 0, Loss: 1.323841
Train - Epoch 59, Batch: 0, Loss: 1.313042
Train - Epoch 60, Batch: 0, Loss: 1.320021
Train - Epoch 61, Batch: 0, Loss: 1.316597
Train - Epoch 62, Batch: 0, Loss: 1.307155
Train - Epoch 63, Batch: 0, Loss: 1.308444
Train - Epoch 64, Batch: 0, Loss: 1.302716
Train - Epoch 65, Batch: 0, Loss: 1.294068
Train - Epoch 66, Batch: 0, Loss: 1.303699
Train - Epoch 67, Batch: 0, Loss: 1.289577
Train - Epoch 68, Batch: 0, Loss: 1.280199
Train - Epoch 69, Batch: 0, Loss: 1.268754
Train - Epoch 70, Batch: 0, Loss: 1.276367
Train - Epoch 71, Batch: 0, Loss: 1.290782
Train - Epoch 72, Batch: 0, Loss: 1.272272
Train - Epoch 73, Batch: 0, Loss: 1.277214
Train - Epoch 74, Batch: 0, Loss: 1.262268
Train - Epoch 75, Batch: 0, Loss: 1.271766
Train - Epoch 76, Batch: 0, Loss: 1.262295
Train - Epoch 77, Batch: 0, Loss: 1.275083
Train - Epoch 78, Batch: 0, Loss: 1.261940
Train - Epoch 79, Batch: 0, Loss: 1.253514
Train - Epoch 80, Batch: 0, Loss: 1.255143
Train - Epoch 81, Batch: 0, Loss: 1.244687
Train - Epoch 82, Batch: 0, Loss: 1.246652
Train - Epoch 83, Batch: 0, Loss: 1.247556
Train - Epoch 84, Batch: 0, Loss: 1.247951
Train - Epoch 85, Batch: 0, Loss: 1.255601
Train - Epoch 86, Batch: 0, Loss: 1.238982
Train - Epoch 87, Batch: 0, Loss: 1.247106
Train - Epoch 88, Batch: 0, Loss: 1.244776
Train - Epoch 89, Batch: 0, Loss: 1.237799
Train - Epoch 90, Batch: 0, Loss: 1.217513
Train - Epoch 91, Batch: 0, Loss: 1.231264
Train - Epoch 92, Batch: 0, Loss: 1.218889
Train - Epoch 93, Batch: 0, Loss: 1.230386
Train - Epoch 94, Batch: 0, Loss: 1.212909
Train - Epoch 95, Batch: 0, Loss: 1.228800
Train - Epoch 96, Batch: 0, Loss: 1.221445
Train - Epoch 97, Batch: 0, Loss: 1.213822
Train - Epoch 98, Batch: 0, Loss: 1.217901
Train - Epoch 99, Batch: 0, Loss: 1.218464
Train - Epoch 100, Batch: 0, Loss: 1.216576
Train - Epoch 101, Batch: 0, Loss: 1.200260
Train - Epoch 102, Batch: 0, Loss: 1.204250
Train - Epoch 103, Batch: 0, Loss: 1.207751
Train - Epoch 104, Batch: 0, Loss: 1.212712
Train - Epoch 105, Batch: 0, Loss: 1.223627
Train - Epoch 106, Batch: 0, Loss: 1.203177
Train - Epoch 107, Batch: 0, Loss: 1.207768
Train - Epoch 108, Batch: 0, Loss: 1.208049
Train - Epoch 109, Batch: 0, Loss: 1.196290
Train - Epoch 110, Batch: 0, Loss: 1.194773
Train - Epoch 111, Batch: 0, Loss: 1.190885
Train - Epoch 112, Batch: 0, Loss: 1.197811
Train - Epoch 113, Batch: 0, Loss: 1.182786
Train - Epoch 114, Batch: 0, Loss: 1.191148
Train - Epoch 115, Batch: 0, Loss: 1.198318
Train - Epoch 116, Batch: 0, Loss: 1.186312
Train - Epoch 117, Batch: 0, Loss: 1.190500
Train - Epoch 118, Batch: 0, Loss: 1.186770
Train - Epoch 119, Batch: 0, Loss: 1.192107
Train - Epoch 120, Batch: 0, Loss: 1.177647
Train - Epoch 121, Batch: 0, Loss: 1.188876
Train - Epoch 122, Batch: 0, Loss: 1.176219
Train - Epoch 123, Batch: 0, Loss: 1.191220
Train - Epoch 124, Batch: 0, Loss: 1.177584
Train - Epoch 125, Batch: 0, Loss: 1.175538
Train - Epoch 126, Batch: 0, Loss: 1.173795
Train - Epoch 127, Batch: 0, Loss: 1.160344
Train - Epoch 128, Batch: 0, Loss: 1.177794
Train - Epoch 129, Batch: 0, Loss: 1.178813
Train - Epoch 130, Batch: 0, Loss: 1.172057
Train - Epoch 131, Batch: 0, Loss: 1.169984
Train - Epoch 132, Batch: 0, Loss: 1.177831
Train - Epoch 133, Batch: 0, Loss: 1.181127
Train - Epoch 134, Batch: 0, Loss: 1.172804
Train - Epoch 135, Batch: 0, Loss: 1.172172
Train - Epoch 136, Batch: 0, Loss: 1.155630
Train - Epoch 137, Batch: 0, Loss: 1.160702
Train - Epoch 138, Batch: 0, Loss: 1.151174
Train - Epoch 139, Batch: 0, Loss: 1.174550
Train - Epoch 140, Batch: 0, Loss: 1.164617
Train - Epoch 141, Batch: 0, Loss: 1.167583
Train - Epoch 142, Batch: 0, Loss: 1.163467
Train - Epoch 143, Batch: 0, Loss: 1.166647
Train - Epoch 144, Batch: 0, Loss: 1.157452
Train - Epoch 145, Batch: 0, Loss: 1.156160
Train - Epoch 146, Batch: 0, Loss: 1.158985
Train - Epoch 147, Batch: 0, Loss: 1.156555
Train - Epoch 148, Batch: 0, Loss: 1.159381
Train - Epoch 149, Batch: 0, Loss: 1.155735
Train - Epoch 150, Batch: 0, Loss: 1.143074
Train - Epoch 151, Batch: 0, Loss: 1.137027
Train - Epoch 152, Batch: 0, Loss: 1.150970
Train - Epoch 153, Batch: 0, Loss: 1.147831
Train - Epoch 154, Batch: 0, Loss: 1.145338
Train - Epoch 155, Batch: 0, Loss: 1.143781
Train - Epoch 156, Batch: 0, Loss: 1.141046
Train - Epoch 157, Batch: 0, Loss: 1.145978
Train - Epoch 158, Batch: 0, Loss: 1.144701
Train - Epoch 159, Batch: 0, Loss: 1.146265
Train - Epoch 160, Batch: 0, Loss: 1.147131
Train - Epoch 161, Batch: 0, Loss: 1.146421
Train - Epoch 162, Batch: 0, Loss: 1.159873
Train - Epoch 163, Batch: 0, Loss: 1.129315
Train - Epoch 164, Batch: 0, Loss: 1.137811
Train - Epoch 165, Batch: 0, Loss: 1.129822
Train - Epoch 166, Batch: 0, Loss: 1.137532
Train - Epoch 167, Batch: 0, Loss: 1.134034
Train - Epoch 168, Batch: 0, Loss: 1.144801
Train - Epoch 169, Batch: 0, Loss: 1.135615
Train - Epoch 170, Batch: 0, Loss: 1.128929
Train - Epoch 171, Batch: 0, Loss: 1.135975
Train - Epoch 172, Batch: 0, Loss: 1.138503
Train - Epoch 173, Batch: 0, Loss: 1.136740
Train - Epoch 174, Batch: 0, Loss: 1.128737
Train - Epoch 175, Batch: 0, Loss: 1.132206
Train - Epoch 176, Batch: 0, Loss: 1.139129
Train - Epoch 177, Batch: 0, Loss: 1.139130
Train - Epoch 178, Batch: 0, Loss: 1.132600
Train - Epoch 179, Batch: 0, Loss: 1.121973
Train - Epoch 180, Batch: 0, Loss: 1.134475
Train - Epoch 181, Batch: 0, Loss: 1.120288
Train - Epoch 182, Batch: 0, Loss: 1.133660
Train - Epoch 183, Batch: 0, Loss: 1.137245
Train - Epoch 184, Batch: 0, Loss: 1.128086
Train - Epoch 185, Batch: 0, Loss: 1.102516
Train - Epoch 186, Batch: 0, Loss: 1.133793
Train - Epoch 187, Batch: 0, Loss: 1.130111
Train - Epoch 188, Batch: 0, Loss: 1.139915
Train - Epoch 189, Batch: 0, Loss: 1.119059
Train - Epoch 190, Batch: 0, Loss: 1.126117
Train - Epoch 191, Batch: 0, Loss: 1.129390
Train - Epoch 192, Batch: 0, Loss: 1.127194
Train - Epoch 193, Batch: 0, Loss: 1.119463
Train - Epoch 194, Batch: 0, Loss: 1.111575
Train - Epoch 195, Batch: 0, Loss: 1.118162
Train - Epoch 196, Batch: 0, Loss: 1.141577
Train - Epoch 197, Batch: 0, Loss: 1.119672
Train - Epoch 198, Batch: 0, Loss: 1.116060
Train - Epoch 199, Batch: 0, Loss: 1.115754
Train - Epoch 200, Batch: 0, Loss: 1.122249
Train - Epoch 201, Batch: 0, Loss: 1.117417
Train - Epoch 202, Batch: 0, Loss: 1.118857
Train - Epoch 203, Batch: 0, Loss: 1.098901
Train - Epoch 204, Batch: 0, Loss: 1.124223
Train - Epoch 205, Batch: 0, Loss: 1.118056
Train - Epoch 206, Batch: 0, Loss: 1.116479
Train - Epoch 207, Batch: 0, Loss: 1.109741
Train - Epoch 208, Batch: 0, Loss: 1.127091
Train - Epoch 209, Batch: 0, Loss: 1.108735
Train - Epoch 210, Batch: 0, Loss: 1.101803
Train - Epoch 211, Batch: 0, Loss: 1.114306
Train - Epoch 212, Batch: 0, Loss: 1.130077
Train - Epoch 213, Batch: 0, Loss: 1.120381
Train - Epoch 214, Batch: 0, Loss: 1.117101
Train - Epoch 215, Batch: 0, Loss: 1.103491
Train - Epoch 216, Batch: 0, Loss: 1.113215
Train - Epoch 217, Batch: 0, Loss: 1.102226
Train - Epoch 218, Batch: 0, Loss: 1.114006
Train - Epoch 219, Batch: 0, Loss: 1.108644
Train - Epoch 220, Batch: 0, Loss: 1.121538
Train - Epoch 221, Batch: 0, Loss: 1.109882
Train - Epoch 222, Batch: 0, Loss: 1.098869
Train - Epoch 223, Batch: 0, Loss: 1.111690
Train - Epoch 224, Batch: 0, Loss: 1.106512
Train - Epoch 225, Batch: 0, Loss: 1.110137
Train - Epoch 226, Batch: 0, Loss: 1.099380
Train - Epoch 227, Batch: 0, Loss: 1.100187
Train - Epoch 228, Batch: 0, Loss: 1.114253
Train - Epoch 229, Batch: 0, Loss: 1.110029
Train - Epoch 230, Batch: 0, Loss: 1.095247
Train - Epoch 231, Batch: 0, Loss: 1.095206
Train - Epoch 232, Batch: 0, Loss: 1.102114
Train - Epoch 233, Batch: 0, Loss: 1.107839
Train - Epoch 234, Batch: 0, Loss: 1.111393
Train - Epoch 235, Batch: 0, Loss: 1.110150
Train - Epoch 236, Batch: 0, Loss: 1.106486
Train - Epoch 237, Batch: 0, Loss: 1.103644
Train - Epoch 238, Batch: 0, Loss: 1.099556
Train - Epoch 239, Batch: 0, Loss: 1.101844
Train - Epoch 240, Batch: 0, Loss: 1.099025
Train - Epoch 241, Batch: 0, Loss: 1.093859
Train - Epoch 242, Batch: 0, Loss: 1.109452
Train - Epoch 243, Batch: 0, Loss: 1.082421
Train - Epoch 244, Batch: 0, Loss: 1.094052
Train - Epoch 245, Batch: 0, Loss: 1.097088
Train - Epoch 246, Batch: 0, Loss: 1.101747
Train - Epoch 247, Batch: 0, Loss: 1.094369
Train - Epoch 248, Batch: 0, Loss: 1.098203
Train - Epoch 249, Batch: 0, Loss: 1.077975
Train - Epoch 250, Batch: 0, Loss: 1.084071
Train - Epoch 251, Batch: 0, Loss: 1.103468
Train - Epoch 252, Batch: 0, Loss: 1.084768
Train - Epoch 253, Batch: 0, Loss: 1.088442
Train - Epoch 254, Batch: 0, Loss: 1.106452
Train - Epoch 255, Batch: 0, Loss: 1.097422
Train - Epoch 256, Batch: 0, Loss: 1.084765
Train - Epoch 257, Batch: 0, Loss: 1.095080
Train - Epoch 258, Batch: 0, Loss: 1.109048
Train - Epoch 259, Batch: 0, Loss: 1.089410
Train - Epoch 260, Batch: 0, Loss: 1.093761
Train - Epoch 261, Batch: 0, Loss: 1.085591
Train - Epoch 262, Batch: 0, Loss: 1.095690
Train - Epoch 263, Batch: 0, Loss: 1.086651
Train - Epoch 264, Batch: 0, Loss: 1.084664
Train - Epoch 265, Batch: 0, Loss: 1.089612
Train - Epoch 266, Batch: 0, Loss: 1.084367
Train - Epoch 267, Batch: 0, Loss: 1.089043
Train - Epoch 268, Batch: 0, Loss: 1.093769
Train - Epoch 269, Batch: 0, Loss: 1.096867
Train - Epoch 270, Batch: 0, Loss: 1.095783
Train - Epoch 271, Batch: 0, Loss: 1.092379
Train - Epoch 272, Batch: 0, Loss: 1.093010
Train - Epoch 273, Batch: 0, Loss: 1.093393
Train - Epoch 274, Batch: 0, Loss: 1.096282
Train - Epoch 275, Batch: 0, Loss: 1.075047
Train - Epoch 276, Batch: 0, Loss: 1.091125
Train - Epoch 277, Batch: 0, Loss: 1.064300
Train - Epoch 278, Batch: 0, Loss: 1.067721
Train - Epoch 279, Batch: 0, Loss: 1.092694
Train - Epoch 280, Batch: 0, Loss: 1.085533
Train - Epoch 281, Batch: 0, Loss: 1.090870
Train - Epoch 282, Batch: 0, Loss: 1.084621
Train - Epoch 283, Batch: 0, Loss: 1.083453
Train - Epoch 284, Batch: 0, Loss: 1.077621
Train - Epoch 285, Batch: 0, Loss: 1.067045
Train - Epoch 286, Batch: 0, Loss: 1.083671
Train - Epoch 287, Batch: 0, Loss: 1.065122
Train - Epoch 288, Batch: 0, Loss: 1.084414
Train - Epoch 289, Batch: 0, Loss: 1.080323
Train - Epoch 290, Batch: 0, Loss: 1.083231
Train - Epoch 291, Batch: 0, Loss: 1.077942
Train - Epoch 292, Batch: 0, Loss: 1.069793
Train - Epoch 293, Batch: 0, Loss: 1.080452
Train - Epoch 294, Batch: 0, Loss: 1.081300
Train - Epoch 295, Batch: 0, Loss: 1.089800
Train - Epoch 296, Batch: 0, Loss: 1.076580
Train - Epoch 297, Batch: 0, Loss: 1.057607
Train - Epoch 298, Batch: 0, Loss: 1.079315
Train - Epoch 299, Batch: 0, Loss: 1.069873
Train - Epoch 300, Batch: 0, Loss: 1.080708
Train - Epoch 301, Batch: 0, Loss: 1.100625
Train - Epoch 302, Batch: 0, Loss: 1.077570
Train - Epoch 303, Batch: 0, Loss: 1.078461
Train - Epoch 304, Batch: 0, Loss: 1.078818
Train - Epoch 305, Batch: 0, Loss: 1.074983
Train - Epoch 306, Batch: 0, Loss: 1.067310
Train - Epoch 307, Batch: 0, Loss: 1.084655
Train - Epoch 308, Batch: 0, Loss: 1.078113
Train - Epoch 309, Batch: 0, Loss: 1.087167
Train - Epoch 310, Batch: 0, Loss: 1.076054
Train - Epoch 311, Batch: 0, Loss: 1.077253
Train - Epoch 312, Batch: 0, Loss: 1.079893
Train - Epoch 313, Batch: 0, Loss: 1.072747
Train - Epoch 314, Batch: 0, Loss: 1.061126
Train - Epoch 315, Batch: 0, Loss: 1.072169
Train - Epoch 316, Batch: 0, Loss: 1.058967
Train - Epoch 317, Batch: 0, Loss: 1.079268
Train - Epoch 318, Batch: 0, Loss: 1.068403
Train - Epoch 319, Batch: 0, Loss: 1.078901
Train - Epoch 320, Batch: 0, Loss: 1.065197
Train - Epoch 321, Batch: 0, Loss: 1.078593
Train - Epoch 322, Batch: 0, Loss: 1.079968
Train - Epoch 323, Batch: 0, Loss: 1.080035
Train - Epoch 324, Batch: 0, Loss: 1.057670
Train - Epoch 325, Batch: 0, Loss: 1.073328
Train - Epoch 326, Batch: 0, Loss: 1.064695
Train - Epoch 327, Batch: 0, Loss: 1.066835
Train - Epoch 328, Batch: 0, Loss: 1.068976
Train - Epoch 329, Batch: 0, Loss: 1.060638
Train - Epoch 330, Batch: 0, Loss: 1.075625
Train - Epoch 331, Batch: 0, Loss: 1.078607
Train - Epoch 332, Batch: 0, Loss: 1.072783
Train - Epoch 333, Batch: 0, Loss: 1.072471
Train - Epoch 334, Batch: 0, Loss: 1.069373
Train - Epoch 335, Batch: 0, Loss: 1.072064
Train - Epoch 336, Batch: 0, Loss: 1.072208
Train - Epoch 337, Batch: 0, Loss: 1.072636
Train - Epoch 338, Batch: 0, Loss: 1.076477
Train - Epoch 339, Batch: 0, Loss: 1.073092
Train - Epoch 340, Batch: 0, Loss: 1.074236
Train - Epoch 341, Batch: 0, Loss: 1.063695
Train - Epoch 342, Batch: 0, Loss: 1.070043
Train - Epoch 343, Batch: 0, Loss: 1.069438
Train - Epoch 344, Batch: 0, Loss: 1.073197
Train - Epoch 345, Batch: 0, Loss: 1.066367
Train - Epoch 346, Batch: 0, Loss: 1.065751
Train - Epoch 347, Batch: 0, Loss: 1.074785
Train - Epoch 348, Batch: 0, Loss: 1.091028
Train - Epoch 349, Batch: 0, Loss: 1.080575
Train - Epoch 350, Batch: 0, Loss: 1.065187
Train - Epoch 351, Batch: 0, Loss: 1.056956
Train - Epoch 352, Batch: 0, Loss: 1.059099
Train - Epoch 353, Batch: 0, Loss: 1.053818
Train - Epoch 354, Batch: 0, Loss: 1.073060
Train - Epoch 355, Batch: 0, Loss: 1.062782
Train - Epoch 356, Batch: 0, Loss: 1.067878
Train - Epoch 357, Batch: 0, Loss: 1.057421
Train - Epoch 358, Batch: 0, Loss: 1.064515
Train - Epoch 359, Batch: 0, Loss: 1.060016
Train - Epoch 360, Batch: 0, Loss: 1.058335
Train - Epoch 361, Batch: 0, Loss: 1.065712
Train - Epoch 362, Batch: 0, Loss: 1.060704
Train - Epoch 363, Batch: 0, Loss: 1.058181
Train - Epoch 364, Batch: 0, Loss: 1.047341
Train - Epoch 365, Batch: 0, Loss: 1.065974
Train - Epoch 366, Batch: 0, Loss: 1.065829
Train - Epoch 367, Batch: 0, Loss: 1.058209
Train - Epoch 368, Batch: 0, Loss: 1.060237
Train - Epoch 369, Batch: 0, Loss: 1.069277
Train - Epoch 370, Batch: 0, Loss: 1.060974
Train - Epoch 371, Batch: 0, Loss: 1.057259
Train - Epoch 372, Batch: 0, Loss: 1.081277
Train - Epoch 373, Batch: 0, Loss: 1.059363/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 374, Batch: 0, Loss: 1.053305
Train - Epoch 375, Batch: 0, Loss: 1.056655
Train - Epoch 376, Batch: 0, Loss: 1.064002
Train - Epoch 377, Batch: 0, Loss: 1.066078
Train - Epoch 378, Batch: 0, Loss: 1.054559
Train - Epoch 379, Batch: 0, Loss: 1.062612
Train - Epoch 380, Batch: 0, Loss: 1.064589
Train - Epoch 381, Batch: 0, Loss: 1.058882
Train - Epoch 382, Batch: 0, Loss: 1.059618
Train - Epoch 383, Batch: 0, Loss: 1.054605
Train - Epoch 384, Batch: 0, Loss: 1.054858
Train - Epoch 385, Batch: 0, Loss: 1.060869
Train - Epoch 386, Batch: 0, Loss: 1.063746
Train - Epoch 387, Batch: 0, Loss: 1.063554
Train - Epoch 388, Batch: 0, Loss: 1.062068
Train - Epoch 389, Batch: 0, Loss: 1.052293
Train - Epoch 390, Batch: 0, Loss: 1.066105
Train - Epoch 391, Batch: 0, Loss: 1.048639
Train - Epoch 392, Batch: 0, Loss: 1.051973
Train - Epoch 393, Batch: 0, Loss: 1.058314
Train - Epoch 394, Batch: 0, Loss: 1.054935
Train - Epoch 395, Batch: 0, Loss: 1.040439
Train - Epoch 396, Batch: 0, Loss: 1.055723
Train - Epoch 397, Batch: 0, Loss: 1.042406
Train - Epoch 398, Batch: 0, Loss: 1.055002
Train - Epoch 399, Batch: 0, Loss: 1.061624
Train - Epoch 400, Batch: 0, Loss: 1.052858
Train - Epoch 401, Batch: 0, Loss: 1.049452
Train - Epoch 402, Batch: 0, Loss: 1.056836
Train - Epoch 403, Batch: 0, Loss: 1.052967
Train - Epoch 404, Batch: 0, Loss: 1.064473
Train - Epoch 405, Batch: 0, Loss: 1.057895
Train - Epoch 406, Batch: 0, Loss: 1.061618
Train - Epoch 407, Batch: 0, Loss: 1.063981
Train - Epoch 408, Batch: 0, Loss: 1.057347
Train - Epoch 409, Batch: 0, Loss: 1.065935
Train - Epoch 410, Batch: 0, Loss: 1.077458
Train - Epoch 411, Batch: 0, Loss: 1.050442
Train - Epoch 412, Batch: 0, Loss: 1.049089
Train - Epoch 413, Batch: 0, Loss: 1.055645
Train - Epoch 414, Batch: 0, Loss: 1.050561
Train - Epoch 415, Batch: 0, Loss: 1.057879
Train - Epoch 416, Batch: 0, Loss: 1.061118
Train - Epoch 417, Batch: 0, Loss: 1.057993
Train - Epoch 418, Batch: 0, Loss: 1.042643
Train - Epoch 419, Batch: 0, Loss: 1.047691
Train - Epoch 420, Batch: 0, Loss: 1.052702
Train - Epoch 421, Batch: 0, Loss: 1.039784
Train - Epoch 422, Batch: 0, Loss: 1.048372
Train - Epoch 423, Batch: 0, Loss: 1.076390
Train - Epoch 424, Batch: 0, Loss: 1.050255
Train - Epoch 425, Batch: 0, Loss: 1.054563
Train - Epoch 426, Batch: 0, Loss: 1.051929
Train - Epoch 427, Batch: 0, Loss: 1.049969
Train - Epoch 428, Batch: 0, Loss: 1.059158
Train - Epoch 429, Batch: 0, Loss: 1.060594
Train - Epoch 430, Batch: 0, Loss: 1.066900
Train - Epoch 431, Batch: 0, Loss: 1.042289
Train - Epoch 432, Batch: 0, Loss: 1.056072
Train - Epoch 433, Batch: 0, Loss: 1.050456
Train - Epoch 434, Batch: 0, Loss: 1.057264
Train - Epoch 435, Batch: 0, Loss: 1.035015
Train - Epoch 436, Batch: 0, Loss: 1.046643
Train - Epoch 437, Batch: 0, Loss: 1.055492
Train - Epoch 438, Batch: 0, Loss: 1.042350
Train - Epoch 439, Batch: 0, Loss: 1.057634
Train - Epoch 440, Batch: 0, Loss: 1.055215
Train - Epoch 441, Batch: 0, Loss: 1.046197
Train - Epoch 442, Batch: 0, Loss: 1.026482
Train - Epoch 443, Batch: 0, Loss: 1.053910
Train - Epoch 444, Batch: 0, Loss: 1.043890
Train - Epoch 445, Batch: 0, Loss: 1.050433
Train - Epoch 446, Batch: 0, Loss: 1.041422
Train - Epoch 447, Batch: 0, Loss: 1.032777
Train - Epoch 448, Batch: 0, Loss: 1.027021
Train - Epoch 449, Batch: 0, Loss: 1.039424
Train - Epoch 450, Batch: 0, Loss: 1.053565
Train - Epoch 451, Batch: 0, Loss: 1.039333
Train - Epoch 452, Batch: 0, Loss: 1.057310
Train - Epoch 453, Batch: 0, Loss: 1.039139
Train - Epoch 454, Batch: 0, Loss: 1.036838
Train - Epoch 455, Batch: 0, Loss: 1.045614
Train - Epoch 456, Batch: 0, Loss: 1.056027
Train - Epoch 457, Batch: 0, Loss: 1.056804
Train - Epoch 458, Batch: 0, Loss: 1.047627
Train - Epoch 459, Batch: 0, Loss: 1.042397
Train - Epoch 460, Batch: 0, Loss: 1.051669
Train - Epoch 461, Batch: 0, Loss: 1.047211
Train - Epoch 462, Batch: 0, Loss: 1.053804
Train - Epoch 463, Batch: 0, Loss: 1.040912
Train - Epoch 464, Batch: 0, Loss: 1.042555
Train - Epoch 465, Batch: 0, Loss: 1.043264
Train - Epoch 466, Batch: 0, Loss: 1.057932
Train - Epoch 467, Batch: 0, Loss: 1.065501
Train - Epoch 468, Batch: 0, Loss: 1.032351
Train - Epoch 469, Batch: 0, Loss: 1.054707
Train - Epoch 470, Batch: 0, Loss: 1.038472
Train - Epoch 471, Batch: 0, Loss: 1.044721
Train - Epoch 472, Batch: 0, Loss: 1.025948
Train - Epoch 473, Batch: 0, Loss: 1.038120
Train - Epoch 474, Batch: 0, Loss: 1.040252
Train - Epoch 475, Batch: 0, Loss: 1.052350
Train - Epoch 476, Batch: 0, Loss: 1.040275
Train - Epoch 477, Batch: 0, Loss: 1.044511
Train - Epoch 478, Batch: 0, Loss: 1.048226
Train - Epoch 479, Batch: 0, Loss: 1.060954
Train - Epoch 480, Batch: 0, Loss: 1.046549
Train - Epoch 481, Batch: 0, Loss: 1.058011
Train - Epoch 482, Batch: 0, Loss: 1.039668
Train - Epoch 483, Batch: 0, Loss: 1.041047
Train - Epoch 484, Batch: 0, Loss: 1.041821
Train - Epoch 485, Batch: 0, Loss: 1.060201
Train - Epoch 486, Batch: 0, Loss: 1.039356
Train - Epoch 487, Batch: 0, Loss: 1.041731
Train - Epoch 488, Batch: 0, Loss: 1.039020
Train - Epoch 489, Batch: 0, Loss: 1.047285
Train - Epoch 490, Batch: 0, Loss: 1.036010
Train - Epoch 491, Batch: 0, Loss: 1.047229
Train - Epoch 492, Batch: 0, Loss: 1.040711
Train - Epoch 493, Batch: 0, Loss: 1.037937
Train - Epoch 494, Batch: 0, Loss: 1.024308
Train - Epoch 495, Batch: 0, Loss: 1.034459
Train - Epoch 496, Batch: 0, Loss: 1.028664
Train - Epoch 497, Batch: 0, Loss: 1.045660
Train - Epoch 498, Batch: 0, Loss: 1.033576
Train - Epoch 499, Batch: 0, Loss: 1.049943
training_time:: 107.41697812080383
training time full:: 107.41704750061035
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.631700
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 25
max_epoch:: 500
batch_size:: 10000
epoch:: 0
tensor([    0, 32774,     6, 32778, 32780,    13,    15,    18,    19, 32788,
        32790, 32793, 32794, 32796, 32798,    38,    40,    42,    44, 32814,
           47,    46, 32817, 32818, 32819,    49,    52, 32816,    57, 32825,
           60, 32829,    62,    61,    65, 32835,    69,    72,    80,    84,
        32852, 32859,    91, 32861,    95,    98, 32868,   101,   100,   102])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
epoch:: 60
epoch:: 61
epoch:: 62
epoch:: 63
epoch:: 64
epoch:: 65
epoch:: 66
epoch:: 67
epoch:: 68
epoch:: 69
epoch:: 70
epoch:: 71
epoch:: 72
epoch:: 73
epoch:: 74
epoch:: 75
epoch:: 76
epoch:: 77
epoch:: 78
epoch:: 79
epoch:: 80
epoch:: 81
epoch:: 82
epoch:: 83
epoch:: 84
epoch:: 85
epoch:: 86
epoch:: 87
epoch:: 88
epoch:: 89
epoch:: 90
epoch:: 91
epoch:: 92
epoch:: 93
epoch:: 94
epoch:: 95
epoch:: 96
epoch:: 97
epoch:: 98
epoch:: 99
epoch:: 100
epoch:: 101
epoch:: 102
epoch:: 103
epoch:: 104
epoch:: 105
epoch:: 106
epoch:: 107
epoch:: 108
epoch:: 109
epoch:: 110
epoch:: 111
epoch:: 112
epoch:: 113
epoch:: 114
epoch:: 115
epoch:: 116
epoch:: 117
epoch:: 118
epoch:: 119
epoch:: 120
epoch:: 121
epoch:: 122
epoch:: 123
epoch:: 124
epoch:: 125
epoch:: 126
epoch:: 127
epoch:: 128
epoch:: 129
epoch:: 130
epoch:: 131
epoch:: 132
epoch:: 133
epoch:: 134
epoch:: 135
epoch:: 136
epoch:: 137
epoch:: 138
epoch:: 139
epoch:: 140
epoch:: 141
epoch:: 142
epoch:: 143
epoch:: 144
epoch:: 145
epoch:: 146
epoch:: 147
epoch:: 148
epoch:: 149
epoch:: 150
epoch:: 151
epoch:: 152
epoch:: 153
epoch:: 154
epoch:: 155
epoch:: 156
epoch:: 157
epoch:: 158
epoch:: 159
epoch:: 160
epoch:: 161
epoch:: 162
epoch:: 163
epoch:: 164
epoch:: 165
epoch:: 166
epoch:: 167
epoch:: 168
epoch:: 169
epoch:: 170
epoch:: 171
epoch:: 172
epoch:: 173
epoch:: 174
epoch:: 175
epoch:: 176
epoch:: 177
epoch:: 178
epoch:: 179
epoch:: 180
epoch:: 181
epoch:: 182
epoch:: 183
epoch:: 184
epoch:: 185
epoch:: 186
epoch:: 187
epoch:: 188
epoch:: 189
epoch:: 190
epoch:: 191
epoch:: 192
epoch:: 193
epoch:: 194
epoch:: 195
epoch:: 196
epoch:: 197
epoch:: 198
epoch:: 199
epoch:: 200
epoch:: 201
epoch:: 202
epoch:: 203
epoch:: 204
epoch:: 205
epoch:: 206
epoch:: 207
epoch:: 208
epoch:: 209
epoch:: 210
epoch:: 211
epoch:: 212
epoch:: 213
epoch:: 214
epoch:: 215
epoch:: 216
epoch:: 217
epoch:: 218
epoch:: 219
epoch:: 220
epoch:: 221
epoch:: 222
epoch:: 223
epoch:: 224
epoch:: 225
epoch:: 226
epoch:: 227
epoch:: 228
epoch:: 229
epoch:: 230
epoch:: 231
epoch:: 232
epoch:: 233
epoch:: 234
epoch:: 235
epoch:: 236
epoch:: 237
epoch:: 238
epoch:: 239
epoch:: 240
epoch:: 241
epoch:: 242
epoch:: 243
epoch:: 244
epoch:: 245
epoch:: 246
epoch:: 247
epoch:: 248
epoch:: 249
epoch:: 250
epoch:: 251
epoch:: 252
epoch:: 253
epoch:: 254
epoch:: 255
epoch:: 256
epoch:: 257
epoch:: 258
epoch:: 259
epoch:: 260
epoch:: 261
epoch:: 262
epoch:: 263
epoch:: 264
epoch:: 265
epoch:: 266
epoch:: 267
epoch:: 268
epoch:: 269
epoch:: 270
epoch:: 271
epoch:: 272
epoch:: 273
epoch:: 274
epoch:: 275
epoch:: 276
epoch:: 277
epoch:: 278
epoch:: 279
epoch:: 280
epoch:: 281
epoch:: 282
epoch:: 283
epoch:: 284
epoch:: 285
epoch:: 286
epoch:: 287
epoch:: 288
epoch:: 289
epoch:: 290
epoch:: 291
epoch:: 292
epoch:: 293
epoch:: 294
epoch:: 295
epoch:: 296
epoch:: 297
epoch:: 298
epoch:: 299
epoch:: 300
epoch:: 301
epoch:: 302
epoch:: 303
epoch:: 304
epoch:: 305
epoch:: 306
epoch:: 307
epoch:: 308
epoch:: 309
epoch:: 310
epoch:: 311
epoch:: 312
epoch:: 313
epoch:: 314
epoch:: 315
epoch:: 316
epoch:: 317
epoch:: 318
epoch:: 319
epoch:: 320
epoch:: 321
epoch:: 322
epoch:: 323
epoch:: 324
epoch:: 325
epoch:: 326
epoch:: 327
epoch:: 328
epoch:: 329
epoch:: 330
epoch:: 331
epoch:: 332
epoch:: 333
epoch:: 334
epoch:: 335
epoch:: 336
epoch:: 337
epoch:: 338
epoch:: 339
epoch:: 340
epoch:: 341
epoch:: 342
epoch:: 343
epoch:: 344
epoch:: 345
epoch:: 346
epoch:: 347
epoch:: 348
epoch:: 349
epoch:: 350
epoch:: 351
epoch:: 352
epoch:: 353
epoch:: 354
epoch:: 355
epoch:: 356
epoch:: 357
epoch:: 358
epoch:: 359
epoch:: 360
epoch:: 361
epoch:: 362
epoch:: 363
epoch:: 364
epoch:: 365
epoch:: 366
epoch:: 367
epoch:: 368
epoch:: 369
epoch:: 370
epoch:: 371
epoch:: 372
epoch:: 373
epoch:: 374
epoch:: 375
epoch:: 376
epoch:: 377
epoch:: 378
epoch:: 379
epoch:: 380
epoch:: 381
epoch:: 382
epoch:: 383
epoch:: 384
epoch:: 385
epoch:: 386
epoch:: 387
epoch:: 388
epoch:: 389
epoch:: 390
epoch:: 391
epoch:: 392
epoch:: 393
epoch:: 394
epoch:: 395
epoch:: 396
epoch:: 397
epoch:: 398
epoch:: 399
epoch:: 400
epoch:: 401
epoch:: 402
epoch:: 403
epoch:: 404
epoch:: 405
epoch:: 406
epoch:: 407
epoch:: 408
epoch:: 409
epoch:: 410
epoch:: 411
epoch:: 412
epoch:: 413
epoch:: 414
epoch:: 415
epoch:: 416
epoch:: 417
epoch:: 418
epoch:: 419
epoch:: 420
epoch:: 421
epoch:: 422
epoch:: 423
epoch:: 424
epoch:: 425
epoch:: 426
epoch:: 427
epoch:: 428
epoch:: 429
epoch:: 430
epoch:: 431
epoch:: 432
epoch:: 433
epoch:: 434
epoch:: 435
epoch:: 436
epoch:: 437
epoch:: 438
epoch:: 439
epoch:: 440
epoch:: 441
epoch:: 442
epoch:: 443
epoch:: 444
epoch:: 445
epoch:: 446
epoch:: 447
epoch:: 448
epoch:: 449
epoch:: 450
epoch:: 451
epoch:: 452
epoch:: 453
epoch:: 454
epoch:: 455
epoch:: 456
epoch:: 457
epoch:: 458
epoch:: 459
epoch:: 460
epoch:: 461
epoch:: 462
epoch:: 463
epoch:: 464
epoch:: 465
epoch:: 466
epoch:: 467
epoch:: 468
epoch:: 469
epoch:: 470
epoch:: 471
epoch:: 472
epoch:: 473
epoch:: 474
epoch:: 475
epoch:: 476
epoch:: 477
epoch:: 478
epoch:: 479
epoch:: 480
epoch:: 481
epoch:: 482
epoch:: 483
epoch:: 484
epoch:: 485
epoch:: 486
epoch:: 487
epoch:: 488
epoch:: 489
epoch:: 490
epoch:: 491
epoch:: 492
epoch:: 493
epoch:: 494
epoch:: 495
epoch:: 496
epoch:: 497
epoch:: 498
epoch:: 499
training time is 80.57597804069519
overhead:: 0
overhead2:: 2.6633946895599365
overhead3:: 0
time_baseline:: 80.57601046562195
curr_diff: 0 tensor(0.0383, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0383, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632000
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 25
max_epoch:: 500
overhead:: 0
overhead2:: 0.07987689971923828
overhead3:: 0.2548995018005371
overhead4:: 10.425946950912476
overhead5:: 0
memory usage:: 5623521280
time_provenance:: 17.96203327178955
curr_diff: 0 tensor(0.0094, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0094, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0455, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0455, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632500
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 25
max_epoch:: 500
overhead:: 0
overhead2:: 0.08173561096191406
overhead3:: 0.2582883834838867
overhead4:: 10.71531629562378
overhead5:: 0
memory usage:: 5625626624
time_provenance:: 18.239782571792603
curr_diff: 0 tensor(0.0094, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0094, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0455, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0455, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632500
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 25
max_epoch:: 500
overhead:: 0
overhead2:: 0.08396577835083008
overhead3:: 0.2623767852783203
overhead4:: 10.852737188339233
overhead5:: 0
memory usage:: 5627990016
time_provenance:: 18.401352643966675
curr_diff: 0 tensor(0.0094, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0094, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0455, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0455, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632300
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 25
max_epoch:: 500
overhead:: 0
overhead2:: 0.15497589111328125
overhead3:: 0.4426262378692627
overhead4:: 18.85857629776001
overhead5:: 0
memory usage:: 5632757760
time_provenance:: 28.268792152404785
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0389, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0389, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632100
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 25
max_epoch:: 500
overhead:: 0
overhead2:: 0.17246079444885254
overhead3:: 0.43041062355041504
overhead4:: 18.8931143283844
overhead5:: 0
memory usage:: 5638889472
time_provenance:: 28.26965308189392
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0389, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0389, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632100
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 25
max_epoch:: 500
overhead:: 0
overhead2:: 0.16234469413757324
overhead3:: 0.46663618087768555
overhead4:: 19.01590895652771
overhead5:: 0
memory usage:: 5631819776
time_provenance:: 28.567331314086914
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0389, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0389, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.631900
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 25
max_epoch:: 500
overhead:: 0
overhead2:: 0.37450575828552246
overhead3:: 0.9989311695098877
overhead4:: 42.64698076248169
overhead5:: 0
memory usage:: 5638746112
time_provenance:: 57.89789628982544
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0383, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0383, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632000
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 25
max_epoch:: 500
overhead:: 0
overhead2:: 0.39453983306884766
overhead3:: 1.0053215026855469
overhead4:: 44.47763466835022
overhead5:: 0
memory usage:: 5625593856
time_provenance:: 59.80085515975952
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0383, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0383, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632000
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 25
max_epoch:: 500
overhead:: 0
overhead2:: 0.4027366638183594
overhead3:: 1.0254194736480713
overhead4:: 44.32210111618042
overhead5:: 0
memory usage:: 5647482880
time_provenance:: 59.70318007469177
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0383, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0383, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632000
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 4 0.0005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 25
max_epoch:: 500
overhead:: 0
overhead2:: 0.7886979579925537
overhead3:: 2.0777394771575928
overhead4:: 82.9022901058197
overhead5:: 0
memory usage:: 5632221184
time_provenance:: 106.17440366744995
curr_diff: 0 tensor(5.0060e-13, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.0060e-13, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0383, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0383, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632000
deletion rate:: 0.001
python3 generate_rand_ids 0.001  cifar10_2 0
tensor([ 7435,  4877,  2445, 26512, 47253,  8856, 15130, 36896,  5282, 28967,
        35751, 40745, 10409, 24235, 25516, 27307,  8239, 22193, 38837, 35638,
        15029,   312, 39992,  3131, 33087, 15735, 45249, 43333, 34379, 38988,
        33741, 10573,  6868, 39900, 41572, 32998, 23782, 10471, 14567, 39406,
        20975, 22513, 33905, 46964, 16628, 12278, 41206, 16888,  9468, 44670])
batch size:: 10000
repetition 0
python3 benchmark_exp_lr.py 0.001 10000 500 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05] Logistic_regression cifar10_2 0 0.0001 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 2.309017
Train - Epoch 1, Batch: 0, Loss: 2.247155
Train - Epoch 2, Batch: 0, Loss: 2.191239
Train - Epoch 3, Batch: 0, Loss: 2.141603
Train - Epoch 4, Batch: 0, Loss: 2.091591
Train - Epoch 5, Batch: 0, Loss: 2.050049
Train - Epoch 6, Batch: 0, Loss: 2.002789
Train - Epoch 7, Batch: 0, Loss: 1.967855
Train - Epoch 8, Batch: 0, Loss: 1.931531
Train - Epoch 9, Batch: 0, Loss: 1.895141
Train - Epoch 10, Batch: 0, Loss: 1.869903
Train - Epoch 11, Batch: 0, Loss: 1.838789
Train - Epoch 12, Batch: 0, Loss: 1.808161
Train - Epoch 13, Batch: 0, Loss: 1.787060
Train - Epoch 14, Batch: 0, Loss: 1.757559
Train - Epoch 15, Batch: 0, Loss: 1.738417
Train - Epoch 16, Batch: 0, Loss: 1.722016
Train - Epoch 17, Batch: 0, Loss: 1.693809
Train - Epoch 18, Batch: 0, Loss: 1.674616
Train - Epoch 19, Batch: 0, Loss: 1.663189
Train - Epoch 20, Batch: 0, Loss: 1.635191
Train - Epoch 21, Batch: 0, Loss: 1.623174
Train - Epoch 22, Batch: 0, Loss: 1.605855
Train - Epoch 23, Batch: 0, Loss: 1.606035
Train - Epoch 24, Batch: 0, Loss: 1.575385
Train - Epoch 25, Batch: 0, Loss: 1.564582
Train - Epoch 26, Batch: 0, Loss: 1.544531
Train - Epoch 27, Batch: 0, Loss: 1.546608
Train - Epoch 28, Batch: 0, Loss: 1.527167
Train - Epoch 29, Batch: 0, Loss: 1.512825
Train - Epoch 30, Batch: 0, Loss: 1.504977
Train - Epoch 31, Batch: 0, Loss: 1.500476
Train - Epoch 32, Batch: 0, Loss: 1.475642
Train - Epoch 33, Batch: 0, Loss: 1.474939
Train - Epoch 34, Batch: 0, Loss: 1.477272
Train - Epoch 35, Batch: 0, Loss: 1.463347
Train - Epoch 36, Batch: 0, Loss: 1.447122
Train - Epoch 37, Batch: 0, Loss: 1.451967
Train - Epoch 38, Batch: 0, Loss: 1.435640
Train - Epoch 39, Batch: 0, Loss: 1.430799
Train - Epoch 40, Batch: 0, Loss: 1.430002
Train - Epoch 41, Batch: 0, Loss: 1.413796
Train - Epoch 42, Batch: 0, Loss: 1.406458
Train - Epoch 43, Batch: 0, Loss: 1.403894
Train - Epoch 44, Batch: 0, Loss: 1.399213
Train - Epoch 45, Batch: 0, Loss: 1.380505
Train - Epoch 46, Batch: 0, Loss: 1.381446
Train - Epoch 47, Batch: 0, Loss: 1.371075
Train - Epoch 48, Batch: 0, Loss: 1.373838
Train - Epoch 49, Batch: 0, Loss: 1.366380
Train - Epoch 50, Batch: 0, Loss: 1.361062
Train - Epoch 51, Batch: 0, Loss: 1.361079
Train - Epoch 52, Batch: 0, Loss: 1.361736
Train - Epoch 53, Batch: 0, Loss: 1.341951
Train - Epoch 54, Batch: 0, Loss: 1.336149
Train - Epoch 55, Batch: 0, Loss: 1.333419
Train - Epoch 56, Batch: 0, Loss: 1.340050
Train - Epoch 57, Batch: 0, Loss: 1.326083
Train - Epoch 58, Batch: 0, Loss: 1.316680
Train - Epoch 59, Batch: 0, Loss: 1.306362
Train - Epoch 60, Batch: 0, Loss: 1.320967
Train - Epoch 61, Batch: 0, Loss: 1.304005
Train - Epoch 62, Batch: 0, Loss: 1.311416
Train - Epoch 63, Batch: 0, Loss: 1.299956
Train - Epoch 64, Batch: 0, Loss: 1.297290
Train - Epoch 65, Batch: 0, Loss: 1.299051
Train - Epoch 66, Batch: 0, Loss: 1.297223
Train - Epoch 67, Batch: 0, Loss: 1.298760
Train - Epoch 68, Batch: 0, Loss: 1.300626
Train - Epoch 69, Batch: 0, Loss: 1.292128
Train - Epoch 70, Batch: 0, Loss: 1.283531
Train - Epoch 71, Batch: 0, Loss: 1.285987
Train - Epoch 72, Batch: 0, Loss: 1.277838
Train - Epoch 73, Batch: 0, Loss: 1.280369
Train - Epoch 74, Batch: 0, Loss: 1.280350
Train - Epoch 75, Batch: 0, Loss: 1.270440
Train - Epoch 76, Batch: 0, Loss: 1.265716
Train - Epoch 77, Batch: 0, Loss: 1.271322
Train - Epoch 78, Batch: 0, Loss: 1.257648
Train - Epoch 79, Batch: 0, Loss: 1.271339
Train - Epoch 80, Batch: 0, Loss: 1.260299
Train - Epoch 81, Batch: 0, Loss: 1.249350
Train - Epoch 82, Batch: 0, Loss: 1.242091
Train - Epoch 83, Batch: 0, Loss: 1.256395
Train - Epoch 84, Batch: 0, Loss: 1.257872
Train - Epoch 85, Batch: 0, Loss: 1.241141
Train - Epoch 86, Batch: 0, Loss: 1.244735
Train - Epoch 87, Batch: 0, Loss: 1.238543
Train - Epoch 88, Batch: 0, Loss: 1.235435
Train - Epoch 89, Batch: 0, Loss: 1.232009
Train - Epoch 90, Batch: 0, Loss: 1.227872
Train - Epoch 91, Batch: 0, Loss: 1.230406
Train - Epoch 92, Batch: 0, Loss: 1.220925
Train - Epoch 93, Batch: 0, Loss: 1.222356
Train - Epoch 94, Batch: 0, Loss: 1.230868
Train - Epoch 95, Batch: 0, Loss: 1.222767
Train - Epoch 96, Batch: 0, Loss: 1.219062
Train - Epoch 97, Batch: 0, Loss: 1.220987
Train - Epoch 98, Batch: 0, Loss: 1.218134
Train - Epoch 99, Batch: 0, Loss: 1.219143
Train - Epoch 100, Batch: 0, Loss: 1.224024
Train - Epoch 101, Batch: 0, Loss: 1.216422
Train - Epoch 102, Batch: 0, Loss: 1.223534
Train - Epoch 103, Batch: 0, Loss: 1.202760
Train - Epoch 104, Batch: 0, Loss: 1.206740
Train - Epoch 105, Batch: 0, Loss: 1.207987
Train - Epoch 106, Batch: 0, Loss: 1.205287
Train - Epoch 107, Batch: 0, Loss: 1.198370
Train - Epoch 108, Batch: 0, Loss: 1.195144
Train - Epoch 109, Batch: 0, Loss: 1.208522
Train - Epoch 110, Batch: 0, Loss: 1.206697
Train - Epoch 111, Batch: 0, Loss: 1.200673
Train - Epoch 112, Batch: 0, Loss: 1.187721
Train - Epoch 113, Batch: 0, Loss: 1.182365
Train - Epoch 114, Batch: 0, Loss: 1.192402
Train - Epoch 115, Batch: 0, Loss: 1.200887
Train - Epoch 116, Batch: 0, Loss: 1.187392
Train - Epoch 117, Batch: 0, Loss: 1.188854
Train - Epoch 118, Batch: 0, Loss: 1.174445
Train - Epoch 119, Batch: 0, Loss: 1.179232
Train - Epoch 120, Batch: 0, Loss: 1.183810
Train - Epoch 121, Batch: 0, Loss: 1.162199
Train - Epoch 122, Batch: 0, Loss: 1.182876
Train - Epoch 123, Batch: 0, Loss: 1.180972
Train - Epoch 124, Batch: 0, Loss: 1.187149
Train - Epoch 125, Batch: 0, Loss: 1.178038
Train - Epoch 126, Batch: 0, Loss: 1.184891
Train - Epoch 127, Batch: 0, Loss: 1.190303
Train - Epoch 128, Batch: 0, Loss: 1.176697
Train - Epoch 129, Batch: 0, Loss: 1.186112
Train - Epoch 130, Batch: 0, Loss: 1.178720
Train - Epoch 131, Batch: 0, Loss: 1.176228
Train - Epoch 132, Batch: 0, Loss: 1.165955
Train - Epoch 133, Batch: 0, Loss: 1.168544
Train - Epoch 134, Batch: 0, Loss: 1.175443
Train - Epoch 135, Batch: 0, Loss: 1.180744
Train - Epoch 136, Batch: 0, Loss: 1.170740
Train - Epoch 137, Batch: 0, Loss: 1.168838
Train - Epoch 138, Batch: 0, Loss: 1.162049
Train - Epoch 139, Batch: 0, Loss: 1.148967
Train - Epoch 140, Batch: 0, Loss: 1.163968
Train - Epoch 141, Batch: 0, Loss: 1.169476
Train - Epoch 142, Batch: 0, Loss: 1.145473
Train - Epoch 143, Batch: 0, Loss: 1.157420
Train - Epoch 144, Batch: 0, Loss: 1.170071
Train - Epoch 145, Batch: 0, Loss: 1.161679
Train - Epoch 146, Batch: 0, Loss: 1.161742
Train - Epoch 147, Batch: 0, Loss: 1.143505
Train - Epoch 148, Batch: 0, Loss: 1.153261
Train - Epoch 149, Batch: 0, Loss: 1.143343
Train - Epoch 150, Batch: 0, Loss: 1.156859
Train - Epoch 151, Batch: 0, Loss: 1.157414
Train - Epoch 152, Batch: 0, Loss: 1.159593
Train - Epoch 153, Batch: 0, Loss: 1.158465
Train - Epoch 154, Batch: 0, Loss: 1.155058
Train - Epoch 155, Batch: 0, Loss: 1.146295
Train - Epoch 156, Batch: 0, Loss: 1.148077
Train - Epoch 157, Batch: 0, Loss: 1.146172
Train - Epoch 158, Batch: 0, Loss: 1.151584
Train - Epoch 159, Batch: 0, Loss: 1.158972
Train - Epoch 160, Batch: 0, Loss: 1.147453
Train - Epoch 161, Batch: 0, Loss: 1.126396
Train - Epoch 162, Batch: 0, Loss: 1.149372
Train - Epoch 163, Batch: 0, Loss: 1.143056
Train - Epoch 164, Batch: 0, Loss: 1.154387
Train - Epoch 165, Batch: 0, Loss: 1.133736
Train - Epoch 166, Batch: 0, Loss: 1.144910
Train - Epoch 167, Batch: 0, Loss: 1.140896
Train - Epoch 168, Batch: 0, Loss: 1.125041
Train - Epoch 169, Batch: 0, Loss: 1.133624
Train - Epoch 170, Batch: 0, Loss: 1.141310
Train - Epoch 171, Batch: 0, Loss: 1.141817
Train - Epoch 172, Batch: 0, Loss: 1.136355
Train - Epoch 173, Batch: 0, Loss: 1.133807
Train - Epoch 174, Batch: 0, Loss: 1.134143
Train - Epoch 175, Batch: 0, Loss: 1.129925
Train - Epoch 176, Batch: 0, Loss: 1.139408
Train - Epoch 177, Batch: 0, Loss: 1.130886
Train - Epoch 178, Batch: 0, Loss: 1.128701
Train - Epoch 179, Batch: 0, Loss: 1.130846
Train - Epoch 180, Batch: 0, Loss: 1.133301
Train - Epoch 181, Batch: 0, Loss: 1.141906
Train - Epoch 182, Batch: 0, Loss: 1.131207
Train - Epoch 183, Batch: 0, Loss: 1.125554
Train - Epoch 184, Batch: 0, Loss: 1.103465
Train - Epoch 185, Batch: 0, Loss: 1.129453
Train - Epoch 186, Batch: 0, Loss: 1.143019
Train - Epoch 187, Batch: 0, Loss: 1.123714
Train - Epoch 188, Batch: 0, Loss: 1.127097
Train - Epoch 189, Batch: 0, Loss: 1.148270
Train - Epoch 190, Batch: 0, Loss: 1.131005
Train - Epoch 191, Batch: 0, Loss: 1.127280
Train - Epoch 192, Batch: 0, Loss: 1.117416
Train - Epoch 193, Batch: 0, Loss: 1.131362
Train - Epoch 194, Batch: 0, Loss: 1.114296
Train - Epoch 195, Batch: 0, Loss: 1.116220
Train - Epoch 196, Batch: 0, Loss: 1.118382
Train - Epoch 197, Batch: 0, Loss: 1.106578
Train - Epoch 198, Batch: 0, Loss: 1.122364
Train - Epoch 199, Batch: 0, Loss: 1.117427
Train - Epoch 200, Batch: 0, Loss: 1.127081
Train - Epoch 201, Batch: 0, Loss: 1.131218
Train - Epoch 202, Batch: 0, Loss: 1.125683
Train - Epoch 203, Batch: 0, Loss: 1.115995
Train - Epoch 204, Batch: 0, Loss: 1.132867
Train - Epoch 205, Batch: 0, Loss: 1.116040
Train - Epoch 206, Batch: 0, Loss: 1.109382
Train - Epoch 207, Batch: 0, Loss: 1.121595
Train - Epoch 208, Batch: 0, Loss: 1.109468
Train - Epoch 209, Batch: 0, Loss: 1.102740
Train - Epoch 210, Batch: 0, Loss: 1.109643
Train - Epoch 211, Batch: 0, Loss: 1.121642
Train - Epoch 212, Batch: 0, Loss: 1.103583
Train - Epoch 213, Batch: 0, Loss: 1.091025
Train - Epoch 214, Batch: 0, Loss: 1.109684
Train - Epoch 215, Batch: 0, Loss: 1.107450
Train - Epoch 216, Batch: 0, Loss: 1.109453
Train - Epoch 217, Batch: 0, Loss: 1.123892
Train - Epoch 218, Batch: 0, Loss: 1.108424
Train - Epoch 219, Batch: 0, Loss: 1.099059
Train - Epoch 220, Batch: 0, Loss: 1.115195
Train - Epoch 221, Batch: 0, Loss: 1.106570
Train - Epoch 222, Batch: 0, Loss: 1.132670
Train - Epoch 223, Batch: 0, Loss: 1.107013
Train - Epoch 224, Batch: 0, Loss: 1.109035
Train - Epoch 225, Batch: 0, Loss: 1.108457
Train - Epoch 226, Batch: 0, Loss: 1.102317
Train - Epoch 227, Batch: 0, Loss: 1.092254
Train - Epoch 228, Batch: 0, Loss: 1.113526
Train - Epoch 229, Batch: 0, Loss: 1.084025
Train - Epoch 230, Batch: 0, Loss: 1.107227
Train - Epoch 231, Batch: 0, Loss: 1.099887
Train - Epoch 232, Batch: 0, Loss: 1.114547
Train - Epoch 233, Batch: 0, Loss: 1.099589
Train - Epoch 234, Batch: 0, Loss: 1.108673
Train - Epoch 235, Batch: 0, Loss: 1.103852
Train - Epoch 236, Batch: 0, Loss: 1.112149
Train - Epoch 237, Batch: 0, Loss: 1.095074
Train - Epoch 238, Batch: 0, Loss: 1.107760
Train - Epoch 239, Batch: 0, Loss: 1.089671
Train - Epoch 240, Batch: 0, Loss: 1.107108
Train - Epoch 241, Batch: 0, Loss: 1.099952
Train - Epoch 242, Batch: 0, Loss: 1.100482
Train - Epoch 243, Batch: 0, Loss: 1.089187
Train - Epoch 244, Batch: 0, Loss: 1.099979
Train - Epoch 245, Batch: 0, Loss: 1.092979
Train - Epoch 246, Batch: 0, Loss: 1.096387
Train - Epoch 247, Batch: 0, Loss: 1.091776
Train - Epoch 248, Batch: 0, Loss: 1.085739
Train - Epoch 249, Batch: 0, Loss: 1.094072
Train - Epoch 250, Batch: 0, Loss: 1.091590
Train - Epoch 251, Batch: 0, Loss: 1.083452
Train - Epoch 252, Batch: 0, Loss: 1.092556
Train - Epoch 253, Batch: 0, Loss: 1.083721
Train - Epoch 254, Batch: 0, Loss: 1.083648
Train - Epoch 255, Batch: 0, Loss: 1.102040
Train - Epoch 256, Batch: 0, Loss: 1.084396
Train - Epoch 257, Batch: 0, Loss: 1.098084
Train - Epoch 258, Batch: 0, Loss: 1.082638
Train - Epoch 259, Batch: 0, Loss: 1.092973
Train - Epoch 260, Batch: 0, Loss: 1.095901
Train - Epoch 261, Batch: 0, Loss: 1.088047
Train - Epoch 262, Batch: 0, Loss: 1.098468
Train - Epoch 263, Batch: 0, Loss: 1.107557
Train - Epoch 264, Batch: 0, Loss: 1.078521
Train - Epoch 265, Batch: 0, Loss: 1.091376
Train - Epoch 266, Batch: 0, Loss: 1.080202
Train - Epoch 267, Batch: 0, Loss: 1.102554
Train - Epoch 268, Batch: 0, Loss: 1.085953
Train - Epoch 269, Batch: 0, Loss: 1.080643
Train - Epoch 270, Batch: 0, Loss: 1.096576
Train - Epoch 271, Batch: 0, Loss: 1.086288
Train - Epoch 272, Batch: 0, Loss: 1.090325
Train - Epoch 273, Batch: 0, Loss: 1.083999
Train - Epoch 274, Batch: 0, Loss: 1.086450
Train - Epoch 275, Batch: 0, Loss: 1.083391
Train - Epoch 276, Batch: 0, Loss: 1.089800
Train - Epoch 277, Batch: 0, Loss: 1.094908
Train - Epoch 278, Batch: 0, Loss: 1.087871
Train - Epoch 279, Batch: 0, Loss: 1.089721
Train - Epoch 280, Batch: 0, Loss: 1.110242
Train - Epoch 281, Batch: 0, Loss: 1.085873
Train - Epoch 282, Batch: 0, Loss: 1.075426
Train - Epoch 283, Batch: 0, Loss: 1.084045
Train - Epoch 284, Batch: 0, Loss: 1.086437
Train - Epoch 285, Batch: 0, Loss: 1.080369
Train - Epoch 286, Batch: 0, Loss: 1.084353
Train - Epoch 287, Batch: 0, Loss: 1.091382
Train - Epoch 288, Batch: 0, Loss: 1.071922
Train - Epoch 289, Batch: 0, Loss: 1.068829
Train - Epoch 290, Batch: 0, Loss: 1.088953
Train - Epoch 291, Batch: 0, Loss: 1.075895
Train - Epoch 292, Batch: 0, Loss: 1.084352
Train - Epoch 293, Batch: 0, Loss: 1.105517
Train - Epoch 294, Batch: 0, Loss: 1.094111
Train - Epoch 295, Batch: 0, Loss: 1.094002
Train - Epoch 296, Batch: 0, Loss: 1.078396
Train - Epoch 297, Batch: 0, Loss: 1.098679
Train - Epoch 298, Batch: 0, Loss: 1.072992
Train - Epoch 299, Batch: 0, Loss: 1.075298
Train - Epoch 300, Batch: 0, Loss: 1.081685
Train - Epoch 301, Batch: 0, Loss: 1.078446
Train - Epoch 302, Batch: 0, Loss: 1.081433
Train - Epoch 303, Batch: 0, Loss: 1.079022
Train - Epoch 304, Batch: 0, Loss: 1.085334
Train - Epoch 305, Batch: 0, Loss: 1.071312
Train - Epoch 306, Batch: 0, Loss: 1.081461
Train - Epoch 307, Batch: 0, Loss: 1.067957
Train - Epoch 308, Batch: 0, Loss: 1.077500
Train - Epoch 309, Batch: 0, Loss: 1.084602
Train - Epoch 310, Batch: 0, Loss: 1.068418
Train - Epoch 311, Batch: 0, Loss: 1.077805
Train - Epoch 312, Batch: 0, Loss: 1.070167
Train - Epoch 313, Batch: 0, Loss: 1.078063
Train - Epoch 314, Batch: 0, Loss: 1.084820
Train - Epoch 315, Batch: 0, Loss: 1.059686
Train - Epoch 316, Batch: 0, Loss: 1.067129
Train - Epoch 317, Batch: 0, Loss: 1.080492
Train - Epoch 318, Batch: 0, Loss: 1.084098
Train - Epoch 319, Batch: 0, Loss: 1.079318
Train - Epoch 320, Batch: 0, Loss: 1.075598
Train - Epoch 321, Batch: 0, Loss: 1.077288
Train - Epoch 322, Batch: 0, Loss: 1.060883
Train - Epoch 323, Batch: 0, Loss: 1.050936
Train - Epoch 324, Batch: 0, Loss: 1.059361
Train - Epoch 325, Batch: 0, Loss: 1.060003
Train - Epoch 326, Batch: 0, Loss: 1.056223
Train - Epoch 327, Batch: 0, Loss: 1.068474
Train - Epoch 328, Batch: 0, Loss: 1.067900
Train - Epoch 329, Batch: 0, Loss: 1.078151
Train - Epoch 330, Batch: 0, Loss: 1.086710
Train - Epoch 331, Batch: 0, Loss: 1.056148
Train - Epoch 332, Batch: 0, Loss: 1.075073
Train - Epoch 333, Batch: 0, Loss: 1.082447
Train - Epoch 334, Batch: 0, Loss: 1.055062
Train - Epoch 335, Batch: 0, Loss: 1.064442
Train - Epoch 336, Batch: 0, Loss: 1.079171
Train - Epoch 337, Batch: 0, Loss: 1.058912
Train - Epoch 338, Batch: 0, Loss: 1.073229
Train - Epoch 339, Batch: 0, Loss: 1.063953
Train - Epoch 340, Batch: 0, Loss: 1.063613
Train - Epoch 341, Batch: 0, Loss: 1.057703
Train - Epoch 342, Batch: 0, Loss: 1.064168
Train - Epoch 343, Batch: 0, Loss: 1.058816
Train - Epoch 344, Batch: 0, Loss: 1.071206
Train - Epoch 345, Batch: 0, Loss: 1.058350
Train - Epoch 346, Batch: 0, Loss: 1.067137
Train - Epoch 347, Batch: 0, Loss: 1.063281
Train - Epoch 348, Batch: 0, Loss: 1.062094
Train - Epoch 349, Batch: 0, Loss: 1.050270
Train - Epoch 350, Batch: 0, Loss: 1.067201
Train - Epoch 351, Batch: 0, Loss: 1.068232
Train - Epoch 352, Batch: 0, Loss: 1.070318
Train - Epoch 353, Batch: 0, Loss: 1.066578
Train - Epoch 354, Batch: 0, Loss: 1.070530
Train - Epoch 355, Batch: 0, Loss: 1.075837
Train - Epoch 356, Batch: 0, Loss: 1.073752
Train - Epoch 357, Batch: 0, Loss: 1.060636
Train - Epoch 358, Batch: 0, Loss: 1.048214
Train - Epoch 359, Batch: 0, Loss: 1.060736
Train - Epoch 360, Batch: 0, Loss: 1.070912
Train - Epoch 361, Batch: 0, Loss: 1.046194
Train - Epoch 362, Batch: 0, Loss: 1.062379
Train - Epoch 363, Batch: 0, Loss: 1.055785
Train - Epoch 364, Batch: 0, Loss: 1.055603
Train - Epoch 365, Batch: 0, Loss: 1.054482
Train - Epoch 366, Batch: 0, Loss: 1.045697
Train - Epoch 367, Batch: 0, Loss: 1.064674
Train - Epoch 368, Batch: 0, Loss: 1.046799
Train - Epoch 369, Batch: 0, Loss: 1.071601
Train - Epoch 370, Batch: 0, Loss: 1.060237
Train - Epoch 371, Batch: 0, Loss: 1.062923
Train - Epoch 372, Batch: 0, Loss: 1.054987
Train - Epoch 373, Batch: 0, Loss: 1.051197/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 374, Batch: 0, Loss: 1.046681
Train - Epoch 375, Batch: 0, Loss: 1.053699
Train - Epoch 376, Batch: 0, Loss: 1.051958
Train - Epoch 377, Batch: 0, Loss: 1.060363
Train - Epoch 378, Batch: 0, Loss: 1.060479
Train - Epoch 379, Batch: 0, Loss: 1.041120
Train - Epoch 380, Batch: 0, Loss: 1.051876
Train - Epoch 381, Batch: 0, Loss: 1.054515
Train - Epoch 382, Batch: 0, Loss: 1.054564
Train - Epoch 383, Batch: 0, Loss: 1.064658
Train - Epoch 384, Batch: 0, Loss: 1.048996
Train - Epoch 385, Batch: 0, Loss: 1.052543
Train - Epoch 386, Batch: 0, Loss: 1.069099
Train - Epoch 387, Batch: 0, Loss: 1.033281
Train - Epoch 388, Batch: 0, Loss: 1.053216
Train - Epoch 389, Batch: 0, Loss: 1.061337
Train - Epoch 390, Batch: 0, Loss: 1.057603
Train - Epoch 391, Batch: 0, Loss: 1.048928
Train - Epoch 392, Batch: 0, Loss: 1.050059
Train - Epoch 393, Batch: 0, Loss: 1.072557
Train - Epoch 394, Batch: 0, Loss: 1.065724
Train - Epoch 395, Batch: 0, Loss: 1.059090
Train - Epoch 396, Batch: 0, Loss: 1.051882
Train - Epoch 397, Batch: 0, Loss: 1.067405
Train - Epoch 398, Batch: 0, Loss: 1.063608
Train - Epoch 399, Batch: 0, Loss: 1.061824
Train - Epoch 400, Batch: 0, Loss: 1.050374
Train - Epoch 401, Batch: 0, Loss: 1.051899
Train - Epoch 402, Batch: 0, Loss: 1.065639
Train - Epoch 403, Batch: 0, Loss: 1.055635
Train - Epoch 404, Batch: 0, Loss: 1.047516
Train - Epoch 405, Batch: 0, Loss: 1.043727
Train - Epoch 406, Batch: 0, Loss: 1.056668
Train - Epoch 407, Batch: 0, Loss: 1.058984
Train - Epoch 408, Batch: 0, Loss: 1.039054
Train - Epoch 409, Batch: 0, Loss: 1.055601
Train - Epoch 410, Batch: 0, Loss: 1.066682
Train - Epoch 411, Batch: 0, Loss: 1.050586
Train - Epoch 412, Batch: 0, Loss: 1.038801
Train - Epoch 413, Batch: 0, Loss: 1.036575
Train - Epoch 414, Batch: 0, Loss: 1.048072
Train - Epoch 415, Batch: 0, Loss: 1.052730
Train - Epoch 416, Batch: 0, Loss: 1.054133
Train - Epoch 417, Batch: 0, Loss: 1.059420
Train - Epoch 418, Batch: 0, Loss: 1.064553
Train - Epoch 419, Batch: 0, Loss: 1.055768
Train - Epoch 420, Batch: 0, Loss: 1.060371
Train - Epoch 421, Batch: 0, Loss: 1.054442
Train - Epoch 422, Batch: 0, Loss: 1.058665
Train - Epoch 423, Batch: 0, Loss: 1.054561
Train - Epoch 424, Batch: 0, Loss: 1.051387
Train - Epoch 425, Batch: 0, Loss: 1.057640
Train - Epoch 426, Batch: 0, Loss: 1.037666
Train - Epoch 427, Batch: 0, Loss: 1.035724
Train - Epoch 428, Batch: 0, Loss: 1.061898
Train - Epoch 429, Batch: 0, Loss: 1.060824
Train - Epoch 430, Batch: 0, Loss: 1.048605
Train - Epoch 431, Batch: 0, Loss: 1.052321
Train - Epoch 432, Batch: 0, Loss: 1.046506
Train - Epoch 433, Batch: 0, Loss: 1.040607
Train - Epoch 434, Batch: 0, Loss: 1.038595
Train - Epoch 435, Batch: 0, Loss: 1.046445
Train - Epoch 436, Batch: 0, Loss: 1.035279
Train - Epoch 437, Batch: 0, Loss: 1.051962
Train - Epoch 438, Batch: 0, Loss: 1.055031
Train - Epoch 439, Batch: 0, Loss: 1.055558
Train - Epoch 440, Batch: 0, Loss: 1.051899
Train - Epoch 441, Batch: 0, Loss: 1.052809
Train - Epoch 442, Batch: 0, Loss: 1.046776
Train - Epoch 443, Batch: 0, Loss: 1.046896
Train - Epoch 444, Batch: 0, Loss: 1.033473
Train - Epoch 445, Batch: 0, Loss: 1.030041
Train - Epoch 446, Batch: 0, Loss: 1.038591
Train - Epoch 447, Batch: 0, Loss: 1.039741
Train - Epoch 448, Batch: 0, Loss: 1.040899
Train - Epoch 449, Batch: 0, Loss: 1.047669
Train - Epoch 450, Batch: 0, Loss: 1.055787
Train - Epoch 451, Batch: 0, Loss: 1.032789
Train - Epoch 452, Batch: 0, Loss: 1.053307
Train - Epoch 453, Batch: 0, Loss: 1.044899
Train - Epoch 454, Batch: 0, Loss: 1.040756
Train - Epoch 455, Batch: 0, Loss: 1.046768
Train - Epoch 456, Batch: 0, Loss: 1.044363
Train - Epoch 457, Batch: 0, Loss: 1.039126
Train - Epoch 458, Batch: 0, Loss: 1.062795
Train - Epoch 459, Batch: 0, Loss: 1.052200
Train - Epoch 460, Batch: 0, Loss: 1.056149
Train - Epoch 461, Batch: 0, Loss: 1.040402
Train - Epoch 462, Batch: 0, Loss: 1.033490
Train - Epoch 463, Batch: 0, Loss: 1.053713
Train - Epoch 464, Batch: 0, Loss: 1.052355
Train - Epoch 465, Batch: 0, Loss: 1.044612
Train - Epoch 466, Batch: 0, Loss: 1.038086
Train - Epoch 467, Batch: 0, Loss: 1.050765
Train - Epoch 468, Batch: 0, Loss: 1.048782
Train - Epoch 469, Batch: 0, Loss: 1.049236
Train - Epoch 470, Batch: 0, Loss: 1.058326
Train - Epoch 471, Batch: 0, Loss: 1.042166
Train - Epoch 472, Batch: 0, Loss: 1.049306
Train - Epoch 473, Batch: 0, Loss: 1.046305
Train - Epoch 474, Batch: 0, Loss: 1.032097
Train - Epoch 475, Batch: 0, Loss: 1.048137
Train - Epoch 476, Batch: 0, Loss: 1.036352
Train - Epoch 477, Batch: 0, Loss: 1.044069
Train - Epoch 478, Batch: 0, Loss: 1.032647
Train - Epoch 479, Batch: 0, Loss: 1.048534
Train - Epoch 480, Batch: 0, Loss: 1.045930
Train - Epoch 481, Batch: 0, Loss: 1.051013
Train - Epoch 482, Batch: 0, Loss: 1.049251
Train - Epoch 483, Batch: 0, Loss: 1.033181
Train - Epoch 484, Batch: 0, Loss: 1.050610
Train - Epoch 485, Batch: 0, Loss: 1.035841
Train - Epoch 486, Batch: 0, Loss: 1.046227
Train - Epoch 487, Batch: 0, Loss: 1.025240
Train - Epoch 488, Batch: 0, Loss: 1.036278
Train - Epoch 489, Batch: 0, Loss: 1.053144
Train - Epoch 490, Batch: 0, Loss: 1.056608
Train - Epoch 491, Batch: 0, Loss: 1.018050
Train - Epoch 492, Batch: 0, Loss: 1.053056
Train - Epoch 493, Batch: 0, Loss: 1.055102
Train - Epoch 494, Batch: 0, Loss: 1.037818
Train - Epoch 495, Batch: 0, Loss: 1.046323
Train - Epoch 496, Batch: 0, Loss: 1.056108
Train - Epoch 497, Batch: 0, Loss: 1.029151
Train - Epoch 498, Batch: 0, Loss: 1.020922
Train - Epoch 499, Batch: 0, Loss: 1.037537
training_time:: 107.29418587684631
training time full:: 107.2942533493042
provenance prepare time:: 0.0
here
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632700
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 50
max_epoch:: 500
batch_size:: 10000
epoch:: 0
tensor([    3, 32773,     7,    13,    17,    22,    24,    25,    32, 32801,
        32802,    35, 32803,    36, 32806, 32810, 32812, 32815,    49,    53,
           54, 32823,    61,    67, 32835,    68,    71, 32840, 32841, 32842,
        32843, 32848,    81, 32850, 32852,    85,    86,    91, 32860, 32862,
           95,    98,    99,   100,   102,   104,   105,   107, 32876,   117])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
epoch:: 60
epoch:: 61
epoch:: 62
epoch:: 63
epoch:: 64
epoch:: 65
epoch:: 66
epoch:: 67
epoch:: 68
epoch:: 69
epoch:: 70
epoch:: 71
epoch:: 72
epoch:: 73
epoch:: 74
epoch:: 75
epoch:: 76
epoch:: 77
epoch:: 78
epoch:: 79
epoch:: 80
epoch:: 81
epoch:: 82
epoch:: 83
epoch:: 84
epoch:: 85
epoch:: 86
epoch:: 87
epoch:: 88
epoch:: 89
epoch:: 90
epoch:: 91
epoch:: 92
epoch:: 93
epoch:: 94
epoch:: 95
epoch:: 96
epoch:: 97
epoch:: 98
epoch:: 99
epoch:: 100
epoch:: 101
epoch:: 102
epoch:: 103
epoch:: 104
epoch:: 105
epoch:: 106
epoch:: 107
epoch:: 108
epoch:: 109
epoch:: 110
epoch:: 111
epoch:: 112
epoch:: 113
epoch:: 114
epoch:: 115
epoch:: 116
epoch:: 117
epoch:: 118
epoch:: 119
epoch:: 120
epoch:: 121
epoch:: 122
epoch:: 123
epoch:: 124
epoch:: 125
epoch:: 126
epoch:: 127
epoch:: 128
epoch:: 129
epoch:: 130
epoch:: 131
epoch:: 132
epoch:: 133
epoch:: 134
epoch:: 135
epoch:: 136
epoch:: 137
epoch:: 138
epoch:: 139
epoch:: 140
epoch:: 141
epoch:: 142
epoch:: 143
epoch:: 144
epoch:: 145
epoch:: 146
epoch:: 147
epoch:: 148
epoch:: 149
epoch:: 150
epoch:: 151
epoch:: 152
epoch:: 153
epoch:: 154
epoch:: 155
epoch:: 156
epoch:: 157
epoch:: 158
epoch:: 159
epoch:: 160
epoch:: 161
epoch:: 162
epoch:: 163
epoch:: 164
epoch:: 165
epoch:: 166
epoch:: 167
epoch:: 168
epoch:: 169
epoch:: 170
epoch:: 171
epoch:: 172
epoch:: 173
epoch:: 174
epoch:: 175
epoch:: 176
epoch:: 177
epoch:: 178
epoch:: 179
epoch:: 180
epoch:: 181
epoch:: 182
epoch:: 183
epoch:: 184
epoch:: 185
epoch:: 186
epoch:: 187
epoch:: 188
epoch:: 189
epoch:: 190
epoch:: 191
epoch:: 192
epoch:: 193
epoch:: 194
epoch:: 195
epoch:: 196
epoch:: 197
epoch:: 198
epoch:: 199
epoch:: 200
epoch:: 201
epoch:: 202
epoch:: 203
epoch:: 204
epoch:: 205
epoch:: 206
epoch:: 207
epoch:: 208
epoch:: 209
epoch:: 210
epoch:: 211
epoch:: 212
epoch:: 213
epoch:: 214
epoch:: 215
epoch:: 216
epoch:: 217
epoch:: 218
epoch:: 219
epoch:: 220
epoch:: 221
epoch:: 222
epoch:: 223
epoch:: 224
epoch:: 225
epoch:: 226
epoch:: 227
epoch:: 228
epoch:: 229
epoch:: 230
epoch:: 231
epoch:: 232
epoch:: 233
epoch:: 234
epoch:: 235
epoch:: 236
epoch:: 237
epoch:: 238
epoch:: 239
epoch:: 240
epoch:: 241
epoch:: 242
epoch:: 243
epoch:: 244
epoch:: 245
epoch:: 246
epoch:: 247
epoch:: 248
epoch:: 249
epoch:: 250
epoch:: 251
epoch:: 252
epoch:: 253
epoch:: 254
epoch:: 255
epoch:: 256
epoch:: 257
epoch:: 258
epoch:: 259
epoch:: 260
epoch:: 261
epoch:: 262
epoch:: 263
epoch:: 264
epoch:: 265
epoch:: 266
epoch:: 267
epoch:: 268
epoch:: 269
epoch:: 270
epoch:: 271
epoch:: 272
epoch:: 273
epoch:: 274
epoch:: 275
epoch:: 276
epoch:: 277
epoch:: 278
epoch:: 279
epoch:: 280
epoch:: 281
epoch:: 282
epoch:: 283
epoch:: 284
epoch:: 285
epoch:: 286
epoch:: 287
epoch:: 288
epoch:: 289
epoch:: 290
epoch:: 291
epoch:: 292
epoch:: 293
epoch:: 294
epoch:: 295
epoch:: 296
epoch:: 297
epoch:: 298
epoch:: 299
epoch:: 300
epoch:: 301
epoch:: 302
epoch:: 303
epoch:: 304
epoch:: 305
epoch:: 306
epoch:: 307
epoch:: 308
epoch:: 309
epoch:: 310
epoch:: 311
epoch:: 312
epoch:: 313
epoch:: 314
epoch:: 315
epoch:: 316
epoch:: 317
epoch:: 318
epoch:: 319
epoch:: 320
epoch:: 321
epoch:: 322
epoch:: 323
epoch:: 324
epoch:: 325
epoch:: 326
epoch:: 327
epoch:: 328
epoch:: 329
epoch:: 330
epoch:: 331
epoch:: 332
epoch:: 333
epoch:: 334
epoch:: 335
epoch:: 336
epoch:: 337
epoch:: 338
epoch:: 339
epoch:: 340
epoch:: 341
epoch:: 342
epoch:: 343
epoch:: 344
epoch:: 345
epoch:: 346
epoch:: 347
epoch:: 348
epoch:: 349
epoch:: 350
epoch:: 351
epoch:: 352
epoch:: 353
epoch:: 354
epoch:: 355
epoch:: 356
epoch:: 357
epoch:: 358
epoch:: 359
epoch:: 360
epoch:: 361
epoch:: 362
epoch:: 363
epoch:: 364
epoch:: 365
epoch:: 366
epoch:: 367
epoch:: 368
epoch:: 369
epoch:: 370
epoch:: 371
epoch:: 372
epoch:: 373
epoch:: 374
epoch:: 375
epoch:: 376
epoch:: 377
epoch:: 378
epoch:: 379
epoch:: 380
epoch:: 381
epoch:: 382
epoch:: 383
epoch:: 384
epoch:: 385
epoch:: 386
epoch:: 387
epoch:: 388
epoch:: 389
epoch:: 390
epoch:: 391
epoch:: 392
epoch:: 393
epoch:: 394
epoch:: 395
epoch:: 396
epoch:: 397
epoch:: 398
epoch:: 399
epoch:: 400
epoch:: 401
epoch:: 402
epoch:: 403
epoch:: 404
epoch:: 405
epoch:: 406
epoch:: 407
epoch:: 408
epoch:: 409
epoch:: 410
epoch:: 411
epoch:: 412
epoch:: 413
epoch:: 414
epoch:: 415
epoch:: 416
epoch:: 417
epoch:: 418
epoch:: 419
epoch:: 420
epoch:: 421
epoch:: 422
epoch:: 423
epoch:: 424
epoch:: 425
epoch:: 426
epoch:: 427
epoch:: 428
epoch:: 429
epoch:: 430
epoch:: 431
epoch:: 432
epoch:: 433
epoch:: 434
epoch:: 435
epoch:: 436
epoch:: 437
epoch:: 438
epoch:: 439
epoch:: 440
epoch:: 441
epoch:: 442
epoch:: 443
epoch:: 444
epoch:: 445
epoch:: 446
epoch:: 447
epoch:: 448
epoch:: 449
epoch:: 450
epoch:: 451
epoch:: 452
epoch:: 453
epoch:: 454
epoch:: 455
epoch:: 456
epoch:: 457
epoch:: 458
epoch:: 459
epoch:: 460
epoch:: 461
epoch:: 462
epoch:: 463
epoch:: 464
epoch:: 465
epoch:: 466
epoch:: 467
epoch:: 468
epoch:: 469
epoch:: 470
epoch:: 471
epoch:: 472
epoch:: 473
epoch:: 474
epoch:: 475
epoch:: 476
epoch:: 477
epoch:: 478
epoch:: 479
epoch:: 480
epoch:: 481
epoch:: 482
epoch:: 483
epoch:: 484
epoch:: 485
epoch:: 486
epoch:: 487
epoch:: 488
epoch:: 489
epoch:: 490
epoch:: 491
epoch:: 492
epoch:: 493
epoch:: 494
epoch:: 495
epoch:: 496
epoch:: 497
epoch:: 498
epoch:: 499
training time is 80.75080728530884
overhead:: 0
overhead2:: 2.631099224090576
overhead3:: 0
time_baseline:: 80.75084137916565
curr_diff: 0 tensor(0.0603, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0603, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632800
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 50
max_epoch:: 500
overhead:: 0
overhead2:: 0.0786600112915039
overhead3:: 0.2507205009460449
overhead4:: 9.869940996170044
overhead5:: 0
memory usage:: 5631979520
time_provenance:: 17.355082035064697
curr_diff: 0 tensor(0.0149, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0149, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0719, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0719, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632500
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 50
max_epoch:: 500
overhead:: 0
overhead2:: 0.08294510841369629
overhead3:: 0.26081228256225586
overhead4:: 10.522425889968872
overhead5:: 0
memory usage:: 5628665856
time_provenance:: 18.111833572387695
curr_diff: 0 tensor(0.0149, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0149, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0719, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0719, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632500
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 50
max_epoch:: 500
overhead:: 0
overhead2:: 0.08512377738952637
overhead3:: 0.26540446281433105
overhead4:: 10.598638772964478
overhead5:: 0
memory usage:: 5645434880
time_provenance:: 18.239736557006836
curr_diff: 0 tensor(0.0149, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0149, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0719, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0719, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632500
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 50
max_epoch:: 500
overhead:: 0
overhead2:: 0.15847134590148926
overhead3:: 0.447157621383667
overhead4:: 18.19023823738098
overhead5:: 0
memory usage:: 5664497664
time_provenance:: 27.638946533203125
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0613, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0613, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633100
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 50
max_epoch:: 500
overhead:: 0
overhead2:: 0.15632247924804688
overhead3:: 0.4448549747467041
overhead4:: 17.92534899711609
overhead5:: 0
memory usage:: 5625397248
time_provenance:: 27.435354709625244
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0613, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0613, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633100
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 50
max_epoch:: 500
overhead:: 0
overhead2:: 0.15905380249023438
overhead3:: 0.4592008590698242
overhead4:: 18.507123708724976
overhead5:: 0
memory usage:: 5640978432
time_provenance:: 28.089410543441772
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0613, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0613, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632700
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 50
max_epoch:: 500
overhead:: 0
overhead2:: 0.38288211822509766
overhead3:: 1.027662754058838
overhead4:: 44.124868869781494
overhead5:: 0
memory usage:: 5626494976
time_provenance:: 59.54163146018982
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0604, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0604, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632900
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 50
max_epoch:: 500
overhead:: 0
overhead2:: 0.40492749214172363
overhead3:: 1.0329217910766602
overhead4:: 43.98356747627258
overhead5:: 0
memory usage:: 5640331264
time_provenance:: 59.43522071838379
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0604, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0604, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632900
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 50
max_epoch:: 500
overhead:: 0
overhead2:: 0.40638303756713867
overhead3:: 1.0420901775360107
overhead4:: 43.91026735305786
overhead5:: 0
memory usage:: 5624561664
time_provenance:: 59.40951895713806
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0604, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0604, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632900
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 0 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 50
max_epoch:: 500
overhead:: 0
overhead2:: 0.7777857780456543
overhead3:: 2.081003189086914
overhead4:: 81.92835736274719
overhead5:: 0
memory usage:: 5631266816
time_provenance:: 105.3832802772522
curr_diff: 0 tensor(5.0026e-13, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.0026e-13, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0603, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0603, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632800
repetition 1
python3 benchmark_exp_lr.py 0.001 10000 500 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05] Logistic_regression cifar10_2 1 0.0001 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 2.314117
Train - Epoch 1, Batch: 0, Loss: 2.250309
Train - Epoch 2, Batch: 0, Loss: 2.194526
Train - Epoch 3, Batch: 0, Loss: 2.142621
Train - Epoch 4, Batch: 0, Loss: 2.094671
Train - Epoch 5, Batch: 0, Loss: 2.052491
Train - Epoch 6, Batch: 0, Loss: 2.011058
Train - Epoch 7, Batch: 0, Loss: 1.973541
Train - Epoch 8, Batch: 0, Loss: 1.933133
Train - Epoch 9, Batch: 0, Loss: 1.901696
Train - Epoch 10, Batch: 0, Loss: 1.871577
Train - Epoch 11, Batch: 0, Loss: 1.846243
Train - Epoch 12, Batch: 0, Loss: 1.813341
Train - Epoch 13, Batch: 0, Loss: 1.789033
Train - Epoch 14, Batch: 0, Loss: 1.756347
Train - Epoch 15, Batch: 0, Loss: 1.731055
Train - Epoch 16, Batch: 0, Loss: 1.722512
Train - Epoch 17, Batch: 0, Loss: 1.692006
Train - Epoch 18, Batch: 0, Loss: 1.682282
Train - Epoch 19, Batch: 0, Loss: 1.659628
Train - Epoch 20, Batch: 0, Loss: 1.639733
Train - Epoch 21, Batch: 0, Loss: 1.629510
Train - Epoch 22, Batch: 0, Loss: 1.609431
Train - Epoch 23, Batch: 0, Loss: 1.598464
Train - Epoch 24, Batch: 0, Loss: 1.581128
Train - Epoch 25, Batch: 0, Loss: 1.563877
Train - Epoch 26, Batch: 0, Loss: 1.553138
Train - Epoch 27, Batch: 0, Loss: 1.547810
Train - Epoch 28, Batch: 0, Loss: 1.527968
Train - Epoch 29, Batch: 0, Loss: 1.521020
Train - Epoch 30, Batch: 0, Loss: 1.505453
Train - Epoch 31, Batch: 0, Loss: 1.497158
Train - Epoch 32, Batch: 0, Loss: 1.493272
Train - Epoch 33, Batch: 0, Loss: 1.481082
Train - Epoch 34, Batch: 0, Loss: 1.478262
Train - Epoch 35, Batch: 0, Loss: 1.466825
Train - Epoch 36, Batch: 0, Loss: 1.441363
Train - Epoch 37, Batch: 0, Loss: 1.447152
Train - Epoch 38, Batch: 0, Loss: 1.436278
Train - Epoch 39, Batch: 0, Loss: 1.431871
Train - Epoch 40, Batch: 0, Loss: 1.421535
Train - Epoch 41, Batch: 0, Loss: 1.414179
Train - Epoch 42, Batch: 0, Loss: 1.419606
Train - Epoch 43, Batch: 0, Loss: 1.405158
Train - Epoch 44, Batch: 0, Loss: 1.393347
Train - Epoch 45, Batch: 0, Loss: 1.386474
Train - Epoch 46, Batch: 0, Loss: 1.382889
Train - Epoch 47, Batch: 0, Loss: 1.386324
Train - Epoch 48, Batch: 0, Loss: 1.370973
Train - Epoch 49, Batch: 0, Loss: 1.361085
Train - Epoch 50, Batch: 0, Loss: 1.353635
Train - Epoch 51, Batch: 0, Loss: 1.361721
Train - Epoch 52, Batch: 0, Loss: 1.354550
Train - Epoch 53, Batch: 0, Loss: 1.345583
Train - Epoch 54, Batch: 0, Loss: 1.333265
Train - Epoch 55, Batch: 0, Loss: 1.338080
Train - Epoch 56, Batch: 0, Loss: 1.334269
Train - Epoch 57, Batch: 0, Loss: 1.335583
Train - Epoch 58, Batch: 0, Loss: 1.330768
Train - Epoch 59, Batch: 0, Loss: 1.322357
Train - Epoch 60, Batch: 0, Loss: 1.311282
Train - Epoch 61, Batch: 0, Loss: 1.317072
Train - Epoch 62, Batch: 0, Loss: 1.314986
Train - Epoch 63, Batch: 0, Loss: 1.304397
Train - Epoch 64, Batch: 0, Loss: 1.300289
Train - Epoch 65, Batch: 0, Loss: 1.298226
Train - Epoch 66, Batch: 0, Loss: 1.299087
Train - Epoch 67, Batch: 0, Loss: 1.289777
Train - Epoch 68, Batch: 0, Loss: 1.292246
Train - Epoch 69, Batch: 0, Loss: 1.292995
Train - Epoch 70, Batch: 0, Loss: 1.280979
Train - Epoch 71, Batch: 0, Loss: 1.270957
Train - Epoch 72, Batch: 0, Loss: 1.278106
Train - Epoch 73, Batch: 0, Loss: 1.275071
Train - Epoch 74, Batch: 0, Loss: 1.263880
Train - Epoch 75, Batch: 0, Loss: 1.264306
Train - Epoch 76, Batch: 0, Loss: 1.267526
Train - Epoch 77, Batch: 0, Loss: 1.264192
Train - Epoch 78, Batch: 0, Loss: 1.267813
Train - Epoch 79, Batch: 0, Loss: 1.261446
Train - Epoch 80, Batch: 0, Loss: 1.252011
Train - Epoch 81, Batch: 0, Loss: 1.265268
Train - Epoch 82, Batch: 0, Loss: 1.249050
Train - Epoch 83, Batch: 0, Loss: 1.246089
Train - Epoch 84, Batch: 0, Loss: 1.234443
Train - Epoch 85, Batch: 0, Loss: 1.248875
Train - Epoch 86, Batch: 0, Loss: 1.254111
Train - Epoch 87, Batch: 0, Loss: 1.246311
Train - Epoch 88, Batch: 0, Loss: 1.231763
Train - Epoch 89, Batch: 0, Loss: 1.234101
Train - Epoch 90, Batch: 0, Loss: 1.232781
Train - Epoch 91, Batch: 0, Loss: 1.235560
Train - Epoch 92, Batch: 0, Loss: 1.232172
Train - Epoch 93, Batch: 0, Loss: 1.230629
Train - Epoch 94, Batch: 0, Loss: 1.233097
Train - Epoch 95, Batch: 0, Loss: 1.228508
Train - Epoch 96, Batch: 0, Loss: 1.214650
Train - Epoch 97, Batch: 0, Loss: 1.218169
Train - Epoch 98, Batch: 0, Loss: 1.220389
Train - Epoch 99, Batch: 0, Loss: 1.209533
Train - Epoch 100, Batch: 0, Loss: 1.209721
Train - Epoch 101, Batch: 0, Loss: 1.217344
Train - Epoch 102, Batch: 0, Loss: 1.214166
Train - Epoch 103, Batch: 0, Loss: 1.217562
Train - Epoch 104, Batch: 0, Loss: 1.222006
Train - Epoch 105, Batch: 0, Loss: 1.210143
Train - Epoch 106, Batch: 0, Loss: 1.211959
Train - Epoch 107, Batch: 0, Loss: 1.207760
Train - Epoch 108, Batch: 0, Loss: 1.207608
Train - Epoch 109, Batch: 0, Loss: 1.189309
Train - Epoch 110, Batch: 0, Loss: 1.194814
Train - Epoch 111, Batch: 0, Loss: 1.196529
Train - Epoch 112, Batch: 0, Loss: 1.204397
Train - Epoch 113, Batch: 0, Loss: 1.188771
Train - Epoch 114, Batch: 0, Loss: 1.205285
Train - Epoch 115, Batch: 0, Loss: 1.191644
Train - Epoch 116, Batch: 0, Loss: 1.177051
Train - Epoch 117, Batch: 0, Loss: 1.187172
Train - Epoch 118, Batch: 0, Loss: 1.188802
Train - Epoch 119, Batch: 0, Loss: 1.187638
Train - Epoch 120, Batch: 0, Loss: 1.182499
Train - Epoch 121, Batch: 0, Loss: 1.185904
Train - Epoch 122, Batch: 0, Loss: 1.189326
Train - Epoch 123, Batch: 0, Loss: 1.195996
Train - Epoch 124, Batch: 0, Loss: 1.171729
Train - Epoch 125, Batch: 0, Loss: 1.173904
Train - Epoch 126, Batch: 0, Loss: 1.199662
Train - Epoch 127, Batch: 0, Loss: 1.177277
Train - Epoch 128, Batch: 0, Loss: 1.175299
Train - Epoch 129, Batch: 0, Loss: 1.161294
Train - Epoch 130, Batch: 0, Loss: 1.172633
Train - Epoch 131, Batch: 0, Loss: 1.163660
Train - Epoch 132, Batch: 0, Loss: 1.164230
Train - Epoch 133, Batch: 0, Loss: 1.170166
Train - Epoch 134, Batch: 0, Loss: 1.165992
Train - Epoch 135, Batch: 0, Loss: 1.167286
Train - Epoch 136, Batch: 0, Loss: 1.172825
Train - Epoch 137, Batch: 0, Loss: 1.159521
Train - Epoch 138, Batch: 0, Loss: 1.161479
Train - Epoch 139, Batch: 0, Loss: 1.178337
Train - Epoch 140, Batch: 0, Loss: 1.161664
Train - Epoch 141, Batch: 0, Loss: 1.163443
Train - Epoch 142, Batch: 0, Loss: 1.175963
Train - Epoch 143, Batch: 0, Loss: 1.165510
Train - Epoch 144, Batch: 0, Loss: 1.156033
Train - Epoch 145, Batch: 0, Loss: 1.155195
Train - Epoch 146, Batch: 0, Loss: 1.166947
Train - Epoch 147, Batch: 0, Loss: 1.158353
Train - Epoch 148, Batch: 0, Loss: 1.165267
Train - Epoch 149, Batch: 0, Loss: 1.167518
Train - Epoch 150, Batch: 0, Loss: 1.152662
Train - Epoch 151, Batch: 0, Loss: 1.158912
Train - Epoch 152, Batch: 0, Loss: 1.138787
Train - Epoch 153, Batch: 0, Loss: 1.154395
Train - Epoch 154, Batch: 0, Loss: 1.154871
Train - Epoch 155, Batch: 0, Loss: 1.154769
Train - Epoch 156, Batch: 0, Loss: 1.139433
Train - Epoch 157, Batch: 0, Loss: 1.164538
Train - Epoch 158, Batch: 0, Loss: 1.144760
Train - Epoch 159, Batch: 0, Loss: 1.162483
Train - Epoch 160, Batch: 0, Loss: 1.149118
Train - Epoch 161, Batch: 0, Loss: 1.144042
Train - Epoch 162, Batch: 0, Loss: 1.153572
Train - Epoch 163, Batch: 0, Loss: 1.152971
Train - Epoch 164, Batch: 0, Loss: 1.143432
Train - Epoch 165, Batch: 0, Loss: 1.145929
Train - Epoch 166, Batch: 0, Loss: 1.151914
Train - Epoch 167, Batch: 0, Loss: 1.132890
Train - Epoch 168, Batch: 0, Loss: 1.141949
Train - Epoch 169, Batch: 0, Loss: 1.127363
Train - Epoch 170, Batch: 0, Loss: 1.144863
Train - Epoch 171, Batch: 0, Loss: 1.131905
Train - Epoch 172, Batch: 0, Loss: 1.136593
Train - Epoch 173, Batch: 0, Loss: 1.129008
Train - Epoch 174, Batch: 0, Loss: 1.132738
Train - Epoch 175, Batch: 0, Loss: 1.143744
Train - Epoch 176, Batch: 0, Loss: 1.136043
Train - Epoch 177, Batch: 0, Loss: 1.144637
Train - Epoch 178, Batch: 0, Loss: 1.144979
Train - Epoch 179, Batch: 0, Loss: 1.132593
Train - Epoch 180, Batch: 0, Loss: 1.131423
Train - Epoch 181, Batch: 0, Loss: 1.126954
Train - Epoch 182, Batch: 0, Loss: 1.124365
Train - Epoch 183, Batch: 0, Loss: 1.129698
Train - Epoch 184, Batch: 0, Loss: 1.137641
Train - Epoch 185, Batch: 0, Loss: 1.127406
Train - Epoch 186, Batch: 0, Loss: 1.152313
Train - Epoch 187, Batch: 0, Loss: 1.122045
Train - Epoch 188, Batch: 0, Loss: 1.128589
Train - Epoch 189, Batch: 0, Loss: 1.126698
Train - Epoch 190, Batch: 0, Loss: 1.131748
Train - Epoch 191, Batch: 0, Loss: 1.132613
Train - Epoch 192, Batch: 0, Loss: 1.130733
Train - Epoch 193, Batch: 0, Loss: 1.114463
Train - Epoch 194, Batch: 0, Loss: 1.130635
Train - Epoch 195, Batch: 0, Loss: 1.121293
Train - Epoch 196, Batch: 0, Loss: 1.126094
Train - Epoch 197, Batch: 0, Loss: 1.125248
Train - Epoch 198, Batch: 0, Loss: 1.114383
Train - Epoch 199, Batch: 0, Loss: 1.107790
Train - Epoch 200, Batch: 0, Loss: 1.118696
Train - Epoch 201, Batch: 0, Loss: 1.110023
Train - Epoch 202, Batch: 0, Loss: 1.118859
Train - Epoch 203, Batch: 0, Loss: 1.118183
Train - Epoch 204, Batch: 0, Loss: 1.123988
Train - Epoch 205, Batch: 0, Loss: 1.125044
Train - Epoch 206, Batch: 0, Loss: 1.133475
Train - Epoch 207, Batch: 0, Loss: 1.104028
Train - Epoch 208, Batch: 0, Loss: 1.109937
Train - Epoch 209, Batch: 0, Loss: 1.098563
Train - Epoch 210, Batch: 0, Loss: 1.110799
Train - Epoch 211, Batch: 0, Loss: 1.115988
Train - Epoch 212, Batch: 0, Loss: 1.109029
Train - Epoch 213, Batch: 0, Loss: 1.114643
Train - Epoch 214, Batch: 0, Loss: 1.117932
Train - Epoch 215, Batch: 0, Loss: 1.112263
Train - Epoch 216, Batch: 0, Loss: 1.109713
Train - Epoch 217, Batch: 0, Loss: 1.114208
Train - Epoch 218, Batch: 0, Loss: 1.106114
Train - Epoch 219, Batch: 0, Loss: 1.097253
Train - Epoch 220, Batch: 0, Loss: 1.103689
Train - Epoch 221, Batch: 0, Loss: 1.114457
Train - Epoch 222, Batch: 0, Loss: 1.107484
Train - Epoch 223, Batch: 0, Loss: 1.110609
Train - Epoch 224, Batch: 0, Loss: 1.096531
Train - Epoch 225, Batch: 0, Loss: 1.117411
Train - Epoch 226, Batch: 0, Loss: 1.105068
Train - Epoch 227, Batch: 0, Loss: 1.110807
Train - Epoch 228, Batch: 0, Loss: 1.089519
Train - Epoch 229, Batch: 0, Loss: 1.105443
Train - Epoch 230, Batch: 0, Loss: 1.099883
Train - Epoch 231, Batch: 0, Loss: 1.106085
Train - Epoch 232, Batch: 0, Loss: 1.099159
Train - Epoch 233, Batch: 0, Loss: 1.081994
Train - Epoch 234, Batch: 0, Loss: 1.097959
Train - Epoch 235, Batch: 0, Loss: 1.104274
Train - Epoch 236, Batch: 0, Loss: 1.105344
Train - Epoch 237, Batch: 0, Loss: 1.116071
Train - Epoch 238, Batch: 0, Loss: 1.097519
Train - Epoch 239, Batch: 0, Loss: 1.101125
Train - Epoch 240, Batch: 0, Loss: 1.081113
Train - Epoch 241, Batch: 0, Loss: 1.115438
Train - Epoch 242, Batch: 0, Loss: 1.110288
Train - Epoch 243, Batch: 0, Loss: 1.097921
Train - Epoch 244, Batch: 0, Loss: 1.102151
Train - Epoch 245, Batch: 0, Loss: 1.099288
Train - Epoch 246, Batch: 0, Loss: 1.093241
Train - Epoch 247, Batch: 0, Loss: 1.090107
Train - Epoch 248, Batch: 0, Loss: 1.097573
Train - Epoch 249, Batch: 0, Loss: 1.092164
Train - Epoch 250, Batch: 0, Loss: 1.087288
Train - Epoch 251, Batch: 0, Loss: 1.090666
Train - Epoch 252, Batch: 0, Loss: 1.088735
Train - Epoch 253, Batch: 0, Loss: 1.100672
Train - Epoch 254, Batch: 0, Loss: 1.083966
Train - Epoch 255, Batch: 0, Loss: 1.092578
Train - Epoch 256, Batch: 0, Loss: 1.095283
Train - Epoch 257, Batch: 0, Loss: 1.096261
Train - Epoch 258, Batch: 0, Loss: 1.095942
Train - Epoch 259, Batch: 0, Loss: 1.083708
Train - Epoch 260, Batch: 0, Loss: 1.086264
Train - Epoch 261, Batch: 0, Loss: 1.077123
Train - Epoch 262, Batch: 0, Loss: 1.092735
Train - Epoch 263, Batch: 0, Loss: 1.085543
Train - Epoch 264, Batch: 0, Loss: 1.090258
Train - Epoch 265, Batch: 0, Loss: 1.080949
Train - Epoch 266, Batch: 0, Loss: 1.088613
Train - Epoch 267, Batch: 0, Loss: 1.082173
Train - Epoch 268, Batch: 0, Loss: 1.089234
Train - Epoch 269, Batch: 0, Loss: 1.089289
Train - Epoch 270, Batch: 0, Loss: 1.082283
Train - Epoch 271, Batch: 0, Loss: 1.094630
Train - Epoch 272, Batch: 0, Loss: 1.094301
Train - Epoch 273, Batch: 0, Loss: 1.085965
Train - Epoch 274, Batch: 0, Loss: 1.080847
Train - Epoch 275, Batch: 0, Loss: 1.081032
Train - Epoch 276, Batch: 0, Loss: 1.090745
Train - Epoch 277, Batch: 0, Loss: 1.082120
Train - Epoch 278, Batch: 0, Loss: 1.087483
Train - Epoch 279, Batch: 0, Loss: 1.078331
Train - Epoch 280, Batch: 0, Loss: 1.088141
Train - Epoch 281, Batch: 0, Loss: 1.089596
Train - Epoch 282, Batch: 0, Loss: 1.079675
Train - Epoch 283, Batch: 0, Loss: 1.084506
Train - Epoch 284, Batch: 0, Loss: 1.093807
Train - Epoch 285, Batch: 0, Loss: 1.076710
Train - Epoch 286, Batch: 0, Loss: 1.082886
Train - Epoch 287, Batch: 0, Loss: 1.076441
Train - Epoch 288, Batch: 0, Loss: 1.079151
Train - Epoch 289, Batch: 0, Loss: 1.077179
Train - Epoch 290, Batch: 0, Loss: 1.078098
Train - Epoch 291, Batch: 0, Loss: 1.092489
Train - Epoch 292, Batch: 0, Loss: 1.079321
Train - Epoch 293, Batch: 0, Loss: 1.068333
Train - Epoch 294, Batch: 0, Loss: 1.083283
Train - Epoch 295, Batch: 0, Loss: 1.077876
Train - Epoch 296, Batch: 0, Loss: 1.082716
Train - Epoch 297, Batch: 0, Loss: 1.078945
Train - Epoch 298, Batch: 0, Loss: 1.081453
Train - Epoch 299, Batch: 0, Loss: 1.064141
Train - Epoch 300, Batch: 0, Loss: 1.074461
Train - Epoch 301, Batch: 0, Loss: 1.081016
Train - Epoch 302, Batch: 0, Loss: 1.074516
Train - Epoch 303, Batch: 0, Loss: 1.088022
Train - Epoch 304, Batch: 0, Loss: 1.065339
Train - Epoch 305, Batch: 0, Loss: 1.089619
Train - Epoch 306, Batch: 0, Loss: 1.089460
Train - Epoch 307, Batch: 0, Loss: 1.063321
Train - Epoch 308, Batch: 0, Loss: 1.063123
Train - Epoch 309, Batch: 0, Loss: 1.070316
Train - Epoch 310, Batch: 0, Loss: 1.067013
Train - Epoch 311, Batch: 0, Loss: 1.066938
Train - Epoch 312, Batch: 0, Loss: 1.091523
Train - Epoch 313, Batch: 0, Loss: 1.080180
Train - Epoch 314, Batch: 0, Loss: 1.066139
Train - Epoch 315, Batch: 0, Loss: 1.074063
Train - Epoch 316, Batch: 0, Loss: 1.082340
Train - Epoch 317, Batch: 0, Loss: 1.067454
Train - Epoch 318, Batch: 0, Loss: 1.075802
Train - Epoch 319, Batch: 0, Loss: 1.068393
Train - Epoch 320, Batch: 0, Loss: 1.084847
Train - Epoch 321, Batch: 0, Loss: 1.069891
Train - Epoch 322, Batch: 0, Loss: 1.078956
Train - Epoch 323, Batch: 0, Loss: 1.074677
Train - Epoch 324, Batch: 0, Loss: 1.058456
Train - Epoch 325, Batch: 0, Loss: 1.073502
Train - Epoch 326, Batch: 0, Loss: 1.071932
Train - Epoch 327, Batch: 0, Loss: 1.078441
Train - Epoch 328, Batch: 0, Loss: 1.048369
Train - Epoch 329, Batch: 0, Loss: 1.076950
Train - Epoch 330, Batch: 0, Loss: 1.058123
Train - Epoch 331, Batch: 0, Loss: 1.080291
Train - Epoch 332, Batch: 0, Loss: 1.076785
Train - Epoch 333, Batch: 0, Loss: 1.066395
Train - Epoch 334, Batch: 0, Loss: 1.061892
Train - Epoch 335, Batch: 0, Loss: 1.059168
Train - Epoch 336, Batch: 0, Loss: 1.067724
Train - Epoch 337, Batch: 0, Loss: 1.074125
Train - Epoch 338, Batch: 0, Loss: 1.070712
Train - Epoch 339, Batch: 0, Loss: 1.060902
Train - Epoch 340, Batch: 0, Loss: 1.070426
Train - Epoch 341, Batch: 0, Loss: 1.066798
Train - Epoch 342, Batch: 0, Loss: 1.058785
Train - Epoch 343, Batch: 0, Loss: 1.058750
Train - Epoch 344, Batch: 0, Loss: 1.077184
Train - Epoch 345, Batch: 0, Loss: 1.075479
Train - Epoch 346, Batch: 0, Loss: 1.070463
Train - Epoch 347, Batch: 0, Loss: 1.060405
Train - Epoch 348, Batch: 0, Loss: 1.063369
Train - Epoch 349, Batch: 0, Loss: 1.071312
Train - Epoch 350, Batch: 0, Loss: 1.076639
Train - Epoch 351, Batch: 0, Loss: 1.058536
Train - Epoch 352, Batch: 0, Loss: 1.068361
Train - Epoch 353, Batch: 0, Loss: 1.060890
Train - Epoch 354, Batch: 0, Loss: 1.045078
Train - Epoch 355, Batch: 0, Loss: 1.063718
Train - Epoch 356, Batch: 0, Loss: 1.062793
Train - Epoch 357, Batch: 0, Loss: 1.060900
Train - Epoch 358, Batch: 0, Loss: 1.071819
Train - Epoch 359, Batch: 0, Loss: 1.060260
Train - Epoch 360, Batch: 0, Loss: 1.064310
Train - Epoch 361, Batch: 0, Loss: 1.066041
Train - Epoch 362, Batch: 0, Loss: 1.076521
Train - Epoch 363, Batch: 0, Loss: 1.052981
Train - Epoch 364, Batch: 0, Loss: 1.044269
Train - Epoch 365, Batch: 0, Loss: 1.060139
Train - Epoch 366, Batch: 0, Loss: 1.049826
Train - Epoch 367, Batch: 0, Loss: 1.045795
Train - Epoch 368, Batch: 0, Loss: 1.061054
Train - Epoch 369, Batch: 0, Loss: 1.057640
Train - Epoch 370, Batch: 0, Loss: 1.069627
Train - Epoch 371, Batch: 0, Loss: 1.052234
Train - Epoch 372, Batch: 0, Loss: 1.059495
Train - Epoch 373, Batch: 0, Loss: 1.048579/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 374, Batch: 0, Loss: 1.068353
Train - Epoch 375, Batch: 0, Loss: 1.066773
Train - Epoch 376, Batch: 0, Loss: 1.046192
Train - Epoch 377, Batch: 0, Loss: 1.067656
Train - Epoch 378, Batch: 0, Loss: 1.064957
Train - Epoch 379, Batch: 0, Loss: 1.058429
Train - Epoch 380, Batch: 0, Loss: 1.041613
Train - Epoch 381, Batch: 0, Loss: 1.054963
Train - Epoch 382, Batch: 0, Loss: 1.058702
Train - Epoch 383, Batch: 0, Loss: 1.058230
Train - Epoch 384, Batch: 0, Loss: 1.051707
Train - Epoch 385, Batch: 0, Loss: 1.062205
Train - Epoch 386, Batch: 0, Loss: 1.078150
Train - Epoch 387, Batch: 0, Loss: 1.050260
Train - Epoch 388, Batch: 0, Loss: 1.061921
Train - Epoch 389, Batch: 0, Loss: 1.059332
Train - Epoch 390, Batch: 0, Loss: 1.056665
Train - Epoch 391, Batch: 0, Loss: 1.060188
Train - Epoch 392, Batch: 0, Loss: 1.059864
Train - Epoch 393, Batch: 0, Loss: 1.061073
Train - Epoch 394, Batch: 0, Loss: 1.069992
Train - Epoch 395, Batch: 0, Loss: 1.055329
Train - Epoch 396, Batch: 0, Loss: 1.052459
Train - Epoch 397, Batch: 0, Loss: 1.047259
Train - Epoch 398, Batch: 0, Loss: 1.069196
Train - Epoch 399, Batch: 0, Loss: 1.040858
Train - Epoch 400, Batch: 0, Loss: 1.056930
Train - Epoch 401, Batch: 0, Loss: 1.058438
Train - Epoch 402, Batch: 0, Loss: 1.054480
Train - Epoch 403, Batch: 0, Loss: 1.060985
Train - Epoch 404, Batch: 0, Loss: 1.052984
Train - Epoch 405, Batch: 0, Loss: 1.053078
Train - Epoch 406, Batch: 0, Loss: 1.051878
Train - Epoch 407, Batch: 0, Loss: 1.057311
Train - Epoch 408, Batch: 0, Loss: 1.049381
Train - Epoch 409, Batch: 0, Loss: 1.052564
Train - Epoch 410, Batch: 0, Loss: 1.045434
Train - Epoch 411, Batch: 0, Loss: 1.040600
Train - Epoch 412, Batch: 0, Loss: 1.060373
Train - Epoch 413, Batch: 0, Loss: 1.069963
Train - Epoch 414, Batch: 0, Loss: 1.044246
Train - Epoch 415, Batch: 0, Loss: 1.046343
Train - Epoch 416, Batch: 0, Loss: 1.067511
Train - Epoch 417, Batch: 0, Loss: 1.052990
Train - Epoch 418, Batch: 0, Loss: 1.055009
Train - Epoch 419, Batch: 0, Loss: 1.051361
Train - Epoch 420, Batch: 0, Loss: 1.038704
Train - Epoch 421, Batch: 0, Loss: 1.043196
Train - Epoch 422, Batch: 0, Loss: 1.054884
Train - Epoch 423, Batch: 0, Loss: 1.064459
Train - Epoch 424, Batch: 0, Loss: 1.041751
Train - Epoch 425, Batch: 0, Loss: 1.045715
Train - Epoch 426, Batch: 0, Loss: 1.053475
Train - Epoch 427, Batch: 0, Loss: 1.049515
Train - Epoch 428, Batch: 0, Loss: 1.045369
Train - Epoch 429, Batch: 0, Loss: 1.061053
Train - Epoch 430, Batch: 0, Loss: 1.042157
Train - Epoch 431, Batch: 0, Loss: 1.043071
Train - Epoch 432, Batch: 0, Loss: 1.042034
Train - Epoch 433, Batch: 0, Loss: 1.044512
Train - Epoch 434, Batch: 0, Loss: 1.052203
Train - Epoch 435, Batch: 0, Loss: 1.055425
Train - Epoch 436, Batch: 0, Loss: 1.049489
Train - Epoch 437, Batch: 0, Loss: 1.050685
Train - Epoch 438, Batch: 0, Loss: 1.033006
Train - Epoch 439, Batch: 0, Loss: 1.057377
Train - Epoch 440, Batch: 0, Loss: 1.044506
Train - Epoch 441, Batch: 0, Loss: 1.050079
Train - Epoch 442, Batch: 0, Loss: 1.048292
Train - Epoch 443, Batch: 0, Loss: 1.046608
Train - Epoch 444, Batch: 0, Loss: 1.051710
Train - Epoch 445, Batch: 0, Loss: 1.041931
Train - Epoch 446, Batch: 0, Loss: 1.036253
Train - Epoch 447, Batch: 0, Loss: 1.052656
Train - Epoch 448, Batch: 0, Loss: 1.046610
Train - Epoch 449, Batch: 0, Loss: 1.045811
Train - Epoch 450, Batch: 0, Loss: 1.039802
Train - Epoch 451, Batch: 0, Loss: 1.059967
Train - Epoch 452, Batch: 0, Loss: 1.046538
Train - Epoch 453, Batch: 0, Loss: 1.053409
Train - Epoch 454, Batch: 0, Loss: 1.026816
Train - Epoch 455, Batch: 0, Loss: 1.056003
Train - Epoch 456, Batch: 0, Loss: 1.048857
Train - Epoch 457, Batch: 0, Loss: 1.057149
Train - Epoch 458, Batch: 0, Loss: 1.043535
Train - Epoch 459, Batch: 0, Loss: 1.047645
Train - Epoch 460, Batch: 0, Loss: 1.042831
Train - Epoch 461, Batch: 0, Loss: 1.044623
Train - Epoch 462, Batch: 0, Loss: 1.034486
Train - Epoch 463, Batch: 0, Loss: 1.049431
Train - Epoch 464, Batch: 0, Loss: 1.040191
Train - Epoch 465, Batch: 0, Loss: 1.053297
Train - Epoch 466, Batch: 0, Loss: 1.027271
Train - Epoch 467, Batch: 0, Loss: 1.040497
Train - Epoch 468, Batch: 0, Loss: 1.036863
Train - Epoch 469, Batch: 0, Loss: 1.030367
Train - Epoch 470, Batch: 0, Loss: 1.047550
Train - Epoch 471, Batch: 0, Loss: 1.043648
Train - Epoch 472, Batch: 0, Loss: 1.047848
Train - Epoch 473, Batch: 0, Loss: 1.055594
Train - Epoch 474, Batch: 0, Loss: 1.046618
Train - Epoch 475, Batch: 0, Loss: 1.035153
Train - Epoch 476, Batch: 0, Loss: 1.044125
Train - Epoch 477, Batch: 0, Loss: 1.050329
Train - Epoch 478, Batch: 0, Loss: 1.039018
Train - Epoch 479, Batch: 0, Loss: 1.031515
Train - Epoch 480, Batch: 0, Loss: 1.038655
Train - Epoch 481, Batch: 0, Loss: 1.057180
Train - Epoch 482, Batch: 0, Loss: 1.033485
Train - Epoch 483, Batch: 0, Loss: 1.030704
Train - Epoch 484, Batch: 0, Loss: 1.039184
Train - Epoch 485, Batch: 0, Loss: 1.040572
Train - Epoch 486, Batch: 0, Loss: 1.049922
Train - Epoch 487, Batch: 0, Loss: 1.041520
Train - Epoch 488, Batch: 0, Loss: 1.027553
Train - Epoch 489, Batch: 0, Loss: 1.022101
Train - Epoch 490, Batch: 0, Loss: 1.039496
Train - Epoch 491, Batch: 0, Loss: 1.029024
Train - Epoch 492, Batch: 0, Loss: 1.034865
Train - Epoch 493, Batch: 0, Loss: 1.019188
Train - Epoch 494, Batch: 0, Loss: 1.057138
Train - Epoch 495, Batch: 0, Loss: 1.041219
Train - Epoch 496, Batch: 0, Loss: 1.032975
Train - Epoch 497, Batch: 0, Loss: 1.041076
Train - Epoch 498, Batch: 0, Loss: 1.036091
Train - Epoch 499, Batch: 0, Loss: 1.042117
training_time:: 106.96019625663757
training time full:: 106.96026182174683
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633500
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 50
max_epoch:: 500
batch_size:: 10000
epoch:: 0
tensor([    0, 32776, 32777,    12, 32780,    15,    16, 32792, 32800,    38,
           42, 32817,    50,    49,    52, 32818,    55, 32825, 32830, 32836,
        32837,    70, 32840, 32841,    78,    79,    91, 32860,    95,    96,
        32866,   100,   103, 32872,   106, 32876, 32880,   116, 32892,   126,
          128,   132,   135, 32904,   141,   145, 32913,   147,   149, 32920])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
epoch:: 60
epoch:: 61
epoch:: 62
epoch:: 63
epoch:: 64
epoch:: 65
epoch:: 66
epoch:: 67
epoch:: 68
epoch:: 69
epoch:: 70
epoch:: 71
epoch:: 72
epoch:: 73
epoch:: 74
epoch:: 75
epoch:: 76
epoch:: 77
epoch:: 78
epoch:: 79
epoch:: 80
epoch:: 81
epoch:: 82
epoch:: 83
epoch:: 84
epoch:: 85
epoch:: 86
epoch:: 87
epoch:: 88
epoch:: 89
epoch:: 90
epoch:: 91
epoch:: 92
epoch:: 93
epoch:: 94
epoch:: 95
epoch:: 96
epoch:: 97
epoch:: 98
epoch:: 99
epoch:: 100
epoch:: 101
epoch:: 102
epoch:: 103
epoch:: 104
epoch:: 105
epoch:: 106
epoch:: 107
epoch:: 108
epoch:: 109
epoch:: 110
epoch:: 111
epoch:: 112
epoch:: 113
epoch:: 114
epoch:: 115
epoch:: 116
epoch:: 117
epoch:: 118
epoch:: 119
epoch:: 120
epoch:: 121
epoch:: 122
epoch:: 123
epoch:: 124
epoch:: 125
epoch:: 126
epoch:: 127
epoch:: 128
epoch:: 129
epoch:: 130
epoch:: 131
epoch:: 132
epoch:: 133
epoch:: 134
epoch:: 135
epoch:: 136
epoch:: 137
epoch:: 138
epoch:: 139
epoch:: 140
epoch:: 141
epoch:: 142
epoch:: 143
epoch:: 144
epoch:: 145
epoch:: 146
epoch:: 147
epoch:: 148
epoch:: 149
epoch:: 150
epoch:: 151
epoch:: 152
epoch:: 153
epoch:: 154
epoch:: 155
epoch:: 156
epoch:: 157
epoch:: 158
epoch:: 159
epoch:: 160
epoch:: 161
epoch:: 162
epoch:: 163
epoch:: 164
epoch:: 165
epoch:: 166
epoch:: 167
epoch:: 168
epoch:: 169
epoch:: 170
epoch:: 171
epoch:: 172
epoch:: 173
epoch:: 174
epoch:: 175
epoch:: 176
epoch:: 177
epoch:: 178
epoch:: 179
epoch:: 180
epoch:: 181
epoch:: 182
epoch:: 183
epoch:: 184
epoch:: 185
epoch:: 186
epoch:: 187
epoch:: 188
epoch:: 189
epoch:: 190
epoch:: 191
epoch:: 192
epoch:: 193
epoch:: 194
epoch:: 195
epoch:: 196
epoch:: 197
epoch:: 198
epoch:: 199
epoch:: 200
epoch:: 201
epoch:: 202
epoch:: 203
epoch:: 204
epoch:: 205
epoch:: 206
epoch:: 207
epoch:: 208
epoch:: 209
epoch:: 210
epoch:: 211
epoch:: 212
epoch:: 213
epoch:: 214
epoch:: 215
epoch:: 216
epoch:: 217
epoch:: 218
epoch:: 219
epoch:: 220
epoch:: 221
epoch:: 222
epoch:: 223
epoch:: 224
epoch:: 225
epoch:: 226
epoch:: 227
epoch:: 228
epoch:: 229
epoch:: 230
epoch:: 231
epoch:: 232
epoch:: 233
epoch:: 234
epoch:: 235
epoch:: 236
epoch:: 237
epoch:: 238
epoch:: 239
epoch:: 240
epoch:: 241
epoch:: 242
epoch:: 243
epoch:: 244
epoch:: 245
epoch:: 246
epoch:: 247
epoch:: 248
epoch:: 249
epoch:: 250
epoch:: 251
epoch:: 252
epoch:: 253
epoch:: 254
epoch:: 255
epoch:: 256
epoch:: 257
epoch:: 258
epoch:: 259
epoch:: 260
epoch:: 261
epoch:: 262
epoch:: 263
epoch:: 264
epoch:: 265
epoch:: 266
epoch:: 267
epoch:: 268
epoch:: 269
epoch:: 270
epoch:: 271
epoch:: 272
epoch:: 273
epoch:: 274
epoch:: 275
epoch:: 276
epoch:: 277
epoch:: 278
epoch:: 279
epoch:: 280
epoch:: 281
epoch:: 282
epoch:: 283
epoch:: 284
epoch:: 285
epoch:: 286
epoch:: 287
epoch:: 288
epoch:: 289
epoch:: 290
epoch:: 291
epoch:: 292
epoch:: 293
epoch:: 294
epoch:: 295
epoch:: 296
epoch:: 297
epoch:: 298
epoch:: 299
epoch:: 300
epoch:: 301
epoch:: 302
epoch:: 303
epoch:: 304
epoch:: 305
epoch:: 306
epoch:: 307
epoch:: 308
epoch:: 309
epoch:: 310
epoch:: 311
epoch:: 312
epoch:: 313
epoch:: 314
epoch:: 315
epoch:: 316
epoch:: 317
epoch:: 318
epoch:: 319
epoch:: 320
epoch:: 321
epoch:: 322
epoch:: 323
epoch:: 324
epoch:: 325
epoch:: 326
epoch:: 327
epoch:: 328
epoch:: 329
epoch:: 330
epoch:: 331
epoch:: 332
epoch:: 333
epoch:: 334
epoch:: 335
epoch:: 336
epoch:: 337
epoch:: 338
epoch:: 339
epoch:: 340
epoch:: 341
epoch:: 342
epoch:: 343
epoch:: 344
epoch:: 345
epoch:: 346
epoch:: 347
epoch:: 348
epoch:: 349
epoch:: 350
epoch:: 351
epoch:: 352
epoch:: 353
epoch:: 354
epoch:: 355
epoch:: 356
epoch:: 357
epoch:: 358
epoch:: 359
epoch:: 360
epoch:: 361
epoch:: 362
epoch:: 363
epoch:: 364
epoch:: 365
epoch:: 366
epoch:: 367
epoch:: 368
epoch:: 369
epoch:: 370
epoch:: 371
epoch:: 372
epoch:: 373
epoch:: 374
epoch:: 375
epoch:: 376
epoch:: 377
epoch:: 378
epoch:: 379
epoch:: 380
epoch:: 381
epoch:: 382
epoch:: 383
epoch:: 384
epoch:: 385
epoch:: 386
epoch:: 387
epoch:: 388
epoch:: 389
epoch:: 390
epoch:: 391
epoch:: 392
epoch:: 393
epoch:: 394
epoch:: 395
epoch:: 396
epoch:: 397
epoch:: 398
epoch:: 399
epoch:: 400
epoch:: 401
epoch:: 402
epoch:: 403
epoch:: 404
epoch:: 405
epoch:: 406
epoch:: 407
epoch:: 408
epoch:: 409
epoch:: 410
epoch:: 411
epoch:: 412
epoch:: 413
epoch:: 414
epoch:: 415
epoch:: 416
epoch:: 417
epoch:: 418
epoch:: 419
epoch:: 420
epoch:: 421
epoch:: 422
epoch:: 423
epoch:: 424
epoch:: 425
epoch:: 426
epoch:: 427
epoch:: 428
epoch:: 429
epoch:: 430
epoch:: 431
epoch:: 432
epoch:: 433
epoch:: 434
epoch:: 435
epoch:: 436
epoch:: 437
epoch:: 438
epoch:: 439
epoch:: 440
epoch:: 441
epoch:: 442
epoch:: 443
epoch:: 444
epoch:: 445
epoch:: 446
epoch:: 447
epoch:: 448
epoch:: 449
epoch:: 450
epoch:: 451
epoch:: 452
epoch:: 453
epoch:: 454
epoch:: 455
epoch:: 456
epoch:: 457
epoch:: 458
epoch:: 459
epoch:: 460
epoch:: 461
epoch:: 462
epoch:: 463
epoch:: 464
epoch:: 465
epoch:: 466
epoch:: 467
epoch:: 468
epoch:: 469
epoch:: 470
epoch:: 471
epoch:: 472
epoch:: 473
epoch:: 474
epoch:: 475
epoch:: 476
epoch:: 477
epoch:: 478
epoch:: 479
epoch:: 480
epoch:: 481
epoch:: 482
epoch:: 483
epoch:: 484
epoch:: 485
epoch:: 486
epoch:: 487
epoch:: 488
epoch:: 489
epoch:: 490
epoch:: 491
epoch:: 492
epoch:: 493
epoch:: 494
epoch:: 495
epoch:: 496
epoch:: 497
epoch:: 498
epoch:: 499
training time is 80.93347239494324
overhead:: 0
overhead2:: 2.642077922821045
overhead3:: 0
time_baseline:: 80.93353533744812
curr_diff: 0 tensor(0.0604, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0604, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633300
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 50
max_epoch:: 500
overhead:: 0
overhead2:: 0.08790230751037598
overhead3:: 0.23795557022094727
overhead4:: 10.02542495727539
overhead5:: 0
memory usage:: 5650149376
time_provenance:: 17.413228273391724
curr_diff: 0 tensor(0.0150, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0150, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0720, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0720, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632700
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 50
max_epoch:: 500
overhead:: 0
overhead2:: 0.08379364013671875
overhead3:: 0.2592930793762207
overhead4:: 10.573583364486694
overhead5:: 0
memory usage:: 5630631936
time_provenance:: 18.162679195404053
curr_diff: 0 tensor(0.0149, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0149, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0720, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0720, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633000
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 50
max_epoch:: 500
overhead:: 0
overhead2:: 0.08337950706481934
overhead3:: 0.2682836055755615
overhead4:: 10.63170075416565
overhead5:: 0
memory usage:: 5689147392
time_provenance:: 18.289814472198486
curr_diff: 0 tensor(0.0150, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0150, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0720, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0720, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632800
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 50
max_epoch:: 500
overhead:: 0
overhead2:: 0.15219783782958984
overhead3:: 0.4493825435638428
overhead4:: 18.202735424041748
overhead5:: 0
memory usage:: 5627965440
time_provenance:: 27.68405055999756
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0613, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0613, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633300
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 50
max_epoch:: 500
overhead:: 0
overhead2:: 0.15898489952087402
overhead3:: 0.4542725086212158
overhead4:: 18.619436264038086
overhead5:: 0
memory usage:: 5635588096
time_provenance:: 28.161105394363403
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0613, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0613, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633300
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 50
max_epoch:: 500
overhead:: 0
overhead2:: 0.16073036193847656
overhead3:: 0.46034789085388184
overhead4:: 18.82408595085144
overhead5:: 0
memory usage:: 5671538688
time_provenance:: 28.430801153182983
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0613, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0613, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633300
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 50
max_epoch:: 500
overhead:: 0
overhead2:: 0.38283848762512207
overhead3:: 1.016594409942627
overhead4:: 44.16299510002136
overhead5:: 0
memory usage:: 5628067840
time_provenance:: 59.561256647109985
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0605, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0605, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633200
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 50
max_epoch:: 500
overhead:: 0
overhead2:: 0.3968038558959961
overhead3:: 1.0516762733459473
overhead4:: 44.35161280632019
overhead5:: 0
memory usage:: 5648162816
time_provenance:: 59.84138345718384
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0605, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0605, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633200
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 50
max_epoch:: 500
overhead:: 0
overhead2:: 0.39134740829467773
overhead3:: 1.036703109741211
overhead4:: 43.63761496543884
overhead5:: 0
memory usage:: 5644308480
time_provenance:: 59.13101363182068
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0605, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0605, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633200
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 1 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 50
max_epoch:: 500
overhead:: 0
overhead2:: 0.797947883605957
overhead3:: 2.1117236614227295
overhead4:: 82.5233633518219
overhead5:: 0
memory usage:: 5613359104
time_provenance:: 105.98088312149048
curr_diff: 0 tensor(5.0030e-13, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.0030e-13, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0604, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0604, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633300
repetition 2
python3 benchmark_exp_lr.py 0.001 10000 500 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05] Logistic_regression cifar10_2 2 0.0001 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 2.298910
Train - Epoch 1, Batch: 0, Loss: 2.238674
Train - Epoch 2, Batch: 0, Loss: 2.182799
Train - Epoch 3, Batch: 0, Loss: 2.131937
Train - Epoch 4, Batch: 0, Loss: 2.085867
Train - Epoch 5, Batch: 0, Loss: 2.048148
Train - Epoch 6, Batch: 0, Loss: 2.001485
Train - Epoch 7, Batch: 0, Loss: 1.964231
Train - Epoch 8, Batch: 0, Loss: 1.926645
Train - Epoch 9, Batch: 0, Loss: 1.894711
Train - Epoch 10, Batch: 0, Loss: 1.861650
Train - Epoch 11, Batch: 0, Loss: 1.829987
Train - Epoch 12, Batch: 0, Loss: 1.810937
Train - Epoch 13, Batch: 0, Loss: 1.786767
Train - Epoch 14, Batch: 0, Loss: 1.762008
Train - Epoch 15, Batch: 0, Loss: 1.734315
Train - Epoch 16, Batch: 0, Loss: 1.706808
Train - Epoch 17, Batch: 0, Loss: 1.692880
Train - Epoch 18, Batch: 0, Loss: 1.668533
Train - Epoch 19, Batch: 0, Loss: 1.655717
Train - Epoch 20, Batch: 0, Loss: 1.641006
Train - Epoch 21, Batch: 0, Loss: 1.619518
Train - Epoch 22, Batch: 0, Loss: 1.604581
Train - Epoch 23, Batch: 0, Loss: 1.583797
Train - Epoch 24, Batch: 0, Loss: 1.580591
Train - Epoch 25, Batch: 0, Loss: 1.565222
Train - Epoch 26, Batch: 0, Loss: 1.557314
Train - Epoch 27, Batch: 0, Loss: 1.544636
Train - Epoch 28, Batch: 0, Loss: 1.522560
Train - Epoch 29, Batch: 0, Loss: 1.522525
Train - Epoch 30, Batch: 0, Loss: 1.497842
Train - Epoch 31, Batch: 0, Loss: 1.495436
Train - Epoch 32, Batch: 0, Loss: 1.479320
Train - Epoch 33, Batch: 0, Loss: 1.480580
Train - Epoch 34, Batch: 0, Loss: 1.453174
Train - Epoch 35, Batch: 0, Loss: 1.461050
Train - Epoch 36, Batch: 0, Loss: 1.444882
Train - Epoch 37, Batch: 0, Loss: 1.451343
Train - Epoch 38, Batch: 0, Loss: 1.426687
Train - Epoch 39, Batch: 0, Loss: 1.421413
Train - Epoch 40, Batch: 0, Loss: 1.420196
Train - Epoch 41, Batch: 0, Loss: 1.416531
Train - Epoch 42, Batch: 0, Loss: 1.409441
Train - Epoch 43, Batch: 0, Loss: 1.393388
Train - Epoch 44, Batch: 0, Loss: 1.397993
Train - Epoch 45, Batch: 0, Loss: 1.388780
Train - Epoch 46, Batch: 0, Loss: 1.377729
Train - Epoch 47, Batch: 0, Loss: 1.378245
Train - Epoch 48, Batch: 0, Loss: 1.369161
Train - Epoch 49, Batch: 0, Loss: 1.361777
Train - Epoch 50, Batch: 0, Loss: 1.350859
Train - Epoch 51, Batch: 0, Loss: 1.355231
Train - Epoch 52, Batch: 0, Loss: 1.357245
Train - Epoch 53, Batch: 0, Loss: 1.358017
Train - Epoch 54, Batch: 0, Loss: 1.340599
Train - Epoch 55, Batch: 0, Loss: 1.333936
Train - Epoch 56, Batch: 0, Loss: 1.335652
Train - Epoch 57, Batch: 0, Loss: 1.332211
Train - Epoch 58, Batch: 0, Loss: 1.310779
Train - Epoch 59, Batch: 0, Loss: 1.317702
Train - Epoch 60, Batch: 0, Loss: 1.318315
Train - Epoch 61, Batch: 0, Loss: 1.319063
Train - Epoch 62, Batch: 0, Loss: 1.307981
Train - Epoch 63, Batch: 0, Loss: 1.312490
Train - Epoch 64, Batch: 0, Loss: 1.301978
Train - Epoch 65, Batch: 0, Loss: 1.299903
Train - Epoch 66, Batch: 0, Loss: 1.291460
Train - Epoch 67, Batch: 0, Loss: 1.294818
Train - Epoch 68, Batch: 0, Loss: 1.281998
Train - Epoch 69, Batch: 0, Loss: 1.292336
Train - Epoch 70, Batch: 0, Loss: 1.263963
Train - Epoch 71, Batch: 0, Loss: 1.278168
Train - Epoch 72, Batch: 0, Loss: 1.270194
Train - Epoch 73, Batch: 0, Loss: 1.278035
Train - Epoch 74, Batch: 0, Loss: 1.286581
Train - Epoch 75, Batch: 0, Loss: 1.258703
Train - Epoch 76, Batch: 0, Loss: 1.258481
Train - Epoch 77, Batch: 0, Loss: 1.253979
Train - Epoch 78, Batch: 0, Loss: 1.266175
Train - Epoch 79, Batch: 0, Loss: 1.255265
Train - Epoch 80, Batch: 0, Loss: 1.249384
Train - Epoch 81, Batch: 0, Loss: 1.264030
Train - Epoch 82, Batch: 0, Loss: 1.257867
Train - Epoch 83, Batch: 0, Loss: 1.245625
Train - Epoch 84, Batch: 0, Loss: 1.250580
Train - Epoch 85, Batch: 0, Loss: 1.250077
Train - Epoch 86, Batch: 0, Loss: 1.248342
Train - Epoch 87, Batch: 0, Loss: 1.238352
Train - Epoch 88, Batch: 0, Loss: 1.239874
Train - Epoch 89, Batch: 0, Loss: 1.238402
Train - Epoch 90, Batch: 0, Loss: 1.221671
Train - Epoch 91, Batch: 0, Loss: 1.232226
Train - Epoch 92, Batch: 0, Loss: 1.209131
Train - Epoch 93, Batch: 0, Loss: 1.232417
Train - Epoch 94, Batch: 0, Loss: 1.218490
Train - Epoch 95, Batch: 0, Loss: 1.247051
Train - Epoch 96, Batch: 0, Loss: 1.216231
Train - Epoch 97, Batch: 0, Loss: 1.233504
Train - Epoch 98, Batch: 0, Loss: 1.202263
Train - Epoch 99, Batch: 0, Loss: 1.205674
Train - Epoch 100, Batch: 0, Loss: 1.221379
Train - Epoch 101, Batch: 0, Loss: 1.204387
Train - Epoch 102, Batch: 0, Loss: 1.231479
Train - Epoch 103, Batch: 0, Loss: 1.201281
Train - Epoch 104, Batch: 0, Loss: 1.209252
Train - Epoch 105, Batch: 0, Loss: 1.203342
Train - Epoch 106, Batch: 0, Loss: 1.205615
Train - Epoch 107, Batch: 0, Loss: 1.192799
Train - Epoch 108, Batch: 0, Loss: 1.201253
Train - Epoch 109, Batch: 0, Loss: 1.215713
Train - Epoch 110, Batch: 0, Loss: 1.190291
Train - Epoch 111, Batch: 0, Loss: 1.194158
Train - Epoch 112, Batch: 0, Loss: 1.194565
Train - Epoch 113, Batch: 0, Loss: 1.194750
Train - Epoch 114, Batch: 0, Loss: 1.183113
Train - Epoch 115, Batch: 0, Loss: 1.192213
Train - Epoch 116, Batch: 0, Loss: 1.187786
Train - Epoch 117, Batch: 0, Loss: 1.180469
Train - Epoch 118, Batch: 0, Loss: 1.174561
Train - Epoch 119, Batch: 0, Loss: 1.181174
Train - Epoch 120, Batch: 0, Loss: 1.177767
Train - Epoch 121, Batch: 0, Loss: 1.178991
Train - Epoch 122, Batch: 0, Loss: 1.176726
Train - Epoch 123, Batch: 0, Loss: 1.183382
Train - Epoch 124, Batch: 0, Loss: 1.167974
Train - Epoch 125, Batch: 0, Loss: 1.175879
Train - Epoch 126, Batch: 0, Loss: 1.172616
Train - Epoch 127, Batch: 0, Loss: 1.184284
Train - Epoch 128, Batch: 0, Loss: 1.168895
Train - Epoch 129, Batch: 0, Loss: 1.171859
Train - Epoch 130, Batch: 0, Loss: 1.165945
Train - Epoch 131, Batch: 0, Loss: 1.161500
Train - Epoch 132, Batch: 0, Loss: 1.158893
Train - Epoch 133, Batch: 0, Loss: 1.165183
Train - Epoch 134, Batch: 0, Loss: 1.185911
Train - Epoch 135, Batch: 0, Loss: 1.155005
Train - Epoch 136, Batch: 0, Loss: 1.170272
Train - Epoch 137, Batch: 0, Loss: 1.164530
Train - Epoch 138, Batch: 0, Loss: 1.166170
Train - Epoch 139, Batch: 0, Loss: 1.157388
Train - Epoch 140, Batch: 0, Loss: 1.156427
Train - Epoch 141, Batch: 0, Loss: 1.173691
Train - Epoch 142, Batch: 0, Loss: 1.163916
Train - Epoch 143, Batch: 0, Loss: 1.162075
Train - Epoch 144, Batch: 0, Loss: 1.164076
Train - Epoch 145, Batch: 0, Loss: 1.169536
Train - Epoch 146, Batch: 0, Loss: 1.150573
Train - Epoch 147, Batch: 0, Loss: 1.157251
Train - Epoch 148, Batch: 0, Loss: 1.153207
Train - Epoch 149, Batch: 0, Loss: 1.155479
Train - Epoch 150, Batch: 0, Loss: 1.160167
Train - Epoch 151, Batch: 0, Loss: 1.144112
Train - Epoch 152, Batch: 0, Loss: 1.140682
Train - Epoch 153, Batch: 0, Loss: 1.155679
Train - Epoch 154, Batch: 0, Loss: 1.162716
Train - Epoch 155, Batch: 0, Loss: 1.150777
Train - Epoch 156, Batch: 0, Loss: 1.147593
Train - Epoch 157, Batch: 0, Loss: 1.152122
Train - Epoch 158, Batch: 0, Loss: 1.155645
Train - Epoch 159, Batch: 0, Loss: 1.138414
Train - Epoch 160, Batch: 0, Loss: 1.144315
Train - Epoch 161, Batch: 0, Loss: 1.136549
Train - Epoch 162, Batch: 0, Loss: 1.141010
Train - Epoch 163, Batch: 0, Loss: 1.143685
Train - Epoch 164, Batch: 0, Loss: 1.145970
Train - Epoch 165, Batch: 0, Loss: 1.134991
Train - Epoch 166, Batch: 0, Loss: 1.128955
Train - Epoch 167, Batch: 0, Loss: 1.133093
Train - Epoch 168, Batch: 0, Loss: 1.127901
Train - Epoch 169, Batch: 0, Loss: 1.122495
Train - Epoch 170, Batch: 0, Loss: 1.137780
Train - Epoch 171, Batch: 0, Loss: 1.136667
Train - Epoch 172, Batch: 0, Loss: 1.136319
Train - Epoch 173, Batch: 0, Loss: 1.123074
Train - Epoch 174, Batch: 0, Loss: 1.123705
Train - Epoch 175, Batch: 0, Loss: 1.137933
Train - Epoch 176, Batch: 0, Loss: 1.144259
Train - Epoch 177, Batch: 0, Loss: 1.123100
Train - Epoch 178, Batch: 0, Loss: 1.126472
Train - Epoch 179, Batch: 0, Loss: 1.130366
Train - Epoch 180, Batch: 0, Loss: 1.129107
Train - Epoch 181, Batch: 0, Loss: 1.131626
Train - Epoch 182, Batch: 0, Loss: 1.114507
Train - Epoch 183, Batch: 0, Loss: 1.121987
Train - Epoch 184, Batch: 0, Loss: 1.138473
Train - Epoch 185, Batch: 0, Loss: 1.137615
Train - Epoch 186, Batch: 0, Loss: 1.129342
Train - Epoch 187, Batch: 0, Loss: 1.119866
Train - Epoch 188, Batch: 0, Loss: 1.129220
Train - Epoch 189, Batch: 0, Loss: 1.126660
Train - Epoch 190, Batch: 0, Loss: 1.122670
Train - Epoch 191, Batch: 0, Loss: 1.128874
Train - Epoch 192, Batch: 0, Loss: 1.118983
Train - Epoch 193, Batch: 0, Loss: 1.126325
Train - Epoch 194, Batch: 0, Loss: 1.128439
Train - Epoch 195, Batch: 0, Loss: 1.128557
Train - Epoch 196, Batch: 0, Loss: 1.111912
Train - Epoch 197, Batch: 0, Loss: 1.121148
Train - Epoch 198, Batch: 0, Loss: 1.117822
Train - Epoch 199, Batch: 0, Loss: 1.118469
Train - Epoch 200, Batch: 0, Loss: 1.111300
Train - Epoch 201, Batch: 0, Loss: 1.123260
Train - Epoch 202, Batch: 0, Loss: 1.109036
Train - Epoch 203, Batch: 0, Loss: 1.099530
Train - Epoch 204, Batch: 0, Loss: 1.114998
Train - Epoch 205, Batch: 0, Loss: 1.101551
Train - Epoch 206, Batch: 0, Loss: 1.113064
Train - Epoch 207, Batch: 0, Loss: 1.115316
Train - Epoch 208, Batch: 0, Loss: 1.110650
Train - Epoch 209, Batch: 0, Loss: 1.122746
Train - Epoch 210, Batch: 0, Loss: 1.119706
Train - Epoch 211, Batch: 0, Loss: 1.126806
Train - Epoch 212, Batch: 0, Loss: 1.109419
Train - Epoch 213, Batch: 0, Loss: 1.106139
Train - Epoch 214, Batch: 0, Loss: 1.119027
Train - Epoch 215, Batch: 0, Loss: 1.110856
Train - Epoch 216, Batch: 0, Loss: 1.116051
Train - Epoch 217, Batch: 0, Loss: 1.103621
Train - Epoch 218, Batch: 0, Loss: 1.103457
Train - Epoch 219, Batch: 0, Loss: 1.107933
Train - Epoch 220, Batch: 0, Loss: 1.120091
Train - Epoch 221, Batch: 0, Loss: 1.105117
Train - Epoch 222, Batch: 0, Loss: 1.110256
Train - Epoch 223, Batch: 0, Loss: 1.104923
Train - Epoch 224, Batch: 0, Loss: 1.103850
Train - Epoch 225, Batch: 0, Loss: 1.096313
Train - Epoch 226, Batch: 0, Loss: 1.113860
Train - Epoch 227, Batch: 0, Loss: 1.102223
Train - Epoch 228, Batch: 0, Loss: 1.099221
Train - Epoch 229, Batch: 0, Loss: 1.083472
Train - Epoch 230, Batch: 0, Loss: 1.107319
Train - Epoch 231, Batch: 0, Loss: 1.094537
Train - Epoch 232, Batch: 0, Loss: 1.104912
Train - Epoch 233, Batch: 0, Loss: 1.095345
Train - Epoch 234, Batch: 0, Loss: 1.102289
Train - Epoch 235, Batch: 0, Loss: 1.107715
Train - Epoch 236, Batch: 0, Loss: 1.111192
Train - Epoch 237, Batch: 0, Loss: 1.110700
Train - Epoch 238, Batch: 0, Loss: 1.106373
Train - Epoch 239, Batch: 0, Loss: 1.097344
Train - Epoch 240, Batch: 0, Loss: 1.093311
Train - Epoch 241, Batch: 0, Loss: 1.102315
Train - Epoch 242, Batch: 0, Loss: 1.114160
Train - Epoch 243, Batch: 0, Loss: 1.088526
Train - Epoch 244, Batch: 0, Loss: 1.100440
Train - Epoch 245, Batch: 0, Loss: 1.087589
Train - Epoch 246, Batch: 0, Loss: 1.079856
Train - Epoch 247, Batch: 0, Loss: 1.086433
Train - Epoch 248, Batch: 0, Loss: 1.089481
Train - Epoch 249, Batch: 0, Loss: 1.094706
Train - Epoch 250, Batch: 0, Loss: 1.103375
Train - Epoch 251, Batch: 0, Loss: 1.106671
Train - Epoch 252, Batch: 0, Loss: 1.073123
Train - Epoch 253, Batch: 0, Loss: 1.089063
Train - Epoch 254, Batch: 0, Loss: 1.087861
Train - Epoch 255, Batch: 0, Loss: 1.096947
Train - Epoch 256, Batch: 0, Loss: 1.099538
Train - Epoch 257, Batch: 0, Loss: 1.086372
Train - Epoch 258, Batch: 0, Loss: 1.091092
Train - Epoch 259, Batch: 0, Loss: 1.091128
Train - Epoch 260, Batch: 0, Loss: 1.089627
Train - Epoch 261, Batch: 0, Loss: 1.091203
Train - Epoch 262, Batch: 0, Loss: 1.100888
Train - Epoch 263, Batch: 0, Loss: 1.080660
Train - Epoch 264, Batch: 0, Loss: 1.082442
Train - Epoch 265, Batch: 0, Loss: 1.084333
Train - Epoch 266, Batch: 0, Loss: 1.095200
Train - Epoch 267, Batch: 0, Loss: 1.103370
Train - Epoch 268, Batch: 0, Loss: 1.088304
Train - Epoch 269, Batch: 0, Loss: 1.090847
Train - Epoch 270, Batch: 0, Loss: 1.094742
Train - Epoch 271, Batch: 0, Loss: 1.071428
Train - Epoch 272, Batch: 0, Loss: 1.090586
Train - Epoch 273, Batch: 0, Loss: 1.088328
Train - Epoch 274, Batch: 0, Loss: 1.071312
Train - Epoch 275, Batch: 0, Loss: 1.070115
Train - Epoch 276, Batch: 0, Loss: 1.080581
Train - Epoch 277, Batch: 0, Loss: 1.086865
Train - Epoch 278, Batch: 0, Loss: 1.077880
Train - Epoch 279, Batch: 0, Loss: 1.091167
Train - Epoch 280, Batch: 0, Loss: 1.082978
Train - Epoch 281, Batch: 0, Loss: 1.087379
Train - Epoch 282, Batch: 0, Loss: 1.088239
Train - Epoch 283, Batch: 0, Loss: 1.084855
Train - Epoch 284, Batch: 0, Loss: 1.084736
Train - Epoch 285, Batch: 0, Loss: 1.078636
Train - Epoch 286, Batch: 0, Loss: 1.082426
Train - Epoch 287, Batch: 0, Loss: 1.083292
Train - Epoch 288, Batch: 0, Loss: 1.081368
Train - Epoch 289, Batch: 0, Loss: 1.078566
Train - Epoch 290, Batch: 0, Loss: 1.076442
Train - Epoch 291, Batch: 0, Loss: 1.072951
Train - Epoch 292, Batch: 0, Loss: 1.074910
Train - Epoch 293, Batch: 0, Loss: 1.082519
Train - Epoch 294, Batch: 0, Loss: 1.080150
Train - Epoch 295, Batch: 0, Loss: 1.084949
Train - Epoch 296, Batch: 0, Loss: 1.079636
Train - Epoch 297, Batch: 0, Loss: 1.066702
Train - Epoch 298, Batch: 0, Loss: 1.067488
Train - Epoch 299, Batch: 0, Loss: 1.081682
Train - Epoch 300, Batch: 0, Loss: 1.085700
Train - Epoch 301, Batch: 0, Loss: 1.078614
Train - Epoch 302, Batch: 0, Loss: 1.074596
Train - Epoch 303, Batch: 0, Loss: 1.078025
Train - Epoch 304, Batch: 0, Loss: 1.088106
Train - Epoch 305, Batch: 0, Loss: 1.068442
Train - Epoch 306, Batch: 0, Loss: 1.078689
Train - Epoch 307, Batch: 0, Loss: 1.055186
Train - Epoch 308, Batch: 0, Loss: 1.082682
Train - Epoch 309, Batch: 0, Loss: 1.070393
Train - Epoch 310, Batch: 0, Loss: 1.075862
Train - Epoch 311, Batch: 0, Loss: 1.073658
Train - Epoch 312, Batch: 0, Loss: 1.098298
Train - Epoch 313, Batch: 0, Loss: 1.080872
Train - Epoch 314, Batch: 0, Loss: 1.084194
Train - Epoch 315, Batch: 0, Loss: 1.062658
Train - Epoch 316, Batch: 0, Loss: 1.077972
Train - Epoch 317, Batch: 0, Loss: 1.060411
Train - Epoch 318, Batch: 0, Loss: 1.078174
Train - Epoch 319, Batch: 0, Loss: 1.083481
Train - Epoch 320, Batch: 0, Loss: 1.077661
Train - Epoch 321, Batch: 0, Loss: 1.078599
Train - Epoch 322, Batch: 0, Loss: 1.070530
Train - Epoch 323, Batch: 0, Loss: 1.074627
Train - Epoch 324, Batch: 0, Loss: 1.074553
Train - Epoch 325, Batch: 0, Loss: 1.075686
Train - Epoch 326, Batch: 0, Loss: 1.067304
Train - Epoch 327, Batch: 0, Loss: 1.073293
Train - Epoch 328, Batch: 0, Loss: 1.053521
Train - Epoch 329, Batch: 0, Loss: 1.064702
Train - Epoch 330, Batch: 0, Loss: 1.076010
Train - Epoch 331, Batch: 0, Loss: 1.070513
Train - Epoch 332, Batch: 0, Loss: 1.067513
Train - Epoch 333, Batch: 0, Loss: 1.059067
Train - Epoch 334, Batch: 0, Loss: 1.043908
Train - Epoch 335, Batch: 0, Loss: 1.061099
Train - Epoch 336, Batch: 0, Loss: 1.065295
Train - Epoch 337, Batch: 0, Loss: 1.077859
Train - Epoch 338, Batch: 0, Loss: 1.062853
Train - Epoch 339, Batch: 0, Loss: 1.072210
Train - Epoch 340, Batch: 0, Loss: 1.071545
Train - Epoch 341, Batch: 0, Loss: 1.060380
Train - Epoch 342, Batch: 0, Loss: 1.076669
Train - Epoch 343, Batch: 0, Loss: 1.054536
Train - Epoch 344, Batch: 0, Loss: 1.064603
Train - Epoch 345, Batch: 0, Loss: 1.057899
Train - Epoch 346, Batch: 0, Loss: 1.069791
Train - Epoch 347, Batch: 0, Loss: 1.069420
Train - Epoch 348, Batch: 0, Loss: 1.073312
Train - Epoch 349, Batch: 0, Loss: 1.067859
Train - Epoch 350, Batch: 0, Loss: 1.056009
Train - Epoch 351, Batch: 0, Loss: 1.061142
Train - Epoch 352, Batch: 0, Loss: 1.078877
Train - Epoch 353, Batch: 0, Loss: 1.074745
Train - Epoch 354, Batch: 0, Loss: 1.072986
Train - Epoch 355, Batch: 0, Loss: 1.074986
Train - Epoch 356, Batch: 0, Loss: 1.076634
Train - Epoch 357, Batch: 0, Loss: 1.063421
Train - Epoch 358, Batch: 0, Loss: 1.079180
Train - Epoch 359, Batch: 0, Loss: 1.052195
Train - Epoch 360, Batch: 0, Loss: 1.057220
Train - Epoch 361, Batch: 0, Loss: 1.050743
Train - Epoch 362, Batch: 0, Loss: 1.068411
Train - Epoch 363, Batch: 0, Loss: 1.071569
Train - Epoch 364, Batch: 0, Loss: 1.049997
Train - Epoch 365, Batch: 0, Loss: 1.073997
Train - Epoch 366, Batch: 0, Loss: 1.053182
Train - Epoch 367, Batch: 0, Loss: 1.047183
Train - Epoch 368, Batch: 0, Loss: 1.058110
Train - Epoch 369, Batch: 0, Loss: 1.060727
Train - Epoch 370, Batch: 0, Loss: 1.054395
Train - Epoch 371, Batch: 0, Loss: 1.053213
Train - Epoch 372, Batch: 0, Loss: 1.071689
Train - Epoch 373, Batch: 0, Loss: 1.054853/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 374, Batch: 0, Loss: 1.075942
Train - Epoch 375, Batch: 0, Loss: 1.058163
Train - Epoch 376, Batch: 0, Loss: 1.052719
Train - Epoch 377, Batch: 0, Loss: 1.067764
Train - Epoch 378, Batch: 0, Loss: 1.073617
Train - Epoch 379, Batch: 0, Loss: 1.058459
Train - Epoch 380, Batch: 0, Loss: 1.057217
Train - Epoch 381, Batch: 0, Loss: 1.055942
Train - Epoch 382, Batch: 0, Loss: 1.060349
Train - Epoch 383, Batch: 0, Loss: 1.065322
Train - Epoch 384, Batch: 0, Loss: 1.046632
Train - Epoch 385, Batch: 0, Loss: 1.067707
Train - Epoch 386, Batch: 0, Loss: 1.067839
Train - Epoch 387, Batch: 0, Loss: 1.055547
Train - Epoch 388, Batch: 0, Loss: 1.050165
Train - Epoch 389, Batch: 0, Loss: 1.061222
Train - Epoch 390, Batch: 0, Loss: 1.053674
Train - Epoch 391, Batch: 0, Loss: 1.044924
Train - Epoch 392, Batch: 0, Loss: 1.060824
Train - Epoch 393, Batch: 0, Loss: 1.056871
Train - Epoch 394, Batch: 0, Loss: 1.078405
Train - Epoch 395, Batch: 0, Loss: 1.056722
Train - Epoch 396, Batch: 0, Loss: 1.052934
Train - Epoch 397, Batch: 0, Loss: 1.044361
Train - Epoch 398, Batch: 0, Loss: 1.058686
Train - Epoch 399, Batch: 0, Loss: 1.050448
Train - Epoch 400, Batch: 0, Loss: 1.057385
Train - Epoch 401, Batch: 0, Loss: 1.062636
Train - Epoch 402, Batch: 0, Loss: 1.068644
Train - Epoch 403, Batch: 0, Loss: 1.055883
Train - Epoch 404, Batch: 0, Loss: 1.055999
Train - Epoch 405, Batch: 0, Loss: 1.060860
Train - Epoch 406, Batch: 0, Loss: 1.064429
Train - Epoch 407, Batch: 0, Loss: 1.028899
Train - Epoch 408, Batch: 0, Loss: 1.057649
Train - Epoch 409, Batch: 0, Loss: 1.051036
Train - Epoch 410, Batch: 0, Loss: 1.055017
Train - Epoch 411, Batch: 0, Loss: 1.059893
Train - Epoch 412, Batch: 0, Loss: 1.055991
Train - Epoch 413, Batch: 0, Loss: 1.044927
Train - Epoch 414, Batch: 0, Loss: 1.039932
Train - Epoch 415, Batch: 0, Loss: 1.062992
Train - Epoch 416, Batch: 0, Loss: 1.054176
Train - Epoch 417, Batch: 0, Loss: 1.055949
Train - Epoch 418, Batch: 0, Loss: 1.053959
Train - Epoch 419, Batch: 0, Loss: 1.043579
Train - Epoch 420, Batch: 0, Loss: 1.043369
Train - Epoch 421, Batch: 0, Loss: 1.040395
Train - Epoch 422, Batch: 0, Loss: 1.047336
Train - Epoch 423, Batch: 0, Loss: 1.063335
Train - Epoch 424, Batch: 0, Loss: 1.059418
Train - Epoch 425, Batch: 0, Loss: 1.051434
Train - Epoch 426, Batch: 0, Loss: 1.043444
Train - Epoch 427, Batch: 0, Loss: 1.062881
Train - Epoch 428, Batch: 0, Loss: 1.055424
Train - Epoch 429, Batch: 0, Loss: 1.039092
Train - Epoch 430, Batch: 0, Loss: 1.052947
Train - Epoch 431, Batch: 0, Loss: 1.038832
Train - Epoch 432, Batch: 0, Loss: 1.024911
Train - Epoch 433, Batch: 0, Loss: 1.055213
Train - Epoch 434, Batch: 0, Loss: 1.060260
Train - Epoch 435, Batch: 0, Loss: 1.031110
Train - Epoch 436, Batch: 0, Loss: 1.039835
Train - Epoch 437, Batch: 0, Loss: 1.055734
Train - Epoch 438, Batch: 0, Loss: 1.038227
Train - Epoch 439, Batch: 0, Loss: 1.061322
Train - Epoch 440, Batch: 0, Loss: 1.046633
Train - Epoch 441, Batch: 0, Loss: 1.051049
Train - Epoch 442, Batch: 0, Loss: 1.045666
Train - Epoch 443, Batch: 0, Loss: 1.059651
Train - Epoch 444, Batch: 0, Loss: 1.047886
Train - Epoch 445, Batch: 0, Loss: 1.049755
Train - Epoch 446, Batch: 0, Loss: 1.034032
Train - Epoch 447, Batch: 0, Loss: 1.042708
Train - Epoch 448, Batch: 0, Loss: 1.047419
Train - Epoch 449, Batch: 0, Loss: 1.040445
Train - Epoch 450, Batch: 0, Loss: 1.036001
Train - Epoch 451, Batch: 0, Loss: 1.039194
Train - Epoch 452, Batch: 0, Loss: 1.023904
Train - Epoch 453, Batch: 0, Loss: 1.044967
Train - Epoch 454, Batch: 0, Loss: 1.046394
Train - Epoch 455, Batch: 0, Loss: 1.036803
Train - Epoch 456, Batch: 0, Loss: 1.034894
Train - Epoch 457, Batch: 0, Loss: 1.033091
Train - Epoch 458, Batch: 0, Loss: 1.040076
Train - Epoch 459, Batch: 0, Loss: 1.053673
Train - Epoch 460, Batch: 0, Loss: 1.039346
Train - Epoch 461, Batch: 0, Loss: 1.044279
Train - Epoch 462, Batch: 0, Loss: 1.029047
Train - Epoch 463, Batch: 0, Loss: 1.034609
Train - Epoch 464, Batch: 0, Loss: 1.046234
Train - Epoch 465, Batch: 0, Loss: 1.067260
Train - Epoch 466, Batch: 0, Loss: 1.034639
Train - Epoch 467, Batch: 0, Loss: 1.045108
Train - Epoch 468, Batch: 0, Loss: 1.047332
Train - Epoch 469, Batch: 0, Loss: 1.051837
Train - Epoch 470, Batch: 0, Loss: 1.031039
Train - Epoch 471, Batch: 0, Loss: 1.044361
Train - Epoch 472, Batch: 0, Loss: 1.045160
Train - Epoch 473, Batch: 0, Loss: 1.037690
Train - Epoch 474, Batch: 0, Loss: 1.032095
Train - Epoch 475, Batch: 0, Loss: 1.043070
Train - Epoch 476, Batch: 0, Loss: 1.050921
Train - Epoch 477, Batch: 0, Loss: 1.042422
Train - Epoch 478, Batch: 0, Loss: 1.055402
Train - Epoch 479, Batch: 0, Loss: 1.029208
Train - Epoch 480, Batch: 0, Loss: 1.068505
Train - Epoch 481, Batch: 0, Loss: 1.048970
Train - Epoch 482, Batch: 0, Loss: 1.043201
Train - Epoch 483, Batch: 0, Loss: 1.048806
Train - Epoch 484, Batch: 0, Loss: 1.037775
Train - Epoch 485, Batch: 0, Loss: 1.035403
Train - Epoch 486, Batch: 0, Loss: 1.034379
Train - Epoch 487, Batch: 0, Loss: 1.022570
Train - Epoch 488, Batch: 0, Loss: 1.036703
Train - Epoch 489, Batch: 0, Loss: 1.031117
Train - Epoch 490, Batch: 0, Loss: 1.041009
Train - Epoch 491, Batch: 0, Loss: 1.040600
Train - Epoch 492, Batch: 0, Loss: 1.026892
Train - Epoch 493, Batch: 0, Loss: 1.033337
Train - Epoch 494, Batch: 0, Loss: 1.043300
Train - Epoch 495, Batch: 0, Loss: 1.040899
Train - Epoch 496, Batch: 0, Loss: 1.042324
Train - Epoch 497, Batch: 0, Loss: 1.040426
Train - Epoch 498, Batch: 0, Loss: 1.022571
Train - Epoch 499, Batch: 0, Loss: 1.027622
training_time:: 107.27271294593811
training time full:: 107.27278256416321
provenance prepare time:: 4.76837158203125e-07
here
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632200
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 50
max_epoch:: 500
batch_size:: 10000
epoch:: 0
tensor([    1,     6, 32775, 32776, 32781, 32783,    31,    33, 32802,    43,
        32811,    45,    46, 32815, 32820,    53,    54, 32823,    55, 32826,
        32829, 32831, 32834,    75, 32844, 32856,    88,    94, 32864,    98,
          104, 32874,   107, 32876,   109,   112,   113,   118, 32886, 32893,
        32896,   131,   136, 32908,   144, 32913,   158,   159, 32927,   164])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
epoch:: 60
epoch:: 61
epoch:: 62
epoch:: 63
epoch:: 64
epoch:: 65
epoch:: 66
epoch:: 67
epoch:: 68
epoch:: 69
epoch:: 70
epoch:: 71
epoch:: 72
epoch:: 73
epoch:: 74
epoch:: 75
epoch:: 76
epoch:: 77
epoch:: 78
epoch:: 79
epoch:: 80
epoch:: 81
epoch:: 82
epoch:: 83
epoch:: 84
epoch:: 85
epoch:: 86
epoch:: 87
epoch:: 88
epoch:: 89
epoch:: 90
epoch:: 91
epoch:: 92
epoch:: 93
epoch:: 94
epoch:: 95
epoch:: 96
epoch:: 97
epoch:: 98
epoch:: 99
epoch:: 100
epoch:: 101
epoch:: 102
epoch:: 103
epoch:: 104
epoch:: 105
epoch:: 106
epoch:: 107
epoch:: 108
epoch:: 109
epoch:: 110
epoch:: 111
epoch:: 112
epoch:: 113
epoch:: 114
epoch:: 115
epoch:: 116
epoch:: 117
epoch:: 118
epoch:: 119
epoch:: 120
epoch:: 121
epoch:: 122
epoch:: 123
epoch:: 124
epoch:: 125
epoch:: 126
epoch:: 127
epoch:: 128
epoch:: 129
epoch:: 130
epoch:: 131
epoch:: 132
epoch:: 133
epoch:: 134
epoch:: 135
epoch:: 136
epoch:: 137
epoch:: 138
epoch:: 139
epoch:: 140
epoch:: 141
epoch:: 142
epoch:: 143
epoch:: 144
epoch:: 145
epoch:: 146
epoch:: 147
epoch:: 148
epoch:: 149
epoch:: 150
epoch:: 151
epoch:: 152
epoch:: 153
epoch:: 154
epoch:: 155
epoch:: 156
epoch:: 157
epoch:: 158
epoch:: 159
epoch:: 160
epoch:: 161
epoch:: 162
epoch:: 163
epoch:: 164
epoch:: 165
epoch:: 166
epoch:: 167
epoch:: 168
epoch:: 169
epoch:: 170
epoch:: 171
epoch:: 172
epoch:: 173
epoch:: 174
epoch:: 175
epoch:: 176
epoch:: 177
epoch:: 178
epoch:: 179
epoch:: 180
epoch:: 181
epoch:: 182
epoch:: 183
epoch:: 184
epoch:: 185
epoch:: 186
epoch:: 187
epoch:: 188
epoch:: 189
epoch:: 190
epoch:: 191
epoch:: 192
epoch:: 193
epoch:: 194
epoch:: 195
epoch:: 196
epoch:: 197
epoch:: 198
epoch:: 199
epoch:: 200
epoch:: 201
epoch:: 202
epoch:: 203
epoch:: 204
epoch:: 205
epoch:: 206
epoch:: 207
epoch:: 208
epoch:: 209
epoch:: 210
epoch:: 211
epoch:: 212
epoch:: 213
epoch:: 214
epoch:: 215
epoch:: 216
epoch:: 217
epoch:: 218
epoch:: 219
epoch:: 220
epoch:: 221
epoch:: 222
epoch:: 223
epoch:: 224
epoch:: 225
epoch:: 226
epoch:: 227
epoch:: 228
epoch:: 229
epoch:: 230
epoch:: 231
epoch:: 232
epoch:: 233
epoch:: 234
epoch:: 235
epoch:: 236
epoch:: 237
epoch:: 238
epoch:: 239
epoch:: 240
epoch:: 241
epoch:: 242
epoch:: 243
epoch:: 244
epoch:: 245
epoch:: 246
epoch:: 247
epoch:: 248
epoch:: 249
epoch:: 250
epoch:: 251
epoch:: 252
epoch:: 253
epoch:: 254
epoch:: 255
epoch:: 256
epoch:: 257
epoch:: 258
epoch:: 259
epoch:: 260
epoch:: 261
epoch:: 262
epoch:: 263
epoch:: 264
epoch:: 265
epoch:: 266
epoch:: 267
epoch:: 268
epoch:: 269
epoch:: 270
epoch:: 271
epoch:: 272
epoch:: 273
epoch:: 274
epoch:: 275
epoch:: 276
epoch:: 277
epoch:: 278
epoch:: 279
epoch:: 280
epoch:: 281
epoch:: 282
epoch:: 283
epoch:: 284
epoch:: 285
epoch:: 286
epoch:: 287
epoch:: 288
epoch:: 289
epoch:: 290
epoch:: 291
epoch:: 292
epoch:: 293
epoch:: 294
epoch:: 295
epoch:: 296
epoch:: 297
epoch:: 298
epoch:: 299
epoch:: 300
epoch:: 301
epoch:: 302
epoch:: 303
epoch:: 304
epoch:: 305
epoch:: 306
epoch:: 307
epoch:: 308
epoch:: 309
epoch:: 310
epoch:: 311
epoch:: 312
epoch:: 313
epoch:: 314
epoch:: 315
epoch:: 316
epoch:: 317
epoch:: 318
epoch:: 319
epoch:: 320
epoch:: 321
epoch:: 322
epoch:: 323
epoch:: 324
epoch:: 325
epoch:: 326
epoch:: 327
epoch:: 328
epoch:: 329
epoch:: 330
epoch:: 331
epoch:: 332
epoch:: 333
epoch:: 334
epoch:: 335
epoch:: 336
epoch:: 337
epoch:: 338
epoch:: 339
epoch:: 340
epoch:: 341
epoch:: 342
epoch:: 343
epoch:: 344
epoch:: 345
epoch:: 346
epoch:: 347
epoch:: 348
epoch:: 349
epoch:: 350
epoch:: 351
epoch:: 352
epoch:: 353
epoch:: 354
epoch:: 355
epoch:: 356
epoch:: 357
epoch:: 358
epoch:: 359
epoch:: 360
epoch:: 361
epoch:: 362
epoch:: 363
epoch:: 364
epoch:: 365
epoch:: 366
epoch:: 367
epoch:: 368
epoch:: 369
epoch:: 370
epoch:: 371
epoch:: 372
epoch:: 373
epoch:: 374
epoch:: 375
epoch:: 376
epoch:: 377
epoch:: 378
epoch:: 379
epoch:: 380
epoch:: 381
epoch:: 382
epoch:: 383
epoch:: 384
epoch:: 385
epoch:: 386
epoch:: 387
epoch:: 388
epoch:: 389
epoch:: 390
epoch:: 391
epoch:: 392
epoch:: 393
epoch:: 394
epoch:: 395
epoch:: 396
epoch:: 397
epoch:: 398
epoch:: 399
epoch:: 400
epoch:: 401
epoch:: 402
epoch:: 403
epoch:: 404
epoch:: 405
epoch:: 406
epoch:: 407
epoch:: 408
epoch:: 409
epoch:: 410
epoch:: 411
epoch:: 412
epoch:: 413
epoch:: 414
epoch:: 415
epoch:: 416
epoch:: 417
epoch:: 418
epoch:: 419
epoch:: 420
epoch:: 421
epoch:: 422
epoch:: 423
epoch:: 424
epoch:: 425
epoch:: 426
epoch:: 427
epoch:: 428
epoch:: 429
epoch:: 430
epoch:: 431
epoch:: 432
epoch:: 433
epoch:: 434
epoch:: 435
epoch:: 436
epoch:: 437
epoch:: 438
epoch:: 439
epoch:: 440
epoch:: 441
epoch:: 442
epoch:: 443
epoch:: 444
epoch:: 445
epoch:: 446
epoch:: 447
epoch:: 448
epoch:: 449
epoch:: 450
epoch:: 451
epoch:: 452
epoch:: 453
epoch:: 454
epoch:: 455
epoch:: 456
epoch:: 457
epoch:: 458
epoch:: 459
epoch:: 460
epoch:: 461
epoch:: 462
epoch:: 463
epoch:: 464
epoch:: 465
epoch:: 466
epoch:: 467
epoch:: 468
epoch:: 469
epoch:: 470
epoch:: 471
epoch:: 472
epoch:: 473
epoch:: 474
epoch:: 475
epoch:: 476
epoch:: 477
epoch:: 478
epoch:: 479
epoch:: 480
epoch:: 481
epoch:: 482
epoch:: 483
epoch:: 484
epoch:: 485
epoch:: 486
epoch:: 487
epoch:: 488
epoch:: 489
epoch:: 490
epoch:: 491
epoch:: 492
epoch:: 493
epoch:: 494
epoch:: 495
epoch:: 496
epoch:: 497
epoch:: 498
epoch:: 499
training time is 80.78262400627136
overhead:: 0
overhead2:: 2.690605401992798
overhead3:: 0
time_baseline:: 80.78265690803528
curr_diff: 0 tensor(0.0603, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0603, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632400
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 50
max_epoch:: 500
overhead:: 0
overhead2:: 0.08019757270812988
overhead3:: 0.2535407543182373
overhead4:: 10.101873636245728
overhead5:: 0
memory usage:: 5634584576
time_provenance:: 17.66659116744995
curr_diff: 0 tensor(0.0150, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0150, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0719, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0719, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.631800
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 50
max_epoch:: 500
overhead:: 0
overhead2:: 0.08642935752868652
overhead3:: 0.258533239364624
overhead4:: 10.323471307754517
overhead5:: 0
memory usage:: 5642457088
time_provenance:: 17.89444875717163
curr_diff: 0 tensor(0.0149, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0149, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0719, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0719, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.631800
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 50
max_epoch:: 500
overhead:: 0
overhead2:: 0.08449363708496094
overhead3:: 0.2669801712036133
overhead4:: 10.376809120178223
overhead5:: 0
memory usage:: 5645660160
time_provenance:: 18.018925189971924
curr_diff: 0 tensor(0.0149, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0149, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0718, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0718, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632000
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 50
max_epoch:: 500
overhead:: 0
overhead2:: 0.1619572639465332
overhead3:: 0.44632720947265625
overhead4:: 18.564146041870117
overhead5:: 0
memory usage:: 5646979072
time_provenance:: 28.054516315460205
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0612, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0612, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632500
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 50
max_epoch:: 500
overhead:: 0
overhead2:: 0.1641979217529297
overhead3:: 0.4666461944580078
overhead4:: 18.673582077026367
overhead5:: 0
memory usage:: 5644480512
time_provenance:: 28.353925704956055
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0612, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0612, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632500
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 50
max_epoch:: 500
overhead:: 0
overhead2:: 0.15852785110473633
overhead3:: 0.4613819122314453
overhead4:: 18.60396718978882
overhead5:: 0
memory usage:: 5627289600
time_provenance:: 28.18517565727234
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0613, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0613, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632300
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 50
max_epoch:: 500
overhead:: 0
overhead2:: 0.37797117233276367
overhead3:: 1.0238072872161865
overhead4:: 42.65554857254028
overhead5:: 0
memory usage:: 5631135744
time_provenance:: 58.079543113708496
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0604, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0604, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632400
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 50
max_epoch:: 500
overhead:: 0
overhead2:: 0.3922078609466553
overhead3:: 1.041184663772583
overhead4:: 44.54371500015259
overhead5:: 0
memory usage:: 5626060800
time_provenance:: 60.03505825996399
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0604, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0604, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632400
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 50
max_epoch:: 500
overhead:: 0
overhead2:: 0.390836238861084
overhead3:: 1.0317213535308838
overhead4:: 44.17030477523804
overhead5:: 0
memory usage:: 5694472192
time_provenance:: 59.63831639289856
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0604, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0604, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632400
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 2 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 50
max_epoch:: 500
overhead:: 0
overhead2:: 0.7836694717407227
overhead3:: 2.0112485885620117
overhead4:: 82.62244272232056
overhead5:: 0
memory usage:: 5626417152
time_provenance:: 106.09774160385132
curr_diff: 0 tensor(5.0199e-13, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.0199e-13, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0603, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0603, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632400
repetition 3
python3 benchmark_exp_lr.py 0.001 10000 500 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05] Logistic_regression cifar10_2 3 0.0001 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 2.311665
Train - Epoch 1, Batch: 0, Loss: 2.248570
Train - Epoch 2, Batch: 0, Loss: 2.193072
Train - Epoch 3, Batch: 0, Loss: 2.141515
Train - Epoch 4, Batch: 0, Loss: 2.095435
Train - Epoch 5, Batch: 0, Loss: 2.049671
Train - Epoch 6, Batch: 0, Loss: 2.007490
Train - Epoch 7, Batch: 0, Loss: 1.972096
Train - Epoch 8, Batch: 0, Loss: 1.936041
Train - Epoch 9, Batch: 0, Loss: 1.902653
Train - Epoch 10, Batch: 0, Loss: 1.871335
Train - Epoch 11, Batch: 0, Loss: 1.841409
Train - Epoch 12, Batch: 0, Loss: 1.814133
Train - Epoch 13, Batch: 0, Loss: 1.788343
Train - Epoch 14, Batch: 0, Loss: 1.754598
Train - Epoch 15, Batch: 0, Loss: 1.737391
Train - Epoch 16, Batch: 0, Loss: 1.720297
Train - Epoch 17, Batch: 0, Loss: 1.692580
Train - Epoch 18, Batch: 0, Loss: 1.675206
Train - Epoch 19, Batch: 0, Loss: 1.662418
Train - Epoch 20, Batch: 0, Loss: 1.644430
Train - Epoch 21, Batch: 0, Loss: 1.621714
Train - Epoch 22, Batch: 0, Loss: 1.617472
Train - Epoch 23, Batch: 0, Loss: 1.591791
Train - Epoch 24, Batch: 0, Loss: 1.581026
Train - Epoch 25, Batch: 0, Loss: 1.569335
Train - Epoch 26, Batch: 0, Loss: 1.545118
Train - Epoch 27, Batch: 0, Loss: 1.546561
Train - Epoch 28, Batch: 0, Loss: 1.531400
Train - Epoch 29, Batch: 0, Loss: 1.527755
Train - Epoch 30, Batch: 0, Loss: 1.508712
Train - Epoch 31, Batch: 0, Loss: 1.502281
Train - Epoch 32, Batch: 0, Loss: 1.489170
Train - Epoch 33, Batch: 0, Loss: 1.479781
Train - Epoch 34, Batch: 0, Loss: 1.484897
Train - Epoch 35, Batch: 0, Loss: 1.458514
Train - Epoch 36, Batch: 0, Loss: 1.460540
Train - Epoch 37, Batch: 0, Loss: 1.443293
Train - Epoch 38, Batch: 0, Loss: 1.440333
Train - Epoch 39, Batch: 0, Loss: 1.434567
Train - Epoch 40, Batch: 0, Loss: 1.419736
Train - Epoch 41, Batch: 0, Loss: 1.419632
Train - Epoch 42, Batch: 0, Loss: 1.401479
Train - Epoch 43, Batch: 0, Loss: 1.409063
Train - Epoch 44, Batch: 0, Loss: 1.406580
Train - Epoch 45, Batch: 0, Loss: 1.385192
Train - Epoch 46, Batch: 0, Loss: 1.381692
Train - Epoch 47, Batch: 0, Loss: 1.372510
Train - Epoch 48, Batch: 0, Loss: 1.376893
Train - Epoch 49, Batch: 0, Loss: 1.365057
Train - Epoch 50, Batch: 0, Loss: 1.357593
Train - Epoch 51, Batch: 0, Loss: 1.356559
Train - Epoch 52, Batch: 0, Loss: 1.350830
Train - Epoch 53, Batch: 0, Loss: 1.370344
Train - Epoch 54, Batch: 0, Loss: 1.339665
Train - Epoch 55, Batch: 0, Loss: 1.344955
Train - Epoch 56, Batch: 0, Loss: 1.336551
Train - Epoch 57, Batch: 0, Loss: 1.325343
Train - Epoch 58, Batch: 0, Loss: 1.323761
Train - Epoch 59, Batch: 0, Loss: 1.318675
Train - Epoch 60, Batch: 0, Loss: 1.313732
Train - Epoch 61, Batch: 0, Loss: 1.309527
Train - Epoch 62, Batch: 0, Loss: 1.315619
Train - Epoch 63, Batch: 0, Loss: 1.306039
Train - Epoch 64, Batch: 0, Loss: 1.303059
Train - Epoch 65, Batch: 0, Loss: 1.307402
Train - Epoch 66, Batch: 0, Loss: 1.291538
Train - Epoch 67, Batch: 0, Loss: 1.292953
Train - Epoch 68, Batch: 0, Loss: 1.286998
Train - Epoch 69, Batch: 0, Loss: 1.290181
Train - Epoch 70, Batch: 0, Loss: 1.289875
Train - Epoch 71, Batch: 0, Loss: 1.270748
Train - Epoch 72, Batch: 0, Loss: 1.269129
Train - Epoch 73, Batch: 0, Loss: 1.260948
Train - Epoch 74, Batch: 0, Loss: 1.269946
Train - Epoch 75, Batch: 0, Loss: 1.274421
Train - Epoch 76, Batch: 0, Loss: 1.272485
Train - Epoch 77, Batch: 0, Loss: 1.267009
Train - Epoch 78, Batch: 0, Loss: 1.266785
Train - Epoch 79, Batch: 0, Loss: 1.271386
Train - Epoch 80, Batch: 0, Loss: 1.252010
Train - Epoch 81, Batch: 0, Loss: 1.244714
Train - Epoch 82, Batch: 0, Loss: 1.253321
Train - Epoch 83, Batch: 0, Loss: 1.243691
Train - Epoch 84, Batch: 0, Loss: 1.244202
Train - Epoch 85, Batch: 0, Loss: 1.251229
Train - Epoch 86, Batch: 0, Loss: 1.256133
Train - Epoch 87, Batch: 0, Loss: 1.246877
Train - Epoch 88, Batch: 0, Loss: 1.229850
Train - Epoch 89, Batch: 0, Loss: 1.238932
Train - Epoch 90, Batch: 0, Loss: 1.230968
Train - Epoch 91, Batch: 0, Loss: 1.230262
Train - Epoch 92, Batch: 0, Loss: 1.241046
Train - Epoch 93, Batch: 0, Loss: 1.218439
Train - Epoch 94, Batch: 0, Loss: 1.224832
Train - Epoch 95, Batch: 0, Loss: 1.241451
Train - Epoch 96, Batch: 0, Loss: 1.233547
Train - Epoch 97, Batch: 0, Loss: 1.220688
Train - Epoch 98, Batch: 0, Loss: 1.223960
Train - Epoch 99, Batch: 0, Loss: 1.227128
Train - Epoch 100, Batch: 0, Loss: 1.207980
Train - Epoch 101, Batch: 0, Loss: 1.200302
Train - Epoch 102, Batch: 0, Loss: 1.215444
Train - Epoch 103, Batch: 0, Loss: 1.212145
Train - Epoch 104, Batch: 0, Loss: 1.203351
Train - Epoch 105, Batch: 0, Loss: 1.196314
Train - Epoch 106, Batch: 0, Loss: 1.213728
Train - Epoch 107, Batch: 0, Loss: 1.206447
Train - Epoch 108, Batch: 0, Loss: 1.204173
Train - Epoch 109, Batch: 0, Loss: 1.189458
Train - Epoch 110, Batch: 0, Loss: 1.198644
Train - Epoch 111, Batch: 0, Loss: 1.198363
Train - Epoch 112, Batch: 0, Loss: 1.188096
Train - Epoch 113, Batch: 0, Loss: 1.188489
Train - Epoch 114, Batch: 0, Loss: 1.193695
Train - Epoch 115, Batch: 0, Loss: 1.197357
Train - Epoch 116, Batch: 0, Loss: 1.182459
Train - Epoch 117, Batch: 0, Loss: 1.193427
Train - Epoch 118, Batch: 0, Loss: 1.188058
Train - Epoch 119, Batch: 0, Loss: 1.171370
Train - Epoch 120, Batch: 0, Loss: 1.188058
Train - Epoch 121, Batch: 0, Loss: 1.192843
Train - Epoch 122, Batch: 0, Loss: 1.196901
Train - Epoch 123, Batch: 0, Loss: 1.189374
Train - Epoch 124, Batch: 0, Loss: 1.173034
Train - Epoch 125, Batch: 0, Loss: 1.187072
Train - Epoch 126, Batch: 0, Loss: 1.186538
Train - Epoch 127, Batch: 0, Loss: 1.196582
Train - Epoch 128, Batch: 0, Loss: 1.185893
Train - Epoch 129, Batch: 0, Loss: 1.158923
Train - Epoch 130, Batch: 0, Loss: 1.162749
Train - Epoch 131, Batch: 0, Loss: 1.166354
Train - Epoch 132, Batch: 0, Loss: 1.178835
Train - Epoch 133, Batch: 0, Loss: 1.166095
Train - Epoch 134, Batch: 0, Loss: 1.158028
Train - Epoch 135, Batch: 0, Loss: 1.169559
Train - Epoch 136, Batch: 0, Loss: 1.161079
Train - Epoch 137, Batch: 0, Loss: 1.173284
Train - Epoch 138, Batch: 0, Loss: 1.160270
Train - Epoch 139, Batch: 0, Loss: 1.154519
Train - Epoch 140, Batch: 0, Loss: 1.169903
Train - Epoch 141, Batch: 0, Loss: 1.174916
Train - Epoch 142, Batch: 0, Loss: 1.153547
Train - Epoch 143, Batch: 0, Loss: 1.163069
Train - Epoch 144, Batch: 0, Loss: 1.155377
Train - Epoch 145, Batch: 0, Loss: 1.165191
Train - Epoch 146, Batch: 0, Loss: 1.163268
Train - Epoch 147, Batch: 0, Loss: 1.153264
Train - Epoch 148, Batch: 0, Loss: 1.163714
Train - Epoch 149, Batch: 0, Loss: 1.155704
Train - Epoch 150, Batch: 0, Loss: 1.152577
Train - Epoch 151, Batch: 0, Loss: 1.158066
Train - Epoch 152, Batch: 0, Loss: 1.142745
Train - Epoch 153, Batch: 0, Loss: 1.143909
Train - Epoch 154, Batch: 0, Loss: 1.157098
Train - Epoch 155, Batch: 0, Loss: 1.155389
Train - Epoch 156, Batch: 0, Loss: 1.154215
Train - Epoch 157, Batch: 0, Loss: 1.149132
Train - Epoch 158, Batch: 0, Loss: 1.140931
Train - Epoch 159, Batch: 0, Loss: 1.144305
Train - Epoch 160, Batch: 0, Loss: 1.143553
Train - Epoch 161, Batch: 0, Loss: 1.139066
Train - Epoch 162, Batch: 0, Loss: 1.139839
Train - Epoch 163, Batch: 0, Loss: 1.144720
Train - Epoch 164, Batch: 0, Loss: 1.155918
Train - Epoch 165, Batch: 0, Loss: 1.145576
Train - Epoch 166, Batch: 0, Loss: 1.149509
Train - Epoch 167, Batch: 0, Loss: 1.144304
Train - Epoch 168, Batch: 0, Loss: 1.145794
Train - Epoch 169, Batch: 0, Loss: 1.136080
Train - Epoch 170, Batch: 0, Loss: 1.147794
Train - Epoch 171, Batch: 0, Loss: 1.139502
Train - Epoch 172, Batch: 0, Loss: 1.130551
Train - Epoch 173, Batch: 0, Loss: 1.143375
Train - Epoch 174, Batch: 0, Loss: 1.133390
Train - Epoch 175, Batch: 0, Loss: 1.133614
Train - Epoch 176, Batch: 0, Loss: 1.126424
Train - Epoch 177, Batch: 0, Loss: 1.118441
Train - Epoch 178, Batch: 0, Loss: 1.127151
Train - Epoch 179, Batch: 0, Loss: 1.123098
Train - Epoch 180, Batch: 0, Loss: 1.131619
Train - Epoch 181, Batch: 0, Loss: 1.113492
Train - Epoch 182, Batch: 0, Loss: 1.119070
Train - Epoch 183, Batch: 0, Loss: 1.134214
Train - Epoch 184, Batch: 0, Loss: 1.139936
Train - Epoch 185, Batch: 0, Loss: 1.110819
Train - Epoch 186, Batch: 0, Loss: 1.123689
Train - Epoch 187, Batch: 0, Loss: 1.126270
Train - Epoch 188, Batch: 0, Loss: 1.116239
Train - Epoch 189, Batch: 0, Loss: 1.134497
Train - Epoch 190, Batch: 0, Loss: 1.123548
Train - Epoch 191, Batch: 0, Loss: 1.123907
Train - Epoch 192, Batch: 0, Loss: 1.125742
Train - Epoch 193, Batch: 0, Loss: 1.127336
Train - Epoch 194, Batch: 0, Loss: 1.126701
Train - Epoch 195, Batch: 0, Loss: 1.126378
Train - Epoch 196, Batch: 0, Loss: 1.125223
Train - Epoch 197, Batch: 0, Loss: 1.113620
Train - Epoch 198, Batch: 0, Loss: 1.124464
Train - Epoch 199, Batch: 0, Loss: 1.118724
Train - Epoch 200, Batch: 0, Loss: 1.113520
Train - Epoch 201, Batch: 0, Loss: 1.112173
Train - Epoch 202, Batch: 0, Loss: 1.117130
Train - Epoch 203, Batch: 0, Loss: 1.130439
Train - Epoch 204, Batch: 0, Loss: 1.126389
Train - Epoch 205, Batch: 0, Loss: 1.108502
Train - Epoch 206, Batch: 0, Loss: 1.129838
Train - Epoch 207, Batch: 0, Loss: 1.117411
Train - Epoch 208, Batch: 0, Loss: 1.118833
Train - Epoch 209, Batch: 0, Loss: 1.111039
Train - Epoch 210, Batch: 0, Loss: 1.115457
Train - Epoch 211, Batch: 0, Loss: 1.108215
Train - Epoch 212, Batch: 0, Loss: 1.123155
Train - Epoch 213, Batch: 0, Loss: 1.109789
Train - Epoch 214, Batch: 0, Loss: 1.111433
Train - Epoch 215, Batch: 0, Loss: 1.122939
Train - Epoch 216, Batch: 0, Loss: 1.111500
Train - Epoch 217, Batch: 0, Loss: 1.105850
Train - Epoch 218, Batch: 0, Loss: 1.113839
Train - Epoch 219, Batch: 0, Loss: 1.114176
Train - Epoch 220, Batch: 0, Loss: 1.111753
Train - Epoch 221, Batch: 0, Loss: 1.106054
Train - Epoch 222, Batch: 0, Loss: 1.096388
Train - Epoch 223, Batch: 0, Loss: 1.108928
Train - Epoch 224, Batch: 0, Loss: 1.101397
Train - Epoch 225, Batch: 0, Loss: 1.110374
Train - Epoch 226, Batch: 0, Loss: 1.109848
Train - Epoch 227, Batch: 0, Loss: 1.111358
Train - Epoch 228, Batch: 0, Loss: 1.098708
Train - Epoch 229, Batch: 0, Loss: 1.111726
Train - Epoch 230, Batch: 0, Loss: 1.091702
Train - Epoch 231, Batch: 0, Loss: 1.090688
Train - Epoch 232, Batch: 0, Loss: 1.096819
Train - Epoch 233, Batch: 0, Loss: 1.097703
Train - Epoch 234, Batch: 0, Loss: 1.090565
Train - Epoch 235, Batch: 0, Loss: 1.098746
Train - Epoch 236, Batch: 0, Loss: 1.099240
Train - Epoch 237, Batch: 0, Loss: 1.106249
Train - Epoch 238, Batch: 0, Loss: 1.097986
Train - Epoch 239, Batch: 0, Loss: 1.097656
Train - Epoch 240, Batch: 0, Loss: 1.111647
Train - Epoch 241, Batch: 0, Loss: 1.100104
Train - Epoch 242, Batch: 0, Loss: 1.094536
Train - Epoch 243, Batch: 0, Loss: 1.116333
Train - Epoch 244, Batch: 0, Loss: 1.096659
Train - Epoch 245, Batch: 0, Loss: 1.087366
Train - Epoch 246, Batch: 0, Loss: 1.094099
Train - Epoch 247, Batch: 0, Loss: 1.090814
Train - Epoch 248, Batch: 0, Loss: 1.089744
Train - Epoch 249, Batch: 0, Loss: 1.084218
Train - Epoch 250, Batch: 0, Loss: 1.085565
Train - Epoch 251, Batch: 0, Loss: 1.106780
Train - Epoch 252, Batch: 0, Loss: 1.085926
Train - Epoch 253, Batch: 0, Loss: 1.100908
Train - Epoch 254, Batch: 0, Loss: 1.092872
Train - Epoch 255, Batch: 0, Loss: 1.107276
Train - Epoch 256, Batch: 0, Loss: 1.086149
Train - Epoch 257, Batch: 0, Loss: 1.085800
Train - Epoch 258, Batch: 0, Loss: 1.081573
Train - Epoch 259, Batch: 0, Loss: 1.082734
Train - Epoch 260, Batch: 0, Loss: 1.093723
Train - Epoch 261, Batch: 0, Loss: 1.086113
Train - Epoch 262, Batch: 0, Loss: 1.088763
Train - Epoch 263, Batch: 0, Loss: 1.085070
Train - Epoch 264, Batch: 0, Loss: 1.073156
Train - Epoch 265, Batch: 0, Loss: 1.081640
Train - Epoch 266, Batch: 0, Loss: 1.084560
Train - Epoch 267, Batch: 0, Loss: 1.091228
Train - Epoch 268, Batch: 0, Loss: 1.088100
Train - Epoch 269, Batch: 0, Loss: 1.080630
Train - Epoch 270, Batch: 0, Loss: 1.074289
Train - Epoch 271, Batch: 0, Loss: 1.087505
Train - Epoch 272, Batch: 0, Loss: 1.088849
Train - Epoch 273, Batch: 0, Loss: 1.092363
Train - Epoch 274, Batch: 0, Loss: 1.094285
Train - Epoch 275, Batch: 0, Loss: 1.079453
Train - Epoch 276, Batch: 0, Loss: 1.080029
Train - Epoch 277, Batch: 0, Loss: 1.081170
Train - Epoch 278, Batch: 0, Loss: 1.066428
Train - Epoch 279, Batch: 0, Loss: 1.092415
Train - Epoch 280, Batch: 0, Loss: 1.081239
Train - Epoch 281, Batch: 0, Loss: 1.091492
Train - Epoch 282, Batch: 0, Loss: 1.087182
Train - Epoch 283, Batch: 0, Loss: 1.104430
Train - Epoch 284, Batch: 0, Loss: 1.085386
Train - Epoch 285, Batch: 0, Loss: 1.072249
Train - Epoch 286, Batch: 0, Loss: 1.091187
Train - Epoch 287, Batch: 0, Loss: 1.089868
Train - Epoch 288, Batch: 0, Loss: 1.074283
Train - Epoch 289, Batch: 0, Loss: 1.086884
Train - Epoch 290, Batch: 0, Loss: 1.078615
Train - Epoch 291, Batch: 0, Loss: 1.083339
Train - Epoch 292, Batch: 0, Loss: 1.061558
Train - Epoch 293, Batch: 0, Loss: 1.083459
Train - Epoch 294, Batch: 0, Loss: 1.075044
Train - Epoch 295, Batch: 0, Loss: 1.078804
Train - Epoch 296, Batch: 0, Loss: 1.080208
Train - Epoch 297, Batch: 0, Loss: 1.075800
Train - Epoch 298, Batch: 0, Loss: 1.068458
Train - Epoch 299, Batch: 0, Loss: 1.072728
Train - Epoch 300, Batch: 0, Loss: 1.077468
Train - Epoch 301, Batch: 0, Loss: 1.092110
Train - Epoch 302, Batch: 0, Loss: 1.080004
Train - Epoch 303, Batch: 0, Loss: 1.078208
Train - Epoch 304, Batch: 0, Loss: 1.081907
Train - Epoch 305, Batch: 0, Loss: 1.067112
Train - Epoch 306, Batch: 0, Loss: 1.072310
Train - Epoch 307, Batch: 0, Loss: 1.082168
Train - Epoch 308, Batch: 0, Loss: 1.080060
Train - Epoch 309, Batch: 0, Loss: 1.062732
Train - Epoch 310, Batch: 0, Loss: 1.075623
Train - Epoch 311, Batch: 0, Loss: 1.062752
Train - Epoch 312, Batch: 0, Loss: 1.062985
Train - Epoch 313, Batch: 0, Loss: 1.067215
Train - Epoch 314, Batch: 0, Loss: 1.080578
Train - Epoch 315, Batch: 0, Loss: 1.081441
Train - Epoch 316, Batch: 0, Loss: 1.071475
Train - Epoch 317, Batch: 0, Loss: 1.076494
Train - Epoch 318, Batch: 0, Loss: 1.085199
Train - Epoch 319, Batch: 0, Loss: 1.078128
Train - Epoch 320, Batch: 0, Loss: 1.066769
Train - Epoch 321, Batch: 0, Loss: 1.083967
Train - Epoch 322, Batch: 0, Loss: 1.058592
Train - Epoch 323, Batch: 0, Loss: 1.072909
Train - Epoch 324, Batch: 0, Loss: 1.072102
Train - Epoch 325, Batch: 0, Loss: 1.074611
Train - Epoch 326, Batch: 0, Loss: 1.054363
Train - Epoch 327, Batch: 0, Loss: 1.064330
Train - Epoch 328, Batch: 0, Loss: 1.067977
Train - Epoch 329, Batch: 0, Loss: 1.062589
Train - Epoch 330, Batch: 0, Loss: 1.069838
Train - Epoch 331, Batch: 0, Loss: 1.081985
Train - Epoch 332, Batch: 0, Loss: 1.064956
Train - Epoch 333, Batch: 0, Loss: 1.081943
Train - Epoch 334, Batch: 0, Loss: 1.087985
Train - Epoch 335, Batch: 0, Loss: 1.087231
Train - Epoch 336, Batch: 0, Loss: 1.083130
Train - Epoch 337, Batch: 0, Loss: 1.070596
Train - Epoch 338, Batch: 0, Loss: 1.072618
Train - Epoch 339, Batch: 0, Loss: 1.057047
Train - Epoch 340, Batch: 0, Loss: 1.080526
Train - Epoch 341, Batch: 0, Loss: 1.068514
Train - Epoch 342, Batch: 0, Loss: 1.059119
Train - Epoch 343, Batch: 0, Loss: 1.061553
Train - Epoch 344, Batch: 0, Loss: 1.059823
Train - Epoch 345, Batch: 0, Loss: 1.071895
Train - Epoch 346, Batch: 0, Loss: 1.072156
Train - Epoch 347, Batch: 0, Loss: 1.063285
Train - Epoch 348, Batch: 0, Loss: 1.073108
Train - Epoch 349, Batch: 0, Loss: 1.070579
Train - Epoch 350, Batch: 0, Loss: 1.061245
Train - Epoch 351, Batch: 0, Loss: 1.070387
Train - Epoch 352, Batch: 0, Loss: 1.055608
Train - Epoch 353, Batch: 0, Loss: 1.061866
Train - Epoch 354, Batch: 0, Loss: 1.067989
Train - Epoch 355, Batch: 0, Loss: 1.059228
Train - Epoch 356, Batch: 0, Loss: 1.048576
Train - Epoch 357, Batch: 0, Loss: 1.062710
Train - Epoch 358, Batch: 0, Loss: 1.068480
Train - Epoch 359, Batch: 0, Loss: 1.067958
Train - Epoch 360, Batch: 0, Loss: 1.061161
Train - Epoch 361, Batch: 0, Loss: 1.056297
Train - Epoch 362, Batch: 0, Loss: 1.070219
Train - Epoch 363, Batch: 0, Loss: 1.063090
Train - Epoch 364, Batch: 0, Loss: 1.078896
Train - Epoch 365, Batch: 0, Loss: 1.061885
Train - Epoch 366, Batch: 0, Loss: 1.067909
Train - Epoch 367, Batch: 0, Loss: 1.069475
Train - Epoch 368, Batch: 0, Loss: 1.061723
Train - Epoch 369, Batch: 0, Loss: 1.074235
Train - Epoch 370, Batch: 0, Loss: 1.074071
Train - Epoch 371, Batch: 0, Loss: 1.060263
Train - Epoch 372, Batch: 0, Loss: 1.058138
Train - Epoch 373, Batch: 0, Loss: 1.061127/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 374, Batch: 0, Loss: 1.058390
Train - Epoch 375, Batch: 0, Loss: 1.056738
Train - Epoch 376, Batch: 0, Loss: 1.063652
Train - Epoch 377, Batch: 0, Loss: 1.068205
Train - Epoch 378, Batch: 0, Loss: 1.061713
Train - Epoch 379, Batch: 0, Loss: 1.045994
Train - Epoch 380, Batch: 0, Loss: 1.041540
Train - Epoch 381, Batch: 0, Loss: 1.057463
Train - Epoch 382, Batch: 0, Loss: 1.057823
Train - Epoch 383, Batch: 0, Loss: 1.058135
Train - Epoch 384, Batch: 0, Loss: 1.061284
Train - Epoch 385, Batch: 0, Loss: 1.055261
Train - Epoch 386, Batch: 0, Loss: 1.058609
Train - Epoch 387, Batch: 0, Loss: 1.061902
Train - Epoch 388, Batch: 0, Loss: 1.054814
Train - Epoch 389, Batch: 0, Loss: 1.071422
Train - Epoch 390, Batch: 0, Loss: 1.051578
Train - Epoch 391, Batch: 0, Loss: 1.059481
Train - Epoch 392, Batch: 0, Loss: 1.058069
Train - Epoch 393, Batch: 0, Loss: 1.058116
Train - Epoch 394, Batch: 0, Loss: 1.064206
Train - Epoch 395, Batch: 0, Loss: 1.063270
Train - Epoch 396, Batch: 0, Loss: 1.047872
Train - Epoch 397, Batch: 0, Loss: 1.073346
Train - Epoch 398, Batch: 0, Loss: 1.051669
Train - Epoch 399, Batch: 0, Loss: 1.067958
Train - Epoch 400, Batch: 0, Loss: 1.066849
Train - Epoch 401, Batch: 0, Loss: 1.042866
Train - Epoch 402, Batch: 0, Loss: 1.043590
Train - Epoch 403, Batch: 0, Loss: 1.064136
Train - Epoch 404, Batch: 0, Loss: 1.072291
Train - Epoch 405, Batch: 0, Loss: 1.047389
Train - Epoch 406, Batch: 0, Loss: 1.053365
Train - Epoch 407, Batch: 0, Loss: 1.055413
Train - Epoch 408, Batch: 0, Loss: 1.052211
Train - Epoch 409, Batch: 0, Loss: 1.055577
Train - Epoch 410, Batch: 0, Loss: 1.039892
Train - Epoch 411, Batch: 0, Loss: 1.062600
Train - Epoch 412, Batch: 0, Loss: 1.051125
Train - Epoch 413, Batch: 0, Loss: 1.051024
Train - Epoch 414, Batch: 0, Loss: 1.058956
Train - Epoch 415, Batch: 0, Loss: 1.043349
Train - Epoch 416, Batch: 0, Loss: 1.043340
Train - Epoch 417, Batch: 0, Loss: 1.049813
Train - Epoch 418, Batch: 0, Loss: 1.050962
Train - Epoch 419, Batch: 0, Loss: 1.040432
Train - Epoch 420, Batch: 0, Loss: 1.049317
Train - Epoch 421, Batch: 0, Loss: 1.054710
Train - Epoch 422, Batch: 0, Loss: 1.056384
Train - Epoch 423, Batch: 0, Loss: 1.041873
Train - Epoch 424, Batch: 0, Loss: 1.047556
Train - Epoch 425, Batch: 0, Loss: 1.047061
Train - Epoch 426, Batch: 0, Loss: 1.033504
Train - Epoch 427, Batch: 0, Loss: 1.042511
Train - Epoch 428, Batch: 0, Loss: 1.043881
Train - Epoch 429, Batch: 0, Loss: 1.049125
Train - Epoch 430, Batch: 0, Loss: 1.052517
Train - Epoch 431, Batch: 0, Loss: 1.054136
Train - Epoch 432, Batch: 0, Loss: 1.062266
Train - Epoch 433, Batch: 0, Loss: 1.056354
Train - Epoch 434, Batch: 0, Loss: 1.046504
Train - Epoch 435, Batch: 0, Loss: 1.063029
Train - Epoch 436, Batch: 0, Loss: 1.039289
Train - Epoch 437, Batch: 0, Loss: 1.051057
Train - Epoch 438, Batch: 0, Loss: 1.050673
Train - Epoch 439, Batch: 0, Loss: 1.037191
Train - Epoch 440, Batch: 0, Loss: 1.034527
Train - Epoch 441, Batch: 0, Loss: 1.057690
Train - Epoch 442, Batch: 0, Loss: 1.072652
Train - Epoch 443, Batch: 0, Loss: 1.033520
Train - Epoch 444, Batch: 0, Loss: 1.058390
Train - Epoch 445, Batch: 0, Loss: 1.042440
Train - Epoch 446, Batch: 0, Loss: 1.040603
Train - Epoch 447, Batch: 0, Loss: 1.046990
Train - Epoch 448, Batch: 0, Loss: 1.057746
Train - Epoch 449, Batch: 0, Loss: 1.037550
Train - Epoch 450, Batch: 0, Loss: 1.036965
Train - Epoch 451, Batch: 0, Loss: 1.049779
Train - Epoch 452, Batch: 0, Loss: 1.047028
Train - Epoch 453, Batch: 0, Loss: 1.036007
Train - Epoch 454, Batch: 0, Loss: 1.054781
Train - Epoch 455, Batch: 0, Loss: 1.048040
Train - Epoch 456, Batch: 0, Loss: 1.042738
Train - Epoch 457, Batch: 0, Loss: 1.057677
Train - Epoch 458, Batch: 0, Loss: 1.044363
Train - Epoch 459, Batch: 0, Loss: 1.046417
Train - Epoch 460, Batch: 0, Loss: 1.049627
Train - Epoch 461, Batch: 0, Loss: 1.048654
Train - Epoch 462, Batch: 0, Loss: 1.045606
Train - Epoch 463, Batch: 0, Loss: 1.037039
Train - Epoch 464, Batch: 0, Loss: 1.041571
Train - Epoch 465, Batch: 0, Loss: 1.047295
Train - Epoch 466, Batch: 0, Loss: 1.051396
Train - Epoch 467, Batch: 0, Loss: 1.048799
Train - Epoch 468, Batch: 0, Loss: 1.059636
Train - Epoch 469, Batch: 0, Loss: 1.050306
Train - Epoch 470, Batch: 0, Loss: 1.036589
Train - Epoch 471, Batch: 0, Loss: 1.041212
Train - Epoch 472, Batch: 0, Loss: 1.041443
Train - Epoch 473, Batch: 0, Loss: 1.030844
Train - Epoch 474, Batch: 0, Loss: 1.041348
Train - Epoch 475, Batch: 0, Loss: 1.044125
Train - Epoch 476, Batch: 0, Loss: 1.045816
Train - Epoch 477, Batch: 0, Loss: 1.038392
Train - Epoch 478, Batch: 0, Loss: 1.025340
Train - Epoch 479, Batch: 0, Loss: 1.032815
Train - Epoch 480, Batch: 0, Loss: 1.035562
Train - Epoch 481, Batch: 0, Loss: 1.037856
Train - Epoch 482, Batch: 0, Loss: 1.060288
Train - Epoch 483, Batch: 0, Loss: 1.038375
Train - Epoch 484, Batch: 0, Loss: 1.036555
Train - Epoch 485, Batch: 0, Loss: 1.037811
Train - Epoch 486, Batch: 0, Loss: 1.032335
Train - Epoch 487, Batch: 0, Loss: 1.032924
Train - Epoch 488, Batch: 0, Loss: 1.021465
Train - Epoch 489, Batch: 0, Loss: 1.036399
Train - Epoch 490, Batch: 0, Loss: 1.041905
Train - Epoch 491, Batch: 0, Loss: 1.046140
Train - Epoch 492, Batch: 0, Loss: 1.036730
Train - Epoch 493, Batch: 0, Loss: 1.043826
Train - Epoch 494, Batch: 0, Loss: 1.042535
Train - Epoch 495, Batch: 0, Loss: 1.049591
Train - Epoch 496, Batch: 0, Loss: 1.034709
Train - Epoch 497, Batch: 0, Loss: 1.040850
Train - Epoch 498, Batch: 0, Loss: 1.040260
Train - Epoch 499, Batch: 0, Loss: 1.049378
training_time:: 107.72138833999634
training time full:: 107.72145771980286
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632700
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 50
max_epoch:: 500
batch_size:: 10000
epoch:: 0
tensor([32768,     4, 32774,    10, 32780,    13,    15, 32784,    20,    28,
        32798,    31,    45, 32816,    50, 32820,    54, 32831,    65, 32834,
        32836,    69,    75, 32847,    81,    82, 32849,    84,    85, 32853,
        32855, 32861, 32864,    97, 32865, 32866, 32869,   110,   111, 32881,
        32882, 32883,   118,   127,   129, 32899, 32901,   136, 32909, 32914])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
epoch:: 60
epoch:: 61
epoch:: 62
epoch:: 63
epoch:: 64
epoch:: 65
epoch:: 66
epoch:: 67
epoch:: 68
epoch:: 69
epoch:: 70
epoch:: 71
epoch:: 72
epoch:: 73
epoch:: 74
epoch:: 75
epoch:: 76
epoch:: 77
epoch:: 78
epoch:: 79
epoch:: 80
epoch:: 81
epoch:: 82
epoch:: 83
epoch:: 84
epoch:: 85
epoch:: 86
epoch:: 87
epoch:: 88
epoch:: 89
epoch:: 90
epoch:: 91
epoch:: 92
epoch:: 93
epoch:: 94
epoch:: 95
epoch:: 96
epoch:: 97
epoch:: 98
epoch:: 99
epoch:: 100
epoch:: 101
epoch:: 102
epoch:: 103
epoch:: 104
epoch:: 105
epoch:: 106
epoch:: 107
epoch:: 108
epoch:: 109
epoch:: 110
epoch:: 111
epoch:: 112
epoch:: 113
epoch:: 114
epoch:: 115
epoch:: 116
epoch:: 117
epoch:: 118
epoch:: 119
epoch:: 120
epoch:: 121
epoch:: 122
epoch:: 123
epoch:: 124
epoch:: 125
epoch:: 126
epoch:: 127
epoch:: 128
epoch:: 129
epoch:: 130
epoch:: 131
epoch:: 132
epoch:: 133
epoch:: 134
epoch:: 135
epoch:: 136
epoch:: 137
epoch:: 138
epoch:: 139
epoch:: 140
epoch:: 141
epoch:: 142
epoch:: 143
epoch:: 144
epoch:: 145
epoch:: 146
epoch:: 147
epoch:: 148
epoch:: 149
epoch:: 150
epoch:: 151
epoch:: 152
epoch:: 153
epoch:: 154
epoch:: 155
epoch:: 156
epoch:: 157
epoch:: 158
epoch:: 159
epoch:: 160
epoch:: 161
epoch:: 162
epoch:: 163
epoch:: 164
epoch:: 165
epoch:: 166
epoch:: 167
epoch:: 168
epoch:: 169
epoch:: 170
epoch:: 171
epoch:: 172
epoch:: 173
epoch:: 174
epoch:: 175
epoch:: 176
epoch:: 177
epoch:: 178
epoch:: 179
epoch:: 180
epoch:: 181
epoch:: 182
epoch:: 183
epoch:: 184
epoch:: 185
epoch:: 186
epoch:: 187
epoch:: 188
epoch:: 189
epoch:: 190
epoch:: 191
epoch:: 192
epoch:: 193
epoch:: 194
epoch:: 195
epoch:: 196
epoch:: 197
epoch:: 198
epoch:: 199
epoch:: 200
epoch:: 201
epoch:: 202
epoch:: 203
epoch:: 204
epoch:: 205
epoch:: 206
epoch:: 207
epoch:: 208
epoch:: 209
epoch:: 210
epoch:: 211
epoch:: 212
epoch:: 213
epoch:: 214
epoch:: 215
epoch:: 216
epoch:: 217
epoch:: 218
epoch:: 219
epoch:: 220
epoch:: 221
epoch:: 222
epoch:: 223
epoch:: 224
epoch:: 225
epoch:: 226
epoch:: 227
epoch:: 228
epoch:: 229
epoch:: 230
epoch:: 231
epoch:: 232
epoch:: 233
epoch:: 234
epoch:: 235
epoch:: 236
epoch:: 237
epoch:: 238
epoch:: 239
epoch:: 240
epoch:: 241
epoch:: 242
epoch:: 243
epoch:: 244
epoch:: 245
epoch:: 246
epoch:: 247
epoch:: 248
epoch:: 249
epoch:: 250
epoch:: 251
epoch:: 252
epoch:: 253
epoch:: 254
epoch:: 255
epoch:: 256
epoch:: 257
epoch:: 258
epoch:: 259
epoch:: 260
epoch:: 261
epoch:: 262
epoch:: 263
epoch:: 264
epoch:: 265
epoch:: 266
epoch:: 267
epoch:: 268
epoch:: 269
epoch:: 270
epoch:: 271
epoch:: 272
epoch:: 273
epoch:: 274
epoch:: 275
epoch:: 276
epoch:: 277
epoch:: 278
epoch:: 279
epoch:: 280
epoch:: 281
epoch:: 282
epoch:: 283
epoch:: 284
epoch:: 285
epoch:: 286
epoch:: 287
epoch:: 288
epoch:: 289
epoch:: 290
epoch:: 291
epoch:: 292
epoch:: 293
epoch:: 294
epoch:: 295
epoch:: 296
epoch:: 297
epoch:: 298
epoch:: 299
epoch:: 300
epoch:: 301
epoch:: 302
epoch:: 303
epoch:: 304
epoch:: 305
epoch:: 306
epoch:: 307
epoch:: 308
epoch:: 309
epoch:: 310
epoch:: 311
epoch:: 312
epoch:: 313
epoch:: 314
epoch:: 315
epoch:: 316
epoch:: 317
epoch:: 318
epoch:: 319
epoch:: 320
epoch:: 321
epoch:: 322
epoch:: 323
epoch:: 324
epoch:: 325
epoch:: 326
epoch:: 327
epoch:: 328
epoch:: 329
epoch:: 330
epoch:: 331
epoch:: 332
epoch:: 333
epoch:: 334
epoch:: 335
epoch:: 336
epoch:: 337
epoch:: 338
epoch:: 339
epoch:: 340
epoch:: 341
epoch:: 342
epoch:: 343
epoch:: 344
epoch:: 345
epoch:: 346
epoch:: 347
epoch:: 348
epoch:: 349
epoch:: 350
epoch:: 351
epoch:: 352
epoch:: 353
epoch:: 354
epoch:: 355
epoch:: 356
epoch:: 357
epoch:: 358
epoch:: 359
epoch:: 360
epoch:: 361
epoch:: 362
epoch:: 363
epoch:: 364
epoch:: 365
epoch:: 366
epoch:: 367
epoch:: 368
epoch:: 369
epoch:: 370
epoch:: 371
epoch:: 372
epoch:: 373
epoch:: 374
epoch:: 375
epoch:: 376
epoch:: 377
epoch:: 378
epoch:: 379
epoch:: 380
epoch:: 381
epoch:: 382
epoch:: 383
epoch:: 384
epoch:: 385
epoch:: 386
epoch:: 387
epoch:: 388
epoch:: 389
epoch:: 390
epoch:: 391
epoch:: 392
epoch:: 393
epoch:: 394
epoch:: 395
epoch:: 396
epoch:: 397
epoch:: 398
epoch:: 399
epoch:: 400
epoch:: 401
epoch:: 402
epoch:: 403
epoch:: 404
epoch:: 405
epoch:: 406
epoch:: 407
epoch:: 408
epoch:: 409
epoch:: 410
epoch:: 411
epoch:: 412
epoch:: 413
epoch:: 414
epoch:: 415
epoch:: 416
epoch:: 417
epoch:: 418
epoch:: 419
epoch:: 420
epoch:: 421
epoch:: 422
epoch:: 423
epoch:: 424
epoch:: 425
epoch:: 426
epoch:: 427
epoch:: 428
epoch:: 429
epoch:: 430
epoch:: 431
epoch:: 432
epoch:: 433
epoch:: 434
epoch:: 435
epoch:: 436
epoch:: 437
epoch:: 438
epoch:: 439
epoch:: 440
epoch:: 441
epoch:: 442
epoch:: 443
epoch:: 444
epoch:: 445
epoch:: 446
epoch:: 447
epoch:: 448
epoch:: 449
epoch:: 450
epoch:: 451
epoch:: 452
epoch:: 453
epoch:: 454
epoch:: 455
epoch:: 456
epoch:: 457
epoch:: 458
epoch:: 459
epoch:: 460
epoch:: 461
epoch:: 462
epoch:: 463
epoch:: 464
epoch:: 465
epoch:: 466
epoch:: 467
epoch:: 468
epoch:: 469
epoch:: 470
epoch:: 471
epoch:: 472
epoch:: 473
epoch:: 474
epoch:: 475
epoch:: 476
epoch:: 477
epoch:: 478
epoch:: 479
epoch:: 480
epoch:: 481
epoch:: 482
epoch:: 483
epoch:: 484
epoch:: 485
epoch:: 486
epoch:: 487
epoch:: 488
epoch:: 489
epoch:: 490
epoch:: 491
epoch:: 492
epoch:: 493
epoch:: 494
epoch:: 495
epoch:: 496
epoch:: 497
epoch:: 498
epoch:: 499
training time is 80.90457701683044
overhead:: 0
overhead2:: 2.639605760574341
overhead3:: 0
time_baseline:: 80.90464162826538
curr_diff: 0 tensor(0.0602, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0602, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632100
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 50
max_epoch:: 500
overhead:: 0
overhead2:: 0.07832026481628418
overhead3:: 0.25307536125183105
overhead4:: 9.983234643936157
overhead5:: 0
memory usage:: 5625384960
time_provenance:: 17.503226280212402
curr_diff: 0 tensor(0.0149, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0149, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0717, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0717, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632100
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 50
max_epoch:: 500
overhead:: 0
overhead2:: 0.08437132835388184
overhead3:: 0.2559211254119873
overhead4:: 9.929646492004395
overhead5:: 0
memory usage:: 5626384384
time_provenance:: 17.477112531661987
curr_diff: 0 tensor(0.0149, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0149, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0717, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0717, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632100
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 50
max_epoch:: 500
overhead:: 0
overhead2:: 0.08320379257202148
overhead3:: 0.2677497863769531
overhead4:: 10.762906551361084
overhead5:: 0
memory usage:: 5631021056
time_provenance:: 18.43602228164673
curr_diff: 0 tensor(0.0149, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0149, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0717, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0717, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632100
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 50
max_epoch:: 500
overhead:: 0
overhead2:: 0.15359926223754883
overhead3:: 0.4456779956817627
overhead4:: 18.2828311920166
overhead5:: 0
memory usage:: 5624897536
time_provenance:: 27.769433736801147
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0611, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0611, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632200
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 50
max_epoch:: 500
overhead:: 0
overhead2:: 0.15780925750732422
overhead3:: 0.4504678249359131
overhead4:: 18.673073530197144
overhead5:: 0
memory usage:: 5626384384
time_provenance:: 28.222330331802368
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0611, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0611, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632200
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 50
max_epoch:: 500
overhead:: 0
overhead2:: 0.16534781455993652
overhead3:: 0.4600040912628174
overhead4:: 18.81713628768921
overhead5:: 0
memory usage:: 5681860608
time_provenance:: 28.427884578704834
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0611, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0611, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632200
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 50
max_epoch:: 500
overhead:: 0
overhead2:: 0.3915598392486572
overhead3:: 1.0315415859222412
overhead4:: 43.756476402282715
overhead5:: 0
memory usage:: 5625425920
time_provenance:: 59.1983106136322
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0602, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0602, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632200
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 50
max_epoch:: 500
overhead:: 0
overhead2:: 0.38288450241088867
overhead3:: 1.0417158603668213
overhead4:: 43.491271018981934
overhead5:: 0
memory usage:: 5637181440
time_provenance:: 58.95946955680847
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0603, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0603, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632200
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 50
max_epoch:: 500
overhead:: 0
overhead2:: 0.3951442241668701
overhead3:: 1.0355384349822998
overhead4:: 44.171886920928955
overhead5:: 0
memory usage:: 5645725696
time_provenance:: 59.67692971229553
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0603, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0603, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632200
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 3 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 50
max_epoch:: 500
overhead:: 0
overhead2:: 0.7876441478729248
overhead3:: 2.090855360031128
overhead4:: 82.60221457481384
overhead5:: 0
memory usage:: 5616517120
time_provenance:: 106.07451367378235
curr_diff: 0 tensor(5.0018e-13, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.0018e-13, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0602, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0602, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632100
repetition 4
python3 benchmark_exp_lr.py 0.001 10000 500 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05] Logistic_regression cifar10_2 4 0.0001 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 2.311484
Train - Epoch 1, Batch: 0, Loss: 2.244821
Train - Epoch 2, Batch: 0, Loss: 2.189309
Train - Epoch 3, Batch: 0, Loss: 2.139456
Train - Epoch 4, Batch: 0, Loss: 2.090468
Train - Epoch 5, Batch: 0, Loss: 2.046821
Train - Epoch 6, Batch: 0, Loss: 2.007085
Train - Epoch 7, Batch: 0, Loss: 1.969894
Train - Epoch 8, Batch: 0, Loss: 1.932417
Train - Epoch 9, Batch: 0, Loss: 1.895918
Train - Epoch 10, Batch: 0, Loss: 1.860266
Train - Epoch 11, Batch: 0, Loss: 1.836356
Train - Epoch 12, Batch: 0, Loss: 1.804079
Train - Epoch 13, Batch: 0, Loss: 1.783558
Train - Epoch 14, Batch: 0, Loss: 1.763917
Train - Epoch 15, Batch: 0, Loss: 1.734008
Train - Epoch 16, Batch: 0, Loss: 1.708789
Train - Epoch 17, Batch: 0, Loss: 1.697015
Train - Epoch 18, Batch: 0, Loss: 1.679677
Train - Epoch 19, Batch: 0, Loss: 1.656666
Train - Epoch 20, Batch: 0, Loss: 1.645002
Train - Epoch 21, Batch: 0, Loss: 1.620127
Train - Epoch 22, Batch: 0, Loss: 1.604141
Train - Epoch 23, Batch: 0, Loss: 1.590433
Train - Epoch 24, Batch: 0, Loss: 1.575727
Train - Epoch 25, Batch: 0, Loss: 1.570462
Train - Epoch 26, Batch: 0, Loss: 1.550855
Train - Epoch 27, Batch: 0, Loss: 1.537129
Train - Epoch 28, Batch: 0, Loss: 1.523874
Train - Epoch 29, Batch: 0, Loss: 1.519845
Train - Epoch 30, Batch: 0, Loss: 1.501716
Train - Epoch 31, Batch: 0, Loss: 1.495383
Train - Epoch 32, Batch: 0, Loss: 1.476999
Train - Epoch 33, Batch: 0, Loss: 1.482668
Train - Epoch 34, Batch: 0, Loss: 1.465399
Train - Epoch 35, Batch: 0, Loss: 1.461660
Train - Epoch 36, Batch: 0, Loss: 1.444707
Train - Epoch 37, Batch: 0, Loss: 1.438278
Train - Epoch 38, Batch: 0, Loss: 1.440459
Train - Epoch 39, Batch: 0, Loss: 1.423903
Train - Epoch 40, Batch: 0, Loss: 1.419634
Train - Epoch 41, Batch: 0, Loss: 1.415704
Train - Epoch 42, Batch: 0, Loss: 1.399956
Train - Epoch 43, Batch: 0, Loss: 1.403410
Train - Epoch 44, Batch: 0, Loss: 1.396914
Train - Epoch 45, Batch: 0, Loss: 1.387229
Train - Epoch 46, Batch: 0, Loss: 1.387057
Train - Epoch 47, Batch: 0, Loss: 1.380527
Train - Epoch 48, Batch: 0, Loss: 1.377371
Train - Epoch 49, Batch: 0, Loss: 1.378577
Train - Epoch 50, Batch: 0, Loss: 1.350112
Train - Epoch 51, Batch: 0, Loss: 1.349390
Train - Epoch 52, Batch: 0, Loss: 1.348385
Train - Epoch 53, Batch: 0, Loss: 1.346815
Train - Epoch 54, Batch: 0, Loss: 1.346165
Train - Epoch 55, Batch: 0, Loss: 1.340325
Train - Epoch 56, Batch: 0, Loss: 1.346953
Train - Epoch 57, Batch: 0, Loss: 1.330794
Train - Epoch 58, Batch: 0, Loss: 1.324720
Train - Epoch 59, Batch: 0, Loss: 1.313265
Train - Epoch 60, Batch: 0, Loss: 1.319829
Train - Epoch 61, Batch: 0, Loss: 1.317086
Train - Epoch 62, Batch: 0, Loss: 1.307430
Train - Epoch 63, Batch: 0, Loss: 1.309243
Train - Epoch 64, Batch: 0, Loss: 1.303789
Train - Epoch 65, Batch: 0, Loss: 1.294339
Train - Epoch 66, Batch: 0, Loss: 1.303604
Train - Epoch 67, Batch: 0, Loss: 1.289806
Train - Epoch 68, Batch: 0, Loss: 1.281156
Train - Epoch 69, Batch: 0, Loss: 1.268360
Train - Epoch 70, Batch: 0, Loss: 1.277232
Train - Epoch 71, Batch: 0, Loss: 1.291208
Train - Epoch 72, Batch: 0, Loss: 1.272319
Train - Epoch 73, Batch: 0, Loss: 1.277782
Train - Epoch 74, Batch: 0, Loss: 1.263497
Train - Epoch 75, Batch: 0, Loss: 1.272287
Train - Epoch 76, Batch: 0, Loss: 1.262462
Train - Epoch 77, Batch: 0, Loss: 1.275293
Train - Epoch 78, Batch: 0, Loss: 1.262118
Train - Epoch 79, Batch: 0, Loss: 1.253632
Train - Epoch 80, Batch: 0, Loss: 1.254735
Train - Epoch 81, Batch: 0, Loss: 1.244380
Train - Epoch 82, Batch: 0, Loss: 1.247014
Train - Epoch 83, Batch: 0, Loss: 1.248397
Train - Epoch 84, Batch: 0, Loss: 1.248500
Train - Epoch 85, Batch: 0, Loss: 1.256811
Train - Epoch 86, Batch: 0, Loss: 1.239188
Train - Epoch 87, Batch: 0, Loss: 1.247146
Train - Epoch 88, Batch: 0, Loss: 1.244678
Train - Epoch 89, Batch: 0, Loss: 1.237412
Train - Epoch 90, Batch: 0, Loss: 1.218079
Train - Epoch 91, Batch: 0, Loss: 1.231144
Train - Epoch 92, Batch: 0, Loss: 1.218933
Train - Epoch 93, Batch: 0, Loss: 1.230318
Train - Epoch 94, Batch: 0, Loss: 1.212902
Train - Epoch 95, Batch: 0, Loss: 1.229056
Train - Epoch 96, Batch: 0, Loss: 1.221398
Train - Epoch 97, Batch: 0, Loss: 1.214069
Train - Epoch 98, Batch: 0, Loss: 1.218029
Train - Epoch 99, Batch: 0, Loss: 1.218773
Train - Epoch 100, Batch: 0, Loss: 1.216795
Train - Epoch 101, Batch: 0, Loss: 1.200640
Train - Epoch 102, Batch: 0, Loss: 1.204697
Train - Epoch 103, Batch: 0, Loss: 1.207747
Train - Epoch 104, Batch: 0, Loss: 1.212561
Train - Epoch 105, Batch: 0, Loss: 1.223418
Train - Epoch 106, Batch: 0, Loss: 1.203886
Train - Epoch 107, Batch: 0, Loss: 1.208061
Train - Epoch 108, Batch: 0, Loss: 1.208307
Train - Epoch 109, Batch: 0, Loss: 1.196558
Train - Epoch 110, Batch: 0, Loss: 1.194979
Train - Epoch 111, Batch: 0, Loss: 1.191235
Train - Epoch 112, Batch: 0, Loss: 1.197876
Train - Epoch 113, Batch: 0, Loss: 1.182876
Train - Epoch 114, Batch: 0, Loss: 1.191182
Train - Epoch 115, Batch: 0, Loss: 1.198354
Train - Epoch 116, Batch: 0, Loss: 1.186548
Train - Epoch 117, Batch: 0, Loss: 1.190598
Train - Epoch 118, Batch: 0, Loss: 1.186540
Train - Epoch 119, Batch: 0, Loss: 1.191676
Train - Epoch 120, Batch: 0, Loss: 1.177983
Train - Epoch 121, Batch: 0, Loss: 1.188428
Train - Epoch 122, Batch: 0, Loss: 1.176342
Train - Epoch 123, Batch: 0, Loss: 1.191557
Train - Epoch 124, Batch: 0, Loss: 1.177752
Train - Epoch 125, Batch: 0, Loss: 1.175870
Train - Epoch 126, Batch: 0, Loss: 1.174105
Train - Epoch 127, Batch: 0, Loss: 1.160815
Train - Epoch 128, Batch: 0, Loss: 1.178143
Train - Epoch 129, Batch: 0, Loss: 1.178907
Train - Epoch 130, Batch: 0, Loss: 1.172383
Train - Epoch 131, Batch: 0, Loss: 1.170570
Train - Epoch 132, Batch: 0, Loss: 1.177645
Train - Epoch 133, Batch: 0, Loss: 1.180902
Train - Epoch 134, Batch: 0, Loss: 1.173537
Train - Epoch 135, Batch: 0, Loss: 1.172311
Train - Epoch 136, Batch: 0, Loss: 1.156101
Train - Epoch 137, Batch: 0, Loss: 1.160593
Train - Epoch 138, Batch: 0, Loss: 1.151302
Train - Epoch 139, Batch: 0, Loss: 1.175236
Train - Epoch 140, Batch: 0, Loss: 1.165044
Train - Epoch 141, Batch: 0, Loss: 1.167843
Train - Epoch 142, Batch: 0, Loss: 1.163680
Train - Epoch 143, Batch: 0, Loss: 1.167372
Train - Epoch 144, Batch: 0, Loss: 1.158252
Train - Epoch 145, Batch: 0, Loss: 1.156152
Train - Epoch 146, Batch: 0, Loss: 1.158936
Train - Epoch 147, Batch: 0, Loss: 1.156424
Train - Epoch 148, Batch: 0, Loss: 1.159561
Train - Epoch 149, Batch: 0, Loss: 1.155449
Train - Epoch 150, Batch: 0, Loss: 1.143234
Train - Epoch 151, Batch: 0, Loss: 1.136952
Train - Epoch 152, Batch: 0, Loss: 1.150836
Train - Epoch 153, Batch: 0, Loss: 1.148325
Train - Epoch 154, Batch: 0, Loss: 1.145065
Train - Epoch 155, Batch: 0, Loss: 1.143815
Train - Epoch 156, Batch: 0, Loss: 1.141335
Train - Epoch 157, Batch: 0, Loss: 1.146027
Train - Epoch 158, Batch: 0, Loss: 1.145090
Train - Epoch 159, Batch: 0, Loss: 1.146838
Train - Epoch 160, Batch: 0, Loss: 1.147173
Train - Epoch 161, Batch: 0, Loss: 1.146373
Train - Epoch 162, Batch: 0, Loss: 1.160098
Train - Epoch 163, Batch: 0, Loss: 1.129642
Train - Epoch 164, Batch: 0, Loss: 1.138021
Train - Epoch 165, Batch: 0, Loss: 1.129811
Train - Epoch 166, Batch: 0, Loss: 1.137804
Train - Epoch 167, Batch: 0, Loss: 1.134646
Train - Epoch 168, Batch: 0, Loss: 1.144833
Train - Epoch 169, Batch: 0, Loss: 1.135959
Train - Epoch 170, Batch: 0, Loss: 1.128910
Train - Epoch 171, Batch: 0, Loss: 1.136374
Train - Epoch 172, Batch: 0, Loss: 1.138615
Train - Epoch 173, Batch: 0, Loss: 1.137032
Train - Epoch 174, Batch: 0, Loss: 1.129160
Train - Epoch 175, Batch: 0, Loss: 1.131930
Train - Epoch 176, Batch: 0, Loss: 1.139558
Train - Epoch 177, Batch: 0, Loss: 1.139398
Train - Epoch 178, Batch: 0, Loss: 1.132983
Train - Epoch 179, Batch: 0, Loss: 1.122308
Train - Epoch 180, Batch: 0, Loss: 1.134796
Train - Epoch 181, Batch: 0, Loss: 1.120641
Train - Epoch 182, Batch: 0, Loss: 1.133874
Train - Epoch 183, Batch: 0, Loss: 1.136973
Train - Epoch 184, Batch: 0, Loss: 1.128533
Train - Epoch 185, Batch: 0, Loss: 1.102899
Train - Epoch 186, Batch: 0, Loss: 1.134264
Train - Epoch 187, Batch: 0, Loss: 1.130569
Train - Epoch 188, Batch: 0, Loss: 1.140036
Train - Epoch 189, Batch: 0, Loss: 1.119399
Train - Epoch 190, Batch: 0, Loss: 1.126678
Train - Epoch 191, Batch: 0, Loss: 1.129690
Train - Epoch 192, Batch: 0, Loss: 1.127385
Train - Epoch 193, Batch: 0, Loss: 1.119311
Train - Epoch 194, Batch: 0, Loss: 1.111653
Train - Epoch 195, Batch: 0, Loss: 1.118438
Train - Epoch 196, Batch: 0, Loss: 1.141842
Train - Epoch 197, Batch: 0, Loss: 1.120081
Train - Epoch 198, Batch: 0, Loss: 1.116174
Train - Epoch 199, Batch: 0, Loss: 1.116124
Train - Epoch 200, Batch: 0, Loss: 1.122452
Train - Epoch 201, Batch: 0, Loss: 1.117231
Train - Epoch 202, Batch: 0, Loss: 1.119141
Train - Epoch 203, Batch: 0, Loss: 1.098797
Train - Epoch 204, Batch: 0, Loss: 1.124157
Train - Epoch 205, Batch: 0, Loss: 1.118371
Train - Epoch 206, Batch: 0, Loss: 1.116280
Train - Epoch 207, Batch: 0, Loss: 1.110215
Train - Epoch 208, Batch: 0, Loss: 1.127342
Train - Epoch 209, Batch: 0, Loss: 1.108677
Train - Epoch 210, Batch: 0, Loss: 1.101542
Train - Epoch 211, Batch: 0, Loss: 1.114618
Train - Epoch 212, Batch: 0, Loss: 1.129747
Train - Epoch 213, Batch: 0, Loss: 1.120454
Train - Epoch 214, Batch: 0, Loss: 1.117253
Train - Epoch 215, Batch: 0, Loss: 1.103592
Train - Epoch 216, Batch: 0, Loss: 1.113739
Train - Epoch 217, Batch: 0, Loss: 1.102215
Train - Epoch 218, Batch: 0, Loss: 1.114416
Train - Epoch 219, Batch: 0, Loss: 1.108706
Train - Epoch 220, Batch: 0, Loss: 1.121432
Train - Epoch 221, Batch: 0, Loss: 1.109454
Train - Epoch 222, Batch: 0, Loss: 1.098704
Train - Epoch 223, Batch: 0, Loss: 1.111654
Train - Epoch 224, Batch: 0, Loss: 1.106307
Train - Epoch 225, Batch: 0, Loss: 1.110322
Train - Epoch 226, Batch: 0, Loss: 1.099615
Train - Epoch 227, Batch: 0, Loss: 1.100379
Train - Epoch 228, Batch: 0, Loss: 1.114399
Train - Epoch 229, Batch: 0, Loss: 1.109454
Train - Epoch 230, Batch: 0, Loss: 1.095035
Train - Epoch 231, Batch: 0, Loss: 1.095662
Train - Epoch 232, Batch: 0, Loss: 1.102561
Train - Epoch 233, Batch: 0, Loss: 1.107863
Train - Epoch 234, Batch: 0, Loss: 1.111429
Train - Epoch 235, Batch: 0, Loss: 1.110388
Train - Epoch 236, Batch: 0, Loss: 1.106391
Train - Epoch 237, Batch: 0, Loss: 1.103610
Train - Epoch 238, Batch: 0, Loss: 1.099502
Train - Epoch 239, Batch: 0, Loss: 1.102232
Train - Epoch 240, Batch: 0, Loss: 1.098862
Train - Epoch 241, Batch: 0, Loss: 1.094081
Train - Epoch 242, Batch: 0, Loss: 1.109603
Train - Epoch 243, Batch: 0, Loss: 1.082328
Train - Epoch 244, Batch: 0, Loss: 1.094269
Train - Epoch 245, Batch: 0, Loss: 1.097393
Train - Epoch 246, Batch: 0, Loss: 1.101819
Train - Epoch 247, Batch: 0, Loss: 1.094243
Train - Epoch 248, Batch: 0, Loss: 1.097980
Train - Epoch 249, Batch: 0, Loss: 1.078081
Train - Epoch 250, Batch: 0, Loss: 1.084281
Train - Epoch 251, Batch: 0, Loss: 1.104108
Train - Epoch 252, Batch: 0, Loss: 1.084987
Train - Epoch 253, Batch: 0, Loss: 1.088629
Train - Epoch 254, Batch: 0, Loss: 1.106321
Train - Epoch 255, Batch: 0, Loss: 1.097273
Train - Epoch 256, Batch: 0, Loss: 1.085308
Train - Epoch 257, Batch: 0, Loss: 1.095171
Train - Epoch 258, Batch: 0, Loss: 1.109289
Train - Epoch 259, Batch: 0, Loss: 1.090004
Train - Epoch 260, Batch: 0, Loss: 1.094182
Train - Epoch 261, Batch: 0, Loss: 1.085404
Train - Epoch 262, Batch: 0, Loss: 1.096030
Train - Epoch 263, Batch: 0, Loss: 1.086890
Train - Epoch 264, Batch: 0, Loss: 1.084999
Train - Epoch 265, Batch: 0, Loss: 1.089295
Train - Epoch 266, Batch: 0, Loss: 1.084004
Train - Epoch 267, Batch: 0, Loss: 1.089173
Train - Epoch 268, Batch: 0, Loss: 1.093723
Train - Epoch 269, Batch: 0, Loss: 1.097046
Train - Epoch 270, Batch: 0, Loss: 1.096285
Train - Epoch 271, Batch: 0, Loss: 1.092768
Train - Epoch 272, Batch: 0, Loss: 1.092950
Train - Epoch 273, Batch: 0, Loss: 1.093566
Train - Epoch 274, Batch: 0, Loss: 1.096401
Train - Epoch 275, Batch: 0, Loss: 1.074880
Train - Epoch 276, Batch: 0, Loss: 1.090753
Train - Epoch 277, Batch: 0, Loss: 1.064495
Train - Epoch 278, Batch: 0, Loss: 1.068011
Train - Epoch 279, Batch: 0, Loss: 1.092627
Train - Epoch 280, Batch: 0, Loss: 1.085296
Train - Epoch 281, Batch: 0, Loss: 1.090836
Train - Epoch 282, Batch: 0, Loss: 1.084805
Train - Epoch 283, Batch: 0, Loss: 1.083676
Train - Epoch 284, Batch: 0, Loss: 1.077525
Train - Epoch 285, Batch: 0, Loss: 1.067160
Train - Epoch 286, Batch: 0, Loss: 1.083859
Train - Epoch 287, Batch: 0, Loss: 1.065248
Train - Epoch 288, Batch: 0, Loss: 1.084182
Train - Epoch 289, Batch: 0, Loss: 1.080551
Train - Epoch 290, Batch: 0, Loss: 1.082982
Train - Epoch 291, Batch: 0, Loss: 1.078469
Train - Epoch 292, Batch: 0, Loss: 1.069795
Train - Epoch 293, Batch: 0, Loss: 1.080624
Train - Epoch 294, Batch: 0, Loss: 1.081276
Train - Epoch 295, Batch: 0, Loss: 1.089878
Train - Epoch 296, Batch: 0, Loss: 1.076435
Train - Epoch 297, Batch: 0, Loss: 1.057853
Train - Epoch 298, Batch: 0, Loss: 1.079475
Train - Epoch 299, Batch: 0, Loss: 1.069857
Train - Epoch 300, Batch: 0, Loss: 1.080631
Train - Epoch 301, Batch: 0, Loss: 1.100022
Train - Epoch 302, Batch: 0, Loss: 1.077310
Train - Epoch 303, Batch: 0, Loss: 1.078698
Train - Epoch 304, Batch: 0, Loss: 1.079353
Train - Epoch 305, Batch: 0, Loss: 1.075311
Train - Epoch 306, Batch: 0, Loss: 1.067616
Train - Epoch 307, Batch: 0, Loss: 1.084652
Train - Epoch 308, Batch: 0, Loss: 1.078146
Train - Epoch 309, Batch: 0, Loss: 1.087016
Train - Epoch 310, Batch: 0, Loss: 1.075903
Train - Epoch 311, Batch: 0, Loss: 1.077615
Train - Epoch 312, Batch: 0, Loss: 1.080024
Train - Epoch 313, Batch: 0, Loss: 1.072688
Train - Epoch 314, Batch: 0, Loss: 1.061276
Train - Epoch 315, Batch: 0, Loss: 1.072068
Train - Epoch 316, Batch: 0, Loss: 1.059012
Train - Epoch 317, Batch: 0, Loss: 1.079285
Train - Epoch 318, Batch: 0, Loss: 1.068735
Train - Epoch 319, Batch: 0, Loss: 1.078672
Train - Epoch 320, Batch: 0, Loss: 1.065268
Train - Epoch 321, Batch: 0, Loss: 1.078599
Train - Epoch 322, Batch: 0, Loss: 1.079742
Train - Epoch 323, Batch: 0, Loss: 1.080178
Train - Epoch 324, Batch: 0, Loss: 1.057585
Train - Epoch 325, Batch: 0, Loss: 1.073403
Train - Epoch 326, Batch: 0, Loss: 1.064813
Train - Epoch 327, Batch: 0, Loss: 1.067502
Train - Epoch 328, Batch: 0, Loss: 1.068594
Train - Epoch 329, Batch: 0, Loss: 1.060209
Train - Epoch 330, Batch: 0, Loss: 1.075933
Train - Epoch 331, Batch: 0, Loss: 1.078931
Train - Epoch 332, Batch: 0, Loss: 1.072977
Train - Epoch 333, Batch: 0, Loss: 1.073128
Train - Epoch 334, Batch: 0, Loss: 1.069308
Train - Epoch 335, Batch: 0, Loss: 1.072147
Train - Epoch 336, Batch: 0, Loss: 1.071862
Train - Epoch 337, Batch: 0, Loss: 1.073061
Train - Epoch 338, Batch: 0, Loss: 1.076826
Train - Epoch 339, Batch: 0, Loss: 1.073437
Train - Epoch 340, Batch: 0, Loss: 1.073917
Train - Epoch 341, Batch: 0, Loss: 1.063971
Train - Epoch 342, Batch: 0, Loss: 1.070032
Train - Epoch 343, Batch: 0, Loss: 1.069212
Train - Epoch 344, Batch: 0, Loss: 1.073434
Train - Epoch 345, Batch: 0, Loss: 1.066386
Train - Epoch 346, Batch: 0, Loss: 1.066033
Train - Epoch 347, Batch: 0, Loss: 1.074909
Train - Epoch 348, Batch: 0, Loss: 1.090759
Train - Epoch 349, Batch: 0, Loss: 1.080818
Train - Epoch 350, Batch: 0, Loss: 1.065388
Train - Epoch 351, Batch: 0, Loss: 1.056775
Train - Epoch 352, Batch: 0, Loss: 1.059211
Train - Epoch 353, Batch: 0, Loss: 1.053763
Train - Epoch 354, Batch: 0, Loss: 1.073303
Train - Epoch 355, Batch: 0, Loss: 1.062860
Train - Epoch 356, Batch: 0, Loss: 1.068307
Train - Epoch 357, Batch: 0, Loss: 1.057339
Train - Epoch 358, Batch: 0, Loss: 1.064548
Train - Epoch 359, Batch: 0, Loss: 1.060075
Train - Epoch 360, Batch: 0, Loss: 1.058330
Train - Epoch 361, Batch: 0, Loss: 1.065821
Train - Epoch 362, Batch: 0, Loss: 1.060616
Train - Epoch 363, Batch: 0, Loss: 1.058553
Train - Epoch 364, Batch: 0, Loss: 1.047077
Train - Epoch 365, Batch: 0, Loss: 1.065940
Train - Epoch 366, Batch: 0, Loss: 1.065652
Train - Epoch 367, Batch: 0, Loss: 1.058091
Train - Epoch 368, Batch: 0, Loss: 1.060311
Train - Epoch 369, Batch: 0, Loss: 1.069063
Train - Epoch 370, Batch: 0, Loss: 1.060977
Train - Epoch 371, Batch: 0, Loss: 1.057239
Train - Epoch 372, Batch: 0, Loss: 1.081476
Train - Epoch 373, Batch: 0, Loss: 1.059635/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 374, Batch: 0, Loss: 1.053219
Train - Epoch 375, Batch: 0, Loss: 1.056472
Train - Epoch 376, Batch: 0, Loss: 1.063884
Train - Epoch 377, Batch: 0, Loss: 1.066187
Train - Epoch 378, Batch: 0, Loss: 1.054611
Train - Epoch 379, Batch: 0, Loss: 1.062761
Train - Epoch 380, Batch: 0, Loss: 1.064592
Train - Epoch 381, Batch: 0, Loss: 1.058979
Train - Epoch 382, Batch: 0, Loss: 1.059472
Train - Epoch 383, Batch: 0, Loss: 1.054636
Train - Epoch 384, Batch: 0, Loss: 1.054717
Train - Epoch 385, Batch: 0, Loss: 1.060770
Train - Epoch 386, Batch: 0, Loss: 1.063914
Train - Epoch 387, Batch: 0, Loss: 1.063680
Train - Epoch 388, Batch: 0, Loss: 1.061715
Train - Epoch 389, Batch: 0, Loss: 1.052414
Train - Epoch 390, Batch: 0, Loss: 1.066403
Train - Epoch 391, Batch: 0, Loss: 1.048699
Train - Epoch 392, Batch: 0, Loss: 1.052110
Train - Epoch 393, Batch: 0, Loss: 1.058080
Train - Epoch 394, Batch: 0, Loss: 1.055490
Train - Epoch 395, Batch: 0, Loss: 1.040595
Train - Epoch 396, Batch: 0, Loss: 1.055119
Train - Epoch 397, Batch: 0, Loss: 1.042361
Train - Epoch 398, Batch: 0, Loss: 1.054927
Train - Epoch 399, Batch: 0, Loss: 1.061740
Train - Epoch 400, Batch: 0, Loss: 1.053417
Train - Epoch 401, Batch: 0, Loss: 1.049448
Train - Epoch 402, Batch: 0, Loss: 1.056983
Train - Epoch 403, Batch: 0, Loss: 1.052650
Train - Epoch 404, Batch: 0, Loss: 1.064326
Train - Epoch 405, Batch: 0, Loss: 1.058175
Train - Epoch 406, Batch: 0, Loss: 1.061521
Train - Epoch 407, Batch: 0, Loss: 1.064010
Train - Epoch 408, Batch: 0, Loss: 1.057597
Train - Epoch 409, Batch: 0, Loss: 1.065887
Train - Epoch 410, Batch: 0, Loss: 1.077532
Train - Epoch 411, Batch: 0, Loss: 1.050425
Train - Epoch 412, Batch: 0, Loss: 1.048924
Train - Epoch 413, Batch: 0, Loss: 1.055603
Train - Epoch 414, Batch: 0, Loss: 1.050409
Train - Epoch 415, Batch: 0, Loss: 1.058345
Train - Epoch 416, Batch: 0, Loss: 1.061196
Train - Epoch 417, Batch: 0, Loss: 1.057979
Train - Epoch 418, Batch: 0, Loss: 1.043116
Train - Epoch 419, Batch: 0, Loss: 1.047834
Train - Epoch 420, Batch: 0, Loss: 1.052430
Train - Epoch 421, Batch: 0, Loss: 1.039723
Train - Epoch 422, Batch: 0, Loss: 1.048483
Train - Epoch 423, Batch: 0, Loss: 1.076526
Train - Epoch 424, Batch: 0, Loss: 1.050514
Train - Epoch 425, Batch: 0, Loss: 1.054449
Train - Epoch 426, Batch: 0, Loss: 1.051853
Train - Epoch 427, Batch: 0, Loss: 1.049801
Train - Epoch 428, Batch: 0, Loss: 1.058894
Train - Epoch 429, Batch: 0, Loss: 1.060452
Train - Epoch 430, Batch: 0, Loss: 1.067077
Train - Epoch 431, Batch: 0, Loss: 1.042544
Train - Epoch 432, Batch: 0, Loss: 1.056349
Train - Epoch 433, Batch: 0, Loss: 1.050463
Train - Epoch 434, Batch: 0, Loss: 1.056937
Train - Epoch 435, Batch: 0, Loss: 1.034769
Train - Epoch 436, Batch: 0, Loss: 1.046868
Train - Epoch 437, Batch: 0, Loss: 1.055607
Train - Epoch 438, Batch: 0, Loss: 1.042256
Train - Epoch 439, Batch: 0, Loss: 1.057864
Train - Epoch 440, Batch: 0, Loss: 1.055607
Train - Epoch 441, Batch: 0, Loss: 1.046279
Train - Epoch 442, Batch: 0, Loss: 1.026746
Train - Epoch 443, Batch: 0, Loss: 1.053687
Train - Epoch 444, Batch: 0, Loss: 1.043962
Train - Epoch 445, Batch: 0, Loss: 1.050382
Train - Epoch 446, Batch: 0, Loss: 1.041392
Train - Epoch 447, Batch: 0, Loss: 1.033148
Train - Epoch 448, Batch: 0, Loss: 1.026782
Train - Epoch 449, Batch: 0, Loss: 1.039255
Train - Epoch 450, Batch: 0, Loss: 1.053475
Train - Epoch 451, Batch: 0, Loss: 1.039192
Train - Epoch 452, Batch: 0, Loss: 1.057040
Train - Epoch 453, Batch: 0, Loss: 1.039221
Train - Epoch 454, Batch: 0, Loss: 1.036777
Train - Epoch 455, Batch: 0, Loss: 1.045325
Train - Epoch 456, Batch: 0, Loss: 1.055783
Train - Epoch 457, Batch: 0, Loss: 1.056890
Train - Epoch 458, Batch: 0, Loss: 1.047293
Train - Epoch 459, Batch: 0, Loss: 1.042317
Train - Epoch 460, Batch: 0, Loss: 1.051605
Train - Epoch 461, Batch: 0, Loss: 1.047398
Train - Epoch 462, Batch: 0, Loss: 1.053806
Train - Epoch 463, Batch: 0, Loss: 1.040826
Train - Epoch 464, Batch: 0, Loss: 1.042263
Train - Epoch 465, Batch: 0, Loss: 1.043455
Train - Epoch 466, Batch: 0, Loss: 1.057942
Train - Epoch 467, Batch: 0, Loss: 1.065406
Train - Epoch 468, Batch: 0, Loss: 1.032468
Train - Epoch 469, Batch: 0, Loss: 1.054738
Train - Epoch 470, Batch: 0, Loss: 1.038010
Train - Epoch 471, Batch: 0, Loss: 1.044767
Train - Epoch 472, Batch: 0, Loss: 1.026164
Train - Epoch 473, Batch: 0, Loss: 1.038026
Train - Epoch 474, Batch: 0, Loss: 1.040661
Train - Epoch 475, Batch: 0, Loss: 1.052179
Train - Epoch 476, Batch: 0, Loss: 1.040222
Train - Epoch 477, Batch: 0, Loss: 1.044446
Train - Epoch 478, Batch: 0, Loss: 1.048541
Train - Epoch 479, Batch: 0, Loss: 1.060929
Train - Epoch 480, Batch: 0, Loss: 1.046692
Train - Epoch 481, Batch: 0, Loss: 1.058037
Train - Epoch 482, Batch: 0, Loss: 1.039792
Train - Epoch 483, Batch: 0, Loss: 1.040684
Train - Epoch 484, Batch: 0, Loss: 1.041681
Train - Epoch 485, Batch: 0, Loss: 1.060227
Train - Epoch 486, Batch: 0, Loss: 1.039130
Train - Epoch 487, Batch: 0, Loss: 1.041792
Train - Epoch 488, Batch: 0, Loss: 1.039366
Train - Epoch 489, Batch: 0, Loss: 1.047198
Train - Epoch 490, Batch: 0, Loss: 1.036016
Train - Epoch 491, Batch: 0, Loss: 1.046926
Train - Epoch 492, Batch: 0, Loss: 1.040491
Train - Epoch 493, Batch: 0, Loss: 1.038339
Train - Epoch 494, Batch: 0, Loss: 1.024215
Train - Epoch 495, Batch: 0, Loss: 1.034749
Train - Epoch 496, Batch: 0, Loss: 1.028699
Train - Epoch 497, Batch: 0, Loss: 1.045941
Train - Epoch 498, Batch: 0, Loss: 1.033464
Train - Epoch 499, Batch: 0, Loss: 1.049908
training_time:: 107.48862290382385
training time full:: 107.488694190979
provenance prepare time:: 4.76837158203125e-07
here
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632000
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 50
max_epoch:: 500
batch_size:: 10000
epoch:: 0
tensor([    0, 32774,     6, 32778, 32780,    13,    15,    18,    19, 32788,
        32790, 32793, 32794, 32796, 32798,    38,    40,    42,    44, 32814,
           47,    46, 32817, 32818, 32819,    49,    52, 32816,    57, 32825,
           60, 32829,    62,    61,    65, 32835,    69,    72,    80,    84,
        32852, 32859,    91, 32861,    95,    98, 32868,   101,   100,   102])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
epoch:: 60
epoch:: 61
epoch:: 62
epoch:: 63
epoch:: 64
epoch:: 65
epoch:: 66
epoch:: 67
epoch:: 68
epoch:: 69
epoch:: 70
epoch:: 71
epoch:: 72
epoch:: 73
epoch:: 74
epoch:: 75
epoch:: 76
epoch:: 77
epoch:: 78
epoch:: 79
epoch:: 80
epoch:: 81
epoch:: 82
epoch:: 83
epoch:: 84
epoch:: 85
epoch:: 86
epoch:: 87
epoch:: 88
epoch:: 89
epoch:: 90
epoch:: 91
epoch:: 92
epoch:: 93
epoch:: 94
epoch:: 95
epoch:: 96
epoch:: 97
epoch:: 98
epoch:: 99
epoch:: 100
epoch:: 101
epoch:: 102
epoch:: 103
epoch:: 104
epoch:: 105
epoch:: 106
epoch:: 107
epoch:: 108
epoch:: 109
epoch:: 110
epoch:: 111
epoch:: 112
epoch:: 113
epoch:: 114
epoch:: 115
epoch:: 116
epoch:: 117
epoch:: 118
epoch:: 119
epoch:: 120
epoch:: 121
epoch:: 122
epoch:: 123
epoch:: 124
epoch:: 125
epoch:: 126
epoch:: 127
epoch:: 128
epoch:: 129
epoch:: 130
epoch:: 131
epoch:: 132
epoch:: 133
epoch:: 134
epoch:: 135
epoch:: 136
epoch:: 137
epoch:: 138
epoch:: 139
epoch:: 140
epoch:: 141
epoch:: 142
epoch:: 143
epoch:: 144
epoch:: 145
epoch:: 146
epoch:: 147
epoch:: 148
epoch:: 149
epoch:: 150
epoch:: 151
epoch:: 152
epoch:: 153
epoch:: 154
epoch:: 155
epoch:: 156
epoch:: 157
epoch:: 158
epoch:: 159
epoch:: 160
epoch:: 161
epoch:: 162
epoch:: 163
epoch:: 164
epoch:: 165
epoch:: 166
epoch:: 167
epoch:: 168
epoch:: 169
epoch:: 170
epoch:: 171
epoch:: 172
epoch:: 173
epoch:: 174
epoch:: 175
epoch:: 176
epoch:: 177
epoch:: 178
epoch:: 179
epoch:: 180
epoch:: 181
epoch:: 182
epoch:: 183
epoch:: 184
epoch:: 185
epoch:: 186
epoch:: 187
epoch:: 188
epoch:: 189
epoch:: 190
epoch:: 191
epoch:: 192
epoch:: 193
epoch:: 194
epoch:: 195
epoch:: 196
epoch:: 197
epoch:: 198
epoch:: 199
epoch:: 200
epoch:: 201
epoch:: 202
epoch:: 203
epoch:: 204
epoch:: 205
epoch:: 206
epoch:: 207
epoch:: 208
epoch:: 209
epoch:: 210
epoch:: 211
epoch:: 212
epoch:: 213
epoch:: 214
epoch:: 215
epoch:: 216
epoch:: 217
epoch:: 218
epoch:: 219
epoch:: 220
epoch:: 221
epoch:: 222
epoch:: 223
epoch:: 224
epoch:: 225
epoch:: 226
epoch:: 227
epoch:: 228
epoch:: 229
epoch:: 230
epoch:: 231
epoch:: 232
epoch:: 233
epoch:: 234
epoch:: 235
epoch:: 236
epoch:: 237
epoch:: 238
epoch:: 239
epoch:: 240
epoch:: 241
epoch:: 242
epoch:: 243
epoch:: 244
epoch:: 245
epoch:: 246
epoch:: 247
epoch:: 248
epoch:: 249
epoch:: 250
epoch:: 251
epoch:: 252
epoch:: 253
epoch:: 254
epoch:: 255
epoch:: 256
epoch:: 257
epoch:: 258
epoch:: 259
epoch:: 260
epoch:: 261
epoch:: 262
epoch:: 263
epoch:: 264
epoch:: 265
epoch:: 266
epoch:: 267
epoch:: 268
epoch:: 269
epoch:: 270
epoch:: 271
epoch:: 272
epoch:: 273
epoch:: 274
epoch:: 275
epoch:: 276
epoch:: 277
epoch:: 278
epoch:: 279
epoch:: 280
epoch:: 281
epoch:: 282
epoch:: 283
epoch:: 284
epoch:: 285
epoch:: 286
epoch:: 287
epoch:: 288
epoch:: 289
epoch:: 290
epoch:: 291
epoch:: 292
epoch:: 293
epoch:: 294
epoch:: 295
epoch:: 296
epoch:: 297
epoch:: 298
epoch:: 299
epoch:: 300
epoch:: 301
epoch:: 302
epoch:: 303
epoch:: 304
epoch:: 305
epoch:: 306
epoch:: 307
epoch:: 308
epoch:: 309
epoch:: 310
epoch:: 311
epoch:: 312
epoch:: 313
epoch:: 314
epoch:: 315
epoch:: 316
epoch:: 317
epoch:: 318
epoch:: 319
epoch:: 320
epoch:: 321
epoch:: 322
epoch:: 323
epoch:: 324
epoch:: 325
epoch:: 326
epoch:: 327
epoch:: 328
epoch:: 329
epoch:: 330
epoch:: 331
epoch:: 332
epoch:: 333
epoch:: 334
epoch:: 335
epoch:: 336
epoch:: 337
epoch:: 338
epoch:: 339
epoch:: 340
epoch:: 341
epoch:: 342
epoch:: 343
epoch:: 344
epoch:: 345
epoch:: 346
epoch:: 347
epoch:: 348
epoch:: 349
epoch:: 350
epoch:: 351
epoch:: 352
epoch:: 353
epoch:: 354
epoch:: 355
epoch:: 356
epoch:: 357
epoch:: 358
epoch:: 359
epoch:: 360
epoch:: 361
epoch:: 362
epoch:: 363
epoch:: 364
epoch:: 365
epoch:: 366
epoch:: 367
epoch:: 368
epoch:: 369
epoch:: 370
epoch:: 371
epoch:: 372
epoch:: 373
epoch:: 374
epoch:: 375
epoch:: 376
epoch:: 377
epoch:: 378
epoch:: 379
epoch:: 380
epoch:: 381
epoch:: 382
epoch:: 383
epoch:: 384
epoch:: 385
epoch:: 386
epoch:: 387
epoch:: 388
epoch:: 389
epoch:: 390
epoch:: 391
epoch:: 392
epoch:: 393
epoch:: 394
epoch:: 395
epoch:: 396
epoch:: 397
epoch:: 398
epoch:: 399
epoch:: 400
epoch:: 401
epoch:: 402
epoch:: 403
epoch:: 404
epoch:: 405
epoch:: 406
epoch:: 407
epoch:: 408
epoch:: 409
epoch:: 410
epoch:: 411
epoch:: 412
epoch:: 413
epoch:: 414
epoch:: 415
epoch:: 416
epoch:: 417
epoch:: 418
epoch:: 419
epoch:: 420
epoch:: 421
epoch:: 422
epoch:: 423
epoch:: 424
epoch:: 425
epoch:: 426
epoch:: 427
epoch:: 428
epoch:: 429
epoch:: 430
epoch:: 431
epoch:: 432
epoch:: 433
epoch:: 434
epoch:: 435
epoch:: 436
epoch:: 437
epoch:: 438
epoch:: 439
epoch:: 440
epoch:: 441
epoch:: 442
epoch:: 443
epoch:: 444
epoch:: 445
epoch:: 446
epoch:: 447
epoch:: 448
epoch:: 449
epoch:: 450
epoch:: 451
epoch:: 452
epoch:: 453
epoch:: 454
epoch:: 455
epoch:: 456
epoch:: 457
epoch:: 458
epoch:: 459
epoch:: 460
epoch:: 461
epoch:: 462
epoch:: 463
epoch:: 464
epoch:: 465
epoch:: 466
epoch:: 467
epoch:: 468
epoch:: 469
epoch:: 470
epoch:: 471
epoch:: 472
epoch:: 473
epoch:: 474
epoch:: 475
epoch:: 476
epoch:: 477
epoch:: 478
epoch:: 479
epoch:: 480
epoch:: 481
epoch:: 482
epoch:: 483
epoch:: 484
epoch:: 485
epoch:: 486
epoch:: 487
epoch:: 488
epoch:: 489
epoch:: 490
epoch:: 491
epoch:: 492
epoch:: 493
epoch:: 494
epoch:: 495
epoch:: 496
epoch:: 497
epoch:: 498
epoch:: 499
training time is 80.49388241767883
overhead:: 0
overhead2:: 2.642778158187866
overhead3:: 0
time_baseline:: 80.49391531944275
curr_diff: 0 tensor(0.0604, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0604, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632400
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 50
max_epoch:: 500
overhead:: 0
overhead2:: 0.07755160331726074
overhead3:: 0.25198888778686523
overhead4:: 9.859248399734497
overhead5:: 0
memory usage:: 5645942784
time_provenance:: 17.38922381401062
curr_diff: 0 tensor(0.0149, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0149, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0719, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0719, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632300
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 50
max_epoch:: 500
overhead:: 0
overhead2:: 0.08251333236694336
overhead3:: 0.2632429599761963
overhead4:: 10.394471168518066
overhead5:: 0
memory usage:: 5626408960
time_provenance:: 18.017812490463257
curr_diff: 0 tensor(0.0149, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0149, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0719, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0719, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632300
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 50
max_epoch:: 500
overhead:: 0
overhead2:: 0.08396124839782715
overhead3:: 0.2656412124633789
overhead4:: 10.427928447723389
overhead5:: 0
memory usage:: 5625929728
time_provenance:: 18.06336784362793
curr_diff: 0 tensor(0.0149, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0149, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0719, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0719, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632000
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 50
max_epoch:: 500
overhead:: 0
overhead2:: 0.15461468696594238
overhead3:: 0.4526560306549072
overhead4:: 18.682985305786133
overhead5:: 0
memory usage:: 5626781696
time_provenance:: 28.205602645874023
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0614, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0614, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632700
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 50
max_epoch:: 500
overhead:: 0
overhead2:: 0.16282868385314941
overhead3:: 0.45713305473327637
overhead4:: 18.66439437866211
overhead5:: 0
memory usage:: 5627625472
time_provenance:: 28.245285987854004
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0614, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0614, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632700
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 50
max_epoch:: 500
overhead:: 0
overhead2:: 0.16661763191223145
overhead3:: 0.4710264205932617
overhead4:: 18.589343309402466
overhead5:: 0
memory usage:: 5664186368
time_provenance:: 28.228485822677612
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0614, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0614, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632700
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 50
max_epoch:: 500
overhead:: 0
overhead2:: 0.3857126235961914
overhead3:: 1.0493812561035156
overhead4:: 43.71344304084778
overhead5:: 0
memory usage:: 5626408960
time_provenance:: 59.14133834838867
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0604, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0604, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632400
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 50
max_epoch:: 500
overhead:: 0
overhead2:: 0.38612794876098633
overhead3:: 1.0132431983947754
overhead4:: 43.34774708747864
overhead5:: 0
memory usage:: 5628616704
time_provenance:: 58.77310228347778
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0604, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0604, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632400
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 50
max_epoch:: 500
overhead:: 0
overhead2:: 0.38446474075317383
overhead3:: 1.0320351123809814
overhead4:: 44.110769510269165
overhead5:: 0
memory usage:: 5645336576
time_provenance:: 59.59351682662964
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0604, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0604, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632400
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 4 0.001 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 50
max_epoch:: 500
overhead:: 0
overhead2:: 0.7697005271911621
overhead3:: 2.090294122695923
overhead4:: 81.81848883628845
overhead5:: 0
memory usage:: 5618950144
time_provenance:: 105.29329204559326
curr_diff: 0 tensor(5.0037e-13, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.0037e-13, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0604, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0604, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632400
deletion rate:: 0.005
python3 generate_rand_ids 0.005  cifar10_2 0
tensor([27146, 22031, 47631,  5649, 42513, 14358, 48152, 40475, 34846, 36896,
        31776, 37924,  4645, 27174, 35878, 36397,  5677,  8239, 21039, 27696,
         6195, 18996, 10291, 39992,  3131,    64, 13895, 28233, 34379, 38988,
        31826, 41044, 18007, 35417, 11866, 48730, 20573, 37983, 40032, 30305,
        46177, 40547, 41572, 10340,  8808,  5737, 31336,  1641, 21100, 37485,
         5743, 36463, 33905, 37491, 41077, 41591, 37880, 23165, 44670, 43134,
         3713,  2691, 26756,  3715, 30856, 32393, 26248, 27788, 48783, 47253,
        40087,  8856, 37018, 43163,  5282, 48802, 26276, 27300, 16550, 17575,
        10409, 24235, 27307, 20141,  2221, 23216, 22193, 43700, 15029, 24758,
        41143, 38074, 20156, 30397, 46272, 45249,  3265, 41671, 22217, 45258,
        25294, 36046,  6866,   723,  6868, 44249, 27866, 23775, 28386, 34019,
        24290, 32998, 23782, 10471, 14567, 14568, 48872, 22251,  9964, 34029,
        14059, 46831, 26865, 44786, 14578, 16628, 47348, 41206,  9468, 23293,
        21761, 35587, 47876,  1797, 10502, 48391, 35589,  7435,  4877, 22285,
         8978, 16150, 15130, 24859, 25375, 13600, 28967, 47911, 40745,   296,
        26410, 27947, 11565, 44023, 35638,  7478,   312, 39735, 29496,  4409,
        25914, 43320, 25404, 33087, 34112, 35651, 38212, 43333, 22347, 10573,
         9549, 32591, 37200, 38751, 29028, 45413, 47979, 20335, 15218, 26483,
        46964, 38771,  7029, 15735,  6518,   888, 42875, 20860, 41343, 33667,
        21379, 42886, 30088, 11145, 37770, 24460,  2445, 36749, 26512, 46480,
        35731, 25492, 20373,  9622, 35223, 46487, 45975,   403, 14751, 43936,
        28578, 35751, 35242, 39851, 25516, 41391, 29619, 29108, 38837, 32691,
         6074, 48059, 12732, 48061, 16320, 33741, 38861, 20441, 19930, 20955,
        39900, 18393, 25054, 46050, 28132, 36325, 39911, 27113, 39406, 20975,
         2542, 22513, 38895, 42995,  6132, 12278, 27127, 16888, 46073, 15870])
batch size:: 10000
repetition 0
python3 benchmark_exp_lr.py 0.001 10000 500 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05] Logistic_regression cifar10_2 0 0.0001 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 2.296853
Train - Epoch 1, Batch: 0, Loss: 2.235755
Train - Epoch 2, Batch: 0, Loss: 2.183000
Train - Epoch 3, Batch: 0, Loss: 2.133107
Train - Epoch 4, Batch: 0, Loss: 2.084510
Train - Epoch 5, Batch: 0, Loss: 2.041979
Train - Epoch 6, Batch: 0, Loss: 1.995618
Train - Epoch 7, Batch: 0, Loss: 1.962401
Train - Epoch 8, Batch: 0, Loss: 1.924321
Train - Epoch 9, Batch: 0, Loss: 1.889566
Train - Epoch 10, Batch: 0, Loss: 1.865748
Train - Epoch 11, Batch: 0, Loss: 1.833669
Train - Epoch 12, Batch: 0, Loss: 1.804286
Train - Epoch 13, Batch: 0, Loss: 1.782829
Train - Epoch 14, Batch: 0, Loss: 1.753523
Train - Epoch 15, Batch: 0, Loss: 1.733665
Train - Epoch 16, Batch: 0, Loss: 1.718045
Train - Epoch 17, Batch: 0, Loss: 1.690435
Train - Epoch 18, Batch: 0, Loss: 1.671814
Train - Epoch 19, Batch: 0, Loss: 1.659534
Train - Epoch 20, Batch: 0, Loss: 1.632630
Train - Epoch 21, Batch: 0, Loss: 1.620033
Train - Epoch 22, Batch: 0, Loss: 1.603685
Train - Epoch 23, Batch: 0, Loss: 1.604481
Train - Epoch 24, Batch: 0, Loss: 1.572323
Train - Epoch 25, Batch: 0, Loss: 1.561876
Train - Epoch 26, Batch: 0, Loss: 1.542420
Train - Epoch 27, Batch: 0, Loss: 1.545389
Train - Epoch 28, Batch: 0, Loss: 1.523798
Train - Epoch 29, Batch: 0, Loss: 1.511276
Train - Epoch 30, Batch: 0, Loss: 1.504014
Train - Epoch 31, Batch: 0, Loss: 1.498074
Train - Epoch 32, Batch: 0, Loss: 1.473740
Train - Epoch 33, Batch: 0, Loss: 1.472201
Train - Epoch 34, Batch: 0, Loss: 1.475474
Train - Epoch 35, Batch: 0, Loss: 1.461903
Train - Epoch 36, Batch: 0, Loss: 1.445230
Train - Epoch 37, Batch: 0, Loss: 1.449776
Train - Epoch 38, Batch: 0, Loss: 1.433816
Train - Epoch 39, Batch: 0, Loss: 1.430027
Train - Epoch 40, Batch: 0, Loss: 1.429617
Train - Epoch 41, Batch: 0, Loss: 1.411800
Train - Epoch 42, Batch: 0, Loss: 1.404976
Train - Epoch 43, Batch: 0, Loss: 1.402598
Train - Epoch 44, Batch: 0, Loss: 1.397785
Train - Epoch 45, Batch: 0, Loss: 1.378704
Train - Epoch 46, Batch: 0, Loss: 1.380451
Train - Epoch 47, Batch: 0, Loss: 1.369778
Train - Epoch 48, Batch: 0, Loss: 1.372303
Train - Epoch 49, Batch: 0, Loss: 1.364934
Train - Epoch 50, Batch: 0, Loss: 1.359969
Train - Epoch 51, Batch: 0, Loss: 1.359297
Train - Epoch 52, Batch: 0, Loss: 1.361188
Train - Epoch 53, Batch: 0, Loss: 1.340581
Train - Epoch 54, Batch: 0, Loss: 1.335903
Train - Epoch 55, Batch: 0, Loss: 1.333220
Train - Epoch 56, Batch: 0, Loss: 1.338883
Train - Epoch 57, Batch: 0, Loss: 1.325251
Train - Epoch 58, Batch: 0, Loss: 1.315901
Train - Epoch 59, Batch: 0, Loss: 1.305873
Train - Epoch 60, Batch: 0, Loss: 1.319419
Train - Epoch 61, Batch: 0, Loss: 1.302448
Train - Epoch 62, Batch: 0, Loss: 1.310635
Train - Epoch 63, Batch: 0, Loss: 1.299745
Train - Epoch 64, Batch: 0, Loss: 1.297040
Train - Epoch 65, Batch: 0, Loss: 1.298200
Train - Epoch 66, Batch: 0, Loss: 1.295947
Train - Epoch 67, Batch: 0, Loss: 1.297418
Train - Epoch 68, Batch: 0, Loss: 1.300308
Train - Epoch 69, Batch: 0, Loss: 1.291166
Train - Epoch 70, Batch: 0, Loss: 1.282424
Train - Epoch 71, Batch: 0, Loss: 1.285032
Train - Epoch 72, Batch: 0, Loss: 1.277161
Train - Epoch 73, Batch: 0, Loss: 1.279194
Train - Epoch 74, Batch: 0, Loss: 1.279411
Train - Epoch 75, Batch: 0, Loss: 1.270344
Train - Epoch 76, Batch: 0, Loss: 1.264508
Train - Epoch 77, Batch: 0, Loss: 1.270956
Train - Epoch 78, Batch: 0, Loss: 1.257054
Train - Epoch 79, Batch: 0, Loss: 1.270807
Train - Epoch 80, Batch: 0, Loss: 1.260002
Train - Epoch 81, Batch: 0, Loss: 1.248665
Train - Epoch 82, Batch: 0, Loss: 1.241545
Train - Epoch 83, Batch: 0, Loss: 1.255941
Train - Epoch 84, Batch: 0, Loss: 1.257490
Train - Epoch 85, Batch: 0, Loss: 1.240461
Train - Epoch 86, Batch: 0, Loss: 1.244356
Train - Epoch 87, Batch: 0, Loss: 1.238019
Train - Epoch 88, Batch: 0, Loss: 1.234737
Train - Epoch 89, Batch: 0, Loss: 1.231621
Train - Epoch 90, Batch: 0, Loss: 1.227248
Train - Epoch 91, Batch: 0, Loss: 1.229957
Train - Epoch 92, Batch: 0, Loss: 1.220347
Train - Epoch 93, Batch: 0, Loss: 1.221889
Train - Epoch 94, Batch: 0, Loss: 1.229743
Train - Epoch 95, Batch: 0, Loss: 1.222532
Train - Epoch 96, Batch: 0, Loss: 1.217980
Train - Epoch 97, Batch: 0, Loss: 1.220351
Train - Epoch 98, Batch: 0, Loss: 1.217615
Train - Epoch 99, Batch: 0, Loss: 1.218711
Train - Epoch 100, Batch: 0, Loss: 1.223627
Train - Epoch 101, Batch: 0, Loss: 1.216054
Train - Epoch 102, Batch: 0, Loss: 1.223231
Train - Epoch 103, Batch: 0, Loss: 1.202834
Train - Epoch 104, Batch: 0, Loss: 1.206073
Train - Epoch 105, Batch: 0, Loss: 1.208123
Train - Epoch 106, Batch: 0, Loss: 1.205373
Train - Epoch 107, Batch: 0, Loss: 1.197773
Train - Epoch 108, Batch: 0, Loss: 1.194607
Train - Epoch 109, Batch: 0, Loss: 1.207369
Train - Epoch 110, Batch: 0, Loss: 1.206945
Train - Epoch 111, Batch: 0, Loss: 1.200667
Train - Epoch 112, Batch: 0, Loss: 1.186853
Train - Epoch 113, Batch: 0, Loss: 1.182091
Train - Epoch 114, Batch: 0, Loss: 1.192022
Train - Epoch 115, Batch: 0, Loss: 1.200159
Train - Epoch 116, Batch: 0, Loss: 1.187239
Train - Epoch 117, Batch: 0, Loss: 1.188946
Train - Epoch 118, Batch: 0, Loss: 1.174276
Train - Epoch 119, Batch: 0, Loss: 1.178825
Train - Epoch 120, Batch: 0, Loss: 1.183246
Train - Epoch 121, Batch: 0, Loss: 1.161795
Train - Epoch 122, Batch: 0, Loss: 1.182401
Train - Epoch 123, Batch: 0, Loss: 1.180550
Train - Epoch 124, Batch: 0, Loss: 1.187274
Train - Epoch 125, Batch: 0, Loss: 1.177861
Train - Epoch 126, Batch: 0, Loss: 1.184607
Train - Epoch 127, Batch: 0, Loss: 1.189731
Train - Epoch 128, Batch: 0, Loss: 1.175845
Train - Epoch 129, Batch: 0, Loss: 1.185942
Train - Epoch 130, Batch: 0, Loss: 1.178787
Train - Epoch 131, Batch: 0, Loss: 1.175419
Train - Epoch 132, Batch: 0, Loss: 1.165961
Train - Epoch 133, Batch: 0, Loss: 1.167932
Train - Epoch 134, Batch: 0, Loss: 1.174663
Train - Epoch 135, Batch: 0, Loss: 1.180613
Train - Epoch 136, Batch: 0, Loss: 1.170534
Train - Epoch 137, Batch: 0, Loss: 1.168668
Train - Epoch 138, Batch: 0, Loss: 1.161946
Train - Epoch 139, Batch: 0, Loss: 1.148149
Train - Epoch 140, Batch: 0, Loss: 1.164220
Train - Epoch 141, Batch: 0, Loss: 1.169466
Train - Epoch 142, Batch: 0, Loss: 1.144716
Train - Epoch 143, Batch: 0, Loss: 1.157089
Train - Epoch 144, Batch: 0, Loss: 1.169184
Train - Epoch 145, Batch: 0, Loss: 1.161328
Train - Epoch 146, Batch: 0, Loss: 1.161972
Train - Epoch 147, Batch: 0, Loss: 1.142994
Train - Epoch 148, Batch: 0, Loss: 1.153266
Train - Epoch 149, Batch: 0, Loss: 1.142673
Train - Epoch 150, Batch: 0, Loss: 1.156922
Train - Epoch 151, Batch: 0, Loss: 1.157080
Train - Epoch 152, Batch: 0, Loss: 1.159310
Train - Epoch 153, Batch: 0, Loss: 1.157943
Train - Epoch 154, Batch: 0, Loss: 1.154964
Train - Epoch 155, Batch: 0, Loss: 1.146000
Train - Epoch 156, Batch: 0, Loss: 1.148334
Train - Epoch 157, Batch: 0, Loss: 1.146031
Train - Epoch 158, Batch: 0, Loss: 1.151715
Train - Epoch 159, Batch: 0, Loss: 1.159328
Train - Epoch 160, Batch: 0, Loss: 1.147241
Train - Epoch 161, Batch: 0, Loss: 1.126477
Train - Epoch 162, Batch: 0, Loss: 1.148988
Train - Epoch 163, Batch: 0, Loss: 1.142939
Train - Epoch 164, Batch: 0, Loss: 1.154292
Train - Epoch 165, Batch: 0, Loss: 1.133557
Train - Epoch 166, Batch: 0, Loss: 1.144622
Train - Epoch 167, Batch: 0, Loss: 1.140208
Train - Epoch 168, Batch: 0, Loss: 1.125075
Train - Epoch 169, Batch: 0, Loss: 1.133430
Train - Epoch 170, Batch: 0, Loss: 1.140872
Train - Epoch 171, Batch: 0, Loss: 1.141359
Train - Epoch 172, Batch: 0, Loss: 1.136901
Train - Epoch 173, Batch: 0, Loss: 1.133734
Train - Epoch 174, Batch: 0, Loss: 1.134391
Train - Epoch 175, Batch: 0, Loss: 1.129813
Train - Epoch 176, Batch: 0, Loss: 1.139357
Train - Epoch 177, Batch: 0, Loss: 1.130762
Train - Epoch 178, Batch: 0, Loss: 1.128259
Train - Epoch 179, Batch: 0, Loss: 1.131322
Train - Epoch 180, Batch: 0, Loss: 1.133063
Train - Epoch 181, Batch: 0, Loss: 1.142014
Train - Epoch 182, Batch: 0, Loss: 1.131065
Train - Epoch 183, Batch: 0, Loss: 1.125730
Train - Epoch 184, Batch: 0, Loss: 1.103303
Train - Epoch 185, Batch: 0, Loss: 1.129700
Train - Epoch 186, Batch: 0, Loss: 1.142386
Train - Epoch 187, Batch: 0, Loss: 1.124066
Train - Epoch 188, Batch: 0, Loss: 1.127249
Train - Epoch 189, Batch: 0, Loss: 1.148132
Train - Epoch 190, Batch: 0, Loss: 1.130500
Train - Epoch 191, Batch: 0, Loss: 1.126996
Train - Epoch 192, Batch: 0, Loss: 1.117743
Train - Epoch 193, Batch: 0, Loss: 1.130723
Train - Epoch 194, Batch: 0, Loss: 1.113969
Train - Epoch 195, Batch: 0, Loss: 1.115726
Train - Epoch 196, Batch: 0, Loss: 1.117914
Train - Epoch 197, Batch: 0, Loss: 1.106401
Train - Epoch 198, Batch: 0, Loss: 1.122370
Train - Epoch 199, Batch: 0, Loss: 1.117087
Train - Epoch 200, Batch: 0, Loss: 1.126696
Train - Epoch 201, Batch: 0, Loss: 1.131273
Train - Epoch 202, Batch: 0, Loss: 1.125502
Train - Epoch 203, Batch: 0, Loss: 1.115494
Train - Epoch 204, Batch: 0, Loss: 1.132602
Train - Epoch 205, Batch: 0, Loss: 1.115877
Train - Epoch 206, Batch: 0, Loss: 1.108876
Train - Epoch 207, Batch: 0, Loss: 1.120896
Train - Epoch 208, Batch: 0, Loss: 1.109217
Train - Epoch 209, Batch: 0, Loss: 1.102383
Train - Epoch 210, Batch: 0, Loss: 1.109525
Train - Epoch 211, Batch: 0, Loss: 1.121619
Train - Epoch 212, Batch: 0, Loss: 1.103482
Train - Epoch 213, Batch: 0, Loss: 1.091113
Train - Epoch 214, Batch: 0, Loss: 1.109622
Train - Epoch 215, Batch: 0, Loss: 1.107564
Train - Epoch 216, Batch: 0, Loss: 1.109272
Train - Epoch 217, Batch: 0, Loss: 1.123859
Train - Epoch 218, Batch: 0, Loss: 1.108680
Train - Epoch 219, Batch: 0, Loss: 1.098980
Train - Epoch 220, Batch: 0, Loss: 1.114530
Train - Epoch 221, Batch: 0, Loss: 1.107068
Train - Epoch 222, Batch: 0, Loss: 1.132611
Train - Epoch 223, Batch: 0, Loss: 1.107075
Train - Epoch 224, Batch: 0, Loss: 1.108759
Train - Epoch 225, Batch: 0, Loss: 1.108200
Train - Epoch 226, Batch: 0, Loss: 1.102146
Train - Epoch 227, Batch: 0, Loss: 1.092082
Train - Epoch 228, Batch: 0, Loss: 1.113843
Train - Epoch 229, Batch: 0, Loss: 1.083621
Train - Epoch 230, Batch: 0, Loss: 1.107491
Train - Epoch 231, Batch: 0, Loss: 1.099582
Train - Epoch 232, Batch: 0, Loss: 1.114364
Train - Epoch 233, Batch: 0, Loss: 1.100104
Train - Epoch 234, Batch: 0, Loss: 1.108970
Train - Epoch 235, Batch: 0, Loss: 1.103632
Train - Epoch 236, Batch: 0, Loss: 1.111916
Train - Epoch 237, Batch: 0, Loss: 1.095238
Train - Epoch 238, Batch: 0, Loss: 1.108122
Train - Epoch 239, Batch: 0, Loss: 1.089554
Train - Epoch 240, Batch: 0, Loss: 1.107129
Train - Epoch 241, Batch: 0, Loss: 1.100032
Train - Epoch 242, Batch: 0, Loss: 1.100492
Train - Epoch 243, Batch: 0, Loss: 1.089208
Train - Epoch 244, Batch: 0, Loss: 1.099981
Train - Epoch 245, Batch: 0, Loss: 1.093079
Train - Epoch 246, Batch: 0, Loss: 1.096436
Train - Epoch 247, Batch: 0, Loss: 1.091882
Train - Epoch 248, Batch: 0, Loss: 1.085575
Train - Epoch 249, Batch: 0, Loss: 1.093916
Train - Epoch 250, Batch: 0, Loss: 1.091371
Train - Epoch 251, Batch: 0, Loss: 1.083048
Train - Epoch 252, Batch: 0, Loss: 1.092496
Train - Epoch 253, Batch: 0, Loss: 1.083626
Train - Epoch 254, Batch: 0, Loss: 1.083695
Train - Epoch 255, Batch: 0, Loss: 1.102078
Train - Epoch 256, Batch: 0, Loss: 1.084412
Train - Epoch 257, Batch: 0, Loss: 1.098041
Train - Epoch 258, Batch: 0, Loss: 1.082243
Train - Epoch 259, Batch: 0, Loss: 1.092809
Train - Epoch 260, Batch: 0, Loss: 1.095737
Train - Epoch 261, Batch: 0, Loss: 1.088119
Train - Epoch 262, Batch: 0, Loss: 1.098623
Train - Epoch 263, Batch: 0, Loss: 1.107458
Train - Epoch 264, Batch: 0, Loss: 1.078607
Train - Epoch 265, Batch: 0, Loss: 1.091227
Train - Epoch 266, Batch: 0, Loss: 1.080129
Train - Epoch 267, Batch: 0, Loss: 1.102399
Train - Epoch 268, Batch: 0, Loss: 1.085812
Train - Epoch 269, Batch: 0, Loss: 1.080555
Train - Epoch 270, Batch: 0, Loss: 1.096815
Train - Epoch 271, Batch: 0, Loss: 1.086116
Train - Epoch 272, Batch: 0, Loss: 1.090354
Train - Epoch 273, Batch: 0, Loss: 1.084284
Train - Epoch 274, Batch: 0, Loss: 1.086534
Train - Epoch 275, Batch: 0, Loss: 1.083303
Train - Epoch 276, Batch: 0, Loss: 1.089691
Train - Epoch 277, Batch: 0, Loss: 1.094728
Train - Epoch 278, Batch: 0, Loss: 1.087877
Train - Epoch 279, Batch: 0, Loss: 1.090167
Train - Epoch 280, Batch: 0, Loss: 1.110565
Train - Epoch 281, Batch: 0, Loss: 1.085667
Train - Epoch 282, Batch: 0, Loss: 1.075704
Train - Epoch 283, Batch: 0, Loss: 1.084232
Train - Epoch 284, Batch: 0, Loss: 1.086838
Train - Epoch 285, Batch: 0, Loss: 1.080482
Train - Epoch 286, Batch: 0, Loss: 1.084130
Train - Epoch 287, Batch: 0, Loss: 1.091435
Train - Epoch 288, Batch: 0, Loss: 1.071844
Train - Epoch 289, Batch: 0, Loss: 1.069259
Train - Epoch 290, Batch: 0, Loss: 1.088832
Train - Epoch 291, Batch: 0, Loss: 1.075716
Train - Epoch 292, Batch: 0, Loss: 1.084221
Train - Epoch 293, Batch: 0, Loss: 1.105461
Train - Epoch 294, Batch: 0, Loss: 1.094148
Train - Epoch 295, Batch: 0, Loss: 1.094090
Train - Epoch 296, Batch: 0, Loss: 1.078823
Train - Epoch 297, Batch: 0, Loss: 1.098704
Train - Epoch 298, Batch: 0, Loss: 1.073459
Train - Epoch 299, Batch: 0, Loss: 1.075311
Train - Epoch 300, Batch: 0, Loss: 1.081800
Train - Epoch 301, Batch: 0, Loss: 1.078290
Train - Epoch 302, Batch: 0, Loss: 1.081143
Train - Epoch 303, Batch: 0, Loss: 1.079048
Train - Epoch 304, Batch: 0, Loss: 1.085532
Train - Epoch 305, Batch: 0, Loss: 1.071644
Train - Epoch 306, Batch: 0, Loss: 1.081465
Train - Epoch 307, Batch: 0, Loss: 1.068122
Train - Epoch 308, Batch: 0, Loss: 1.077153
Train - Epoch 309, Batch: 0, Loss: 1.084726
Train - Epoch 310, Batch: 0, Loss: 1.068494
Train - Epoch 311, Batch: 0, Loss: 1.077782
Train - Epoch 312, Batch: 0, Loss: 1.070014
Train - Epoch 313, Batch: 0, Loss: 1.078079
Train - Epoch 314, Batch: 0, Loss: 1.085022
Train - Epoch 315, Batch: 0, Loss: 1.059771
Train - Epoch 316, Batch: 0, Loss: 1.066912
Train - Epoch 317, Batch: 0, Loss: 1.080862
Train - Epoch 318, Batch: 0, Loss: 1.084310
Train - Epoch 319, Batch: 0, Loss: 1.079091
Train - Epoch 320, Batch: 0, Loss: 1.075554
Train - Epoch 321, Batch: 0, Loss: 1.077132
Train - Epoch 322, Batch: 0, Loss: 1.060998
Train - Epoch 323, Batch: 0, Loss: 1.050858
Train - Epoch 324, Batch: 0, Loss: 1.059494
Train - Epoch 325, Batch: 0, Loss: 1.059829
Train - Epoch 326, Batch: 0, Loss: 1.056516
Train - Epoch 327, Batch: 0, Loss: 1.068561
Train - Epoch 328, Batch: 0, Loss: 1.067753
Train - Epoch 329, Batch: 0, Loss: 1.078370
Train - Epoch 330, Batch: 0, Loss: 1.086807
Train - Epoch 331, Batch: 0, Loss: 1.056485
Train - Epoch 332, Batch: 0, Loss: 1.075191
Train - Epoch 333, Batch: 0, Loss: 1.082657
Train - Epoch 334, Batch: 0, Loss: 1.055485
Train - Epoch 335, Batch: 0, Loss: 1.064634
Train - Epoch 336, Batch: 0, Loss: 1.079540
Train - Epoch 337, Batch: 0, Loss: 1.058819
Train - Epoch 338, Batch: 0, Loss: 1.073798
Train - Epoch 339, Batch: 0, Loss: 1.064123
Train - Epoch 340, Batch: 0, Loss: 1.063980
Train - Epoch 341, Batch: 0, Loss: 1.057801
Train - Epoch 342, Batch: 0, Loss: 1.064301
Train - Epoch 343, Batch: 0, Loss: 1.058655
Train - Epoch 344, Batch: 0, Loss: 1.071357
Train - Epoch 345, Batch: 0, Loss: 1.057985
Train - Epoch 346, Batch: 0, Loss: 1.067083
Train - Epoch 347, Batch: 0, Loss: 1.062962
Train - Epoch 348, Batch: 0, Loss: 1.062199
Train - Epoch 349, Batch: 0, Loss: 1.049488
Train - Epoch 350, Batch: 0, Loss: 1.066922
Train - Epoch 351, Batch: 0, Loss: 1.068266
Train - Epoch 352, Batch: 0, Loss: 1.070165
Train - Epoch 353, Batch: 0, Loss: 1.066547
Train - Epoch 354, Batch: 0, Loss: 1.070885
Train - Epoch 355, Batch: 0, Loss: 1.075732
Train - Epoch 356, Batch: 0, Loss: 1.073987
Train - Epoch 357, Batch: 0, Loss: 1.060858
Train - Epoch 358, Batch: 0, Loss: 1.048544
Train - Epoch 359, Batch: 0, Loss: 1.060808
Train - Epoch 360, Batch: 0, Loss: 1.070967
Train - Epoch 361, Batch: 0, Loss: 1.046291
Train - Epoch 362, Batch: 0, Loss: 1.062501
Train - Epoch 363, Batch: 0, Loss: 1.055754
Train - Epoch 364, Batch: 0, Loss: 1.055615
Train - Epoch 365, Batch: 0, Loss: 1.054034
Train - Epoch 366, Batch: 0, Loss: 1.045620
Train - Epoch 367, Batch: 0, Loss: 1.064230
Train - Epoch 368, Batch: 0, Loss: 1.046883
Train - Epoch 369, Batch: 0, Loss: 1.071725
Train - Epoch 370, Batch: 0, Loss: 1.060089
Train - Epoch 371, Batch: 0, Loss: 1.063004
Train - Epoch 372, Batch: 0, Loss: 1.055275
Train - Epoch 373, Batch: 0, Loss: 1.051445/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 374, Batch: 0, Loss: 1.046401
Train - Epoch 375, Batch: 0, Loss: 1.053563
Train - Epoch 376, Batch: 0, Loss: 1.052238
Train - Epoch 377, Batch: 0, Loss: 1.059787
Train - Epoch 378, Batch: 0, Loss: 1.060211
Train - Epoch 379, Batch: 0, Loss: 1.041296
Train - Epoch 380, Batch: 0, Loss: 1.051997
Train - Epoch 381, Batch: 0, Loss: 1.054260
Train - Epoch 382, Batch: 0, Loss: 1.054443
Train - Epoch 383, Batch: 0, Loss: 1.064970
Train - Epoch 384, Batch: 0, Loss: 1.048952
Train - Epoch 385, Batch: 0, Loss: 1.052616
Train - Epoch 386, Batch: 0, Loss: 1.068828
Train - Epoch 387, Batch: 0, Loss: 1.033618
Train - Epoch 388, Batch: 0, Loss: 1.053316
Train - Epoch 389, Batch: 0, Loss: 1.061333
Train - Epoch 390, Batch: 0, Loss: 1.057776
Train - Epoch 391, Batch: 0, Loss: 1.048751
Train - Epoch 392, Batch: 0, Loss: 1.050517
Train - Epoch 393, Batch: 0, Loss: 1.072224
Train - Epoch 394, Batch: 0, Loss: 1.065693
Train - Epoch 395, Batch: 0, Loss: 1.058925
Train - Epoch 396, Batch: 0, Loss: 1.052009
Train - Epoch 397, Batch: 0, Loss: 1.067428
Train - Epoch 398, Batch: 0, Loss: 1.063742
Train - Epoch 399, Batch: 0, Loss: 1.061630
Train - Epoch 400, Batch: 0, Loss: 1.050517
Train - Epoch 401, Batch: 0, Loss: 1.051840
Train - Epoch 402, Batch: 0, Loss: 1.065635
Train - Epoch 403, Batch: 0, Loss: 1.055852
Train - Epoch 404, Batch: 0, Loss: 1.047580
Train - Epoch 405, Batch: 0, Loss: 1.043680
Train - Epoch 406, Batch: 0, Loss: 1.056694
Train - Epoch 407, Batch: 0, Loss: 1.059307
Train - Epoch 408, Batch: 0, Loss: 1.039080
Train - Epoch 409, Batch: 0, Loss: 1.055891
Train - Epoch 410, Batch: 0, Loss: 1.066308
Train - Epoch 411, Batch: 0, Loss: 1.050513
Train - Epoch 412, Batch: 0, Loss: 1.038807
Train - Epoch 413, Batch: 0, Loss: 1.036607
Train - Epoch 414, Batch: 0, Loss: 1.047885
Train - Epoch 415, Batch: 0, Loss: 1.052344
Train - Epoch 416, Batch: 0, Loss: 1.054750
Train - Epoch 417, Batch: 0, Loss: 1.059295
Train - Epoch 418, Batch: 0, Loss: 1.064752
Train - Epoch 419, Batch: 0, Loss: 1.055907
Train - Epoch 420, Batch: 0, Loss: 1.060350
Train - Epoch 421, Batch: 0, Loss: 1.054501
Train - Epoch 422, Batch: 0, Loss: 1.059040
Train - Epoch 423, Batch: 0, Loss: 1.054448
Train - Epoch 424, Batch: 0, Loss: 1.051799
Train - Epoch 425, Batch: 0, Loss: 1.057553
Train - Epoch 426, Batch: 0, Loss: 1.037704
Train - Epoch 427, Batch: 0, Loss: 1.035800
Train - Epoch 428, Batch: 0, Loss: 1.061908
Train - Epoch 429, Batch: 0, Loss: 1.060888
Train - Epoch 430, Batch: 0, Loss: 1.048640
Train - Epoch 431, Batch: 0, Loss: 1.052302
Train - Epoch 432, Batch: 0, Loss: 1.046720
Train - Epoch 433, Batch: 0, Loss: 1.040412
Train - Epoch 434, Batch: 0, Loss: 1.038835
Train - Epoch 435, Batch: 0, Loss: 1.046286
Train - Epoch 436, Batch: 0, Loss: 1.035498
Train - Epoch 437, Batch: 0, Loss: 1.051912
Train - Epoch 438, Batch: 0, Loss: 1.054799
Train - Epoch 439, Batch: 0, Loss: 1.055629
Train - Epoch 440, Batch: 0, Loss: 1.051979
Train - Epoch 441, Batch: 0, Loss: 1.052886
Train - Epoch 442, Batch: 0, Loss: 1.046999
Train - Epoch 443, Batch: 0, Loss: 1.047219
Train - Epoch 444, Batch: 0, Loss: 1.033286
Train - Epoch 445, Batch: 0, Loss: 1.030093
Train - Epoch 446, Batch: 0, Loss: 1.038590
Train - Epoch 447, Batch: 0, Loss: 1.039831
Train - Epoch 448, Batch: 0, Loss: 1.040742
Train - Epoch 449, Batch: 0, Loss: 1.047663
Train - Epoch 450, Batch: 0, Loss: 1.055987
Train - Epoch 451, Batch: 0, Loss: 1.032831
Train - Epoch 452, Batch: 0, Loss: 1.053266
Train - Epoch 453, Batch: 0, Loss: 1.045019
Train - Epoch 454, Batch: 0, Loss: 1.040558
Train - Epoch 455, Batch: 0, Loss: 1.046715
Train - Epoch 456, Batch: 0, Loss: 1.044637
Train - Epoch 457, Batch: 0, Loss: 1.039102
Train - Epoch 458, Batch: 0, Loss: 1.062783
Train - Epoch 459, Batch: 0, Loss: 1.052431
Train - Epoch 460, Batch: 0, Loss: 1.056321
Train - Epoch 461, Batch: 0, Loss: 1.040313
Train - Epoch 462, Batch: 0, Loss: 1.033656
Train - Epoch 463, Batch: 0, Loss: 1.053669
Train - Epoch 464, Batch: 0, Loss: 1.052225
Train - Epoch 465, Batch: 0, Loss: 1.044234
Train - Epoch 466, Batch: 0, Loss: 1.038745
Train - Epoch 467, Batch: 0, Loss: 1.050704
Train - Epoch 468, Batch: 0, Loss: 1.048647
Train - Epoch 469, Batch: 0, Loss: 1.049145
Train - Epoch 470, Batch: 0, Loss: 1.058317
Train - Epoch 471, Batch: 0, Loss: 1.042000
Train - Epoch 472, Batch: 0, Loss: 1.049218
Train - Epoch 473, Batch: 0, Loss: 1.046503
Train - Epoch 474, Batch: 0, Loss: 1.032475
Train - Epoch 475, Batch: 0, Loss: 1.047986
Train - Epoch 476, Batch: 0, Loss: 1.036305
Train - Epoch 477, Batch: 0, Loss: 1.044144
Train - Epoch 478, Batch: 0, Loss: 1.032572
Train - Epoch 479, Batch: 0, Loss: 1.048544
Train - Epoch 480, Batch: 0, Loss: 1.045994
Train - Epoch 481, Batch: 0, Loss: 1.050813
Train - Epoch 482, Batch: 0, Loss: 1.049258
Train - Epoch 483, Batch: 0, Loss: 1.033116
Train - Epoch 484, Batch: 0, Loss: 1.050750
Train - Epoch 485, Batch: 0, Loss: 1.035833
Train - Epoch 486, Batch: 0, Loss: 1.046383
Train - Epoch 487, Batch: 0, Loss: 1.025287
Train - Epoch 488, Batch: 0, Loss: 1.036118
Train - Epoch 489, Batch: 0, Loss: 1.053049
Train - Epoch 490, Batch: 0, Loss: 1.056851
Train - Epoch 491, Batch: 0, Loss: 1.018036
Train - Epoch 492, Batch: 0, Loss: 1.053343
Train - Epoch 493, Batch: 0, Loss: 1.055053
Train - Epoch 494, Batch: 0, Loss: 1.037194
Train - Epoch 495, Batch: 0, Loss: 1.046250
Train - Epoch 496, Batch: 0, Loss: 1.056146
Train - Epoch 497, Batch: 0, Loss: 1.029303
Train - Epoch 498, Batch: 0, Loss: 1.020864
Train - Epoch 499, Batch: 0, Loss: 1.037547
training_time:: 107.45230436325073
training time full:: 107.45237398147583
provenance prepare time:: 4.76837158203125e-07
here
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.631000
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 250
max_epoch:: 500
batch_size:: 10000
epoch:: 0
tensor([    3, 32773,     7,    13,    17,    22,    24,    25,    32, 32801,
        32802,    35, 32803,    36, 32806, 32810, 32812, 32815,    49,    53,
           54, 32823,    61,    67, 32835,    68,    71, 32840, 32841, 32842,
        32843, 32848,    81, 32850, 32852,    85,    86,    91, 32860, 32862,
           95,    98,    99,   100,   102,   104,   105,   107, 32876,   117])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
epoch:: 60
epoch:: 61
epoch:: 62
epoch:: 63
epoch:: 64
epoch:: 65
epoch:: 66
epoch:: 67
epoch:: 68
epoch:: 69
epoch:: 70
epoch:: 71
epoch:: 72
epoch:: 73
epoch:: 74
epoch:: 75
epoch:: 76
epoch:: 77
epoch:: 78
epoch:: 79
epoch:: 80
epoch:: 81
epoch:: 82
epoch:: 83
epoch:: 84
epoch:: 85
epoch:: 86
epoch:: 87
epoch:: 88
epoch:: 89
epoch:: 90
epoch:: 91
epoch:: 92
epoch:: 93
epoch:: 94
epoch:: 95
epoch:: 96
epoch:: 97
epoch:: 98
epoch:: 99
epoch:: 100
epoch:: 101
epoch:: 102
epoch:: 103
epoch:: 104
epoch:: 105
epoch:: 106
epoch:: 107
epoch:: 108
epoch:: 109
epoch:: 110
epoch:: 111
epoch:: 112
epoch:: 113
epoch:: 114
epoch:: 115
epoch:: 116
epoch:: 117
epoch:: 118
epoch:: 119
epoch:: 120
epoch:: 121
epoch:: 122
epoch:: 123
epoch:: 124
epoch:: 125
epoch:: 126
epoch:: 127
epoch:: 128
epoch:: 129
epoch:: 130
epoch:: 131
epoch:: 132
epoch:: 133
epoch:: 134
epoch:: 135
epoch:: 136
epoch:: 137
epoch:: 138
epoch:: 139
epoch:: 140
epoch:: 141
epoch:: 142
epoch:: 143
epoch:: 144
epoch:: 145
epoch:: 146
epoch:: 147
epoch:: 148
epoch:: 149
epoch:: 150
epoch:: 151
epoch:: 152
epoch:: 153
epoch:: 154
epoch:: 155
epoch:: 156
epoch:: 157
epoch:: 158
epoch:: 159
epoch:: 160
epoch:: 161
epoch:: 162
epoch:: 163
epoch:: 164
epoch:: 165
epoch:: 166
epoch:: 167
epoch:: 168
epoch:: 169
epoch:: 170
epoch:: 171
epoch:: 172
epoch:: 173
epoch:: 174
epoch:: 175
epoch:: 176
epoch:: 177
epoch:: 178
epoch:: 179
epoch:: 180
epoch:: 181
epoch:: 182
epoch:: 183
epoch:: 184
epoch:: 185
epoch:: 186
epoch:: 187
epoch:: 188
epoch:: 189
epoch:: 190
epoch:: 191
epoch:: 192
epoch:: 193
epoch:: 194
epoch:: 195
epoch:: 196
epoch:: 197
epoch:: 198
epoch:: 199
epoch:: 200
epoch:: 201
epoch:: 202
epoch:: 203
epoch:: 204
epoch:: 205
epoch:: 206
epoch:: 207
epoch:: 208
epoch:: 209
epoch:: 210
epoch:: 211
epoch:: 212
epoch:: 213
epoch:: 214
epoch:: 215
epoch:: 216
epoch:: 217
epoch:: 218
epoch:: 219
epoch:: 220
epoch:: 221
epoch:: 222
epoch:: 223
epoch:: 224
epoch:: 225
epoch:: 226
epoch:: 227
epoch:: 228
epoch:: 229
epoch:: 230
epoch:: 231
epoch:: 232
epoch:: 233
epoch:: 234
epoch:: 235
epoch:: 236
epoch:: 237
epoch:: 238
epoch:: 239
epoch:: 240
epoch:: 241
epoch:: 242
epoch:: 243
epoch:: 244
epoch:: 245
epoch:: 246
epoch:: 247
epoch:: 248
epoch:: 249
epoch:: 250
epoch:: 251
epoch:: 252
epoch:: 253
epoch:: 254
epoch:: 255
epoch:: 256
epoch:: 257
epoch:: 258
epoch:: 259
epoch:: 260
epoch:: 261
epoch:: 262
epoch:: 263
epoch:: 264
epoch:: 265
epoch:: 266
epoch:: 267
epoch:: 268
epoch:: 269
epoch:: 270
epoch:: 271
epoch:: 272
epoch:: 273
epoch:: 274
epoch:: 275
epoch:: 276
epoch:: 277
epoch:: 278
epoch:: 279
epoch:: 280
epoch:: 281
epoch:: 282
epoch:: 283
epoch:: 284
epoch:: 285
epoch:: 286
epoch:: 287
epoch:: 288
epoch:: 289
epoch:: 290
epoch:: 291
epoch:: 292
epoch:: 293
epoch:: 294
epoch:: 295
epoch:: 296
epoch:: 297
epoch:: 298
epoch:: 299
epoch:: 300
epoch:: 301
epoch:: 302
epoch:: 303
epoch:: 304
epoch:: 305
epoch:: 306
epoch:: 307
epoch:: 308
epoch:: 309
epoch:: 310
epoch:: 311
epoch:: 312
epoch:: 313
epoch:: 314
epoch:: 315
epoch:: 316
epoch:: 317
epoch:: 318
epoch:: 319
epoch:: 320
epoch:: 321
epoch:: 322
epoch:: 323
epoch:: 324
epoch:: 325
epoch:: 326
epoch:: 327
epoch:: 328
epoch:: 329
epoch:: 330
epoch:: 331
epoch:: 332
epoch:: 333
epoch:: 334
epoch:: 335
epoch:: 336
epoch:: 337
epoch:: 338
epoch:: 339
epoch:: 340
epoch:: 341
epoch:: 342
epoch:: 343
epoch:: 344
epoch:: 345
epoch:: 346
epoch:: 347
epoch:: 348
epoch:: 349
epoch:: 350
epoch:: 351
epoch:: 352
epoch:: 353
epoch:: 354
epoch:: 355
epoch:: 356
epoch:: 357
epoch:: 358
epoch:: 359
epoch:: 360
epoch:: 361
epoch:: 362
epoch:: 363
epoch:: 364
epoch:: 365
epoch:: 366
epoch:: 367
epoch:: 368
epoch:: 369
epoch:: 370
epoch:: 371
epoch:: 372
epoch:: 373
epoch:: 374
epoch:: 375
epoch:: 376
epoch:: 377
epoch:: 378
epoch:: 379
epoch:: 380
epoch:: 381
epoch:: 382
epoch:: 383
epoch:: 384
epoch:: 385
epoch:: 386
epoch:: 387
epoch:: 388
epoch:: 389
epoch:: 390
epoch:: 391
epoch:: 392
epoch:: 393
epoch:: 394
epoch:: 395
epoch:: 396
epoch:: 397
epoch:: 398
epoch:: 399
epoch:: 400
epoch:: 401
epoch:: 402
epoch:: 403
epoch:: 404
epoch:: 405
epoch:: 406
epoch:: 407
epoch:: 408
epoch:: 409
epoch:: 410
epoch:: 411
epoch:: 412
epoch:: 413
epoch:: 414
epoch:: 415
epoch:: 416
epoch:: 417
epoch:: 418
epoch:: 419
epoch:: 420
epoch:: 421
epoch:: 422
epoch:: 423
epoch:: 424
epoch:: 425
epoch:: 426
epoch:: 427
epoch:: 428
epoch:: 429
epoch:: 430
epoch:: 431
epoch:: 432
epoch:: 433
epoch:: 434
epoch:: 435
epoch:: 436
epoch:: 437
epoch:: 438
epoch:: 439
epoch:: 440
epoch:: 441
epoch:: 442
epoch:: 443
epoch:: 444
epoch:: 445
epoch:: 446
epoch:: 447
epoch:: 448
epoch:: 449
epoch:: 450
epoch:: 451
epoch:: 452
epoch:: 453
epoch:: 454
epoch:: 455
epoch:: 456
epoch:: 457
epoch:: 458
epoch:: 459
epoch:: 460
epoch:: 461
epoch:: 462
epoch:: 463
epoch:: 464
epoch:: 465
epoch:: 466
epoch:: 467
epoch:: 468
epoch:: 469
epoch:: 470
epoch:: 471
epoch:: 472
epoch:: 473
epoch:: 474
epoch:: 475
epoch:: 476
epoch:: 477
epoch:: 478
epoch:: 479
epoch:: 480
epoch:: 481
epoch:: 482
epoch:: 483
epoch:: 484
epoch:: 485
epoch:: 486
epoch:: 487
epoch:: 488
epoch:: 489
epoch:: 490
epoch:: 491
epoch:: 492
epoch:: 493
epoch:: 494
epoch:: 495
epoch:: 496
epoch:: 497
epoch:: 498
epoch:: 499
training time is 80.48747253417969
overhead:: 0
overhead2:: 2.7036213874816895
overhead3:: 0
time_baseline:: 80.48750948905945
curr_diff: 0 tensor(0.1355, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1355, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632300
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 0 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 250
max_epoch:: 500
overhead:: 0
overhead2:: 0.08132457733154297
overhead3:: 0.25996899604797363
overhead4:: 10.253082990646362
overhead5:: 0
memory usage:: 5642010624
time_provenance:: 18.381993293762207
curr_diff: 0 tensor(0.0334, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0334, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1611, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1611, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632000
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 0 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 250
max_epoch:: 500
overhead:: 0
overhead2:: 0.08403992652893066
overhead3:: 0.26568007469177246
overhead4:: 10.724812746047974
overhead5:: 0
memory usage:: 5631188992
time_provenance:: 18.95976758003235
curr_diff: 0 tensor(0.0333, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0333, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1611, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1611, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632700
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 0 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 250
max_epoch:: 500
overhead:: 0
overhead2:: 0.09376835823059082
overhead3:: 0.267519474029541
overhead4:: 11.02120304107666
overhead5:: 0
memory usage:: 5630283776
time_provenance:: 19.31779718399048
curr_diff: 0 tensor(0.0333, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0333, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1611, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1611, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632700
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 0 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 250
max_epoch:: 500
overhead:: 0
overhead2:: 0.1613917350769043
overhead3:: 0.4605066776275635
overhead4:: 18.809083223342896
overhead5:: 0
memory usage:: 5647470592
time_provenance:: 29.096237659454346
curr_diff: 0 tensor(0.0031, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0031, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1378, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1378, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632200
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 0 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 250
max_epoch:: 500
overhead:: 0
overhead2:: 0.16547703742980957
overhead3:: 0.45980095863342285
overhead4:: 18.771329641342163
overhead5:: 0
memory usage:: 5629370368
time_provenance:: 29.05422019958496
curr_diff: 0 tensor(0.0031, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0031, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1378, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1378, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632200
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 0 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 250
max_epoch:: 500
overhead:: 0
overhead2:: 0.1723315715789795
overhead3:: 0.46657323837280273
overhead4:: 19.418827056884766
overhead5:: 0
memory usage:: 5633445888
time_provenance:: 29.785231828689575
curr_diff: 0 tensor(0.0033, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0033, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1379, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1379, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.631600
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 0 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 250
max_epoch:: 500
overhead:: 0
overhead2:: 0.4096798896789551
overhead3:: 1.0276763439178467
overhead4:: 44.54131555557251
overhead5:: 0
memory usage:: 5640523776
time_provenance:: 61.19382429122925
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1357, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1357, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632300
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 0 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 250
max_epoch:: 500
overhead:: 0
overhead2:: 0.4238851070404053
overhead3:: 1.0256164073944092
overhead4:: 45.23653554916382
overhead5:: 0
memory usage:: 5656264704
time_provenance:: 61.93590497970581
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1357, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1357, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632300
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 0 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 250
max_epoch:: 500
overhead:: 0
overhead2:: 0.4368898868560791
overhead3:: 0.9969205856323242
overhead4:: 44.61384987831116
overhead5:: 0
memory usage:: 5646917632
time_provenance:: 61.33286142349243
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1357, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1357, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632300
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 0 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 250
max_epoch:: 500
overhead:: 0
overhead2:: 0.8448381423950195
overhead3:: 2.078413486480713
overhead4:: 82.08766889572144
overhead5:: 0
memory usage:: 5626437632
time_provenance:: 107.10374927520752
curr_diff: 0 tensor(4.9980e-13, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.9980e-13, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1355, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1355, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632300
repetition 1
python3 benchmark_exp_lr.py 0.001 10000 500 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05] Logistic_regression cifar10_2 1 0.0001 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 2.304476
Train - Epoch 1, Batch: 0, Loss: 2.244656
Train - Epoch 2, Batch: 0, Loss: 2.188073
Train - Epoch 3, Batch: 0, Loss: 2.138833
Train - Epoch 4, Batch: 0, Loss: 2.091607
Train - Epoch 5, Batch: 0, Loss: 2.048463
Train - Epoch 6, Batch: 0, Loss: 2.007424
Train - Epoch 7, Batch: 0, Loss: 1.969512
Train - Epoch 8, Batch: 0, Loss: 1.930790
Train - Epoch 9, Batch: 0, Loss: 1.898925
Train - Epoch 10, Batch: 0, Loss: 1.869326
Train - Epoch 11, Batch: 0, Loss: 1.843279
Train - Epoch 12, Batch: 0, Loss: 1.810365
Train - Epoch 13, Batch: 0, Loss: 1.786578
Train - Epoch 14, Batch: 0, Loss: 1.754739
Train - Epoch 15, Batch: 0, Loss: 1.729629
Train - Epoch 16, Batch: 0, Loss: 1.720914
Train - Epoch 17, Batch: 0, Loss: 1.689532
Train - Epoch 18, Batch: 0, Loss: 1.680022
Train - Epoch 19, Batch: 0, Loss: 1.657975
Train - Epoch 20, Batch: 0, Loss: 1.637238
Train - Epoch 21, Batch: 0, Loss: 1.627604
Train - Epoch 22, Batch: 0, Loss: 1.608172
Train - Epoch 23, Batch: 0, Loss: 1.596205
Train - Epoch 24, Batch: 0, Loss: 1.578886
Train - Epoch 25, Batch: 0, Loss: 1.562818
Train - Epoch 26, Batch: 0, Loss: 1.552046
Train - Epoch 27, Batch: 0, Loss: 1.547246
Train - Epoch 28, Batch: 0, Loss: 1.526004
Train - Epoch 29, Batch: 0, Loss: 1.518869
Train - Epoch 30, Batch: 0, Loss: 1.503765
Train - Epoch 31, Batch: 0, Loss: 1.495955
Train - Epoch 32, Batch: 0, Loss: 1.492050
Train - Epoch 33, Batch: 0, Loss: 1.478888
Train - Epoch 34, Batch: 0, Loss: 1.478150
Train - Epoch 35, Batch: 0, Loss: 1.465477
Train - Epoch 36, Batch: 0, Loss: 1.439362
Train - Epoch 37, Batch: 0, Loss: 1.446391
Train - Epoch 38, Batch: 0, Loss: 1.434637
Train - Epoch 39, Batch: 0, Loss: 1.429910
Train - Epoch 40, Batch: 0, Loss: 1.420699
Train - Epoch 41, Batch: 0, Loss: 1.412972
Train - Epoch 42, Batch: 0, Loss: 1.418239
Train - Epoch 43, Batch: 0, Loss: 1.403748
Train - Epoch 44, Batch: 0, Loss: 1.392221
Train - Epoch 45, Batch: 0, Loss: 1.385231
Train - Epoch 46, Batch: 0, Loss: 1.381957
Train - Epoch 47, Batch: 0, Loss: 1.385124
Train - Epoch 48, Batch: 0, Loss: 1.369974
Train - Epoch 49, Batch: 0, Loss: 1.360703
Train - Epoch 50, Batch: 0, Loss: 1.352082
Train - Epoch 51, Batch: 0, Loss: 1.360300
Train - Epoch 52, Batch: 0, Loss: 1.353741
Train - Epoch 53, Batch: 0, Loss: 1.344498
Train - Epoch 54, Batch: 0, Loss: 1.332986
Train - Epoch 55, Batch: 0, Loss: 1.336798
Train - Epoch 56, Batch: 0, Loss: 1.333337
Train - Epoch 57, Batch: 0, Loss: 1.334418
Train - Epoch 58, Batch: 0, Loss: 1.329344
Train - Epoch 59, Batch: 0, Loss: 1.321208
Train - Epoch 60, Batch: 0, Loss: 1.310401
Train - Epoch 61, Batch: 0, Loss: 1.315979
Train - Epoch 62, Batch: 0, Loss: 1.313225
Train - Epoch 63, Batch: 0, Loss: 1.303410
Train - Epoch 64, Batch: 0, Loss: 1.299506
Train - Epoch 65, Batch: 0, Loss: 1.296708
Train - Epoch 66, Batch: 0, Loss: 1.298361
Train - Epoch 67, Batch: 0, Loss: 1.288176
Train - Epoch 68, Batch: 0, Loss: 1.291006
Train - Epoch 69, Batch: 0, Loss: 1.291988
Train - Epoch 70, Batch: 0, Loss: 1.280611
Train - Epoch 71, Batch: 0, Loss: 1.269653
Train - Epoch 72, Batch: 0, Loss: 1.277305
Train - Epoch 73, Batch: 0, Loss: 1.274835
Train - Epoch 74, Batch: 0, Loss: 1.264009
Train - Epoch 75, Batch: 0, Loss: 1.263255
Train - Epoch 76, Batch: 0, Loss: 1.267116
Train - Epoch 77, Batch: 0, Loss: 1.263288
Train - Epoch 78, Batch: 0, Loss: 1.267328
Train - Epoch 79, Batch: 0, Loss: 1.261157
Train - Epoch 80, Batch: 0, Loss: 1.250987
Train - Epoch 81, Batch: 0, Loss: 1.264869
Train - Epoch 82, Batch: 0, Loss: 1.247889
Train - Epoch 83, Batch: 0, Loss: 1.245144
Train - Epoch 84, Batch: 0, Loss: 1.234084
Train - Epoch 85, Batch: 0, Loss: 1.247348
Train - Epoch 86, Batch: 0, Loss: 1.253672
Train - Epoch 87, Batch: 0, Loss: 1.245982
Train - Epoch 88, Batch: 0, Loss: 1.231234
Train - Epoch 89, Batch: 0, Loss: 1.233578
Train - Epoch 90, Batch: 0, Loss: 1.232281
Train - Epoch 91, Batch: 0, Loss: 1.235499
Train - Epoch 92, Batch: 0, Loss: 1.232089
Train - Epoch 93, Batch: 0, Loss: 1.230038
Train - Epoch 94, Batch: 0, Loss: 1.233014
Train - Epoch 95, Batch: 0, Loss: 1.227507
Train - Epoch 96, Batch: 0, Loss: 1.214001
Train - Epoch 97, Batch: 0, Loss: 1.217601
Train - Epoch 98, Batch: 0, Loss: 1.218949
Train - Epoch 99, Batch: 0, Loss: 1.208457
Train - Epoch 100, Batch: 0, Loss: 1.209643
Train - Epoch 101, Batch: 0, Loss: 1.216836
Train - Epoch 102, Batch: 0, Loss: 1.213217
Train - Epoch 103, Batch: 0, Loss: 1.216496
Train - Epoch 104, Batch: 0, Loss: 1.221279
Train - Epoch 105, Batch: 0, Loss: 1.209159
Train - Epoch 106, Batch: 0, Loss: 1.211247
Train - Epoch 107, Batch: 0, Loss: 1.207293
Train - Epoch 108, Batch: 0, Loss: 1.207344
Train - Epoch 109, Batch: 0, Loss: 1.188536
Train - Epoch 110, Batch: 0, Loss: 1.194088
Train - Epoch 111, Batch: 0, Loss: 1.195800
Train - Epoch 112, Batch: 0, Loss: 1.203499
Train - Epoch 113, Batch: 0, Loss: 1.187908
Train - Epoch 114, Batch: 0, Loss: 1.204349
Train - Epoch 115, Batch: 0, Loss: 1.191133
Train - Epoch 116, Batch: 0, Loss: 1.175929
Train - Epoch 117, Batch: 0, Loss: 1.186463
Train - Epoch 118, Batch: 0, Loss: 1.188521
Train - Epoch 119, Batch: 0, Loss: 1.187499
Train - Epoch 120, Batch: 0, Loss: 1.181915
Train - Epoch 121, Batch: 0, Loss: 1.185575
Train - Epoch 122, Batch: 0, Loss: 1.188500
Train - Epoch 123, Batch: 0, Loss: 1.195351
Train - Epoch 124, Batch: 0, Loss: 1.170611
Train - Epoch 125, Batch: 0, Loss: 1.173389
Train - Epoch 126, Batch: 0, Loss: 1.199255
Train - Epoch 127, Batch: 0, Loss: 1.176183
Train - Epoch 128, Batch: 0, Loss: 1.174922
Train - Epoch 129, Batch: 0, Loss: 1.160398
Train - Epoch 130, Batch: 0, Loss: 1.171635
Train - Epoch 131, Batch: 0, Loss: 1.162656
Train - Epoch 132, Batch: 0, Loss: 1.163873
Train - Epoch 133, Batch: 0, Loss: 1.169956
Train - Epoch 134, Batch: 0, Loss: 1.165259
Train - Epoch 135, Batch: 0, Loss: 1.166828
Train - Epoch 136, Batch: 0, Loss: 1.172110
Train - Epoch 137, Batch: 0, Loss: 1.158946
Train - Epoch 138, Batch: 0, Loss: 1.161376
Train - Epoch 139, Batch: 0, Loss: 1.178080
Train - Epoch 140, Batch: 0, Loss: 1.161448
Train - Epoch 141, Batch: 0, Loss: 1.162740
Train - Epoch 142, Batch: 0, Loss: 1.175514
Train - Epoch 143, Batch: 0, Loss: 1.164970
Train - Epoch 144, Batch: 0, Loss: 1.155770
Train - Epoch 145, Batch: 0, Loss: 1.154516
Train - Epoch 146, Batch: 0, Loss: 1.166643
Train - Epoch 147, Batch: 0, Loss: 1.157904
Train - Epoch 148, Batch: 0, Loss: 1.164631
Train - Epoch 149, Batch: 0, Loss: 1.167386
Train - Epoch 150, Batch: 0, Loss: 1.151825
Train - Epoch 151, Batch: 0, Loss: 1.158318
Train - Epoch 152, Batch: 0, Loss: 1.138519
Train - Epoch 153, Batch: 0, Loss: 1.153517
Train - Epoch 154, Batch: 0, Loss: 1.154229
Train - Epoch 155, Batch: 0, Loss: 1.154249
Train - Epoch 156, Batch: 0, Loss: 1.139137
Train - Epoch 157, Batch: 0, Loss: 1.163865
Train - Epoch 158, Batch: 0, Loss: 1.143832
Train - Epoch 159, Batch: 0, Loss: 1.161688
Train - Epoch 160, Batch: 0, Loss: 1.148487
Train - Epoch 161, Batch: 0, Loss: 1.143709
Train - Epoch 162, Batch: 0, Loss: 1.152780
Train - Epoch 163, Batch: 0, Loss: 1.151953
Train - Epoch 164, Batch: 0, Loss: 1.142950
Train - Epoch 165, Batch: 0, Loss: 1.144976
Train - Epoch 166, Batch: 0, Loss: 1.151063
Train - Epoch 167, Batch: 0, Loss: 1.132615
Train - Epoch 168, Batch: 0, Loss: 1.140967
Train - Epoch 169, Batch: 0, Loss: 1.127137
Train - Epoch 170, Batch: 0, Loss: 1.144366
Train - Epoch 171, Batch: 0, Loss: 1.131246
Train - Epoch 172, Batch: 0, Loss: 1.135888
Train - Epoch 173, Batch: 0, Loss: 1.128328
Train - Epoch 174, Batch: 0, Loss: 1.132401
Train - Epoch 175, Batch: 0, Loss: 1.143274
Train - Epoch 176, Batch: 0, Loss: 1.135789
Train - Epoch 177, Batch: 0, Loss: 1.143870
Train - Epoch 178, Batch: 0, Loss: 1.144145
Train - Epoch 179, Batch: 0, Loss: 1.132366
Train - Epoch 180, Batch: 0, Loss: 1.131190
Train - Epoch 181, Batch: 0, Loss: 1.126574
Train - Epoch 182, Batch: 0, Loss: 1.124219
Train - Epoch 183, Batch: 0, Loss: 1.128903
Train - Epoch 184, Batch: 0, Loss: 1.137047
Train - Epoch 185, Batch: 0, Loss: 1.126652
Train - Epoch 186, Batch: 0, Loss: 1.151782
Train - Epoch 187, Batch: 0, Loss: 1.121326
Train - Epoch 188, Batch: 0, Loss: 1.128535
Train - Epoch 189, Batch: 0, Loss: 1.126326
Train - Epoch 190, Batch: 0, Loss: 1.131519
Train - Epoch 191, Batch: 0, Loss: 1.132286
Train - Epoch 192, Batch: 0, Loss: 1.130603
Train - Epoch 193, Batch: 0, Loss: 1.113884
Train - Epoch 194, Batch: 0, Loss: 1.130003
Train - Epoch 195, Batch: 0, Loss: 1.121038
Train - Epoch 196, Batch: 0, Loss: 1.125891
Train - Epoch 197, Batch: 0, Loss: 1.124739
Train - Epoch 198, Batch: 0, Loss: 1.113896
Train - Epoch 199, Batch: 0, Loss: 1.107602
Train - Epoch 200, Batch: 0, Loss: 1.118388
Train - Epoch 201, Batch: 0, Loss: 1.109771
Train - Epoch 202, Batch: 0, Loss: 1.118417
Train - Epoch 203, Batch: 0, Loss: 1.117634
Train - Epoch 204, Batch: 0, Loss: 1.123799
Train - Epoch 205, Batch: 0, Loss: 1.124698
Train - Epoch 206, Batch: 0, Loss: 1.132555
Train - Epoch 207, Batch: 0, Loss: 1.103214
Train - Epoch 208, Batch: 0, Loss: 1.109419
Train - Epoch 209, Batch: 0, Loss: 1.098328
Train - Epoch 210, Batch: 0, Loss: 1.110521
Train - Epoch 211, Batch: 0, Loss: 1.115690
Train - Epoch 212, Batch: 0, Loss: 1.108927
Train - Epoch 213, Batch: 0, Loss: 1.114548
Train - Epoch 214, Batch: 0, Loss: 1.117763
Train - Epoch 215, Batch: 0, Loss: 1.111931
Train - Epoch 216, Batch: 0, Loss: 1.109079
Train - Epoch 217, Batch: 0, Loss: 1.113508
Train - Epoch 218, Batch: 0, Loss: 1.104967
Train - Epoch 219, Batch: 0, Loss: 1.096797
Train - Epoch 220, Batch: 0, Loss: 1.102925
Train - Epoch 221, Batch: 0, Loss: 1.114137
Train - Epoch 222, Batch: 0, Loss: 1.106846
Train - Epoch 223, Batch: 0, Loss: 1.109770
Train - Epoch 224, Batch: 0, Loss: 1.095627
Train - Epoch 225, Batch: 0, Loss: 1.117013
Train - Epoch 226, Batch: 0, Loss: 1.104776
Train - Epoch 227, Batch: 0, Loss: 1.110345
Train - Epoch 228, Batch: 0, Loss: 1.089600
Train - Epoch 229, Batch: 0, Loss: 1.104948
Train - Epoch 230, Batch: 0, Loss: 1.099436
Train - Epoch 231, Batch: 0, Loss: 1.105925
Train - Epoch 232, Batch: 0, Loss: 1.098769
Train - Epoch 233, Batch: 0, Loss: 1.081160
Train - Epoch 234, Batch: 0, Loss: 1.097393
Train - Epoch 235, Batch: 0, Loss: 1.103574
Train - Epoch 236, Batch: 0, Loss: 1.105293
Train - Epoch 237, Batch: 0, Loss: 1.115928
Train - Epoch 238, Batch: 0, Loss: 1.097177
Train - Epoch 239, Batch: 0, Loss: 1.100957
Train - Epoch 240, Batch: 0, Loss: 1.080652
Train - Epoch 241, Batch: 0, Loss: 1.114699
Train - Epoch 242, Batch: 0, Loss: 1.109932
Train - Epoch 243, Batch: 0, Loss: 1.097744
Train - Epoch 244, Batch: 0, Loss: 1.102183
Train - Epoch 245, Batch: 0, Loss: 1.098813
Train - Epoch 246, Batch: 0, Loss: 1.092511
Train - Epoch 247, Batch: 0, Loss: 1.089802
Train - Epoch 248, Batch: 0, Loss: 1.097271
Train - Epoch 249, Batch: 0, Loss: 1.092042
Train - Epoch 250, Batch: 0, Loss: 1.086843
Train - Epoch 251, Batch: 0, Loss: 1.089887
Train - Epoch 252, Batch: 0, Loss: 1.088223
Train - Epoch 253, Batch: 0, Loss: 1.100192
Train - Epoch 254, Batch: 0, Loss: 1.083111
Train - Epoch 255, Batch: 0, Loss: 1.091984
Train - Epoch 256, Batch: 0, Loss: 1.094550
Train - Epoch 257, Batch: 0, Loss: 1.095471
Train - Epoch 258, Batch: 0, Loss: 1.095375
Train - Epoch 259, Batch: 0, Loss: 1.083449
Train - Epoch 260, Batch: 0, Loss: 1.085252
Train - Epoch 261, Batch: 0, Loss: 1.076605
Train - Epoch 262, Batch: 0, Loss: 1.092254
Train - Epoch 263, Batch: 0, Loss: 1.084887
Train - Epoch 264, Batch: 0, Loss: 1.089598
Train - Epoch 265, Batch: 0, Loss: 1.080838
Train - Epoch 266, Batch: 0, Loss: 1.087929
Train - Epoch 267, Batch: 0, Loss: 1.081413
Train - Epoch 268, Batch: 0, Loss: 1.088472
Train - Epoch 269, Batch: 0, Loss: 1.088815
Train - Epoch 270, Batch: 0, Loss: 1.082124
Train - Epoch 271, Batch: 0, Loss: 1.093900
Train - Epoch 272, Batch: 0, Loss: 1.093812
Train - Epoch 273, Batch: 0, Loss: 1.085932
Train - Epoch 274, Batch: 0, Loss: 1.080751
Train - Epoch 275, Batch: 0, Loss: 1.080719
Train - Epoch 276, Batch: 0, Loss: 1.090087
Train - Epoch 277, Batch: 0, Loss: 1.081859
Train - Epoch 278, Batch: 0, Loss: 1.087190
Train - Epoch 279, Batch: 0, Loss: 1.077974
Train - Epoch 280, Batch: 0, Loss: 1.087614
Train - Epoch 281, Batch: 0, Loss: 1.089164
Train - Epoch 282, Batch: 0, Loss: 1.079479
Train - Epoch 283, Batch: 0, Loss: 1.084317
Train - Epoch 284, Batch: 0, Loss: 1.093378
Train - Epoch 285, Batch: 0, Loss: 1.076602
Train - Epoch 286, Batch: 0, Loss: 1.082355
Train - Epoch 287, Batch: 0, Loss: 1.075966
Train - Epoch 288, Batch: 0, Loss: 1.078256
Train - Epoch 289, Batch: 0, Loss: 1.076762
Train - Epoch 290, Batch: 0, Loss: 1.077319
Train - Epoch 291, Batch: 0, Loss: 1.092389
Train - Epoch 292, Batch: 0, Loss: 1.078669
Train - Epoch 293, Batch: 0, Loss: 1.067795
Train - Epoch 294, Batch: 0, Loss: 1.082668
Train - Epoch 295, Batch: 0, Loss: 1.077410
Train - Epoch 296, Batch: 0, Loss: 1.082219
Train - Epoch 297, Batch: 0, Loss: 1.078550
Train - Epoch 298, Batch: 0, Loss: 1.081052
Train - Epoch 299, Batch: 0, Loss: 1.063980
Train - Epoch 300, Batch: 0, Loss: 1.074197
Train - Epoch 301, Batch: 0, Loss: 1.080744
Train - Epoch 302, Batch: 0, Loss: 1.073997
Train - Epoch 303, Batch: 0, Loss: 1.087712
Train - Epoch 304, Batch: 0, Loss: 1.064949
Train - Epoch 305, Batch: 0, Loss: 1.089029
Train - Epoch 306, Batch: 0, Loss: 1.089129
Train - Epoch 307, Batch: 0, Loss: 1.063297
Train - Epoch 308, Batch: 0, Loss: 1.062516
Train - Epoch 309, Batch: 0, Loss: 1.069861
Train - Epoch 310, Batch: 0, Loss: 1.066779
Train - Epoch 311, Batch: 0, Loss: 1.066714
Train - Epoch 312, Batch: 0, Loss: 1.091333
Train - Epoch 313, Batch: 0, Loss: 1.080152
Train - Epoch 314, Batch: 0, Loss: 1.065800
Train - Epoch 315, Batch: 0, Loss: 1.073536
Train - Epoch 316, Batch: 0, Loss: 1.081910
Train - Epoch 317, Batch: 0, Loss: 1.067683
Train - Epoch 318, Batch: 0, Loss: 1.075631
Train - Epoch 319, Batch: 0, Loss: 1.068068
Train - Epoch 320, Batch: 0, Loss: 1.084804
Train - Epoch 321, Batch: 0, Loss: 1.069752
Train - Epoch 322, Batch: 0, Loss: 1.078864
Train - Epoch 323, Batch: 0, Loss: 1.074176
Train - Epoch 324, Batch: 0, Loss: 1.058209
Train - Epoch 325, Batch: 0, Loss: 1.073304
Train - Epoch 326, Batch: 0, Loss: 1.071957
Train - Epoch 327, Batch: 0, Loss: 1.078224
Train - Epoch 328, Batch: 0, Loss: 1.047989
Train - Epoch 329, Batch: 0, Loss: 1.076666
Train - Epoch 330, Batch: 0, Loss: 1.057549
Train - Epoch 331, Batch: 0, Loss: 1.079875
Train - Epoch 332, Batch: 0, Loss: 1.076769
Train - Epoch 333, Batch: 0, Loss: 1.066596
Train - Epoch 334, Batch: 0, Loss: 1.061507
Train - Epoch 335, Batch: 0, Loss: 1.059043
Train - Epoch 336, Batch: 0, Loss: 1.067331
Train - Epoch 337, Batch: 0, Loss: 1.073701
Train - Epoch 338, Batch: 0, Loss: 1.070607
Train - Epoch 339, Batch: 0, Loss: 1.060659
Train - Epoch 340, Batch: 0, Loss: 1.070017
Train - Epoch 341, Batch: 0, Loss: 1.066332
Train - Epoch 342, Batch: 0, Loss: 1.058563
Train - Epoch 343, Batch: 0, Loss: 1.058507
Train - Epoch 344, Batch: 0, Loss: 1.076863
Train - Epoch 345, Batch: 0, Loss: 1.075381
Train - Epoch 346, Batch: 0, Loss: 1.070056
Train - Epoch 347, Batch: 0, Loss: 1.059910
Train - Epoch 348, Batch: 0, Loss: 1.062835
Train - Epoch 349, Batch: 0, Loss: 1.070752
Train - Epoch 350, Batch: 0, Loss: 1.076197
Train - Epoch 351, Batch: 0, Loss: 1.058400
Train - Epoch 352, Batch: 0, Loss: 1.067984
Train - Epoch 353, Batch: 0, Loss: 1.060438
Train - Epoch 354, Batch: 0, Loss: 1.045384
Train - Epoch 355, Batch: 0, Loss: 1.063466
Train - Epoch 356, Batch: 0, Loss: 1.061904
Train - Epoch 357, Batch: 0, Loss: 1.060518
Train - Epoch 358, Batch: 0, Loss: 1.071069
Train - Epoch 359, Batch: 0, Loss: 1.059634
Train - Epoch 360, Batch: 0, Loss: 1.063800
Train - Epoch 361, Batch: 0, Loss: 1.065585
Train - Epoch 362, Batch: 0, Loss: 1.075904
Train - Epoch 363, Batch: 0, Loss: 1.053366
Train - Epoch 364, Batch: 0, Loss: 1.043936
Train - Epoch 365, Batch: 0, Loss: 1.059801
Train - Epoch 366, Batch: 0, Loss: 1.049470
Train - Epoch 367, Batch: 0, Loss: 1.044955
Train - Epoch 368, Batch: 0, Loss: 1.060841
Train - Epoch 369, Batch: 0, Loss: 1.056805
Train - Epoch 370, Batch: 0, Loss: 1.069159
Train - Epoch 371, Batch: 0, Loss: 1.052148
Train - Epoch 372, Batch: 0, Loss: 1.059226
Train - Epoch 373, Batch: 0, Loss: 1.048584/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 374, Batch: 0, Loss: 1.068001
Train - Epoch 375, Batch: 0, Loss: 1.066195
Train - Epoch 376, Batch: 0, Loss: 1.045747
Train - Epoch 377, Batch: 0, Loss: 1.067198
Train - Epoch 378, Batch: 0, Loss: 1.064507
Train - Epoch 379, Batch: 0, Loss: 1.058186
Train - Epoch 380, Batch: 0, Loss: 1.041282
Train - Epoch 381, Batch: 0, Loss: 1.054987
Train - Epoch 382, Batch: 0, Loss: 1.058090
Train - Epoch 383, Batch: 0, Loss: 1.057875
Train - Epoch 384, Batch: 0, Loss: 1.051217
Train - Epoch 385, Batch: 0, Loss: 1.062048
Train - Epoch 386, Batch: 0, Loss: 1.077956
Train - Epoch 387, Batch: 0, Loss: 1.049604
Train - Epoch 388, Batch: 0, Loss: 1.061978
Train - Epoch 389, Batch: 0, Loss: 1.059210
Train - Epoch 390, Batch: 0, Loss: 1.056273
Train - Epoch 391, Batch: 0, Loss: 1.059599
Train - Epoch 392, Batch: 0, Loss: 1.059450
Train - Epoch 393, Batch: 0, Loss: 1.060728
Train - Epoch 394, Batch: 0, Loss: 1.069588
Train - Epoch 395, Batch: 0, Loss: 1.055078
Train - Epoch 396, Batch: 0, Loss: 1.051454
Train - Epoch 397, Batch: 0, Loss: 1.047199
Train - Epoch 398, Batch: 0, Loss: 1.068923
Train - Epoch 399, Batch: 0, Loss: 1.040637
Train - Epoch 400, Batch: 0, Loss: 1.056403
Train - Epoch 401, Batch: 0, Loss: 1.058068
Train - Epoch 402, Batch: 0, Loss: 1.053693
Train - Epoch 403, Batch: 0, Loss: 1.060775
Train - Epoch 404, Batch: 0, Loss: 1.052802
Train - Epoch 405, Batch: 0, Loss: 1.052779
Train - Epoch 406, Batch: 0, Loss: 1.051661
Train - Epoch 407, Batch: 0, Loss: 1.056921
Train - Epoch 408, Batch: 0, Loss: 1.049013
Train - Epoch 409, Batch: 0, Loss: 1.051850
Train - Epoch 410, Batch: 0, Loss: 1.045005
Train - Epoch 411, Batch: 0, Loss: 1.040394
Train - Epoch 412, Batch: 0, Loss: 1.060090
Train - Epoch 413, Batch: 0, Loss: 1.069647
Train - Epoch 414, Batch: 0, Loss: 1.044061
Train - Epoch 415, Batch: 0, Loss: 1.045872
Train - Epoch 416, Batch: 0, Loss: 1.067016
Train - Epoch 417, Batch: 0, Loss: 1.052447
Train - Epoch 418, Batch: 0, Loss: 1.054647
Train - Epoch 419, Batch: 0, Loss: 1.051144
Train - Epoch 420, Batch: 0, Loss: 1.038494
Train - Epoch 421, Batch: 0, Loss: 1.042776
Train - Epoch 422, Batch: 0, Loss: 1.054857
Train - Epoch 423, Batch: 0, Loss: 1.064062
Train - Epoch 424, Batch: 0, Loss: 1.041204
Train - Epoch 425, Batch: 0, Loss: 1.045534
Train - Epoch 426, Batch: 0, Loss: 1.053126
Train - Epoch 427, Batch: 0, Loss: 1.049510
Train - Epoch 428, Batch: 0, Loss: 1.044726
Train - Epoch 429, Batch: 0, Loss: 1.060737
Train - Epoch 430, Batch: 0, Loss: 1.041724
Train - Epoch 431, Batch: 0, Loss: 1.043125
Train - Epoch 432, Batch: 0, Loss: 1.042205
Train - Epoch 433, Batch: 0, Loss: 1.043788
Train - Epoch 434, Batch: 0, Loss: 1.051593
Train - Epoch 435, Batch: 0, Loss: 1.054760
Train - Epoch 436, Batch: 0, Loss: 1.048475
Train - Epoch 437, Batch: 0, Loss: 1.050572
Train - Epoch 438, Batch: 0, Loss: 1.032694
Train - Epoch 439, Batch: 0, Loss: 1.057253
Train - Epoch 440, Batch: 0, Loss: 1.044219
Train - Epoch 441, Batch: 0, Loss: 1.049762
Train - Epoch 442, Batch: 0, Loss: 1.048082
Train - Epoch 443, Batch: 0, Loss: 1.046265
Train - Epoch 444, Batch: 0, Loss: 1.051139
Train - Epoch 445, Batch: 0, Loss: 1.041691
Train - Epoch 446, Batch: 0, Loss: 1.036232
Train - Epoch 447, Batch: 0, Loss: 1.052562
Train - Epoch 448, Batch: 0, Loss: 1.046538
Train - Epoch 449, Batch: 0, Loss: 1.045784
Train - Epoch 450, Batch: 0, Loss: 1.039669
Train - Epoch 451, Batch: 0, Loss: 1.059676
Train - Epoch 452, Batch: 0, Loss: 1.046306
Train - Epoch 453, Batch: 0, Loss: 1.052998
Train - Epoch 454, Batch: 0, Loss: 1.026494
Train - Epoch 455, Batch: 0, Loss: 1.055666
Train - Epoch 456, Batch: 0, Loss: 1.048635
Train - Epoch 457, Batch: 0, Loss: 1.057279
Train - Epoch 458, Batch: 0, Loss: 1.043465
Train - Epoch 459, Batch: 0, Loss: 1.047880
Train - Epoch 460, Batch: 0, Loss: 1.042738
Train - Epoch 461, Batch: 0, Loss: 1.044542
Train - Epoch 462, Batch: 0, Loss: 1.034475
Train - Epoch 463, Batch: 0, Loss: 1.049185
Train - Epoch 464, Batch: 0, Loss: 1.039902
Train - Epoch 465, Batch: 0, Loss: 1.052986
Train - Epoch 466, Batch: 0, Loss: 1.026560
Train - Epoch 467, Batch: 0, Loss: 1.040078
Train - Epoch 468, Batch: 0, Loss: 1.036652
Train - Epoch 469, Batch: 0, Loss: 1.029969
Train - Epoch 470, Batch: 0, Loss: 1.047391
Train - Epoch 471, Batch: 0, Loss: 1.043370
Train - Epoch 472, Batch: 0, Loss: 1.047533
Train - Epoch 473, Batch: 0, Loss: 1.055228
Train - Epoch 474, Batch: 0, Loss: 1.046469
Train - Epoch 475, Batch: 0, Loss: 1.034849
Train - Epoch 476, Batch: 0, Loss: 1.043722
Train - Epoch 477, Batch: 0, Loss: 1.049878
Train - Epoch 478, Batch: 0, Loss: 1.038963
Train - Epoch 479, Batch: 0, Loss: 1.031306
Train - Epoch 480, Batch: 0, Loss: 1.038306
Train - Epoch 481, Batch: 0, Loss: 1.056984
Train - Epoch 482, Batch: 0, Loss: 1.033328
Train - Epoch 483, Batch: 0, Loss: 1.030081
Train - Epoch 484, Batch: 0, Loss: 1.038959
Train - Epoch 485, Batch: 0, Loss: 1.040506
Train - Epoch 486, Batch: 0, Loss: 1.049589
Train - Epoch 487, Batch: 0, Loss: 1.041176
Train - Epoch 488, Batch: 0, Loss: 1.027619
Train - Epoch 489, Batch: 0, Loss: 1.021969
Train - Epoch 490, Batch: 0, Loss: 1.038786
Train - Epoch 491, Batch: 0, Loss: 1.028872
Train - Epoch 492, Batch: 0, Loss: 1.034474
Train - Epoch 493, Batch: 0, Loss: 1.018864
Train - Epoch 494, Batch: 0, Loss: 1.056466
Train - Epoch 495, Batch: 0, Loss: 1.040787
Train - Epoch 496, Batch: 0, Loss: 1.032889
Train - Epoch 497, Batch: 0, Loss: 1.040876
Train - Epoch 498, Batch: 0, Loss: 1.035763
Train - Epoch 499, Batch: 0, Loss: 1.041859
training_time:: 108.27370405197144
training time full:: 108.27377343177795
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632100
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 250
max_epoch:: 500
batch_size:: 10000
epoch:: 0
tensor([    0, 32776, 32777,    12, 32780,    15,    16, 32792, 32800,    38,
           42, 32817,    50,    49,    52, 32818,    55, 32825, 32830, 32836,
        32837,    70, 32840, 32841,    78,    79,    91, 32860,    95,    96,
        32866,   100,   103, 32872,   106, 32876, 32880,   116, 32892,   126,
          128,   132,   135, 32904,   141,   145, 32913,   147,   149, 32920])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
epoch:: 60
epoch:: 61
epoch:: 62
epoch:: 63
epoch:: 64
epoch:: 65
epoch:: 66
epoch:: 67
epoch:: 68
epoch:: 69
epoch:: 70
epoch:: 71
epoch:: 72
epoch:: 73
epoch:: 74
epoch:: 75
epoch:: 76
epoch:: 77
epoch:: 78
epoch:: 79
epoch:: 80
epoch:: 81
epoch:: 82
epoch:: 83
epoch:: 84
epoch:: 85
epoch:: 86
epoch:: 87
epoch:: 88
epoch:: 89
epoch:: 90
epoch:: 91
epoch:: 92
epoch:: 93
epoch:: 94
epoch:: 95
epoch:: 96
epoch:: 97
epoch:: 98
epoch:: 99
epoch:: 100
epoch:: 101
epoch:: 102
epoch:: 103
epoch:: 104
epoch:: 105
epoch:: 106
epoch:: 107
epoch:: 108
epoch:: 109
epoch:: 110
epoch:: 111
epoch:: 112
epoch:: 113
epoch:: 114
epoch:: 115
epoch:: 116
epoch:: 117
epoch:: 118
epoch:: 119
epoch:: 120
epoch:: 121
epoch:: 122
epoch:: 123
epoch:: 124
epoch:: 125
epoch:: 126
epoch:: 127
epoch:: 128
epoch:: 129
epoch:: 130
epoch:: 131
epoch:: 132
epoch:: 133
epoch:: 134
epoch:: 135
epoch:: 136
epoch:: 137
epoch:: 138
epoch:: 139
epoch:: 140
epoch:: 141
epoch:: 142
epoch:: 143
epoch:: 144
epoch:: 145
epoch:: 146
epoch:: 147
epoch:: 148
epoch:: 149
epoch:: 150
epoch:: 151
epoch:: 152
epoch:: 153
epoch:: 154
epoch:: 155
epoch:: 156
epoch:: 157
epoch:: 158
epoch:: 159
epoch:: 160
epoch:: 161
epoch:: 162
epoch:: 163
epoch:: 164
epoch:: 165
epoch:: 166
epoch:: 167
epoch:: 168
epoch:: 169
epoch:: 170
epoch:: 171
epoch:: 172
epoch:: 173
epoch:: 174
epoch:: 175
epoch:: 176
epoch:: 177
epoch:: 178
epoch:: 179
epoch:: 180
epoch:: 181
epoch:: 182
epoch:: 183
epoch:: 184
epoch:: 185
epoch:: 186
epoch:: 187
epoch:: 188
epoch:: 189
epoch:: 190
epoch:: 191
epoch:: 192
epoch:: 193
epoch:: 194
epoch:: 195
epoch:: 196
epoch:: 197
epoch:: 198
epoch:: 199
epoch:: 200
epoch:: 201
epoch:: 202
epoch:: 203
epoch:: 204
epoch:: 205
epoch:: 206
epoch:: 207
epoch:: 208
epoch:: 209
epoch:: 210
epoch:: 211
epoch:: 212
epoch:: 213
epoch:: 214
epoch:: 215
epoch:: 216
epoch:: 217
epoch:: 218
epoch:: 219
epoch:: 220
epoch:: 221
epoch:: 222
epoch:: 223
epoch:: 224
epoch:: 225
epoch:: 226
epoch:: 227
epoch:: 228
epoch:: 229
epoch:: 230
epoch:: 231
epoch:: 232
epoch:: 233
epoch:: 234
epoch:: 235
epoch:: 236
epoch:: 237
epoch:: 238
epoch:: 239
epoch:: 240
epoch:: 241
epoch:: 242
epoch:: 243
epoch:: 244
epoch:: 245
epoch:: 246
epoch:: 247
epoch:: 248
epoch:: 249
epoch:: 250
epoch:: 251
epoch:: 252
epoch:: 253
epoch:: 254
epoch:: 255
epoch:: 256
epoch:: 257
epoch:: 258
epoch:: 259
epoch:: 260
epoch:: 261
epoch:: 262
epoch:: 263
epoch:: 264
epoch:: 265
epoch:: 266
epoch:: 267
epoch:: 268
epoch:: 269
epoch:: 270
epoch:: 271
epoch:: 272
epoch:: 273
epoch:: 274
epoch:: 275
epoch:: 276
epoch:: 277
epoch:: 278
epoch:: 279
epoch:: 280
epoch:: 281
epoch:: 282
epoch:: 283
epoch:: 284
epoch:: 285
epoch:: 286
epoch:: 287
epoch:: 288
epoch:: 289
epoch:: 290
epoch:: 291
epoch:: 292
epoch:: 293
epoch:: 294
epoch:: 295
epoch:: 296
epoch:: 297
epoch:: 298
epoch:: 299
epoch:: 300
epoch:: 301
epoch:: 302
epoch:: 303
epoch:: 304
epoch:: 305
epoch:: 306
epoch:: 307
epoch:: 308
epoch:: 309
epoch:: 310
epoch:: 311
epoch:: 312
epoch:: 313
epoch:: 314
epoch:: 315
epoch:: 316
epoch:: 317
epoch:: 318
epoch:: 319
epoch:: 320
epoch:: 321
epoch:: 322
epoch:: 323
epoch:: 324
epoch:: 325
epoch:: 326
epoch:: 327
epoch:: 328
epoch:: 329
epoch:: 330
epoch:: 331
epoch:: 332
epoch:: 333
epoch:: 334
epoch:: 335
epoch:: 336
epoch:: 337
epoch:: 338
epoch:: 339
epoch:: 340
epoch:: 341
epoch:: 342
epoch:: 343
epoch:: 344
epoch:: 345
epoch:: 346
epoch:: 347
epoch:: 348
epoch:: 349
epoch:: 350
epoch:: 351
epoch:: 352
epoch:: 353
epoch:: 354
epoch:: 355
epoch:: 356
epoch:: 357
epoch:: 358
epoch:: 359
epoch:: 360
epoch:: 361
epoch:: 362
epoch:: 363
epoch:: 364
epoch:: 365
epoch:: 366
epoch:: 367
epoch:: 368
epoch:: 369
epoch:: 370
epoch:: 371
epoch:: 372
epoch:: 373
epoch:: 374
epoch:: 375
epoch:: 376
epoch:: 377
epoch:: 378
epoch:: 379
epoch:: 380
epoch:: 381
epoch:: 382
epoch:: 383
epoch:: 384
epoch:: 385
epoch:: 386
epoch:: 387
epoch:: 388
epoch:: 389
epoch:: 390
epoch:: 391
epoch:: 392
epoch:: 393
epoch:: 394
epoch:: 395
epoch:: 396
epoch:: 397
epoch:: 398
epoch:: 399
epoch:: 400
epoch:: 401
epoch:: 402
epoch:: 403
epoch:: 404
epoch:: 405
epoch:: 406
epoch:: 407
epoch:: 408
epoch:: 409
epoch:: 410
epoch:: 411
epoch:: 412
epoch:: 413
epoch:: 414
epoch:: 415
epoch:: 416
epoch:: 417
epoch:: 418
epoch:: 419
epoch:: 420
epoch:: 421
epoch:: 422
epoch:: 423
epoch:: 424
epoch:: 425
epoch:: 426
epoch:: 427
epoch:: 428
epoch:: 429
epoch:: 430
epoch:: 431
epoch:: 432
epoch:: 433
epoch:: 434
epoch:: 435
epoch:: 436
epoch:: 437
epoch:: 438
epoch:: 439
epoch:: 440
epoch:: 441
epoch:: 442
epoch:: 443
epoch:: 444
epoch:: 445
epoch:: 446
epoch:: 447
epoch:: 448
epoch:: 449
epoch:: 450
epoch:: 451
epoch:: 452
epoch:: 453
epoch:: 454
epoch:: 455
epoch:: 456
epoch:: 457
epoch:: 458
epoch:: 459
epoch:: 460
epoch:: 461
epoch:: 462
epoch:: 463
epoch:: 464
epoch:: 465
epoch:: 466
epoch:: 467
epoch:: 468
epoch:: 469
epoch:: 470
epoch:: 471
epoch:: 472
epoch:: 473
epoch:: 474
epoch:: 475
epoch:: 476
epoch:: 477
epoch:: 478
epoch:: 479
epoch:: 480
epoch:: 481
epoch:: 482
epoch:: 483
epoch:: 484
epoch:: 485
epoch:: 486
epoch:: 487
epoch:: 488
epoch:: 489
epoch:: 490
epoch:: 491
epoch:: 492
epoch:: 493
epoch:: 494
epoch:: 495
epoch:: 496
epoch:: 497
epoch:: 498
epoch:: 499
training time is 80.90483403205872
overhead:: 0
overhead2:: 2.657212018966675
overhead3:: 0
time_baseline:: 80.90487146377563
curr_diff: 0 tensor(0.1354, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1354, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.631200
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 1 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 250
max_epoch:: 500
overhead:: 0
overhead2:: 0.08347320556640625
overhead3:: 0.2574794292449951
overhead4:: 10.348729610443115
overhead5:: 0
memory usage:: 5629005824
time_provenance:: 18.51259160041809
curr_diff: 0 tensor(0.0334, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0334, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1609, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1609, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632300
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 1 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 250
max_epoch:: 500
overhead:: 0
overhead2:: 0.08550357818603516
overhead3:: 0.267841100692749
overhead4:: 10.730000734329224
overhead5:: 0
memory usage:: 5627187200
time_provenance:: 18.943262577056885
curr_diff: 0 tensor(0.0334, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0334, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1610, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1610, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632500
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 1 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 250
max_epoch:: 500
overhead:: 0
overhead2:: 0.08742213249206543
overhead3:: 0.267275333404541
overhead4:: 10.83363389968872
overhead5:: 0
memory usage:: 5630394368
time_provenance:: 19.073890209197998
curr_diff: 0 tensor(0.0334, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0334, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1610, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1610, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632300
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 1 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 250
max_epoch:: 500
overhead:: 0
overhead2:: 0.16669726371765137
overhead3:: 0.4584650993347168
overhead4:: 19.05047059059143
overhead5:: 0
memory usage:: 5630713856
time_provenance:: 29.315199613571167
curr_diff: 0 tensor(0.0031, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0031, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1377, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1377, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.631100
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 1 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 250
max_epoch:: 500
overhead:: 0
overhead2:: 0.1753232479095459
overhead3:: 0.4687337875366211
overhead4:: 18.693515300750732
overhead5:: 0
memory usage:: 5648154624
time_provenance:: 29.00568914413452
curr_diff: 0 tensor(0.0031, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0031, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1377, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1377, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.631100
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 1 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 250
max_epoch:: 500
overhead:: 0
overhead2:: 0.1665325164794922
overhead3:: 0.47539734840393066
overhead4:: 19.211781978607178
overhead5:: 0
memory usage:: 5645803520
time_provenance:: 29.54247760772705
curr_diff: 0 tensor(0.0031, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0031, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1377, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1377, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.631100
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 1 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 250
max_epoch:: 500
overhead:: 0
overhead2:: 0.40819621086120605
overhead3:: 1.0187010765075684
overhead4:: 44.57798790931702
overhead5:: 0
memory usage:: 5649584128
time_provenance:: 61.10339093208313
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1356, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1356, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.631100
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 1 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 250
max_epoch:: 500
overhead:: 0
overhead2:: 0.42348289489746094
overhead3:: 1.0436015129089355
overhead4:: 45.23327350616455
overhead5:: 0
memory usage:: 5673799680
time_provenance:: 61.79974102973938
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1356, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1356, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.631100
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 1 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 250
max_epoch:: 500
overhead:: 0
overhead2:: 0.4316396713256836
overhead3:: 1.0580551624298096
overhead4:: 45.48520064353943
overhead5:: 0
memory usage:: 5637496832
time_provenance:: 62.09819316864014
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1356, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1356, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.631100
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 1 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 250
max_epoch:: 500
overhead:: 0
overhead2:: 0.8198611736297607
overhead3:: 2.0499165058135986
overhead4:: 83.32774448394775
overhead5:: 0
memory usage:: 5621493760
time_provenance:: 108.13589096069336
curr_diff: 0 tensor(5.0199e-13, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.0199e-13, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1., dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1354, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1354, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.631200
repetition 2
python3 benchmark_exp_lr.py 0.001 10000 500 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05] Logistic_regression cifar10_2 2 0.0001 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 2.310324
Train - Epoch 1, Batch: 0, Loss: 2.249112
Train - Epoch 2, Batch: 0, Loss: 2.193015
Train - Epoch 3, Batch: 0, Loss: 2.141829
Train - Epoch 4, Batch: 0, Loss: 2.093926
Train - Epoch 5, Batch: 0, Loss: 2.055052
Train - Epoch 6, Batch: 0, Loss: 2.009092
Train - Epoch 7, Batch: 0, Loss: 1.972841
Train - Epoch 8, Batch: 0, Loss: 1.933265
Train - Epoch 9, Batch: 0, Loss: 1.900914
Train - Epoch 10, Batch: 0, Loss: 1.867511
Train - Epoch 11, Batch: 0, Loss: 1.835578
Train - Epoch 12, Batch: 0, Loss: 1.815968
Train - Epoch 13, Batch: 0, Loss: 1.791327
Train - Epoch 14, Batch: 0, Loss: 1.766418
Train - Epoch 15, Batch: 0, Loss: 1.738492
Train - Epoch 16, Batch: 0, Loss: 1.711723
Train - Epoch 17, Batch: 0, Loss: 1.697584
Train - Epoch 18, Batch: 0, Loss: 1.673116
Train - Epoch 19, Batch: 0, Loss: 1.658832
Train - Epoch 20, Batch: 0, Loss: 1.645767
Train - Epoch 21, Batch: 0, Loss: 1.623441
Train - Epoch 22, Batch: 0, Loss: 1.608309
Train - Epoch 23, Batch: 0, Loss: 1.587357
Train - Epoch 24, Batch: 0, Loss: 1.583711
Train - Epoch 25, Batch: 0, Loss: 1.569261
Train - Epoch 26, Batch: 0, Loss: 1.560523
Train - Epoch 27, Batch: 0, Loss: 1.547812
Train - Epoch 28, Batch: 0, Loss: 1.526490
Train - Epoch 29, Batch: 0, Loss: 1.526689
Train - Epoch 30, Batch: 0, Loss: 1.501210
Train - Epoch 31, Batch: 0, Loss: 1.498563
Train - Epoch 32, Batch: 0, Loss: 1.482058
Train - Epoch 33, Batch: 0, Loss: 1.483235
Train - Epoch 34, Batch: 0, Loss: 1.456296
Train - Epoch 35, Batch: 0, Loss: 1.463853
Train - Epoch 36, Batch: 0, Loss: 1.447657
Train - Epoch 37, Batch: 0, Loss: 1.453412
Train - Epoch 38, Batch: 0, Loss: 1.429023
Train - Epoch 39, Batch: 0, Loss: 1.423104
Train - Epoch 40, Batch: 0, Loss: 1.421891
Train - Epoch 41, Batch: 0, Loss: 1.417702
Train - Epoch 42, Batch: 0, Loss: 1.411828
Train - Epoch 43, Batch: 0, Loss: 1.395889
Train - Epoch 44, Batch: 0, Loss: 1.399738
Train - Epoch 45, Batch: 0, Loss: 1.390895
Train - Epoch 46, Batch: 0, Loss: 1.379475
Train - Epoch 47, Batch: 0, Loss: 1.379991
Train - Epoch 48, Batch: 0, Loss: 1.371030
Train - Epoch 49, Batch: 0, Loss: 1.363443
Train - Epoch 50, Batch: 0, Loss: 1.352682
Train - Epoch 51, Batch: 0, Loss: 1.356987
Train - Epoch 52, Batch: 0, Loss: 1.358946
Train - Epoch 53, Batch: 0, Loss: 1.359909
Train - Epoch 54, Batch: 0, Loss: 1.342769
Train - Epoch 55, Batch: 0, Loss: 1.334880
Train - Epoch 56, Batch: 0, Loss: 1.337841
Train - Epoch 57, Batch: 0, Loss: 1.333916
Train - Epoch 58, Batch: 0, Loss: 1.311979
Train - Epoch 59, Batch: 0, Loss: 1.319126
Train - Epoch 60, Batch: 0, Loss: 1.319505
Train - Epoch 61, Batch: 0, Loss: 1.320772
Train - Epoch 62, Batch: 0, Loss: 1.309611
Train - Epoch 63, Batch: 0, Loss: 1.314290
Train - Epoch 64, Batch: 0, Loss: 1.303685
Train - Epoch 65, Batch: 0, Loss: 1.301218
Train - Epoch 66, Batch: 0, Loss: 1.293058
Train - Epoch 67, Batch: 0, Loss: 1.296181
Train - Epoch 68, Batch: 0, Loss: 1.282710
Train - Epoch 69, Batch: 0, Loss: 1.294130
Train - Epoch 70, Batch: 0, Loss: 1.265800
Train - Epoch 71, Batch: 0, Loss: 1.279099
Train - Epoch 72, Batch: 0, Loss: 1.271267
Train - Epoch 73, Batch: 0, Loss: 1.280242
Train - Epoch 74, Batch: 0, Loss: 1.288216
Train - Epoch 75, Batch: 0, Loss: 1.260164
Train - Epoch 76, Batch: 0, Loss: 1.260254
Train - Epoch 77, Batch: 0, Loss: 1.255248
Train - Epoch 78, Batch: 0, Loss: 1.267577
Train - Epoch 79, Batch: 0, Loss: 1.256602
Train - Epoch 80, Batch: 0, Loss: 1.251082
Train - Epoch 81, Batch: 0, Loss: 1.265037
Train - Epoch 82, Batch: 0, Loss: 1.259321
Train - Epoch 83, Batch: 0, Loss: 1.247319
Train - Epoch 84, Batch: 0, Loss: 1.252068
Train - Epoch 85, Batch: 0, Loss: 1.250908
Train - Epoch 86, Batch: 0, Loss: 1.250185
Train - Epoch 87, Batch: 0, Loss: 1.238796
Train - Epoch 88, Batch: 0, Loss: 1.240938
Train - Epoch 89, Batch: 0, Loss: 1.239816
Train - Epoch 90, Batch: 0, Loss: 1.222545
Train - Epoch 91, Batch: 0, Loss: 1.233709
Train - Epoch 92, Batch: 0, Loss: 1.210248
Train - Epoch 93, Batch: 0, Loss: 1.233122
Train - Epoch 94, Batch: 0, Loss: 1.220003
Train - Epoch 95, Batch: 0, Loss: 1.248517
Train - Epoch 96, Batch: 0, Loss: 1.217618
Train - Epoch 97, Batch: 0, Loss: 1.235144
Train - Epoch 98, Batch: 0, Loss: 1.202981
Train - Epoch 99, Batch: 0, Loss: 1.207300
Train - Epoch 100, Batch: 0, Loss: 1.222298
Train - Epoch 101, Batch: 0, Loss: 1.206184
Train - Epoch 102, Batch: 0, Loss: 1.232547
Train - Epoch 103, Batch: 0, Loss: 1.202284
Train - Epoch 104, Batch: 0, Loss: 1.210612
Train - Epoch 105, Batch: 0, Loss: 1.204229
Train - Epoch 106, Batch: 0, Loss: 1.206696
Train - Epoch 107, Batch: 0, Loss: 1.193665
Train - Epoch 108, Batch: 0, Loss: 1.202182
Train - Epoch 109, Batch: 0, Loss: 1.216578
Train - Epoch 110, Batch: 0, Loss: 1.191068
Train - Epoch 111, Batch: 0, Loss: 1.195297
Train - Epoch 112, Batch: 0, Loss: 1.195664
Train - Epoch 113, Batch: 0, Loss: 1.195687
Train - Epoch 114, Batch: 0, Loss: 1.183728
Train - Epoch 115, Batch: 0, Loss: 1.193284
Train - Epoch 116, Batch: 0, Loss: 1.189163
Train - Epoch 117, Batch: 0, Loss: 1.181364
Train - Epoch 118, Batch: 0, Loss: 1.175295
Train - Epoch 119, Batch: 0, Loss: 1.182288
Train - Epoch 120, Batch: 0, Loss: 1.178891
Train - Epoch 121, Batch: 0, Loss: 1.179929
Train - Epoch 122, Batch: 0, Loss: 1.177729
Train - Epoch 123, Batch: 0, Loss: 1.184489
Train - Epoch 124, Batch: 0, Loss: 1.168851
Train - Epoch 125, Batch: 0, Loss: 1.176472
Train - Epoch 126, Batch: 0, Loss: 1.173686
Train - Epoch 127, Batch: 0, Loss: 1.185484
Train - Epoch 128, Batch: 0, Loss: 1.169948
Train - Epoch 129, Batch: 0, Loss: 1.172708
Train - Epoch 130, Batch: 0, Loss: 1.167138
Train - Epoch 131, Batch: 0, Loss: 1.162173
Train - Epoch 132, Batch: 0, Loss: 1.159861
Train - Epoch 133, Batch: 0, Loss: 1.166289
Train - Epoch 134, Batch: 0, Loss: 1.186498
Train - Epoch 135, Batch: 0, Loss: 1.156294
Train - Epoch 136, Batch: 0, Loss: 1.171513
Train - Epoch 137, Batch: 0, Loss: 1.165145
Train - Epoch 138, Batch: 0, Loss: 1.167011
Train - Epoch 139, Batch: 0, Loss: 1.158682
Train - Epoch 140, Batch: 0, Loss: 1.156993
Train - Epoch 141, Batch: 0, Loss: 1.174451
Train - Epoch 142, Batch: 0, Loss: 1.164981
Train - Epoch 143, Batch: 0, Loss: 1.163025
Train - Epoch 144, Batch: 0, Loss: 1.165090
Train - Epoch 145, Batch: 0, Loss: 1.170308
Train - Epoch 146, Batch: 0, Loss: 1.151481
Train - Epoch 147, Batch: 0, Loss: 1.158358
Train - Epoch 148, Batch: 0, Loss: 1.154054
Train - Epoch 149, Batch: 0, Loss: 1.155925
Train - Epoch 150, Batch: 0, Loss: 1.161073
Train - Epoch 151, Batch: 0, Loss: 1.144907
Train - Epoch 152, Batch: 0, Loss: 1.142114
Train - Epoch 153, Batch: 0, Loss: 1.156174
Train - Epoch 154, Batch: 0, Loss: 1.163829
Train - Epoch 155, Batch: 0, Loss: 1.151531
Train - Epoch 156, Batch: 0, Loss: 1.148525
Train - Epoch 157, Batch: 0, Loss: 1.152342
Train - Epoch 158, Batch: 0, Loss: 1.156084
Train - Epoch 159, Batch: 0, Loss: 1.139759
Train - Epoch 160, Batch: 0, Loss: 1.144792
Train - Epoch 161, Batch: 0, Loss: 1.137869
Train - Epoch 162, Batch: 0, Loss: 1.142215
Train - Epoch 163, Batch: 0, Loss: 1.144435
Train - Epoch 164, Batch: 0, Loss: 1.146675
Train - Epoch 165, Batch: 0, Loss: 1.135575
Train - Epoch 166, Batch: 0, Loss: 1.129627
Train - Epoch 167, Batch: 0, Loss: 1.133852
Train - Epoch 168, Batch: 0, Loss: 1.128614
Train - Epoch 169, Batch: 0, Loss: 1.123099
Train - Epoch 170, Batch: 0, Loss: 1.138501
Train - Epoch 171, Batch: 0, Loss: 1.137420
Train - Epoch 172, Batch: 0, Loss: 1.136644
Train - Epoch 173, Batch: 0, Loss: 1.124063
Train - Epoch 174, Batch: 0, Loss: 1.124839
Train - Epoch 175, Batch: 0, Loss: 1.139122
Train - Epoch 176, Batch: 0, Loss: 1.145140
Train - Epoch 177, Batch: 0, Loss: 1.123782
Train - Epoch 178, Batch: 0, Loss: 1.127051
Train - Epoch 179, Batch: 0, Loss: 1.131562
Train - Epoch 180, Batch: 0, Loss: 1.129636
Train - Epoch 181, Batch: 0, Loss: 1.132101
Train - Epoch 182, Batch: 0, Loss: 1.115768
Train - Epoch 183, Batch: 0, Loss: 1.122598
Train - Epoch 184, Batch: 0, Loss: 1.139051
Train - Epoch 185, Batch: 0, Loss: 1.138450
Train - Epoch 186, Batch: 0, Loss: 1.130277
Train - Epoch 187, Batch: 0, Loss: 1.120441
Train - Epoch 188, Batch: 0, Loss: 1.129923
Train - Epoch 189, Batch: 0, Loss: 1.127389
Train - Epoch 190, Batch: 0, Loss: 1.123838
Train - Epoch 191, Batch: 0, Loss: 1.129439
Train - Epoch 192, Batch: 0, Loss: 1.120055
Train - Epoch 193, Batch: 0, Loss: 1.127073
Train - Epoch 194, Batch: 0, Loss: 1.128953
Train - Epoch 195, Batch: 0, Loss: 1.129403
Train - Epoch 196, Batch: 0, Loss: 1.112489
Train - Epoch 197, Batch: 0, Loss: 1.122230
Train - Epoch 198, Batch: 0, Loss: 1.118362
Train - Epoch 199, Batch: 0, Loss: 1.119010
Train - Epoch 200, Batch: 0, Loss: 1.111279
Train - Epoch 201, Batch: 0, Loss: 1.124230
Train - Epoch 202, Batch: 0, Loss: 1.109358
Train - Epoch 203, Batch: 0, Loss: 1.099994
Train - Epoch 204, Batch: 0, Loss: 1.115933
Train - Epoch 205, Batch: 0, Loss: 1.102224
Train - Epoch 206, Batch: 0, Loss: 1.113893
Train - Epoch 207, Batch: 0, Loss: 1.115759
Train - Epoch 208, Batch: 0, Loss: 1.111374
Train - Epoch 209, Batch: 0, Loss: 1.123424
Train - Epoch 210, Batch: 0, Loss: 1.120479
Train - Epoch 211, Batch: 0, Loss: 1.127545
Train - Epoch 212, Batch: 0, Loss: 1.109648
Train - Epoch 213, Batch: 0, Loss: 1.106732
Train - Epoch 214, Batch: 0, Loss: 1.119710
Train - Epoch 215, Batch: 0, Loss: 1.111309
Train - Epoch 216, Batch: 0, Loss: 1.116635
Train - Epoch 217, Batch: 0, Loss: 1.104180
Train - Epoch 218, Batch: 0, Loss: 1.104143
Train - Epoch 219, Batch: 0, Loss: 1.108953
Train - Epoch 220, Batch: 0, Loss: 1.120389
Train - Epoch 221, Batch: 0, Loss: 1.105727
Train - Epoch 222, Batch: 0, Loss: 1.110926
Train - Epoch 223, Batch: 0, Loss: 1.105304
Train - Epoch 224, Batch: 0, Loss: 1.104788
Train - Epoch 225, Batch: 0, Loss: 1.097101
Train - Epoch 226, Batch: 0, Loss: 1.114871
Train - Epoch 227, Batch: 0, Loss: 1.102949
Train - Epoch 228, Batch: 0, Loss: 1.099213
Train - Epoch 229, Batch: 0, Loss: 1.084040
Train - Epoch 230, Batch: 0, Loss: 1.107539
Train - Epoch 231, Batch: 0, Loss: 1.095230
Train - Epoch 232, Batch: 0, Loss: 1.105248
Train - Epoch 233, Batch: 0, Loss: 1.095664
Train - Epoch 234, Batch: 0, Loss: 1.103001
Train - Epoch 235, Batch: 0, Loss: 1.108410
Train - Epoch 236, Batch: 0, Loss: 1.112112
Train - Epoch 237, Batch: 0, Loss: 1.111337
Train - Epoch 238, Batch: 0, Loss: 1.107303
Train - Epoch 239, Batch: 0, Loss: 1.097381
Train - Epoch 240, Batch: 0, Loss: 1.093879
Train - Epoch 241, Batch: 0, Loss: 1.102798
Train - Epoch 242, Batch: 0, Loss: 1.114765
Train - Epoch 243, Batch: 0, Loss: 1.089047
Train - Epoch 244, Batch: 0, Loss: 1.101155
Train - Epoch 245, Batch: 0, Loss: 1.088030
Train - Epoch 246, Batch: 0, Loss: 1.080641
Train - Epoch 247, Batch: 0, Loss: 1.086770
Train - Epoch 248, Batch: 0, Loss: 1.090867
Train - Epoch 249, Batch: 0, Loss: 1.095127
Train - Epoch 250, Batch: 0, Loss: 1.103916
Train - Epoch 251, Batch: 0, Loss: 1.107624
Train - Epoch 252, Batch: 0, Loss: 1.073854
Train - Epoch 253, Batch: 0, Loss: 1.089657
Train - Epoch 254, Batch: 0, Loss: 1.088706
Train - Epoch 255, Batch: 0, Loss: 1.097577
Train - Epoch 256, Batch: 0, Loss: 1.099782
Train - Epoch 257, Batch: 0, Loss: 1.086431
Train - Epoch 258, Batch: 0, Loss: 1.091386
Train - Epoch 259, Batch: 0, Loss: 1.091543
Train - Epoch 260, Batch: 0, Loss: 1.090123
Train - Epoch 261, Batch: 0, Loss: 1.091813
Train - Epoch 262, Batch: 0, Loss: 1.101567
Train - Epoch 263, Batch: 0, Loss: 1.081167
Train - Epoch 264, Batch: 0, Loss: 1.083138
Train - Epoch 265, Batch: 0, Loss: 1.084836
Train - Epoch 266, Batch: 0, Loss: 1.095742
Train - Epoch 267, Batch: 0, Loss: 1.103977
Train - Epoch 268, Batch: 0, Loss: 1.089102
Train - Epoch 269, Batch: 0, Loss: 1.091407
Train - Epoch 270, Batch: 0, Loss: 1.094572
Train - Epoch 271, Batch: 0, Loss: 1.072291
Train - Epoch 272, Batch: 0, Loss: 1.091091
Train - Epoch 273, Batch: 0, Loss: 1.089209
Train - Epoch 274, Batch: 0, Loss: 1.072032
Train - Epoch 275, Batch: 0, Loss: 1.070734
Train - Epoch 276, Batch: 0, Loss: 1.080936
Train - Epoch 277, Batch: 0, Loss: 1.086920
Train - Epoch 278, Batch: 0, Loss: 1.078758
Train - Epoch 279, Batch: 0, Loss: 1.092148
Train - Epoch 280, Batch: 0, Loss: 1.083628
Train - Epoch 281, Batch: 0, Loss: 1.087763
Train - Epoch 282, Batch: 0, Loss: 1.089038
Train - Epoch 283, Batch: 0, Loss: 1.085290
Train - Epoch 284, Batch: 0, Loss: 1.085390
Train - Epoch 285, Batch: 0, Loss: 1.079010
Train - Epoch 286, Batch: 0, Loss: 1.083079
Train - Epoch 287, Batch: 0, Loss: 1.083714
Train - Epoch 288, Batch: 0, Loss: 1.081889
Train - Epoch 289, Batch: 0, Loss: 1.079245
Train - Epoch 290, Batch: 0, Loss: 1.077235
Train - Epoch 291, Batch: 0, Loss: 1.073661
Train - Epoch 292, Batch: 0, Loss: 1.075805
Train - Epoch 293, Batch: 0, Loss: 1.083029
Train - Epoch 294, Batch: 0, Loss: 1.080935
Train - Epoch 295, Batch: 0, Loss: 1.085684
Train - Epoch 296, Batch: 0, Loss: 1.080284
Train - Epoch 297, Batch: 0, Loss: 1.067502
Train - Epoch 298, Batch: 0, Loss: 1.068147
Train - Epoch 299, Batch: 0, Loss: 1.082522
Train - Epoch 300, Batch: 0, Loss: 1.086486
Train - Epoch 301, Batch: 0, Loss: 1.079179
Train - Epoch 302, Batch: 0, Loss: 1.075084
Train - Epoch 303, Batch: 0, Loss: 1.078489
Train - Epoch 304, Batch: 0, Loss: 1.088517
Train - Epoch 305, Batch: 0, Loss: 1.068548
Train - Epoch 306, Batch: 0, Loss: 1.079119
Train - Epoch 307, Batch: 0, Loss: 1.055850
Train - Epoch 308, Batch: 0, Loss: 1.083499
Train - Epoch 309, Batch: 0, Loss: 1.070942
Train - Epoch 310, Batch: 0, Loss: 1.076519
Train - Epoch 311, Batch: 0, Loss: 1.073912
Train - Epoch 312, Batch: 0, Loss: 1.098891
Train - Epoch 313, Batch: 0, Loss: 1.081504
Train - Epoch 314, Batch: 0, Loss: 1.084680
Train - Epoch 315, Batch: 0, Loss: 1.062851
Train - Epoch 316, Batch: 0, Loss: 1.078133
Train - Epoch 317, Batch: 0, Loss: 1.060844
Train - Epoch 318, Batch: 0, Loss: 1.078437
Train - Epoch 319, Batch: 0, Loss: 1.083963
Train - Epoch 320, Batch: 0, Loss: 1.078238
Train - Epoch 321, Batch: 0, Loss: 1.079266
Train - Epoch 322, Batch: 0, Loss: 1.071106
Train - Epoch 323, Batch: 0, Loss: 1.075184
Train - Epoch 324, Batch: 0, Loss: 1.074500
Train - Epoch 325, Batch: 0, Loss: 1.076295
Train - Epoch 326, Batch: 0, Loss: 1.067580
Train - Epoch 327, Batch: 0, Loss: 1.074032
Train - Epoch 328, Batch: 0, Loss: 1.053764
Train - Epoch 329, Batch: 0, Loss: 1.065284
Train - Epoch 330, Batch: 0, Loss: 1.076810
Train - Epoch 331, Batch: 0, Loss: 1.070850
Train - Epoch 332, Batch: 0, Loss: 1.068341
Train - Epoch 333, Batch: 0, Loss: 1.059770
Train - Epoch 334, Batch: 0, Loss: 1.044763
Train - Epoch 335, Batch: 0, Loss: 1.061971
Train - Epoch 336, Batch: 0, Loss: 1.065698
Train - Epoch 337, Batch: 0, Loss: 1.078523
Train - Epoch 338, Batch: 0, Loss: 1.063060
Train - Epoch 339, Batch: 0, Loss: 1.072917
Train - Epoch 340, Batch: 0, Loss: 1.071882
Train - Epoch 341, Batch: 0, Loss: 1.061097
Train - Epoch 342, Batch: 0, Loss: 1.077167
Train - Epoch 343, Batch: 0, Loss: 1.054955
Train - Epoch 344, Batch: 0, Loss: 1.064927
Train - Epoch 345, Batch: 0, Loss: 1.058489
Train - Epoch 346, Batch: 0, Loss: 1.070072
Train - Epoch 347, Batch: 0, Loss: 1.070574
Train - Epoch 348, Batch: 0, Loss: 1.073706
Train - Epoch 349, Batch: 0, Loss: 1.068470
Train - Epoch 350, Batch: 0, Loss: 1.056531
Train - Epoch 351, Batch: 0, Loss: 1.061741
Train - Epoch 352, Batch: 0, Loss: 1.079472
Train - Epoch 353, Batch: 0, Loss: 1.075509
Train - Epoch 354, Batch: 0, Loss: 1.073363
Train - Epoch 355, Batch: 0, Loss: 1.075785
Train - Epoch 356, Batch: 0, Loss: 1.077054
Train - Epoch 357, Batch: 0, Loss: 1.064100
Train - Epoch 358, Batch: 0, Loss: 1.079682
Train - Epoch 359, Batch: 0, Loss: 1.052778
Train - Epoch 360, Batch: 0, Loss: 1.057680
Train - Epoch 361, Batch: 0, Loss: 1.051027
Train - Epoch 362, Batch: 0, Loss: 1.068862
Train - Epoch 363, Batch: 0, Loss: 1.072506
Train - Epoch 364, Batch: 0, Loss: 1.050487
Train - Epoch 365, Batch: 0, Loss: 1.074498
Train - Epoch 366, Batch: 0, Loss: 1.053694
Train - Epoch 367, Batch: 0, Loss: 1.047623
Train - Epoch 368, Batch: 0, Loss: 1.058396
Train - Epoch 369, Batch: 0, Loss: 1.061546
Train - Epoch 370, Batch: 0, Loss: 1.055248
Train - Epoch 371, Batch: 0, Loss: 1.053397
Train - Epoch 372, Batch: 0, Loss: 1.071856
Train - Epoch 373, Batch: 0, Loss: 1.055328/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 374, Batch: 0, Loss: 1.075962
Train - Epoch 375, Batch: 0, Loss: 1.058619
Train - Epoch 376, Batch: 0, Loss: 1.053677
Train - Epoch 377, Batch: 0, Loss: 1.068161
Train - Epoch 378, Batch: 0, Loss: 1.074007
Train - Epoch 379, Batch: 0, Loss: 1.059204
Train - Epoch 380, Batch: 0, Loss: 1.057614
Train - Epoch 381, Batch: 0, Loss: 1.056425
Train - Epoch 382, Batch: 0, Loss: 1.061095
Train - Epoch 383, Batch: 0, Loss: 1.065464
Train - Epoch 384, Batch: 0, Loss: 1.047304
Train - Epoch 385, Batch: 0, Loss: 1.067854
Train - Epoch 386, Batch: 0, Loss: 1.068064
Train - Epoch 387, Batch: 0, Loss: 1.055939
Train - Epoch 388, Batch: 0, Loss: 1.050494
Train - Epoch 389, Batch: 0, Loss: 1.061616
Train - Epoch 390, Batch: 0, Loss: 1.054335
Train - Epoch 391, Batch: 0, Loss: 1.045708
Train - Epoch 392, Batch: 0, Loss: 1.061280
Train - Epoch 393, Batch: 0, Loss: 1.057249
Train - Epoch 394, Batch: 0, Loss: 1.079115
Train - Epoch 395, Batch: 0, Loss: 1.057216
Train - Epoch 396, Batch: 0, Loss: 1.053598
Train - Epoch 397, Batch: 0, Loss: 1.045059
Train - Epoch 398, Batch: 0, Loss: 1.059473
Train - Epoch 399, Batch: 0, Loss: 1.050883
Train - Epoch 400, Batch: 0, Loss: 1.057875
Train - Epoch 401, Batch: 0, Loss: 1.063453
Train - Epoch 402, Batch: 0, Loss: 1.069197
Train - Epoch 403, Batch: 0, Loss: 1.056061
Train - Epoch 404, Batch: 0, Loss: 1.056440
Train - Epoch 405, Batch: 0, Loss: 1.061769
Train - Epoch 406, Batch: 0, Loss: 1.065034
Train - Epoch 407, Batch: 0, Loss: 1.028857
Train - Epoch 408, Batch: 0, Loss: 1.057974
Train - Epoch 409, Batch: 0, Loss: 1.051648
Train - Epoch 410, Batch: 0, Loss: 1.055538
Train - Epoch 411, Batch: 0, Loss: 1.060078
Train - Epoch 412, Batch: 0, Loss: 1.056406
Train - Epoch 413, Batch: 0, Loss: 1.044910
Train - Epoch 414, Batch: 0, Loss: 1.040515
Train - Epoch 415, Batch: 0, Loss: 1.063573
Train - Epoch 416, Batch: 0, Loss: 1.054596
Train - Epoch 417, Batch: 0, Loss: 1.056571
Train - Epoch 418, Batch: 0, Loss: 1.054142
Train - Epoch 419, Batch: 0, Loss: 1.044051
Train - Epoch 420, Batch: 0, Loss: 1.043929
Train - Epoch 421, Batch: 0, Loss: 1.041023
Train - Epoch 422, Batch: 0, Loss: 1.047769
Train - Epoch 423, Batch: 0, Loss: 1.063746
Train - Epoch 424, Batch: 0, Loss: 1.059868
Train - Epoch 425, Batch: 0, Loss: 1.051667
Train - Epoch 426, Batch: 0, Loss: 1.044017
Train - Epoch 427, Batch: 0, Loss: 1.063633
Train - Epoch 428, Batch: 0, Loss: 1.056015
Train - Epoch 429, Batch: 0, Loss: 1.039691
Train - Epoch 430, Batch: 0, Loss: 1.053414
Train - Epoch 431, Batch: 0, Loss: 1.039162
Train - Epoch 432, Batch: 0, Loss: 1.025311
Train - Epoch 433, Batch: 0, Loss: 1.055890
Train - Epoch 434, Batch: 0, Loss: 1.060683
Train - Epoch 435, Batch: 0, Loss: 1.031842
Train - Epoch 436, Batch: 0, Loss: 1.039879
Train - Epoch 437, Batch: 0, Loss: 1.055896
Train - Epoch 438, Batch: 0, Loss: 1.038748
Train - Epoch 439, Batch: 0, Loss: 1.061447
Train - Epoch 440, Batch: 0, Loss: 1.047166
Train - Epoch 441, Batch: 0, Loss: 1.051324
Train - Epoch 442, Batch: 0, Loss: 1.046027
Train - Epoch 443, Batch: 0, Loss: 1.060070
Train - Epoch 444, Batch: 0, Loss: 1.048264
Train - Epoch 445, Batch: 0, Loss: 1.050075
Train - Epoch 446, Batch: 0, Loss: 1.034514
Train - Epoch 447, Batch: 0, Loss: 1.042979
Train - Epoch 448, Batch: 0, Loss: 1.047759
Train - Epoch 449, Batch: 0, Loss: 1.040922
Train - Epoch 450, Batch: 0, Loss: 1.036165
Train - Epoch 451, Batch: 0, Loss: 1.039591
Train - Epoch 452, Batch: 0, Loss: 1.024202
Train - Epoch 453, Batch: 0, Loss: 1.045556
Train - Epoch 454, Batch: 0, Loss: 1.046621
Train - Epoch 455, Batch: 0, Loss: 1.037389
Train - Epoch 456, Batch: 0, Loss: 1.035635
Train - Epoch 457, Batch: 0, Loss: 1.033289
Train - Epoch 458, Batch: 0, Loss: 1.041079
Train - Epoch 459, Batch: 0, Loss: 1.054463
Train - Epoch 460, Batch: 0, Loss: 1.040076
Train - Epoch 461, Batch: 0, Loss: 1.044616
Train - Epoch 462, Batch: 0, Loss: 1.029649
Train - Epoch 463, Batch: 0, Loss: 1.035188
Train - Epoch 464, Batch: 0, Loss: 1.047182
Train - Epoch 465, Batch: 0, Loss: 1.067688
Train - Epoch 466, Batch: 0, Loss: 1.035369
Train - Epoch 467, Batch: 0, Loss: 1.045633
Train - Epoch 468, Batch: 0, Loss: 1.047572
Train - Epoch 469, Batch: 0, Loss: 1.052255
Train - Epoch 470, Batch: 0, Loss: 1.031171
Train - Epoch 471, Batch: 0, Loss: 1.044833
Train - Epoch 472, Batch: 0, Loss: 1.045971
Train - Epoch 473, Batch: 0, Loss: 1.037680
Train - Epoch 474, Batch: 0, Loss: 1.032596
Train - Epoch 475, Batch: 0, Loss: 1.043266
Train - Epoch 476, Batch: 0, Loss: 1.051499
Train - Epoch 477, Batch: 0, Loss: 1.042465
Train - Epoch 478, Batch: 0, Loss: 1.055774
Train - Epoch 479, Batch: 0, Loss: 1.029366
Train - Epoch 480, Batch: 0, Loss: 1.068961
Train - Epoch 481, Batch: 0, Loss: 1.048937
Train - Epoch 482, Batch: 0, Loss: 1.043432
Train - Epoch 483, Batch: 0, Loss: 1.049170
Train - Epoch 484, Batch: 0, Loss: 1.038093
Train - Epoch 485, Batch: 0, Loss: 1.035910
Train - Epoch 486, Batch: 0, Loss: 1.034926
Train - Epoch 487, Batch: 0, Loss: 1.022997
Train - Epoch 488, Batch: 0, Loss: 1.037480
Train - Epoch 489, Batch: 0, Loss: 1.031606
Train - Epoch 490, Batch: 0, Loss: 1.041193
Train - Epoch 491, Batch: 0, Loss: 1.041225
Train - Epoch 492, Batch: 0, Loss: 1.027518
Train - Epoch 493, Batch: 0, Loss: 1.033693
Train - Epoch 494, Batch: 0, Loss: 1.043715
Train - Epoch 495, Batch: 0, Loss: 1.041023
Train - Epoch 496, Batch: 0, Loss: 1.042802
Train - Epoch 497, Batch: 0, Loss: 1.040754
Train - Epoch 498, Batch: 0, Loss: 1.023206
Train - Epoch 499, Batch: 0, Loss: 1.028203
training_time:: 107.85879588127136
training time full:: 107.85886478424072
provenance prepare time:: 4.76837158203125e-07
here
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632500
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 250
max_epoch:: 500
batch_size:: 10000
epoch:: 0
tensor([    1,     6, 32775, 32776, 32781, 32783,    31,    33, 32802,    43,
        32811,    45,    46, 32815, 32820,    53,    54, 32823,    55, 32826,
        32829, 32831, 32834,    75, 32844, 32856,    88,    94, 32864,    98,
          104, 32874,   107, 32876,   109,   112,   113,   118, 32886, 32893,
        32896,   131,   136, 32908,   144, 32913,   158,   159, 32927,   164])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
epoch:: 60
epoch:: 61
epoch:: 62
epoch:: 63
epoch:: 64
epoch:: 65
epoch:: 66
epoch:: 67
epoch:: 68
epoch:: 69
epoch:: 70
epoch:: 71
epoch:: 72
epoch:: 73
epoch:: 74
epoch:: 75
epoch:: 76
epoch:: 77
epoch:: 78
epoch:: 79
epoch:: 80
epoch:: 81
epoch:: 82
epoch:: 83
epoch:: 84
epoch:: 85
epoch:: 86
epoch:: 87
epoch:: 88
epoch:: 89
epoch:: 90
epoch:: 91
epoch:: 92
epoch:: 93
epoch:: 94
epoch:: 95
epoch:: 96
epoch:: 97
epoch:: 98
epoch:: 99
epoch:: 100
epoch:: 101
epoch:: 102
epoch:: 103
epoch:: 104
epoch:: 105
epoch:: 106
epoch:: 107
epoch:: 108
epoch:: 109
epoch:: 110
epoch:: 111
epoch:: 112
epoch:: 113
epoch:: 114
epoch:: 115
epoch:: 116
epoch:: 117
epoch:: 118
epoch:: 119
epoch:: 120
epoch:: 121
epoch:: 122
epoch:: 123
epoch:: 124
epoch:: 125
epoch:: 126
epoch:: 127
epoch:: 128
epoch:: 129
epoch:: 130
epoch:: 131
epoch:: 132
epoch:: 133
epoch:: 134
epoch:: 135
epoch:: 136
epoch:: 137
epoch:: 138
epoch:: 139
epoch:: 140
epoch:: 141
epoch:: 142
epoch:: 143
epoch:: 144
epoch:: 145
epoch:: 146
epoch:: 147
epoch:: 148
epoch:: 149
epoch:: 150
epoch:: 151
epoch:: 152
epoch:: 153
epoch:: 154
epoch:: 155
epoch:: 156
epoch:: 157
epoch:: 158
epoch:: 159
epoch:: 160
epoch:: 161
epoch:: 162
epoch:: 163
epoch:: 164
epoch:: 165
epoch:: 166
epoch:: 167
epoch:: 168
epoch:: 169
epoch:: 170
epoch:: 171
epoch:: 172
epoch:: 173
epoch:: 174
epoch:: 175
epoch:: 176
epoch:: 177
epoch:: 178
epoch:: 179
epoch:: 180
epoch:: 181
epoch:: 182
epoch:: 183
epoch:: 184
epoch:: 185
epoch:: 186
epoch:: 187
epoch:: 188
epoch:: 189
epoch:: 190
epoch:: 191
epoch:: 192
epoch:: 193
epoch:: 194
epoch:: 195
epoch:: 196
epoch:: 197
epoch:: 198
epoch:: 199
epoch:: 200
epoch:: 201
epoch:: 202
epoch:: 203
epoch:: 204
epoch:: 205
epoch:: 206
epoch:: 207
epoch:: 208
epoch:: 209
epoch:: 210
epoch:: 211
epoch:: 212
epoch:: 213
epoch:: 214
epoch:: 215
epoch:: 216
epoch:: 217
epoch:: 218
epoch:: 219
epoch:: 220
epoch:: 221
epoch:: 222
epoch:: 223
epoch:: 224
epoch:: 225
epoch:: 226
epoch:: 227
epoch:: 228
epoch:: 229
epoch:: 230
epoch:: 231
epoch:: 232
epoch:: 233
epoch:: 234
epoch:: 235
epoch:: 236
epoch:: 237
epoch:: 238
epoch:: 239
epoch:: 240
epoch:: 241
epoch:: 242
epoch:: 243
epoch:: 244
epoch:: 245
epoch:: 246
epoch:: 247
epoch:: 248
epoch:: 249
epoch:: 250
epoch:: 251
epoch:: 252
epoch:: 253
epoch:: 254
epoch:: 255
epoch:: 256
epoch:: 257
epoch:: 258
epoch:: 259
epoch:: 260
epoch:: 261
epoch:: 262
epoch:: 263
epoch:: 264
epoch:: 265
epoch:: 266
epoch:: 267
epoch:: 268
epoch:: 269
epoch:: 270
epoch:: 271
epoch:: 272
epoch:: 273
epoch:: 274
epoch:: 275
epoch:: 276
epoch:: 277
epoch:: 278
epoch:: 279
epoch:: 280
epoch:: 281
epoch:: 282
epoch:: 283
epoch:: 284
epoch:: 285
epoch:: 286
epoch:: 287
epoch:: 288
epoch:: 289
epoch:: 290
epoch:: 291
epoch:: 292
epoch:: 293
epoch:: 294
epoch:: 295
epoch:: 296
epoch:: 297
epoch:: 298
epoch:: 299
epoch:: 300
epoch:: 301
epoch:: 302
epoch:: 303
epoch:: 304
epoch:: 305
epoch:: 306
epoch:: 307
epoch:: 308
epoch:: 309
epoch:: 310
epoch:: 311
epoch:: 312
epoch:: 313
epoch:: 314
epoch:: 315
epoch:: 316
epoch:: 317
epoch:: 318
epoch:: 319
epoch:: 320
epoch:: 321
epoch:: 322
epoch:: 323
epoch:: 324
epoch:: 325
epoch:: 326
epoch:: 327
epoch:: 328
epoch:: 329
epoch:: 330
epoch:: 331
epoch:: 332
epoch:: 333
epoch:: 334
epoch:: 335
epoch:: 336
epoch:: 337
epoch:: 338
epoch:: 339
epoch:: 340
epoch:: 341
epoch:: 342
epoch:: 343
epoch:: 344
epoch:: 345
epoch:: 346
epoch:: 347
epoch:: 348
epoch:: 349
epoch:: 350
epoch:: 351
epoch:: 352
epoch:: 353
epoch:: 354
epoch:: 355
epoch:: 356
epoch:: 357
epoch:: 358
epoch:: 359
epoch:: 360
epoch:: 361
epoch:: 362
epoch:: 363
epoch:: 364
epoch:: 365
epoch:: 366
epoch:: 367
epoch:: 368
epoch:: 369
epoch:: 370
epoch:: 371
epoch:: 372
epoch:: 373
epoch:: 374
epoch:: 375
epoch:: 376
epoch:: 377
epoch:: 378
epoch:: 379
epoch:: 380
epoch:: 381
epoch:: 382
epoch:: 383
epoch:: 384
epoch:: 385
epoch:: 386
epoch:: 387
epoch:: 388
epoch:: 389
epoch:: 390
epoch:: 391
epoch:: 392
epoch:: 393
epoch:: 394
epoch:: 395
epoch:: 396
epoch:: 397
epoch:: 398
epoch:: 399
epoch:: 400
epoch:: 401
epoch:: 402
epoch:: 403
epoch:: 404
epoch:: 405
epoch:: 406
epoch:: 407
epoch:: 408
epoch:: 409
epoch:: 410
epoch:: 411
epoch:: 412
epoch:: 413
epoch:: 414
epoch:: 415
epoch:: 416
epoch:: 417
epoch:: 418
epoch:: 419
epoch:: 420
epoch:: 421
epoch:: 422
epoch:: 423
epoch:: 424
epoch:: 425
epoch:: 426
epoch:: 427
epoch:: 428
epoch:: 429
epoch:: 430
epoch:: 431
epoch:: 432
epoch:: 433
epoch:: 434
epoch:: 435
epoch:: 436
epoch:: 437
epoch:: 438
epoch:: 439
epoch:: 440
epoch:: 441
epoch:: 442
epoch:: 443
epoch:: 444
epoch:: 445
epoch:: 446
epoch:: 447
epoch:: 448
epoch:: 449
epoch:: 450
epoch:: 451
epoch:: 452
epoch:: 453
epoch:: 454
epoch:: 455
epoch:: 456
epoch:: 457
epoch:: 458
epoch:: 459
epoch:: 460
epoch:: 461
epoch:: 462
epoch:: 463
epoch:: 464
epoch:: 465
epoch:: 466
epoch:: 467
epoch:: 468
epoch:: 469
epoch:: 470
epoch:: 471
epoch:: 472
epoch:: 473
epoch:: 474
epoch:: 475
epoch:: 476
epoch:: 477
epoch:: 478
epoch:: 479
epoch:: 480
epoch:: 481
epoch:: 482
epoch:: 483
epoch:: 484
epoch:: 485
epoch:: 486
epoch:: 487
epoch:: 488
epoch:: 489
epoch:: 490
epoch:: 491
epoch:: 492
epoch:: 493
epoch:: 494
epoch:: 495
epoch:: 496
epoch:: 497
epoch:: 498
epoch:: 499
training time is 80.41948246955872
overhead:: 0
overhead2:: 2.662909746170044
overhead3:: 0
time_baseline:: 80.41952061653137
curr_diff: 0 tensor(0.1355, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1355, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632900
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 2 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 250
max_epoch:: 500
overhead:: 0
overhead2:: 0.08765101432800293
overhead3:: 0.2540760040283203
overhead4:: 10.487115859985352
overhead5:: 0
memory usage:: 5643591680
time_provenance:: 18.60833740234375
curr_diff: 0 tensor(0.0333, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0333, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1610, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1610, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633000
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 2 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 250
max_epoch:: 500
overhead:: 0
overhead2:: 0.08496832847595215
overhead3:: 0.26788783073425293
overhead4:: 10.70409345626831
overhead5:: 0
memory usage:: 5629534208
time_provenance:: 18.92593741416931
curr_diff: 0 tensor(0.0334, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0334, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1610, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1610, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633000
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 2 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 250
max_epoch:: 500
overhead:: 0
overhead2:: 0.09125423431396484
overhead3:: 0.2701735496520996
overhead4:: 10.978256702423096
overhead5:: 0
memory usage:: 5636890624
time_provenance:: 19.22114896774292
curr_diff: 0 tensor(0.0334, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0334, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1610, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1610, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633000
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 2 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 250
max_epoch:: 500
overhead:: 0
overhead2:: 0.16633105278015137
overhead3:: 0.45104551315307617
overhead4:: 18.719293355941772
overhead5:: 0
memory usage:: 5647294464
time_provenance:: 28.923252820968628
curr_diff: 0 tensor(0.0032, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0032, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1378, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1378, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633200
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 2 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 250
max_epoch:: 500
overhead:: 0
overhead2:: 0.1672821044921875
overhead3:: 0.46120548248291016
overhead4:: 18.98123288154602
overhead5:: 0
memory usage:: 5628915712
time_provenance:: 29.260865211486816
curr_diff: 0 tensor(0.0030, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0030, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1376, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1376, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632700
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 2 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 250
max_epoch:: 500
overhead:: 0
overhead2:: 0.17421317100524902
overhead3:: 0.4592301845550537
overhead4:: 19.272831201553345
overhead5:: 0
memory usage:: 5645635584
time_provenance:: 29.576324701309204
curr_diff: 0 tensor(0.0032, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0032, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1378, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1378, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633200
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 2 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 250
max_epoch:: 500
overhead:: 0
overhead2:: 0.4244067668914795
overhead3:: 0.9962959289550781
overhead4:: 44.75498676300049
overhead5:: 0
memory usage:: 5630861312
time_provenance:: 61.274373054504395
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1356, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1356, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632800
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 2 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 250
max_epoch:: 500
overhead:: 0
overhead2:: 0.4222433567047119
overhead3:: 1.054335117340088
overhead4:: 44.83438301086426
overhead5:: 0
memory usage:: 5657042944
time_provenance:: 61.418532371520996
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1356, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1356, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632800
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 2 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 250
max_epoch:: 500
overhead:: 0
overhead2:: 0.4130544662475586
overhead3:: 1.029695987701416
overhead4:: 44.553932905197144
overhead5:: 0
memory usage:: 5629706240
time_provenance:: 61.14289402961731
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1356, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1356, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632800
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 2 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 250
max_epoch:: 500
overhead:: 0
overhead2:: 0.8626968860626221
overhead3:: 2.1063215732574463
overhead4:: 82.20333313941956
overhead5:: 0
memory usage:: 5627518976
time_provenance:: 107.02982306480408
curr_diff: 0 tensor(4.9944e-13, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.9944e-13, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1355, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1355, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632900
repetition 3
python3 benchmark_exp_lr.py 0.001 10000 500 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05] Logistic_regression cifar10_2 3 0.0001 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 2.307999
Train - Epoch 1, Batch: 0, Loss: 2.244582
Train - Epoch 2, Batch: 0, Loss: 2.190008
Train - Epoch 3, Batch: 0, Loss: 2.139068
Train - Epoch 4, Batch: 0, Loss: 2.092451
Train - Epoch 5, Batch: 0, Loss: 2.047121
Train - Epoch 6, Batch: 0, Loss: 2.005863
Train - Epoch 7, Batch: 0, Loss: 1.968838
Train - Epoch 8, Batch: 0, Loss: 1.933192
Train - Epoch 9, Batch: 0, Loss: 1.899960
Train - Epoch 10, Batch: 0, Loss: 1.868879
Train - Epoch 11, Batch: 0, Loss: 1.839612
Train - Epoch 12, Batch: 0, Loss: 1.811477
Train - Epoch 13, Batch: 0, Loss: 1.786801
Train - Epoch 14, Batch: 0, Loss: 1.752032
Train - Epoch 15, Batch: 0, Loss: 1.735154
Train - Epoch 16, Batch: 0, Loss: 1.718067
Train - Epoch 17, Batch: 0, Loss: 1.690131
Train - Epoch 18, Batch: 0, Loss: 1.673021
Train - Epoch 19, Batch: 0, Loss: 1.661647
Train - Epoch 20, Batch: 0, Loss: 1.642407
Train - Epoch 21, Batch: 0, Loss: 1.619336
Train - Epoch 22, Batch: 0, Loss: 1.615799
Train - Epoch 23, Batch: 0, Loss: 1.589562
Train - Epoch 24, Batch: 0, Loss: 1.578641
Train - Epoch 25, Batch: 0, Loss: 1.567758
Train - Epoch 26, Batch: 0, Loss: 1.543525
Train - Epoch 27, Batch: 0, Loss: 1.544602
Train - Epoch 28, Batch: 0, Loss: 1.529316
Train - Epoch 29, Batch: 0, Loss: 1.525781
Train - Epoch 30, Batch: 0, Loss: 1.506876
Train - Epoch 31, Batch: 0, Loss: 1.499985
Train - Epoch 32, Batch: 0, Loss: 1.487571
Train - Epoch 33, Batch: 0, Loss: 1.478070
Train - Epoch 34, Batch: 0, Loss: 1.483430
Train - Epoch 35, Batch: 0, Loss: 1.456988
Train - Epoch 36, Batch: 0, Loss: 1.459012
Train - Epoch 37, Batch: 0, Loss: 1.441945
Train - Epoch 38, Batch: 0, Loss: 1.438557
Train - Epoch 39, Batch: 0, Loss: 1.432638
Train - Epoch 40, Batch: 0, Loss: 1.417983
Train - Epoch 41, Batch: 0, Loss: 1.417222
Train - Epoch 42, Batch: 0, Loss: 1.399811
Train - Epoch 43, Batch: 0, Loss: 1.407236
Train - Epoch 44, Batch: 0, Loss: 1.405524
Train - Epoch 45, Batch: 0, Loss: 1.383714
Train - Epoch 46, Batch: 0, Loss: 1.380416
Train - Epoch 47, Batch: 0, Loss: 1.371612
Train - Epoch 48, Batch: 0, Loss: 1.375548
Train - Epoch 49, Batch: 0, Loss: 1.363930
Train - Epoch 50, Batch: 0, Loss: 1.356297
Train - Epoch 51, Batch: 0, Loss: 1.355575
Train - Epoch 52, Batch: 0, Loss: 1.349867
Train - Epoch 53, Batch: 0, Loss: 1.368696
Train - Epoch 54, Batch: 0, Loss: 1.338075
Train - Epoch 55, Batch: 0, Loss: 1.343611
Train - Epoch 56, Batch: 0, Loss: 1.335712
Train - Epoch 57, Batch: 0, Loss: 1.324480
Train - Epoch 58, Batch: 0, Loss: 1.323268
Train - Epoch 59, Batch: 0, Loss: 1.317766
Train - Epoch 60, Batch: 0, Loss: 1.313062
Train - Epoch 61, Batch: 0, Loss: 1.309094
Train - Epoch 62, Batch: 0, Loss: 1.314853
Train - Epoch 63, Batch: 0, Loss: 1.305417
Train - Epoch 64, Batch: 0, Loss: 1.301458
Train - Epoch 65, Batch: 0, Loss: 1.306867
Train - Epoch 66, Batch: 0, Loss: 1.290032
Train - Epoch 67, Batch: 0, Loss: 1.291476
Train - Epoch 68, Batch: 0, Loss: 1.285657
Train - Epoch 69, Batch: 0, Loss: 1.289627
Train - Epoch 70, Batch: 0, Loss: 1.288936
Train - Epoch 71, Batch: 0, Loss: 1.270197
Train - Epoch 72, Batch: 0, Loss: 1.268646
Train - Epoch 73, Batch: 0, Loss: 1.260241
Train - Epoch 74, Batch: 0, Loss: 1.268596
Train - Epoch 75, Batch: 0, Loss: 1.273379
Train - Epoch 76, Batch: 0, Loss: 1.272633
Train - Epoch 77, Batch: 0, Loss: 1.266360
Train - Epoch 78, Batch: 0, Loss: 1.266579
Train - Epoch 79, Batch: 0, Loss: 1.270293
Train - Epoch 80, Batch: 0, Loss: 1.250810
Train - Epoch 81, Batch: 0, Loss: 1.243369
Train - Epoch 82, Batch: 0, Loss: 1.252925
Train - Epoch 83, Batch: 0, Loss: 1.243733
Train - Epoch 84, Batch: 0, Loss: 1.244257
Train - Epoch 85, Batch: 0, Loss: 1.250570
Train - Epoch 86, Batch: 0, Loss: 1.255889
Train - Epoch 87, Batch: 0, Loss: 1.245688
Train - Epoch 88, Batch: 0, Loss: 1.228316
Train - Epoch 89, Batch: 0, Loss: 1.238362
Train - Epoch 90, Batch: 0, Loss: 1.230241
Train - Epoch 91, Batch: 0, Loss: 1.229362
Train - Epoch 92, Batch: 0, Loss: 1.240455
Train - Epoch 93, Batch: 0, Loss: 1.218162
Train - Epoch 94, Batch: 0, Loss: 1.224687
Train - Epoch 95, Batch: 0, Loss: 1.241424
Train - Epoch 96, Batch: 0, Loss: 1.232883
Train - Epoch 97, Batch: 0, Loss: 1.219575
Train - Epoch 98, Batch: 0, Loss: 1.223545
Train - Epoch 99, Batch: 0, Loss: 1.226831
Train - Epoch 100, Batch: 0, Loss: 1.207704
Train - Epoch 101, Batch: 0, Loss: 1.200091
Train - Epoch 102, Batch: 0, Loss: 1.214893
Train - Epoch 103, Batch: 0, Loss: 1.211932
Train - Epoch 104, Batch: 0, Loss: 1.203009
Train - Epoch 105, Batch: 0, Loss: 1.195906
Train - Epoch 106, Batch: 0, Loss: 1.213405
Train - Epoch 107, Batch: 0, Loss: 1.206097
Train - Epoch 108, Batch: 0, Loss: 1.203831
Train - Epoch 109, Batch: 0, Loss: 1.188676
Train - Epoch 110, Batch: 0, Loss: 1.198339
Train - Epoch 111, Batch: 0, Loss: 1.197913
Train - Epoch 112, Batch: 0, Loss: 1.186813
Train - Epoch 113, Batch: 0, Loss: 1.187848
Train - Epoch 114, Batch: 0, Loss: 1.193004
Train - Epoch 115, Batch: 0, Loss: 1.196746
Train - Epoch 116, Batch: 0, Loss: 1.182143
Train - Epoch 117, Batch: 0, Loss: 1.193029
Train - Epoch 118, Batch: 0, Loss: 1.187254
Train - Epoch 119, Batch: 0, Loss: 1.171168
Train - Epoch 120, Batch: 0, Loss: 1.187118
Train - Epoch 121, Batch: 0, Loss: 1.192164
Train - Epoch 122, Batch: 0, Loss: 1.196258
Train - Epoch 123, Batch: 0, Loss: 1.188627
Train - Epoch 124, Batch: 0, Loss: 1.172743
Train - Epoch 125, Batch: 0, Loss: 1.187339
Train - Epoch 126, Batch: 0, Loss: 1.186079
Train - Epoch 127, Batch: 0, Loss: 1.196253
Train - Epoch 128, Batch: 0, Loss: 1.185824
Train - Epoch 129, Batch: 0, Loss: 1.158342
Train - Epoch 130, Batch: 0, Loss: 1.162259
Train - Epoch 131, Batch: 0, Loss: 1.165249
Train - Epoch 132, Batch: 0, Loss: 1.178205
Train - Epoch 133, Batch: 0, Loss: 1.166020
Train - Epoch 134, Batch: 0, Loss: 1.157962
Train - Epoch 135, Batch: 0, Loss: 1.168746
Train - Epoch 136, Batch: 0, Loss: 1.160521
Train - Epoch 137, Batch: 0, Loss: 1.173071
Train - Epoch 138, Batch: 0, Loss: 1.159885
Train - Epoch 139, Batch: 0, Loss: 1.153840
Train - Epoch 140, Batch: 0, Loss: 1.169391
Train - Epoch 141, Batch: 0, Loss: 1.174340
Train - Epoch 142, Batch: 0, Loss: 1.152990
Train - Epoch 143, Batch: 0, Loss: 1.163158
Train - Epoch 144, Batch: 0, Loss: 1.155102
Train - Epoch 145, Batch: 0, Loss: 1.164573
Train - Epoch 146, Batch: 0, Loss: 1.163188
Train - Epoch 147, Batch: 0, Loss: 1.153142
Train - Epoch 148, Batch: 0, Loss: 1.162759
Train - Epoch 149, Batch: 0, Loss: 1.155099
Train - Epoch 150, Batch: 0, Loss: 1.152537
Train - Epoch 151, Batch: 0, Loss: 1.157722
Train - Epoch 152, Batch: 0, Loss: 1.142153
Train - Epoch 153, Batch: 0, Loss: 1.143745
Train - Epoch 154, Batch: 0, Loss: 1.156424
Train - Epoch 155, Batch: 0, Loss: 1.155381
Train - Epoch 156, Batch: 0, Loss: 1.154146
Train - Epoch 157, Batch: 0, Loss: 1.148635
Train - Epoch 158, Batch: 0, Loss: 1.140444
Train - Epoch 159, Batch: 0, Loss: 1.144478
Train - Epoch 160, Batch: 0, Loss: 1.143075
Train - Epoch 161, Batch: 0, Loss: 1.138603
Train - Epoch 162, Batch: 0, Loss: 1.139078
Train - Epoch 163, Batch: 0, Loss: 1.143914
Train - Epoch 164, Batch: 0, Loss: 1.155686
Train - Epoch 165, Batch: 0, Loss: 1.145124
Train - Epoch 166, Batch: 0, Loss: 1.149419
Train - Epoch 167, Batch: 0, Loss: 1.143843
Train - Epoch 168, Batch: 0, Loss: 1.145032
Train - Epoch 169, Batch: 0, Loss: 1.136416
Train - Epoch 170, Batch: 0, Loss: 1.147345
Train - Epoch 171, Batch: 0, Loss: 1.138769
Train - Epoch 172, Batch: 0, Loss: 1.129816
Train - Epoch 173, Batch: 0, Loss: 1.142956
Train - Epoch 174, Batch: 0, Loss: 1.132798
Train - Epoch 175, Batch: 0, Loss: 1.133261
Train - Epoch 176, Batch: 0, Loss: 1.126306
Train - Epoch 177, Batch: 0, Loss: 1.118309
Train - Epoch 178, Batch: 0, Loss: 1.126712
Train - Epoch 179, Batch: 0, Loss: 1.122618
Train - Epoch 180, Batch: 0, Loss: 1.131703
Train - Epoch 181, Batch: 0, Loss: 1.113251
Train - Epoch 182, Batch: 0, Loss: 1.117853
Train - Epoch 183, Batch: 0, Loss: 1.133784
Train - Epoch 184, Batch: 0, Loss: 1.140024
Train - Epoch 185, Batch: 0, Loss: 1.110593
Train - Epoch 186, Batch: 0, Loss: 1.123352
Train - Epoch 187, Batch: 0, Loss: 1.125826
Train - Epoch 188, Batch: 0, Loss: 1.115856
Train - Epoch 189, Batch: 0, Loss: 1.134247
Train - Epoch 190, Batch: 0, Loss: 1.123783
Train - Epoch 191, Batch: 0, Loss: 1.123621
Train - Epoch 192, Batch: 0, Loss: 1.125552
Train - Epoch 193, Batch: 0, Loss: 1.126965
Train - Epoch 194, Batch: 0, Loss: 1.125422
Train - Epoch 195, Batch: 0, Loss: 1.126178
Train - Epoch 196, Batch: 0, Loss: 1.124953
Train - Epoch 197, Batch: 0, Loss: 1.114188
Train - Epoch 198, Batch: 0, Loss: 1.124332
Train - Epoch 199, Batch: 0, Loss: 1.118703
Train - Epoch 200, Batch: 0, Loss: 1.112715
Train - Epoch 201, Batch: 0, Loss: 1.111749
Train - Epoch 202, Batch: 0, Loss: 1.116757
Train - Epoch 203, Batch: 0, Loss: 1.129932
Train - Epoch 204, Batch: 0, Loss: 1.125704
Train - Epoch 205, Batch: 0, Loss: 1.108140
Train - Epoch 206, Batch: 0, Loss: 1.129840
Train - Epoch 207, Batch: 0, Loss: 1.117081
Train - Epoch 208, Batch: 0, Loss: 1.118612
Train - Epoch 209, Batch: 0, Loss: 1.111049
Train - Epoch 210, Batch: 0, Loss: 1.115140
Train - Epoch 211, Batch: 0, Loss: 1.108255
Train - Epoch 212, Batch: 0, Loss: 1.123290
Train - Epoch 213, Batch: 0, Loss: 1.109143
Train - Epoch 214, Batch: 0, Loss: 1.111272
Train - Epoch 215, Batch: 0, Loss: 1.122717
Train - Epoch 216, Batch: 0, Loss: 1.111229
Train - Epoch 217, Batch: 0, Loss: 1.105692
Train - Epoch 218, Batch: 0, Loss: 1.113385
Train - Epoch 219, Batch: 0, Loss: 1.113848
Train - Epoch 220, Batch: 0, Loss: 1.111327
Train - Epoch 221, Batch: 0, Loss: 1.105785
Train - Epoch 222, Batch: 0, Loss: 1.096419
Train - Epoch 223, Batch: 0, Loss: 1.109056
Train - Epoch 224, Batch: 0, Loss: 1.100868
Train - Epoch 225, Batch: 0, Loss: 1.109792
Train - Epoch 226, Batch: 0, Loss: 1.109678
Train - Epoch 227, Batch: 0, Loss: 1.111189
Train - Epoch 228, Batch: 0, Loss: 1.098013
Train - Epoch 229, Batch: 0, Loss: 1.111236
Train - Epoch 230, Batch: 0, Loss: 1.091490
Train - Epoch 231, Batch: 0, Loss: 1.090693
Train - Epoch 232, Batch: 0, Loss: 1.096758
Train - Epoch 233, Batch: 0, Loss: 1.097486
Train - Epoch 234, Batch: 0, Loss: 1.090627
Train - Epoch 235, Batch: 0, Loss: 1.098527
Train - Epoch 236, Batch: 0, Loss: 1.099042
Train - Epoch 237, Batch: 0, Loss: 1.105669
Train - Epoch 238, Batch: 0, Loss: 1.097702
Train - Epoch 239, Batch: 0, Loss: 1.097392
Train - Epoch 240, Batch: 0, Loss: 1.111309
Train - Epoch 241, Batch: 0, Loss: 1.100023
Train - Epoch 242, Batch: 0, Loss: 1.094197
Train - Epoch 243, Batch: 0, Loss: 1.115816
Train - Epoch 244, Batch: 0, Loss: 1.096201
Train - Epoch 245, Batch: 0, Loss: 1.086632
Train - Epoch 246, Batch: 0, Loss: 1.093734
Train - Epoch 247, Batch: 0, Loss: 1.090976
Train - Epoch 248, Batch: 0, Loss: 1.089739
Train - Epoch 249, Batch: 0, Loss: 1.083904
Train - Epoch 250, Batch: 0, Loss: 1.084645
Train - Epoch 251, Batch: 0, Loss: 1.106343
Train - Epoch 252, Batch: 0, Loss: 1.085913
Train - Epoch 253, Batch: 0, Loss: 1.100656
Train - Epoch 254, Batch: 0, Loss: 1.092799
Train - Epoch 255, Batch: 0, Loss: 1.107682
Train - Epoch 256, Batch: 0, Loss: 1.085936
Train - Epoch 257, Batch: 0, Loss: 1.085431
Train - Epoch 258, Batch: 0, Loss: 1.081192
Train - Epoch 259, Batch: 0, Loss: 1.082426
Train - Epoch 260, Batch: 0, Loss: 1.093569
Train - Epoch 261, Batch: 0, Loss: 1.085357
Train - Epoch 262, Batch: 0, Loss: 1.088264
Train - Epoch 263, Batch: 0, Loss: 1.084990
Train - Epoch 264, Batch: 0, Loss: 1.072562
Train - Epoch 265, Batch: 0, Loss: 1.082022
Train - Epoch 266, Batch: 0, Loss: 1.084043
Train - Epoch 267, Batch: 0, Loss: 1.091021
Train - Epoch 268, Batch: 0, Loss: 1.087796
Train - Epoch 269, Batch: 0, Loss: 1.080097
Train - Epoch 270, Batch: 0, Loss: 1.074246
Train - Epoch 271, Batch: 0, Loss: 1.087329
Train - Epoch 272, Batch: 0, Loss: 1.088635
Train - Epoch 273, Batch: 0, Loss: 1.091860
Train - Epoch 274, Batch: 0, Loss: 1.094168
Train - Epoch 275, Batch: 0, Loss: 1.079574
Train - Epoch 276, Batch: 0, Loss: 1.079414
Train - Epoch 277, Batch: 0, Loss: 1.080522
Train - Epoch 278, Batch: 0, Loss: 1.066365
Train - Epoch 279, Batch: 0, Loss: 1.092021
Train - Epoch 280, Batch: 0, Loss: 1.080997
Train - Epoch 281, Batch: 0, Loss: 1.091119
Train - Epoch 282, Batch: 0, Loss: 1.087324
Train - Epoch 283, Batch: 0, Loss: 1.104305
Train - Epoch 284, Batch: 0, Loss: 1.085371
Train - Epoch 285, Batch: 0, Loss: 1.071654
Train - Epoch 286, Batch: 0, Loss: 1.090526
Train - Epoch 287, Batch: 0, Loss: 1.089387
Train - Epoch 288, Batch: 0, Loss: 1.074135
Train - Epoch 289, Batch: 0, Loss: 1.086518
Train - Epoch 290, Batch: 0, Loss: 1.078544
Train - Epoch 291, Batch: 0, Loss: 1.082908
Train - Epoch 292, Batch: 0, Loss: 1.061289
Train - Epoch 293, Batch: 0, Loss: 1.083178
Train - Epoch 294, Batch: 0, Loss: 1.075115
Train - Epoch 295, Batch: 0, Loss: 1.078327
Train - Epoch 296, Batch: 0, Loss: 1.079802
Train - Epoch 297, Batch: 0, Loss: 1.076006
Train - Epoch 298, Batch: 0, Loss: 1.068591
Train - Epoch 299, Batch: 0, Loss: 1.072904
Train - Epoch 300, Batch: 0, Loss: 1.077423
Train - Epoch 301, Batch: 0, Loss: 1.091589
Train - Epoch 302, Batch: 0, Loss: 1.080306
Train - Epoch 303, Batch: 0, Loss: 1.077617
Train - Epoch 304, Batch: 0, Loss: 1.081467
Train - Epoch 305, Batch: 0, Loss: 1.066836
Train - Epoch 306, Batch: 0, Loss: 1.072264
Train - Epoch 307, Batch: 0, Loss: 1.081660
Train - Epoch 308, Batch: 0, Loss: 1.079367
Train - Epoch 309, Batch: 0, Loss: 1.062431
Train - Epoch 310, Batch: 0, Loss: 1.075608
Train - Epoch 311, Batch: 0, Loss: 1.062786
Train - Epoch 312, Batch: 0, Loss: 1.063293
Train - Epoch 313, Batch: 0, Loss: 1.067217
Train - Epoch 314, Batch: 0, Loss: 1.080514
Train - Epoch 315, Batch: 0, Loss: 1.080998
Train - Epoch 316, Batch: 0, Loss: 1.071565
Train - Epoch 317, Batch: 0, Loss: 1.075909
Train - Epoch 318, Batch: 0, Loss: 1.084775
Train - Epoch 319, Batch: 0, Loss: 1.077257
Train - Epoch 320, Batch: 0, Loss: 1.066433
Train - Epoch 321, Batch: 0, Loss: 1.083759
Train - Epoch 322, Batch: 0, Loss: 1.057877
Train - Epoch 323, Batch: 0, Loss: 1.073070
Train - Epoch 324, Batch: 0, Loss: 1.071813
Train - Epoch 325, Batch: 0, Loss: 1.074267
Train - Epoch 326, Batch: 0, Loss: 1.053957
Train - Epoch 327, Batch: 0, Loss: 1.064044
Train - Epoch 328, Batch: 0, Loss: 1.067921
Train - Epoch 329, Batch: 0, Loss: 1.062224
Train - Epoch 330, Batch: 0, Loss: 1.069451
Train - Epoch 331, Batch: 0, Loss: 1.081486
Train - Epoch 332, Batch: 0, Loss: 1.064712
Train - Epoch 333, Batch: 0, Loss: 1.081323
Train - Epoch 334, Batch: 0, Loss: 1.087940
Train - Epoch 335, Batch: 0, Loss: 1.087289
Train - Epoch 336, Batch: 0, Loss: 1.083251
Train - Epoch 337, Batch: 0, Loss: 1.070419
Train - Epoch 338, Batch: 0, Loss: 1.072247
Train - Epoch 339, Batch: 0, Loss: 1.056350
Train - Epoch 340, Batch: 0, Loss: 1.080565
Train - Epoch 341, Batch: 0, Loss: 1.068239
Train - Epoch 342, Batch: 0, Loss: 1.058696
Train - Epoch 343, Batch: 0, Loss: 1.061286
Train - Epoch 344, Batch: 0, Loss: 1.059535
Train - Epoch 345, Batch: 0, Loss: 1.071580
Train - Epoch 346, Batch: 0, Loss: 1.071795
Train - Epoch 347, Batch: 0, Loss: 1.063409
Train - Epoch 348, Batch: 0, Loss: 1.072923
Train - Epoch 349, Batch: 0, Loss: 1.069997
Train - Epoch 350, Batch: 0, Loss: 1.061159
Train - Epoch 351, Batch: 0, Loss: 1.070067
Train - Epoch 352, Batch: 0, Loss: 1.055185
Train - Epoch 353, Batch: 0, Loss: 1.061816
Train - Epoch 354, Batch: 0, Loss: 1.067594
Train - Epoch 355, Batch: 0, Loss: 1.059062
Train - Epoch 356, Batch: 0, Loss: 1.047873
Train - Epoch 357, Batch: 0, Loss: 1.062419
Train - Epoch 358, Batch: 0, Loss: 1.068450
Train - Epoch 359, Batch: 0, Loss: 1.067749
Train - Epoch 360, Batch: 0, Loss: 1.061319
Train - Epoch 361, Batch: 0, Loss: 1.056369
Train - Epoch 362, Batch: 0, Loss: 1.069365
Train - Epoch 363, Batch: 0, Loss: 1.062850
Train - Epoch 364, Batch: 0, Loss: 1.078406
Train - Epoch 365, Batch: 0, Loss: 1.061502
Train - Epoch 366, Batch: 0, Loss: 1.067350
Train - Epoch 367, Batch: 0, Loss: 1.068952
Train - Epoch 368, Batch: 0, Loss: 1.061388
Train - Epoch 369, Batch: 0, Loss: 1.074360
Train - Epoch 370, Batch: 0, Loss: 1.073983
Train - Epoch 371, Batch: 0, Loss: 1.060040
Train - Epoch 372, Batch: 0, Loss: 1.058011
Train - Epoch 373, Batch: 0, Loss: 1.060867/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 374, Batch: 0, Loss: 1.057735
Train - Epoch 375, Batch: 0, Loss: 1.056628
Train - Epoch 376, Batch: 0, Loss: 1.063274
Train - Epoch 377, Batch: 0, Loss: 1.067781
Train - Epoch 378, Batch: 0, Loss: 1.061811
Train - Epoch 379, Batch: 0, Loss: 1.045557
Train - Epoch 380, Batch: 0, Loss: 1.041310
Train - Epoch 381, Batch: 0, Loss: 1.057205
Train - Epoch 382, Batch: 0, Loss: 1.057566
Train - Epoch 383, Batch: 0, Loss: 1.058139
Train - Epoch 384, Batch: 0, Loss: 1.060826
Train - Epoch 385, Batch: 0, Loss: 1.054659
Train - Epoch 386, Batch: 0, Loss: 1.058400
Train - Epoch 387, Batch: 0, Loss: 1.061427
Train - Epoch 388, Batch: 0, Loss: 1.054386
Train - Epoch 389, Batch: 0, Loss: 1.071170
Train - Epoch 390, Batch: 0, Loss: 1.051416
Train - Epoch 391, Batch: 0, Loss: 1.059107
Train - Epoch 392, Batch: 0, Loss: 1.057786
Train - Epoch 393, Batch: 0, Loss: 1.058182
Train - Epoch 394, Batch: 0, Loss: 1.063988
Train - Epoch 395, Batch: 0, Loss: 1.062934
Train - Epoch 396, Batch: 0, Loss: 1.047874
Train - Epoch 397, Batch: 0, Loss: 1.072930
Train - Epoch 398, Batch: 0, Loss: 1.051431
Train - Epoch 399, Batch: 0, Loss: 1.067964
Train - Epoch 400, Batch: 0, Loss: 1.066326
Train - Epoch 401, Batch: 0, Loss: 1.042732
Train - Epoch 402, Batch: 0, Loss: 1.043419
Train - Epoch 403, Batch: 0, Loss: 1.063733
Train - Epoch 404, Batch: 0, Loss: 1.072129
Train - Epoch 405, Batch: 0, Loss: 1.047189
Train - Epoch 406, Batch: 0, Loss: 1.053269
Train - Epoch 407, Batch: 0, Loss: 1.055042
Train - Epoch 408, Batch: 0, Loss: 1.051821
Train - Epoch 409, Batch: 0, Loss: 1.055606
Train - Epoch 410, Batch: 0, Loss: 1.039557
Train - Epoch 411, Batch: 0, Loss: 1.061896
Train - Epoch 412, Batch: 0, Loss: 1.050750
Train - Epoch 413, Batch: 0, Loss: 1.050538
Train - Epoch 414, Batch: 0, Loss: 1.058786
Train - Epoch 415, Batch: 0, Loss: 1.043311
Train - Epoch 416, Batch: 0, Loss: 1.043049
Train - Epoch 417, Batch: 0, Loss: 1.049463
Train - Epoch 418, Batch: 0, Loss: 1.051085
Train - Epoch 419, Batch: 0, Loss: 1.040537
Train - Epoch 420, Batch: 0, Loss: 1.049493
Train - Epoch 421, Batch: 0, Loss: 1.054493
Train - Epoch 422, Batch: 0, Loss: 1.056032
Train - Epoch 423, Batch: 0, Loss: 1.041552
Train - Epoch 424, Batch: 0, Loss: 1.047372
Train - Epoch 425, Batch: 0, Loss: 1.046813
Train - Epoch 426, Batch: 0, Loss: 1.033457
Train - Epoch 427, Batch: 0, Loss: 1.042217
Train - Epoch 428, Batch: 0, Loss: 1.043507
Train - Epoch 429, Batch: 0, Loss: 1.048852
Train - Epoch 430, Batch: 0, Loss: 1.052645
Train - Epoch 431, Batch: 0, Loss: 1.053661
Train - Epoch 432, Batch: 0, Loss: 1.062327
Train - Epoch 433, Batch: 0, Loss: 1.056108
Train - Epoch 434, Batch: 0, Loss: 1.046466
Train - Epoch 435, Batch: 0, Loss: 1.062625
Train - Epoch 436, Batch: 0, Loss: 1.038814
Train - Epoch 437, Batch: 0, Loss: 1.051057
Train - Epoch 438, Batch: 0, Loss: 1.050555
Train - Epoch 439, Batch: 0, Loss: 1.036996
Train - Epoch 440, Batch: 0, Loss: 1.034252
Train - Epoch 441, Batch: 0, Loss: 1.057806
Train - Epoch 442, Batch: 0, Loss: 1.072551
Train - Epoch 443, Batch: 0, Loss: 1.033232
Train - Epoch 444, Batch: 0, Loss: 1.057966
Train - Epoch 445, Batch: 0, Loss: 1.041802
Train - Epoch 446, Batch: 0, Loss: 1.040657
Train - Epoch 447, Batch: 0, Loss: 1.046740
Train - Epoch 448, Batch: 0, Loss: 1.057795
Train - Epoch 449, Batch: 0, Loss: 1.037358
Train - Epoch 450, Batch: 0, Loss: 1.036673
Train - Epoch 451, Batch: 0, Loss: 1.049381
Train - Epoch 452, Batch: 0, Loss: 1.046994
Train - Epoch 453, Batch: 0, Loss: 1.035785
Train - Epoch 454, Batch: 0, Loss: 1.054275
Train - Epoch 455, Batch: 0, Loss: 1.047908
Train - Epoch 456, Batch: 0, Loss: 1.042494
Train - Epoch 457, Batch: 0, Loss: 1.057339
Train - Epoch 458, Batch: 0, Loss: 1.044235
Train - Epoch 459, Batch: 0, Loss: 1.045963
Train - Epoch 460, Batch: 0, Loss: 1.049721
Train - Epoch 461, Batch: 0, Loss: 1.048182
Train - Epoch 462, Batch: 0, Loss: 1.044774
Train - Epoch 463, Batch: 0, Loss: 1.036993
Train - Epoch 464, Batch: 0, Loss: 1.041245
Train - Epoch 465, Batch: 0, Loss: 1.047539
Train - Epoch 466, Batch: 0, Loss: 1.051015
Train - Epoch 467, Batch: 0, Loss: 1.048681
Train - Epoch 468, Batch: 0, Loss: 1.059449
Train - Epoch 469, Batch: 0, Loss: 1.050203
Train - Epoch 470, Batch: 0, Loss: 1.036391
Train - Epoch 471, Batch: 0, Loss: 1.041079
Train - Epoch 472, Batch: 0, Loss: 1.041180
Train - Epoch 473, Batch: 0, Loss: 1.030931
Train - Epoch 474, Batch: 0, Loss: 1.041015
Train - Epoch 475, Batch: 0, Loss: 1.043981
Train - Epoch 476, Batch: 0, Loss: 1.045687
Train - Epoch 477, Batch: 0, Loss: 1.038285
Train - Epoch 478, Batch: 0, Loss: 1.025340
Train - Epoch 479, Batch: 0, Loss: 1.032428
Train - Epoch 480, Batch: 0, Loss: 1.035389
Train - Epoch 481, Batch: 0, Loss: 1.037622
Train - Epoch 482, Batch: 0, Loss: 1.060015
Train - Epoch 483, Batch: 0, Loss: 1.038062
Train - Epoch 484, Batch: 0, Loss: 1.036482
Train - Epoch 485, Batch: 0, Loss: 1.037810
Train - Epoch 486, Batch: 0, Loss: 1.032436
Train - Epoch 487, Batch: 0, Loss: 1.032664
Train - Epoch 488, Batch: 0, Loss: 1.021097
Train - Epoch 489, Batch: 0, Loss: 1.036336
Train - Epoch 490, Batch: 0, Loss: 1.041628
Train - Epoch 491, Batch: 0, Loss: 1.046007
Train - Epoch 492, Batch: 0, Loss: 1.036451
Train - Epoch 493, Batch: 0, Loss: 1.043454
Train - Epoch 494, Batch: 0, Loss: 1.042491
Train - Epoch 495, Batch: 0, Loss: 1.049572
Train - Epoch 496, Batch: 0, Loss: 1.034225
Train - Epoch 497, Batch: 0, Loss: 1.040604
Train - Epoch 498, Batch: 0, Loss: 1.040075
Train - Epoch 499, Batch: 0, Loss: 1.049553
training_time:: 106.33746695518494
training time full:: 106.33753681182861
provenance prepare time:: 0.0
here
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633700
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 250
max_epoch:: 500
batch_size:: 10000
epoch:: 0
tensor([32768,     4, 32774,    10, 32780,    13,    15, 32784,    20,    28,
        32798,    31,    45, 32816,    50, 32820,    54, 32831,    65, 32834,
        32836,    69,    75, 32847,    81,    82, 32849,    84,    85, 32853,
        32855, 32861, 32864,    97, 32865, 32866, 32869,   110,   111, 32881,
        32882, 32883,   118,   127,   129, 32899, 32901,   136, 32909, 32914])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
epoch:: 60
epoch:: 61
epoch:: 62
epoch:: 63
epoch:: 64
epoch:: 65
epoch:: 66
epoch:: 67
epoch:: 68
epoch:: 69
epoch:: 70
epoch:: 71
epoch:: 72
epoch:: 73
epoch:: 74
epoch:: 75
epoch:: 76
epoch:: 77
epoch:: 78
epoch:: 79
epoch:: 80
epoch:: 81
epoch:: 82
epoch:: 83
epoch:: 84
epoch:: 85
epoch:: 86
epoch:: 87
epoch:: 88
epoch:: 89
epoch:: 90
epoch:: 91
epoch:: 92
epoch:: 93
epoch:: 94
epoch:: 95
epoch:: 96
epoch:: 97
epoch:: 98
epoch:: 99
epoch:: 100
epoch:: 101
epoch:: 102
epoch:: 103
epoch:: 104
epoch:: 105
epoch:: 106
epoch:: 107
epoch:: 108
epoch:: 109
epoch:: 110
epoch:: 111
epoch:: 112
epoch:: 113
epoch:: 114
epoch:: 115
epoch:: 116
epoch:: 117
epoch:: 118
epoch:: 119
epoch:: 120
epoch:: 121
epoch:: 122
epoch:: 123
epoch:: 124
epoch:: 125
epoch:: 126
epoch:: 127
epoch:: 128
epoch:: 129
epoch:: 130
epoch:: 131
epoch:: 132
epoch:: 133
epoch:: 134
epoch:: 135
epoch:: 136
epoch:: 137
epoch:: 138
epoch:: 139
epoch:: 140
epoch:: 141
epoch:: 142
epoch:: 143
epoch:: 144
epoch:: 145
epoch:: 146
epoch:: 147
epoch:: 148
epoch:: 149
epoch:: 150
epoch:: 151
epoch:: 152
epoch:: 153
epoch:: 154
epoch:: 155
epoch:: 156
epoch:: 157
epoch:: 158
epoch:: 159
epoch:: 160
epoch:: 161
epoch:: 162
epoch:: 163
epoch:: 164
epoch:: 165
epoch:: 166
epoch:: 167
epoch:: 168
epoch:: 169
epoch:: 170
epoch:: 171
epoch:: 172
epoch:: 173
epoch:: 174
epoch:: 175
epoch:: 176
epoch:: 177
epoch:: 178
epoch:: 179
epoch:: 180
epoch:: 181
epoch:: 182
epoch:: 183
epoch:: 184
epoch:: 185
epoch:: 186
epoch:: 187
epoch:: 188
epoch:: 189
epoch:: 190
epoch:: 191
epoch:: 192
epoch:: 193
epoch:: 194
epoch:: 195
epoch:: 196
epoch:: 197
epoch:: 198
epoch:: 199
epoch:: 200
epoch:: 201
epoch:: 202
epoch:: 203
epoch:: 204
epoch:: 205
epoch:: 206
epoch:: 207
epoch:: 208
epoch:: 209
epoch:: 210
epoch:: 211
epoch:: 212
epoch:: 213
epoch:: 214
epoch:: 215
epoch:: 216
epoch:: 217
epoch:: 218
epoch:: 219
epoch:: 220
epoch:: 221
epoch:: 222
epoch:: 223
epoch:: 224
epoch:: 225
epoch:: 226
epoch:: 227
epoch:: 228
epoch:: 229
epoch:: 230
epoch:: 231
epoch:: 232
epoch:: 233
epoch:: 234
epoch:: 235
epoch:: 236
epoch:: 237
epoch:: 238
epoch:: 239
epoch:: 240
epoch:: 241
epoch:: 242
epoch:: 243
epoch:: 244
epoch:: 245
epoch:: 246
epoch:: 247
epoch:: 248
epoch:: 249
epoch:: 250
epoch:: 251
epoch:: 252
epoch:: 253
epoch:: 254
epoch:: 255
epoch:: 256
epoch:: 257
epoch:: 258
epoch:: 259
epoch:: 260
epoch:: 261
epoch:: 262
epoch:: 263
epoch:: 264
epoch:: 265
epoch:: 266
epoch:: 267
epoch:: 268
epoch:: 269
epoch:: 270
epoch:: 271
epoch:: 272
epoch:: 273
epoch:: 274
epoch:: 275
epoch:: 276
epoch:: 277
epoch:: 278
epoch:: 279
epoch:: 280
epoch:: 281
epoch:: 282
epoch:: 283
epoch:: 284
epoch:: 285
epoch:: 286
epoch:: 287
epoch:: 288
epoch:: 289
epoch:: 290
epoch:: 291
epoch:: 292
epoch:: 293
epoch:: 294
epoch:: 295
epoch:: 296
epoch:: 297
epoch:: 298
epoch:: 299
epoch:: 300
epoch:: 301
epoch:: 302
epoch:: 303
epoch:: 304
epoch:: 305
epoch:: 306
epoch:: 307
epoch:: 308
epoch:: 309
epoch:: 310
epoch:: 311
epoch:: 312
epoch:: 313
epoch:: 314
epoch:: 315
epoch:: 316
epoch:: 317
epoch:: 318
epoch:: 319
epoch:: 320
epoch:: 321
epoch:: 322
epoch:: 323
epoch:: 324
epoch:: 325
epoch:: 326
epoch:: 327
epoch:: 328
epoch:: 329
epoch:: 330
epoch:: 331
epoch:: 332
epoch:: 333
epoch:: 334
epoch:: 335
epoch:: 336
epoch:: 337
epoch:: 338
epoch:: 339
epoch:: 340
epoch:: 341
epoch:: 342
epoch:: 343
epoch:: 344
epoch:: 345
epoch:: 346
epoch:: 347
epoch:: 348
epoch:: 349
epoch:: 350
epoch:: 351
epoch:: 352
epoch:: 353
epoch:: 354
epoch:: 355
epoch:: 356
epoch:: 357
epoch:: 358
epoch:: 359
epoch:: 360
epoch:: 361
epoch:: 362
epoch:: 363
epoch:: 364
epoch:: 365
epoch:: 366
epoch:: 367
epoch:: 368
epoch:: 369
epoch:: 370
epoch:: 371
epoch:: 372
epoch:: 373
epoch:: 374
epoch:: 375
epoch:: 376
epoch:: 377
epoch:: 378
epoch:: 379
epoch:: 380
epoch:: 381
epoch:: 382
epoch:: 383
epoch:: 384
epoch:: 385
epoch:: 386
epoch:: 387
epoch:: 388
epoch:: 389
epoch:: 390
epoch:: 391
epoch:: 392
epoch:: 393
epoch:: 394
epoch:: 395
epoch:: 396
epoch:: 397
epoch:: 398
epoch:: 399
epoch:: 400
epoch:: 401
epoch:: 402
epoch:: 403
epoch:: 404
epoch:: 405
epoch:: 406
epoch:: 407
epoch:: 408
epoch:: 409
epoch:: 410
epoch:: 411
epoch:: 412
epoch:: 413
epoch:: 414
epoch:: 415
epoch:: 416
epoch:: 417
epoch:: 418
epoch:: 419
epoch:: 420
epoch:: 421
epoch:: 422
epoch:: 423
epoch:: 424
epoch:: 425
epoch:: 426
epoch:: 427
epoch:: 428
epoch:: 429
epoch:: 430
epoch:: 431
epoch:: 432
epoch:: 433
epoch:: 434
epoch:: 435
epoch:: 436
epoch:: 437
epoch:: 438
epoch:: 439
epoch:: 440
epoch:: 441
epoch:: 442
epoch:: 443
epoch:: 444
epoch:: 445
epoch:: 446
epoch:: 447
epoch:: 448
epoch:: 449
epoch:: 450
epoch:: 451
epoch:: 452
epoch:: 453
epoch:: 454
epoch:: 455
epoch:: 456
epoch:: 457
epoch:: 458
epoch:: 459
epoch:: 460
epoch:: 461
epoch:: 462
epoch:: 463
epoch:: 464
epoch:: 465
epoch:: 466
epoch:: 467
epoch:: 468
epoch:: 469
epoch:: 470
epoch:: 471
epoch:: 472
epoch:: 473
epoch:: 474
epoch:: 475
epoch:: 476
epoch:: 477
epoch:: 478
epoch:: 479
epoch:: 480
epoch:: 481
epoch:: 482
epoch:: 483
epoch:: 484
epoch:: 485
epoch:: 486
epoch:: 487
epoch:: 488
epoch:: 489
epoch:: 490
epoch:: 491
epoch:: 492
epoch:: 493
epoch:: 494
epoch:: 495
epoch:: 496
epoch:: 497
epoch:: 498
epoch:: 499
training time is 80.5527229309082
overhead:: 0
overhead2:: 2.6794795989990234
overhead3:: 0
time_baseline:: 80.5527606010437
curr_diff: 0 tensor(0.1355, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1355, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633900
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 3 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 250
max_epoch:: 500
overhead:: 0
overhead2:: 0.08252978324890137
overhead3:: 0.2547576427459717
overhead4:: 10.250568389892578
overhead5:: 0
memory usage:: 5641764864
time_provenance:: 18.38409924507141
curr_diff: 0 tensor(0.0332, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0332, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1609, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1609, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633800
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 3 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 250
max_epoch:: 500
overhead:: 0
overhead2:: 0.08668732643127441
overhead3:: 0.26485109329223633
overhead4:: 10.607972621917725
overhead5:: 0
memory usage:: 5626114048
time_provenance:: 18.829588890075684
curr_diff: 0 tensor(0.0332, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0332, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1610, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1610, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633800
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 3 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 250
max_epoch:: 500
overhead:: 0
overhead2:: 0.09028458595275879
overhead3:: 0.27167272567749023
overhead4:: 10.944790601730347
overhead5:: 0
memory usage:: 5647671296
time_provenance:: 19.23284339904785
curr_diff: 0 tensor(0.0334, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0334, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1610, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1610, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633000
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 3 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 250
max_epoch:: 500
overhead:: 0
overhead2:: 0.16396236419677734
overhead3:: 0.4567687511444092
overhead4:: 18.957011699676514
overhead5:: 0
memory usage:: 5647491072
time_provenance:: 29.226205825805664
curr_diff: 0 tensor(0.0031, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0031, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1377, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1377, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633700
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 3 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 250
max_epoch:: 500
overhead:: 0
overhead2:: 0.16809940338134766
overhead3:: 0.46516919136047363
overhead4:: 19.373963594436646
overhead5:: 0
memory usage:: 5638529024
time_provenance:: 29.711081504821777
curr_diff: 0 tensor(0.0031, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0031, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1377, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1377, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633800
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 3 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 250
max_epoch:: 500
overhead:: 0
overhead2:: 0.17097234725952148
overhead3:: 0.47469568252563477
overhead4:: 19.58660340309143
overhead5:: 0
memory usage:: 5629722624
time_provenance:: 29.91664433479309
curr_diff: 0 tensor(0.0031, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0031, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1377, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1377, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633700
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 3 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 250
max_epoch:: 500
overhead:: 0
overhead2:: 0.4068160057067871
overhead3:: 1.0377733707427979
overhead4:: 44.390963077545166
overhead5:: 0
memory usage:: 5630099456
time_provenance:: 60.92161703109741
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1356, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1356, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633800
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 3 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 250
max_epoch:: 500
overhead:: 0
overhead2:: 0.41991233825683594
overhead3:: 1.0328688621520996
overhead4:: 45.012640953063965
overhead5:: 0
memory usage:: 5644472320
time_provenance:: 61.585373401641846
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1356, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1356, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633800
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 3 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 250
max_epoch:: 500
overhead:: 0
overhead2:: 0.41835880279541016
overhead3:: 1.0606536865234375
overhead4:: 45.21191620826721
overhead5:: 0
memory usage:: 5652054016
time_provenance:: 61.81866812705994
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1356, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1356, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633800
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 3 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 250
max_epoch:: 500
overhead:: 0
overhead2:: 0.823915958404541
overhead3:: 2.0140573978424072
overhead4:: 82.34337282180786
overhead5:: 0
memory usage:: 5632647168
time_provenance:: 107.14262104034424
curr_diff: 0 tensor(5.0070e-13, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.0070e-13, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1355, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1355, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633900
repetition 4
python3 benchmark_exp_lr.py 0.001 10000 500 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05] Logistic_regression cifar10_2 4 0.0001 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 2.303715
Train - Epoch 1, Batch: 0, Loss: 2.240496
Train - Epoch 2, Batch: 0, Loss: 2.185365
Train - Epoch 3, Batch: 0, Loss: 2.137666
Train - Epoch 4, Batch: 0, Loss: 2.086977
Train - Epoch 5, Batch: 0, Loss: 2.044221
Train - Epoch 6, Batch: 0, Loss: 2.005321
Train - Epoch 7, Batch: 0, Loss: 1.966532
Train - Epoch 8, Batch: 0, Loss: 1.929405
Train - Epoch 9, Batch: 0, Loss: 1.894426
Train - Epoch 10, Batch: 0, Loss: 1.858416
Train - Epoch 11, Batch: 0, Loss: 1.835262
Train - Epoch 12, Batch: 0, Loss: 1.804233
Train - Epoch 13, Batch: 0, Loss: 1.783106
Train - Epoch 14, Batch: 0, Loss: 1.763234
Train - Epoch 15, Batch: 0, Loss: 1.733694
Train - Epoch 16, Batch: 0, Loss: 1.708050
Train - Epoch 17, Batch: 0, Loss: 1.696488
Train - Epoch 18, Batch: 0, Loss: 1.678582
Train - Epoch 19, Batch: 0, Loss: 1.655942
Train - Epoch 20, Batch: 0, Loss: 1.645209
Train - Epoch 21, Batch: 0, Loss: 1.619404
Train - Epoch 22, Batch: 0, Loss: 1.604294
Train - Epoch 23, Batch: 0, Loss: 1.591226
Train - Epoch 24, Batch: 0, Loss: 1.575595
Train - Epoch 25, Batch: 0, Loss: 1.571768
Train - Epoch 26, Batch: 0, Loss: 1.551926
Train - Epoch 27, Batch: 0, Loss: 1.537629
Train - Epoch 28, Batch: 0, Loss: 1.523780
Train - Epoch 29, Batch: 0, Loss: 1.520215
Train - Epoch 30, Batch: 0, Loss: 1.502190
Train - Epoch 31, Batch: 0, Loss: 1.496801
Train - Epoch 32, Batch: 0, Loss: 1.477943
Train - Epoch 33, Batch: 0, Loss: 1.482922
Train - Epoch 34, Batch: 0, Loss: 1.465435
Train - Epoch 35, Batch: 0, Loss: 1.461725
Train - Epoch 36, Batch: 0, Loss: 1.445274
Train - Epoch 37, Batch: 0, Loss: 1.438532
Train - Epoch 38, Batch: 0, Loss: 1.441183
Train - Epoch 39, Batch: 0, Loss: 1.424734
Train - Epoch 40, Batch: 0, Loss: 1.419970
Train - Epoch 41, Batch: 0, Loss: 1.416287
Train - Epoch 42, Batch: 0, Loss: 1.400542
Train - Epoch 43, Batch: 0, Loss: 1.404186
Train - Epoch 44, Batch: 0, Loss: 1.397421
Train - Epoch 45, Batch: 0, Loss: 1.387129
Train - Epoch 46, Batch: 0, Loss: 1.387039
Train - Epoch 47, Batch: 0, Loss: 1.380928
Train - Epoch 48, Batch: 0, Loss: 1.377890
Train - Epoch 49, Batch: 0, Loss: 1.378643
Train - Epoch 50, Batch: 0, Loss: 1.351694
Train - Epoch 51, Batch: 0, Loss: 1.349928
Train - Epoch 52, Batch: 0, Loss: 1.348596
Train - Epoch 53, Batch: 0, Loss: 1.347075
Train - Epoch 54, Batch: 0, Loss: 1.346715
Train - Epoch 55, Batch: 0, Loss: 1.341500
Train - Epoch 56, Batch: 0, Loss: 1.347546
Train - Epoch 57, Batch: 0, Loss: 1.331262
Train - Epoch 58, Batch: 0, Loss: 1.324343
Train - Epoch 59, Batch: 0, Loss: 1.313859
Train - Epoch 60, Batch: 0, Loss: 1.321108
Train - Epoch 61, Batch: 0, Loss: 1.317466
Train - Epoch 62, Batch: 0, Loss: 1.307406
Train - Epoch 63, Batch: 0, Loss: 1.309727
Train - Epoch 64, Batch: 0, Loss: 1.303862
Train - Epoch 65, Batch: 0, Loss: 1.294932
Train - Epoch 66, Batch: 0, Loss: 1.304396
Train - Epoch 67, Batch: 0, Loss: 1.290499
Train - Epoch 68, Batch: 0, Loss: 1.281601
Train - Epoch 69, Batch: 0, Loss: 1.269114
Train - Epoch 70, Batch: 0, Loss: 1.277259
Train - Epoch 71, Batch: 0, Loss: 1.291270
Train - Epoch 72, Batch: 0, Loss: 1.272658
Train - Epoch 73, Batch: 0, Loss: 1.277880
Train - Epoch 74, Batch: 0, Loss: 1.263473
Train - Epoch 75, Batch: 0, Loss: 1.272697
Train - Epoch 76, Batch: 0, Loss: 1.262609
Train - Epoch 77, Batch: 0, Loss: 1.275909
Train - Epoch 78, Batch: 0, Loss: 1.262511
Train - Epoch 79, Batch: 0, Loss: 1.254072
Train - Epoch 80, Batch: 0, Loss: 1.255285
Train - Epoch 81, Batch: 0, Loss: 1.244373
Train - Epoch 82, Batch: 0, Loss: 1.247502
Train - Epoch 83, Batch: 0, Loss: 1.248137
Train - Epoch 84, Batch: 0, Loss: 1.248037
Train - Epoch 85, Batch: 0, Loss: 1.256270
Train - Epoch 86, Batch: 0, Loss: 1.239572
Train - Epoch 87, Batch: 0, Loss: 1.247642
Train - Epoch 88, Batch: 0, Loss: 1.244436
Train - Epoch 89, Batch: 0, Loss: 1.237765
Train - Epoch 90, Batch: 0, Loss: 1.218532
Train - Epoch 91, Batch: 0, Loss: 1.231311
Train - Epoch 92, Batch: 0, Loss: 1.219009
Train - Epoch 93, Batch: 0, Loss: 1.231579
Train - Epoch 94, Batch: 0, Loss: 1.212814
Train - Epoch 95, Batch: 0, Loss: 1.229708
Train - Epoch 96, Batch: 0, Loss: 1.222236
Train - Epoch 97, Batch: 0, Loss: 1.214599
Train - Epoch 98, Batch: 0, Loss: 1.218642
Train - Epoch 99, Batch: 0, Loss: 1.219125
Train - Epoch 100, Batch: 0, Loss: 1.216795
Train - Epoch 101, Batch: 0, Loss: 1.200840
Train - Epoch 102, Batch: 0, Loss: 1.204253
Train - Epoch 103, Batch: 0, Loss: 1.208634
Train - Epoch 104, Batch: 0, Loss: 1.213091
Train - Epoch 105, Batch: 0, Loss: 1.223747
Train - Epoch 106, Batch: 0, Loss: 1.203592
Train - Epoch 107, Batch: 0, Loss: 1.208588
Train - Epoch 108, Batch: 0, Loss: 1.208615
Train - Epoch 109, Batch: 0, Loss: 1.197030
Train - Epoch 110, Batch: 0, Loss: 1.195322
Train - Epoch 111, Batch: 0, Loss: 1.191139
Train - Epoch 112, Batch: 0, Loss: 1.198250
Train - Epoch 113, Batch: 0, Loss: 1.183152
Train - Epoch 114, Batch: 0, Loss: 1.191765
Train - Epoch 115, Batch: 0, Loss: 1.198595
Train - Epoch 116, Batch: 0, Loss: 1.186358
Train - Epoch 117, Batch: 0, Loss: 1.190827
Train - Epoch 118, Batch: 0, Loss: 1.186928
Train - Epoch 119, Batch: 0, Loss: 1.192282
Train - Epoch 120, Batch: 0, Loss: 1.178319
Train - Epoch 121, Batch: 0, Loss: 1.189329
Train - Epoch 122, Batch: 0, Loss: 1.176780
Train - Epoch 123, Batch: 0, Loss: 1.191867
Train - Epoch 124, Batch: 0, Loss: 1.178305
Train - Epoch 125, Batch: 0, Loss: 1.175539
Train - Epoch 126, Batch: 0, Loss: 1.173780
Train - Epoch 127, Batch: 0, Loss: 1.160634
Train - Epoch 128, Batch: 0, Loss: 1.178329
Train - Epoch 129, Batch: 0, Loss: 1.179090
Train - Epoch 130, Batch: 0, Loss: 1.172399
Train - Epoch 131, Batch: 0, Loss: 1.170361
Train - Epoch 132, Batch: 0, Loss: 1.177625
Train - Epoch 133, Batch: 0, Loss: 1.181123
Train - Epoch 134, Batch: 0, Loss: 1.172885
Train - Epoch 135, Batch: 0, Loss: 1.172722
Train - Epoch 136, Batch: 0, Loss: 1.155953
Train - Epoch 137, Batch: 0, Loss: 1.160927
Train - Epoch 138, Batch: 0, Loss: 1.151115
Train - Epoch 139, Batch: 0, Loss: 1.174873
Train - Epoch 140, Batch: 0, Loss: 1.165232
Train - Epoch 141, Batch: 0, Loss: 1.168284
Train - Epoch 142, Batch: 0, Loss: 1.163807
Train - Epoch 143, Batch: 0, Loss: 1.167238
Train - Epoch 144, Batch: 0, Loss: 1.158272
Train - Epoch 145, Batch: 0, Loss: 1.156646
Train - Epoch 146, Batch: 0, Loss: 1.159366
Train - Epoch 147, Batch: 0, Loss: 1.157135
Train - Epoch 148, Batch: 0, Loss: 1.159682
Train - Epoch 149, Batch: 0, Loss: 1.155758
Train - Epoch 150, Batch: 0, Loss: 1.143575
Train - Epoch 151, Batch: 0, Loss: 1.137703
Train - Epoch 152, Batch: 0, Loss: 1.151074
Train - Epoch 153, Batch: 0, Loss: 1.148611
Train - Epoch 154, Batch: 0, Loss: 1.145986
Train - Epoch 155, Batch: 0, Loss: 1.143895
Train - Epoch 156, Batch: 0, Loss: 1.141287
Train - Epoch 157, Batch: 0, Loss: 1.146676
Train - Epoch 158, Batch: 0, Loss: 1.145423
Train - Epoch 159, Batch: 0, Loss: 1.146779
Train - Epoch 160, Batch: 0, Loss: 1.147243
Train - Epoch 161, Batch: 0, Loss: 1.146701
Train - Epoch 162, Batch: 0, Loss: 1.160187
Train - Epoch 163, Batch: 0, Loss: 1.129526
Train - Epoch 164, Batch: 0, Loss: 1.138505
Train - Epoch 165, Batch: 0, Loss: 1.130314
Train - Epoch 166, Batch: 0, Loss: 1.137899
Train - Epoch 167, Batch: 0, Loss: 1.134631
Train - Epoch 168, Batch: 0, Loss: 1.144916
Train - Epoch 169, Batch: 0, Loss: 1.135602
Train - Epoch 170, Batch: 0, Loss: 1.129504
Train - Epoch 171, Batch: 0, Loss: 1.136527
Train - Epoch 172, Batch: 0, Loss: 1.138939
Train - Epoch 173, Batch: 0, Loss: 1.137251
Train - Epoch 174, Batch: 0, Loss: 1.129161
Train - Epoch 175, Batch: 0, Loss: 1.132316
Train - Epoch 176, Batch: 0, Loss: 1.139327
Train - Epoch 177, Batch: 0, Loss: 1.139787
Train - Epoch 178, Batch: 0, Loss: 1.133363
Train - Epoch 179, Batch: 0, Loss: 1.121955
Train - Epoch 180, Batch: 0, Loss: 1.135067
Train - Epoch 181, Batch: 0, Loss: 1.121051
Train - Epoch 182, Batch: 0, Loss: 1.134006
Train - Epoch 183, Batch: 0, Loss: 1.137407
Train - Epoch 184, Batch: 0, Loss: 1.128654
Train - Epoch 185, Batch: 0, Loss: 1.102958
Train - Epoch 186, Batch: 0, Loss: 1.134129
Train - Epoch 187, Batch: 0, Loss: 1.130383
Train - Epoch 188, Batch: 0, Loss: 1.140017
Train - Epoch 189, Batch: 0, Loss: 1.119167
Train - Epoch 190, Batch: 0, Loss: 1.126234
Train - Epoch 191, Batch: 0, Loss: 1.129762
Train - Epoch 192, Batch: 0, Loss: 1.128018
Train - Epoch 193, Batch: 0, Loss: 1.119416
Train - Epoch 194, Batch: 0, Loss: 1.111848
Train - Epoch 195, Batch: 0, Loss: 1.118887
Train - Epoch 196, Batch: 0, Loss: 1.141907
Train - Epoch 197, Batch: 0, Loss: 1.120110
Train - Epoch 198, Batch: 0, Loss: 1.116111
Train - Epoch 199, Batch: 0, Loss: 1.116304
Train - Epoch 200, Batch: 0, Loss: 1.122743
Train - Epoch 201, Batch: 0, Loss: 1.117764
Train - Epoch 202, Batch: 0, Loss: 1.119134
Train - Epoch 203, Batch: 0, Loss: 1.098776
Train - Epoch 204, Batch: 0, Loss: 1.124555
Train - Epoch 205, Batch: 0, Loss: 1.118463
Train - Epoch 206, Batch: 0, Loss: 1.116498
Train - Epoch 207, Batch: 0, Loss: 1.110066
Train - Epoch 208, Batch: 0, Loss: 1.127464
Train - Epoch 209, Batch: 0, Loss: 1.108849
Train - Epoch 210, Batch: 0, Loss: 1.101563
Train - Epoch 211, Batch: 0, Loss: 1.114690
Train - Epoch 212, Batch: 0, Loss: 1.130134
Train - Epoch 213, Batch: 0, Loss: 1.120751
Train - Epoch 214, Batch: 0, Loss: 1.117211
Train - Epoch 215, Batch: 0, Loss: 1.103735
Train - Epoch 216, Batch: 0, Loss: 1.113631
Train - Epoch 217, Batch: 0, Loss: 1.102083
Train - Epoch 218, Batch: 0, Loss: 1.114561
Train - Epoch 219, Batch: 0, Loss: 1.109023
Train - Epoch 220, Batch: 0, Loss: 1.121886
Train - Epoch 221, Batch: 0, Loss: 1.109571
Train - Epoch 222, Batch: 0, Loss: 1.099070
Train - Epoch 223, Batch: 0, Loss: 1.111587
Train - Epoch 224, Batch: 0, Loss: 1.106401
Train - Epoch 225, Batch: 0, Loss: 1.110012
Train - Epoch 226, Batch: 0, Loss: 1.099431
Train - Epoch 227, Batch: 0, Loss: 1.100212
Train - Epoch 228, Batch: 0, Loss: 1.114360
Train - Epoch 229, Batch: 0, Loss: 1.109874
Train - Epoch 230, Batch: 0, Loss: 1.095487
Train - Epoch 231, Batch: 0, Loss: 1.095585
Train - Epoch 232, Batch: 0, Loss: 1.102801
Train - Epoch 233, Batch: 0, Loss: 1.107684
Train - Epoch 234, Batch: 0, Loss: 1.111746
Train - Epoch 235, Batch: 0, Loss: 1.110613
Train - Epoch 236, Batch: 0, Loss: 1.106374
Train - Epoch 237, Batch: 0, Loss: 1.103998
Train - Epoch 238, Batch: 0, Loss: 1.099485
Train - Epoch 239, Batch: 0, Loss: 1.102771
Train - Epoch 240, Batch: 0, Loss: 1.099143
Train - Epoch 241, Batch: 0, Loss: 1.093979
Train - Epoch 242, Batch: 0, Loss: 1.110042
Train - Epoch 243, Batch: 0, Loss: 1.082853
Train - Epoch 244, Batch: 0, Loss: 1.093945
Train - Epoch 245, Batch: 0, Loss: 1.097482
Train - Epoch 246, Batch: 0, Loss: 1.101817
Train - Epoch 247, Batch: 0, Loss: 1.094374
Train - Epoch 248, Batch: 0, Loss: 1.098124
Train - Epoch 249, Batch: 0, Loss: 1.078231
Train - Epoch 250, Batch: 0, Loss: 1.084810
Train - Epoch 251, Batch: 0, Loss: 1.103877
Train - Epoch 252, Batch: 0, Loss: 1.085288
Train - Epoch 253, Batch: 0, Loss: 1.089151
Train - Epoch 254, Batch: 0, Loss: 1.106579
Train - Epoch 255, Batch: 0, Loss: 1.097154
Train - Epoch 256, Batch: 0, Loss: 1.085454
Train - Epoch 257, Batch: 0, Loss: 1.095080
Train - Epoch 258, Batch: 0, Loss: 1.109282
Train - Epoch 259, Batch: 0, Loss: 1.089811
Train - Epoch 260, Batch: 0, Loss: 1.093967
Train - Epoch 261, Batch: 0, Loss: 1.086020
Train - Epoch 262, Batch: 0, Loss: 1.096105
Train - Epoch 263, Batch: 0, Loss: 1.086937
Train - Epoch 264, Batch: 0, Loss: 1.084922
Train - Epoch 265, Batch: 0, Loss: 1.089910
Train - Epoch 266, Batch: 0, Loss: 1.084782
Train - Epoch 267, Batch: 0, Loss: 1.088999
Train - Epoch 268, Batch: 0, Loss: 1.093772
Train - Epoch 269, Batch: 0, Loss: 1.096883
Train - Epoch 270, Batch: 0, Loss: 1.096275
Train - Epoch 271, Batch: 0, Loss: 1.092691
Train - Epoch 272, Batch: 0, Loss: 1.093061
Train - Epoch 273, Batch: 0, Loss: 1.093896
Train - Epoch 274, Batch: 0, Loss: 1.096382
Train - Epoch 275, Batch: 0, Loss: 1.075065
Train - Epoch 276, Batch: 0, Loss: 1.091262
Train - Epoch 277, Batch: 0, Loss: 1.064496
Train - Epoch 278, Batch: 0, Loss: 1.068219
Train - Epoch 279, Batch: 0, Loss: 1.093253
Train - Epoch 280, Batch: 0, Loss: 1.085439
Train - Epoch 281, Batch: 0, Loss: 1.091078
Train - Epoch 282, Batch: 0, Loss: 1.084931
Train - Epoch 283, Batch: 0, Loss: 1.083797
Train - Epoch 284, Batch: 0, Loss: 1.077771
Train - Epoch 285, Batch: 0, Loss: 1.067684
Train - Epoch 286, Batch: 0, Loss: 1.083867
Train - Epoch 287, Batch: 0, Loss: 1.065519
Train - Epoch 288, Batch: 0, Loss: 1.084854
Train - Epoch 289, Batch: 0, Loss: 1.080592
Train - Epoch 290, Batch: 0, Loss: 1.083214
Train - Epoch 291, Batch: 0, Loss: 1.078407
Train - Epoch 292, Batch: 0, Loss: 1.070429
Train - Epoch 293, Batch: 0, Loss: 1.080241
Train - Epoch 294, Batch: 0, Loss: 1.081561
Train - Epoch 295, Batch: 0, Loss: 1.090285
Train - Epoch 296, Batch: 0, Loss: 1.076954
Train - Epoch 297, Batch: 0, Loss: 1.057906
Train - Epoch 298, Batch: 0, Loss: 1.079302
Train - Epoch 299, Batch: 0, Loss: 1.069632
Train - Epoch 300, Batch: 0, Loss: 1.080763
Train - Epoch 301, Batch: 0, Loss: 1.100319
Train - Epoch 302, Batch: 0, Loss: 1.077762
Train - Epoch 303, Batch: 0, Loss: 1.078811
Train - Epoch 304, Batch: 0, Loss: 1.079690
Train - Epoch 305, Batch: 0, Loss: 1.075107
Train - Epoch 306, Batch: 0, Loss: 1.067602
Train - Epoch 307, Batch: 0, Loss: 1.084545
Train - Epoch 308, Batch: 0, Loss: 1.078343
Train - Epoch 309, Batch: 0, Loss: 1.087102
Train - Epoch 310, Batch: 0, Loss: 1.075901
Train - Epoch 311, Batch: 0, Loss: 1.077250
Train - Epoch 312, Batch: 0, Loss: 1.079822
Train - Epoch 313, Batch: 0, Loss: 1.073041
Train - Epoch 314, Batch: 0, Loss: 1.061638
Train - Epoch 315, Batch: 0, Loss: 1.072231
Train - Epoch 316, Batch: 0, Loss: 1.059165
Train - Epoch 317, Batch: 0, Loss: 1.079736
Train - Epoch 318, Batch: 0, Loss: 1.068711
Train - Epoch 319, Batch: 0, Loss: 1.079254
Train - Epoch 320, Batch: 0, Loss: 1.065224
Train - Epoch 321, Batch: 0, Loss: 1.079151
Train - Epoch 322, Batch: 0, Loss: 1.080222
Train - Epoch 323, Batch: 0, Loss: 1.080532
Train - Epoch 324, Batch: 0, Loss: 1.058012
Train - Epoch 325, Batch: 0, Loss: 1.073358
Train - Epoch 326, Batch: 0, Loss: 1.065016
Train - Epoch 327, Batch: 0, Loss: 1.067446
Train - Epoch 328, Batch: 0, Loss: 1.068902
Train - Epoch 329, Batch: 0, Loss: 1.060388
Train - Epoch 330, Batch: 0, Loss: 1.075814
Train - Epoch 331, Batch: 0, Loss: 1.078810
Train - Epoch 332, Batch: 0, Loss: 1.073092
Train - Epoch 333, Batch: 0, Loss: 1.072782
Train - Epoch 334, Batch: 0, Loss: 1.069957
Train - Epoch 335, Batch: 0, Loss: 1.072211
Train - Epoch 336, Batch: 0, Loss: 1.072406
Train - Epoch 337, Batch: 0, Loss: 1.073060
Train - Epoch 338, Batch: 0, Loss: 1.076771
Train - Epoch 339, Batch: 0, Loss: 1.073553
Train - Epoch 340, Batch: 0, Loss: 1.073970
Train - Epoch 341, Batch: 0, Loss: 1.063715
Train - Epoch 342, Batch: 0, Loss: 1.069568
Train - Epoch 343, Batch: 0, Loss: 1.069317
Train - Epoch 344, Batch: 0, Loss: 1.073537
Train - Epoch 345, Batch: 0, Loss: 1.066583
Train - Epoch 346, Batch: 0, Loss: 1.065994
Train - Epoch 347, Batch: 0, Loss: 1.075043
Train - Epoch 348, Batch: 0, Loss: 1.091107
Train - Epoch 349, Batch: 0, Loss: 1.080691
Train - Epoch 350, Batch: 0, Loss: 1.065756
Train - Epoch 351, Batch: 0, Loss: 1.056804
Train - Epoch 352, Batch: 0, Loss: 1.059637
Train - Epoch 353, Batch: 0, Loss: 1.053775
Train - Epoch 354, Batch: 0, Loss: 1.073539
Train - Epoch 355, Batch: 0, Loss: 1.063151
Train - Epoch 356, Batch: 0, Loss: 1.068029
Train - Epoch 357, Batch: 0, Loss: 1.057307
Train - Epoch 358, Batch: 0, Loss: 1.064655
Train - Epoch 359, Batch: 0, Loss: 1.060317
Train - Epoch 360, Batch: 0, Loss: 1.058864
Train - Epoch 361, Batch: 0, Loss: 1.065840
Train - Epoch 362, Batch: 0, Loss: 1.061121
Train - Epoch 363, Batch: 0, Loss: 1.058452
Train - Epoch 364, Batch: 0, Loss: 1.047367
Train - Epoch 365, Batch: 0, Loss: 1.065976
Train - Epoch 366, Batch: 0, Loss: 1.065445
Train - Epoch 367, Batch: 0, Loss: 1.058232
Train - Epoch 368, Batch: 0, Loss: 1.060490
Train - Epoch 369, Batch: 0, Loss: 1.069492
Train - Epoch 370, Batch: 0, Loss: 1.061072
Train - Epoch 371, Batch: 0, Loss: 1.057128
Train - Epoch 372, Batch: 0, Loss: 1.081490
Train - Epoch 373, Batch: 0, Loss: 1.059781/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 374, Batch: 0, Loss: 1.053138
Train - Epoch 375, Batch: 0, Loss: 1.056640
Train - Epoch 376, Batch: 0, Loss: 1.064083
Train - Epoch 377, Batch: 0, Loss: 1.066269
Train - Epoch 378, Batch: 0, Loss: 1.054803
Train - Epoch 379, Batch: 0, Loss: 1.063087
Train - Epoch 380, Batch: 0, Loss: 1.064760
Train - Epoch 381, Batch: 0, Loss: 1.058787
Train - Epoch 382, Batch: 0, Loss: 1.059813
Train - Epoch 383, Batch: 0, Loss: 1.054791
Train - Epoch 384, Batch: 0, Loss: 1.055179
Train - Epoch 385, Batch: 0, Loss: 1.061161
Train - Epoch 386, Batch: 0, Loss: 1.064216
Train - Epoch 387, Batch: 0, Loss: 1.064190
Train - Epoch 388, Batch: 0, Loss: 1.061998
Train - Epoch 389, Batch: 0, Loss: 1.052597
Train - Epoch 390, Batch: 0, Loss: 1.066555
Train - Epoch 391, Batch: 0, Loss: 1.048935
Train - Epoch 392, Batch: 0, Loss: 1.052154
Train - Epoch 393, Batch: 0, Loss: 1.058279
Train - Epoch 394, Batch: 0, Loss: 1.054756
Train - Epoch 395, Batch: 0, Loss: 1.040624
Train - Epoch 396, Batch: 0, Loss: 1.055547
Train - Epoch 397, Batch: 0, Loss: 1.042474
Train - Epoch 398, Batch: 0, Loss: 1.054846
Train - Epoch 399, Batch: 0, Loss: 1.061837
Train - Epoch 400, Batch: 0, Loss: 1.053353
Train - Epoch 401, Batch: 0, Loss: 1.049776
Train - Epoch 402, Batch: 0, Loss: 1.057335
Train - Epoch 403, Batch: 0, Loss: 1.053018
Train - Epoch 404, Batch: 0, Loss: 1.064206
Train - Epoch 405, Batch: 0, Loss: 1.058513
Train - Epoch 406, Batch: 0, Loss: 1.061457
Train - Epoch 407, Batch: 0, Loss: 1.064250
Train - Epoch 408, Batch: 0, Loss: 1.057684
Train - Epoch 409, Batch: 0, Loss: 1.065971
Train - Epoch 410, Batch: 0, Loss: 1.077711
Train - Epoch 411, Batch: 0, Loss: 1.050641
Train - Epoch 412, Batch: 0, Loss: 1.048793
Train - Epoch 413, Batch: 0, Loss: 1.055854
Train - Epoch 414, Batch: 0, Loss: 1.050777
Train - Epoch 415, Batch: 0, Loss: 1.058361
Train - Epoch 416, Batch: 0, Loss: 1.061533
Train - Epoch 417, Batch: 0, Loss: 1.058216
Train - Epoch 418, Batch: 0, Loss: 1.042854
Train - Epoch 419, Batch: 0, Loss: 1.047628
Train - Epoch 420, Batch: 0, Loss: 1.052952
Train - Epoch 421, Batch: 0, Loss: 1.039604
Train - Epoch 422, Batch: 0, Loss: 1.048557
Train - Epoch 423, Batch: 0, Loss: 1.077077
Train - Epoch 424, Batch: 0, Loss: 1.050629
Train - Epoch 425, Batch: 0, Loss: 1.055107
Train - Epoch 426, Batch: 0, Loss: 1.052150
Train - Epoch 427, Batch: 0, Loss: 1.050137
Train - Epoch 428, Batch: 0, Loss: 1.059150
Train - Epoch 429, Batch: 0, Loss: 1.061062
Train - Epoch 430, Batch: 0, Loss: 1.067106
Train - Epoch 431, Batch: 0, Loss: 1.042531
Train - Epoch 432, Batch: 0, Loss: 1.056277
Train - Epoch 433, Batch: 0, Loss: 1.050506
Train - Epoch 434, Batch: 0, Loss: 1.057240
Train - Epoch 435, Batch: 0, Loss: 1.035099
Train - Epoch 436, Batch: 0, Loss: 1.046797
Train - Epoch 437, Batch: 0, Loss: 1.055691
Train - Epoch 438, Batch: 0, Loss: 1.042707
Train - Epoch 439, Batch: 0, Loss: 1.057956
Train - Epoch 440, Batch: 0, Loss: 1.055606
Train - Epoch 441, Batch: 0, Loss: 1.046189
Train - Epoch 442, Batch: 0, Loss: 1.027038
Train - Epoch 443, Batch: 0, Loss: 1.053891
Train - Epoch 444, Batch: 0, Loss: 1.044127
Train - Epoch 445, Batch: 0, Loss: 1.050528
Train - Epoch 446, Batch: 0, Loss: 1.041587
Train - Epoch 447, Batch: 0, Loss: 1.033209
Train - Epoch 448, Batch: 0, Loss: 1.027066
Train - Epoch 449, Batch: 0, Loss: 1.039507
Train - Epoch 450, Batch: 0, Loss: 1.053843
Train - Epoch 451, Batch: 0, Loss: 1.039468
Train - Epoch 452, Batch: 0, Loss: 1.057445
Train - Epoch 453, Batch: 0, Loss: 1.039189
Train - Epoch 454, Batch: 0, Loss: 1.036691
Train - Epoch 455, Batch: 0, Loss: 1.045485
Train - Epoch 456, Batch: 0, Loss: 1.056324
Train - Epoch 457, Batch: 0, Loss: 1.056791
Train - Epoch 458, Batch: 0, Loss: 1.048023
Train - Epoch 459, Batch: 0, Loss: 1.042657
Train - Epoch 460, Batch: 0, Loss: 1.051631
Train - Epoch 461, Batch: 0, Loss: 1.047199
Train - Epoch 462, Batch: 0, Loss: 1.053954
Train - Epoch 463, Batch: 0, Loss: 1.040955
Train - Epoch 464, Batch: 0, Loss: 1.042572
Train - Epoch 465, Batch: 0, Loss: 1.043400
Train - Epoch 466, Batch: 0, Loss: 1.058188
Train - Epoch 467, Batch: 0, Loss: 1.065355
Train - Epoch 468, Batch: 0, Loss: 1.032572
Train - Epoch 469, Batch: 0, Loss: 1.054778
Train - Epoch 470, Batch: 0, Loss: 1.038437
Train - Epoch 471, Batch: 0, Loss: 1.045102
Train - Epoch 472, Batch: 0, Loss: 1.026043
Train - Epoch 473, Batch: 0, Loss: 1.038119
Train - Epoch 474, Batch: 0, Loss: 1.040359
Train - Epoch 475, Batch: 0, Loss: 1.052069
Train - Epoch 476, Batch: 0, Loss: 1.040444
Train - Epoch 477, Batch: 0, Loss: 1.044488
Train - Epoch 478, Batch: 0, Loss: 1.048398
Train - Epoch 479, Batch: 0, Loss: 1.060914
Train - Epoch 480, Batch: 0, Loss: 1.046577
Train - Epoch 481, Batch: 0, Loss: 1.058505
Train - Epoch 482, Batch: 0, Loss: 1.040136
Train - Epoch 483, Batch: 0, Loss: 1.041037
Train - Epoch 484, Batch: 0, Loss: 1.042167
Train - Epoch 485, Batch: 0, Loss: 1.060678
Train - Epoch 486, Batch: 0, Loss: 1.039360
Train - Epoch 487, Batch: 0, Loss: 1.041947
Train - Epoch 488, Batch: 0, Loss: 1.039236
Train - Epoch 489, Batch: 0, Loss: 1.047494
Train - Epoch 490, Batch: 0, Loss: 1.036495
Train - Epoch 491, Batch: 0, Loss: 1.046985
Train - Epoch 492, Batch: 0, Loss: 1.041138
Train - Epoch 493, Batch: 0, Loss: 1.038341
Train - Epoch 494, Batch: 0, Loss: 1.024486
Train - Epoch 495, Batch: 0, Loss: 1.035076
Train - Epoch 496, Batch: 0, Loss: 1.028706
Train - Epoch 497, Batch: 0, Loss: 1.046041
Train - Epoch 498, Batch: 0, Loss: 1.033393
Train - Epoch 499, Batch: 0, Loss: 1.050214
training_time:: 107.03342366218567
training time full:: 107.0334939956665
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632200
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 250
max_epoch:: 500
batch_size:: 10000
epoch:: 0
tensor([    0, 32774,     6, 32778, 32780,    13,    15,    18,    19, 32788,
        32790, 32793, 32794, 32796, 32798,    38,    40,    42,    44, 32814,
           47,    46, 32817, 32818, 32819,    49,    52, 32816,    57, 32825,
           60, 32829,    62,    61,    65, 32835,    69,    72,    80,    84,
        32852, 32859,    91, 32861,    95,    98, 32868,   101,   100,   102])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
epoch:: 60
epoch:: 61
epoch:: 62
epoch:: 63
epoch:: 64
epoch:: 65
epoch:: 66
epoch:: 67
epoch:: 68
epoch:: 69
epoch:: 70
epoch:: 71
epoch:: 72
epoch:: 73
epoch:: 74
epoch:: 75
epoch:: 76
epoch:: 77
epoch:: 78
epoch:: 79
epoch:: 80
epoch:: 81
epoch:: 82
epoch:: 83
epoch:: 84
epoch:: 85
epoch:: 86
epoch:: 87
epoch:: 88
epoch:: 89
epoch:: 90
epoch:: 91
epoch:: 92
epoch:: 93
epoch:: 94
epoch:: 95
epoch:: 96
epoch:: 97
epoch:: 98
epoch:: 99
epoch:: 100
epoch:: 101
epoch:: 102
epoch:: 103
epoch:: 104
epoch:: 105
epoch:: 106
epoch:: 107
epoch:: 108
epoch:: 109
epoch:: 110
epoch:: 111
epoch:: 112
epoch:: 113
epoch:: 114
epoch:: 115
epoch:: 116
epoch:: 117
epoch:: 118
epoch:: 119
epoch:: 120
epoch:: 121
epoch:: 122
epoch:: 123
epoch:: 124
epoch:: 125
epoch:: 126
epoch:: 127
epoch:: 128
epoch:: 129
epoch:: 130
epoch:: 131
epoch:: 132
epoch:: 133
epoch:: 134
epoch:: 135
epoch:: 136
epoch:: 137
epoch:: 138
epoch:: 139
epoch:: 140
epoch:: 141
epoch:: 142
epoch:: 143
epoch:: 144
epoch:: 145
epoch:: 146
epoch:: 147
epoch:: 148
epoch:: 149
epoch:: 150
epoch:: 151
epoch:: 152
epoch:: 153
epoch:: 154
epoch:: 155
epoch:: 156
epoch:: 157
epoch:: 158
epoch:: 159
epoch:: 160
epoch:: 161
epoch:: 162
epoch:: 163
epoch:: 164
epoch:: 165
epoch:: 166
epoch:: 167
epoch:: 168
epoch:: 169
epoch:: 170
epoch:: 171
epoch:: 172
epoch:: 173
epoch:: 174
epoch:: 175
epoch:: 176
epoch:: 177
epoch:: 178
epoch:: 179
epoch:: 180
epoch:: 181
epoch:: 182
epoch:: 183
epoch:: 184
epoch:: 185
epoch:: 186
epoch:: 187
epoch:: 188
epoch:: 189
epoch:: 190
epoch:: 191
epoch:: 192
epoch:: 193
epoch:: 194
epoch:: 195
epoch:: 196
epoch:: 197
epoch:: 198
epoch:: 199
epoch:: 200
epoch:: 201
epoch:: 202
epoch:: 203
epoch:: 204
epoch:: 205
epoch:: 206
epoch:: 207
epoch:: 208
epoch:: 209
epoch:: 210
epoch:: 211
epoch:: 212
epoch:: 213
epoch:: 214
epoch:: 215
epoch:: 216
epoch:: 217
epoch:: 218
epoch:: 219
epoch:: 220
epoch:: 221
epoch:: 222
epoch:: 223
epoch:: 224
epoch:: 225
epoch:: 226
epoch:: 227
epoch:: 228
epoch:: 229
epoch:: 230
epoch:: 231
epoch:: 232
epoch:: 233
epoch:: 234
epoch:: 235
epoch:: 236
epoch:: 237
epoch:: 238
epoch:: 239
epoch:: 240
epoch:: 241
epoch:: 242
epoch:: 243
epoch:: 244
epoch:: 245
epoch:: 246
epoch:: 247
epoch:: 248
epoch:: 249
epoch:: 250
epoch:: 251
epoch:: 252
epoch:: 253
epoch:: 254
epoch:: 255
epoch:: 256
epoch:: 257
epoch:: 258
epoch:: 259
epoch:: 260
epoch:: 261
epoch:: 262
epoch:: 263
epoch:: 264
epoch:: 265
epoch:: 266
epoch:: 267
epoch:: 268
epoch:: 269
epoch:: 270
epoch:: 271
epoch:: 272
epoch:: 273
epoch:: 274
epoch:: 275
epoch:: 276
epoch:: 277
epoch:: 278
epoch:: 279
epoch:: 280
epoch:: 281
epoch:: 282
epoch:: 283
epoch:: 284
epoch:: 285
epoch:: 286
epoch:: 287
epoch:: 288
epoch:: 289
epoch:: 290
epoch:: 291
epoch:: 292
epoch:: 293
epoch:: 294
epoch:: 295
epoch:: 296
epoch:: 297
epoch:: 298
epoch:: 299
epoch:: 300
epoch:: 301
epoch:: 302
epoch:: 303
epoch:: 304
epoch:: 305
epoch:: 306
epoch:: 307
epoch:: 308
epoch:: 309
epoch:: 310
epoch:: 311
epoch:: 312
epoch:: 313
epoch:: 314
epoch:: 315
epoch:: 316
epoch:: 317
epoch:: 318
epoch:: 319
epoch:: 320
epoch:: 321
epoch:: 322
epoch:: 323
epoch:: 324
epoch:: 325
epoch:: 326
epoch:: 327
epoch:: 328
epoch:: 329
epoch:: 330
epoch:: 331
epoch:: 332
epoch:: 333
epoch:: 334
epoch:: 335
epoch:: 336
epoch:: 337
epoch:: 338
epoch:: 339
epoch:: 340
epoch:: 341
epoch:: 342
epoch:: 343
epoch:: 344
epoch:: 345
epoch:: 346
epoch:: 347
epoch:: 348
epoch:: 349
epoch:: 350
epoch:: 351
epoch:: 352
epoch:: 353
epoch:: 354
epoch:: 355
epoch:: 356
epoch:: 357
epoch:: 358
epoch:: 359
epoch:: 360
epoch:: 361
epoch:: 362
epoch:: 363
epoch:: 364
epoch:: 365
epoch:: 366
epoch:: 367
epoch:: 368
epoch:: 369
epoch:: 370
epoch:: 371
epoch:: 372
epoch:: 373
epoch:: 374
epoch:: 375
epoch:: 376
epoch:: 377
epoch:: 378
epoch:: 379
epoch:: 380
epoch:: 381
epoch:: 382
epoch:: 383
epoch:: 384
epoch:: 385
epoch:: 386
epoch:: 387
epoch:: 388
epoch:: 389
epoch:: 390
epoch:: 391
epoch:: 392
epoch:: 393
epoch:: 394
epoch:: 395
epoch:: 396
epoch:: 397
epoch:: 398
epoch:: 399
epoch:: 400
epoch:: 401
epoch:: 402
epoch:: 403
epoch:: 404
epoch:: 405
epoch:: 406
epoch:: 407
epoch:: 408
epoch:: 409
epoch:: 410
epoch:: 411
epoch:: 412
epoch:: 413
epoch:: 414
epoch:: 415
epoch:: 416
epoch:: 417
epoch:: 418
epoch:: 419
epoch:: 420
epoch:: 421
epoch:: 422
epoch:: 423
epoch:: 424
epoch:: 425
epoch:: 426
epoch:: 427
epoch:: 428
epoch:: 429
epoch:: 430
epoch:: 431
epoch:: 432
epoch:: 433
epoch:: 434
epoch:: 435
epoch:: 436
epoch:: 437
epoch:: 438
epoch:: 439
epoch:: 440
epoch:: 441
epoch:: 442
epoch:: 443
epoch:: 444
epoch:: 445
epoch:: 446
epoch:: 447
epoch:: 448
epoch:: 449
epoch:: 450
epoch:: 451
epoch:: 452
epoch:: 453
epoch:: 454
epoch:: 455
epoch:: 456
epoch:: 457
epoch:: 458
epoch:: 459
epoch:: 460
epoch:: 461
epoch:: 462
epoch:: 463
epoch:: 464
epoch:: 465
epoch:: 466
epoch:: 467
epoch:: 468
epoch:: 469
epoch:: 470
epoch:: 471
epoch:: 472
epoch:: 473
epoch:: 474
epoch:: 475
epoch:: 476
epoch:: 477
epoch:: 478
epoch:: 479
epoch:: 480
epoch:: 481
epoch:: 482
epoch:: 483
epoch:: 484
epoch:: 485
epoch:: 486
epoch:: 487
epoch:: 488
epoch:: 489
epoch:: 490
epoch:: 491
epoch:: 492
epoch:: 493
epoch:: 494
epoch:: 495
epoch:: 496
epoch:: 497
epoch:: 498
epoch:: 499
training time is 80.48448872566223
overhead:: 0
overhead2:: 2.709947347640991
overhead3:: 0
time_baseline:: 80.4845278263092
curr_diff: 0 tensor(0.1354, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1354, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632600
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 4 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 250
max_epoch:: 500
overhead:: 0
overhead2:: 0.08123564720153809
overhead3:: 0.26108598709106445
overhead4:: 10.400356769561768
overhead5:: 0
memory usage:: 5625184256
time_provenance:: 18.51786470413208
curr_diff: 0 tensor(0.0332, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0332, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1608, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1608, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632600
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 4 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 250
max_epoch:: 500
overhead:: 0
overhead2:: 0.08704710006713867
overhead3:: 0.2658205032348633
overhead4:: 10.662748336791992
overhead5:: 0
memory usage:: 5628256256
time_provenance:: 18.908633708953857
curr_diff: 0 tensor(0.0332, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0332, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1608, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1608, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632600
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 4 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 250
max_epoch:: 500
overhead:: 0
overhead2:: 0.08865499496459961
overhead3:: 0.25949573516845703
overhead4:: 10.760048389434814
overhead5:: 0
memory usage:: 5652504576
time_provenance:: 18.951776266098022
curr_diff: 0 tensor(0.0332, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0332, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1608, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1608, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632600
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 4 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 250
max_epoch:: 500
overhead:: 0
overhead2:: 0.164947509765625
overhead3:: 0.4575972557067871
overhead4:: 18.354538679122925
overhead5:: 0
memory usage:: 5633232896
time_provenance:: 28.60305404663086
curr_diff: 0 tensor(0.0032, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0032, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1377, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1377, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632500
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 4 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 250
max_epoch:: 500
overhead:: 0
overhead2:: 0.17121100425720215
overhead3:: 0.4519033432006836
overhead4:: 19.110641956329346
overhead5:: 0
memory usage:: 5626060800
time_provenance:: 29.46390676498413
curr_diff: 0 tensor(0.0032, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0032, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1377, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1377, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632500
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 4 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 250
max_epoch:: 500
overhead:: 0
overhead2:: 0.16510796546936035
overhead3:: 0.4623069763183594
overhead4:: 18.853938341140747
overhead5:: 0
memory usage:: 5635457024
time_provenance:: 29.126721382141113
curr_diff: 0 tensor(0.0030, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0030, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1376, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1376, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632700
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 4 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 250
max_epoch:: 500
overhead:: 0
overhead2:: 0.4130589962005615
overhead3:: 1.0197601318359375
overhead4:: 43.62409472465515
overhead5:: 0
memory usage:: 5630636032
time_provenance:: 60.138718366622925
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1355, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1355, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632600
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 4 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 250
max_epoch:: 500
overhead:: 0
overhead2:: 0.42734646797180176
overhead3:: 1.0452723503112793
overhead4:: 45.290178537368774
overhead5:: 0
memory usage:: 5636161536
time_provenance:: 61.85823583602905
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1355, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1355, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632600
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 4 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 250
max_epoch:: 500
overhead:: 0
overhead2:: 0.41335320472717285
overhead3:: 1.0399739742279053
overhead4:: 44.72459435462952
overhead5:: 0
memory usage:: 5637296128
time_provenance:: 61.318121910095215
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1355, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1355, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632600
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 4 0.005 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 250
max_epoch:: 500
overhead:: 0
overhead2:: 0.8359153270721436
overhead3:: 2.050788640975952
overhead4:: 82.35128903388977
overhead5:: 0
memory usage:: 5624102912
time_provenance:: 107.17487168312073
curr_diff: 0 tensor(5.0051e-13, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.0051e-13, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1354, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1354, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9999, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632600
deletion rate:: 0.01
python3 generate_rand_ids 0.01  cifar10_2 0
tensor([24576, 27652,  4102,  1030,  4105, 17420, 35854, 15378, 14358, 48152,
        24601, 23578,  3098, 14364,  2077, 34846, 48157, 36896, 31776, 24608,
        37924, 35878, 10278, 12331,  1069,  8239, 27696,  6195, 10291,  9269,
        10293, 39992,  3131, 30779, 24635,    64, 44097,  3139,  6215, 23625,
        38988, 16462, 22609, 31826, 41044, 22619, 33884, 20573, 45150, 37983,
        40032, 46177, 10340, 26727, 46184,   109, 47216, 33905, 44147, 28787,
        41077,  5239, 47223, 15481, 26744, 25725, 43134, 33918, 24702, 15486,
        26756, 21639, 30856, 27788, 35980, 30861, 23694,  2191, 23699, 47253,
        40087, 19608, 37018, 43163,  5282, 16550, 17575, 10409, 26796,  2221,
        15534,  5299, 16563, 16565, 24758, 41143, 38074, 42171, 25788,  5308,
        22716, 46272, 45249,  3265, 49344, 13511, 45258, 33997, 36046, 26829,
        44240, 14547, 30932, 44245, 44249, 27866, 41178, 37084,  1242, 23775,
        39135, 34019, 32998, 23782, 10471, 14567, 14568, 49388, 34029, 43247,
        26865, 14578, 16628, 47348, 41206, 48374, 23800, 24826, 18683,  9468,
        18685, 14590, 21761, 10502, 48391,  7435, 42251, 42255, 43281, 49431,
         9496, 38168, 24859, 47387, 27933, 21790, 23836, 13600,  9504, 48418,
        28967,   296, 35113, 27947, 11565, 47413,  7478,   312,  4409, 25914,
        43320, 40254, 33087, 34112, 17729, 38212, 43333, 14666, 10573,  9549,
        37200, 45399, 45403, 29028, 45413, 13671, 45416, 31081, 12283, 45427,
         6518, 15735, 49526, 20860, 36220, 41343, 30088,  2445,  5518, 46480,
          403, 46483, 12693,  9622, 35223, 46487, 42396, 14751, 41377, 18850,
        27043, 39333, 35242, 33195, 41391, 47538, 10674, 29108, 20918, 46521,
        12732,  5569,  5573,  3531, 39371, 19921, 21973, 19930, 20955, 49629,
        25054,  9694, 28132, 36325, 27113, 39406, 20975,  2542, 27127, 16888,
         3577, 11773, 15870, 40455, 27146, 27147, 47631, 22031,  5649, 42513,
        40475,  6684, 29217, 41507, 24100,  4645, 27174, 40487, 11815, 36397,
         5677, 21039, 23087, 18996, 40501, 35387,  6717, 40512, 13895, 28233,
        34379, 45644, 45645,  1619, 18007, 44631, 35417, 11866, 48730, 37467,
        19039, 42592, 30305, 27232, 40547, 41572,  9827,  8808,  5737, 31336,
         1641, 21100, 37485, 28269,  5743, 36463, 29293, 29295, 37491, 13940,
        48758, 41591, 13942, 36473,  2682, 23165, 44670, 36478, 15997,  3713,
        34434,  2691,  3715, 38533, 26248, 32393,  7820, 48783, 49814, 37526,
         8856,  8860, 48802, 33443, 26276, 27300, 17061, 49832, 24235, 27307,
        20141, 12971, 20142, 23216, 22193, 26287, 43700, 15029, 21176, 35513,
        29371, 20156, 30397, 31421, 41671, 22217, 40649, 25294, 30414,  6866,
          723,  6868, 38610, 33495, 49883, 36571, 28386, 24290, 41702, 48872,
        25321, 22251,  9964, 14059,  4846, 46831, 44786, 39672, 43770, 23293,
        13054,   766, 29440, 35587, 47876,  1797, 35589,  4877, 22285, 13072,
         8978, 16150,  7959, 15130,  5915, 30494, 25375, 47911, 40745, 26410,
         9001, 23339, 18219, 21292, 16174, 31530, 35638, 39735, 29496, 41784,
        25404, 18242, 35651, 25411, 30535, 22347, 41804, 32591, 21336, 46937,
        10078, 38751, 39775, 17248, 10082, 39779, 29540, 47979, 31595, 28525,
        20335, 15218, 26483, 46964, 38771,  7029,  4978,   888, 27509, 36727,
        42875, 17277, 33662, 36735, 22400, 48000, 21377, 33667, 21379, 30593,
        42886, 13182, 30599, 11145, 37770,  6024, 24460, 36749, 26512, 35731,
        25492, 20373, 34707, 45975, 23443,  5019, 43936, 28578, 10146, 35751,
        28584, 39851, 25516, 23468, 45997, 28592, 27568, 29619, 32691, 38837,
        47027,  6074, 48059, 23484, 48061, 16320, 27593, 24522, 33741, 38861,
        20441, 18393, 39900, 41951, 46050, 39911, 34797, 38895, 38896, 22513,
        42995,  6132, 49140, 12278, 44023, 37880, 46073, 35829,  5115,  2044])
batch size:: 10000
repetition 0
python3 benchmark_exp_lr.py 0.001 10000 500 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05] Logistic_regression cifar10_2 0 0.0001 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 2.305855
Train - Epoch 1, Batch: 0, Loss: 2.244473
Train - Epoch 2, Batch: 0, Loss: 2.190710
Train - Epoch 3, Batch: 0, Loss: 2.140442
Train - Epoch 4, Batch: 0, Loss: 2.090859
Train - Epoch 5, Batch: 0, Loss: 2.048001
Train - Epoch 6, Batch: 0, Loss: 2.001817
Train - Epoch 7, Batch: 0, Loss: 1.966936
Train - Epoch 8, Batch: 0, Loss: 1.930789
Train - Epoch 9, Batch: 0, Loss: 1.894182
Train - Epoch 10, Batch: 0, Loss: 1.869797
Train - Epoch 11, Batch: 0, Loss: 1.838369
Train - Epoch 12, Batch: 0, Loss: 1.807731
Train - Epoch 13, Batch: 0, Loss: 1.786676
Train - Epoch 14, Batch: 0, Loss: 1.756905
Train - Epoch 15, Batch: 0, Loss: 1.737886
Train - Epoch 16, Batch: 0, Loss: 1.721420
Train - Epoch 17, Batch: 0, Loss: 1.693187
Train - Epoch 18, Batch: 0, Loss: 1.674234
Train - Epoch 19, Batch: 0, Loss: 1.662509
Train - Epoch 20, Batch: 0, Loss: 1.635914
Train - Epoch 21, Batch: 0, Loss: 1.622520
Train - Epoch 22, Batch: 0, Loss: 1.606320
Train - Epoch 23, Batch: 0, Loss: 1.606298
Train - Epoch 24, Batch: 0, Loss: 1.574428
Train - Epoch 25, Batch: 0, Loss: 1.564377
Train - Epoch 26, Batch: 0, Loss: 1.545127
Train - Epoch 27, Batch: 0, Loss: 1.547426
Train - Epoch 28, Batch: 0, Loss: 1.526450
Train - Epoch 29, Batch: 0, Loss: 1.513152
Train - Epoch 30, Batch: 0, Loss: 1.505151
Train - Epoch 31, Batch: 0, Loss: 1.500718
Train - Epoch 32, Batch: 0, Loss: 1.476037
Train - Epoch 33, Batch: 0, Loss: 1.474441
Train - Epoch 34, Batch: 0, Loss: 1.476938
Train - Epoch 35, Batch: 0, Loss: 1.463640
Train - Epoch 36, Batch: 0, Loss: 1.446761
Train - Epoch 37, Batch: 0, Loss: 1.451540
Train - Epoch 38, Batch: 0, Loss: 1.435114
Train - Epoch 39, Batch: 0, Loss: 1.430834
Train - Epoch 40, Batch: 0, Loss: 1.430296
Train - Epoch 41, Batch: 0, Loss: 1.412932
Train - Epoch 42, Batch: 0, Loss: 1.406293
Train - Epoch 43, Batch: 0, Loss: 1.403933
Train - Epoch 44, Batch: 0, Loss: 1.399244
Train - Epoch 45, Batch: 0, Loss: 1.380463
Train - Epoch 46, Batch: 0, Loss: 1.381720
Train - Epoch 47, Batch: 0, Loss: 1.371158
Train - Epoch 48, Batch: 0, Loss: 1.373233
Train - Epoch 49, Batch: 0, Loss: 1.366598
Train - Epoch 50, Batch: 0, Loss: 1.361179
Train - Epoch 51, Batch: 0, Loss: 1.360738
Train - Epoch 52, Batch: 0, Loss: 1.361834
Train - Epoch 53, Batch: 0, Loss: 1.342025
Train - Epoch 54, Batch: 0, Loss: 1.336707
Train - Epoch 55, Batch: 0, Loss: 1.334535
Train - Epoch 56, Batch: 0, Loss: 1.339772
Train - Epoch 57, Batch: 0, Loss: 1.326183
Train - Epoch 58, Batch: 0, Loss: 1.317439
Train - Epoch 59, Batch: 0, Loss: 1.306557
Train - Epoch 60, Batch: 0, Loss: 1.320843
Train - Epoch 61, Batch: 0, Loss: 1.303875
Train - Epoch 62, Batch: 0, Loss: 1.311627
Train - Epoch 63, Batch: 0, Loss: 1.300350
Train - Epoch 64, Batch: 0, Loss: 1.297450
Train - Epoch 65, Batch: 0, Loss: 1.298929
Train - Epoch 66, Batch: 0, Loss: 1.296775
Train - Epoch 67, Batch: 0, Loss: 1.298508
Train - Epoch 68, Batch: 0, Loss: 1.300349
Train - Epoch 69, Batch: 0, Loss: 1.291473
Train - Epoch 70, Batch: 0, Loss: 1.283010
Train - Epoch 71, Batch: 0, Loss: 1.286201
Train - Epoch 72, Batch: 0, Loss: 1.278368
Train - Epoch 73, Batch: 0, Loss: 1.280106
Train - Epoch 74, Batch: 0, Loss: 1.280259
Train - Epoch 75, Batch: 0, Loss: 1.270801
Train - Epoch 76, Batch: 0, Loss: 1.265365
Train - Epoch 77, Batch: 0, Loss: 1.271080
Train - Epoch 78, Batch: 0, Loss: 1.258066
Train - Epoch 79, Batch: 0, Loss: 1.270965
Train - Epoch 80, Batch: 0, Loss: 1.260039
Train - Epoch 81, Batch: 0, Loss: 1.248775
Train - Epoch 82, Batch: 0, Loss: 1.242480
Train - Epoch 83, Batch: 0, Loss: 1.256369
Train - Epoch 84, Batch: 0, Loss: 1.257480
Train - Epoch 85, Batch: 0, Loss: 1.241140
Train - Epoch 86, Batch: 0, Loss: 1.244938
Train - Epoch 87, Batch: 0, Loss: 1.238509
Train - Epoch 88, Batch: 0, Loss: 1.235351
Train - Epoch 89, Batch: 0, Loss: 1.231996
Train - Epoch 90, Batch: 0, Loss: 1.227493
Train - Epoch 91, Batch: 0, Loss: 1.230184
Train - Epoch 92, Batch: 0, Loss: 1.220501
Train - Epoch 93, Batch: 0, Loss: 1.222584
Train - Epoch 94, Batch: 0, Loss: 1.230445
Train - Epoch 95, Batch: 0, Loss: 1.223058
Train - Epoch 96, Batch: 0, Loss: 1.218867
Train - Epoch 97, Batch: 0, Loss: 1.221047
Train - Epoch 98, Batch: 0, Loss: 1.218371
Train - Epoch 99, Batch: 0, Loss: 1.219458
Train - Epoch 100, Batch: 0, Loss: 1.224486
Train - Epoch 101, Batch: 0, Loss: 1.216495
Train - Epoch 102, Batch: 0, Loss: 1.223072
Train - Epoch 103, Batch: 0, Loss: 1.203406
Train - Epoch 104, Batch: 0, Loss: 1.206869
Train - Epoch 105, Batch: 0, Loss: 1.208257
Train - Epoch 106, Batch: 0, Loss: 1.205926
Train - Epoch 107, Batch: 0, Loss: 1.198253
Train - Epoch 108, Batch: 0, Loss: 1.195226
Train - Epoch 109, Batch: 0, Loss: 1.208490
Train - Epoch 110, Batch: 0, Loss: 1.207066
Train - Epoch 111, Batch: 0, Loss: 1.200252
Train - Epoch 112, Batch: 0, Loss: 1.187072
Train - Epoch 113, Batch: 0, Loss: 1.182415
Train - Epoch 114, Batch: 0, Loss: 1.192065
Train - Epoch 115, Batch: 0, Loss: 1.200667
Train - Epoch 116, Batch: 0, Loss: 1.187782
Train - Epoch 117, Batch: 0, Loss: 1.188958
Train - Epoch 118, Batch: 0, Loss: 1.173692
Train - Epoch 119, Batch: 0, Loss: 1.179464
Train - Epoch 120, Batch: 0, Loss: 1.184037
Train - Epoch 121, Batch: 0, Loss: 1.162479
Train - Epoch 122, Batch: 0, Loss: 1.183094
Train - Epoch 123, Batch: 0, Loss: 1.181039
Train - Epoch 124, Batch: 0, Loss: 1.187604
Train - Epoch 125, Batch: 0, Loss: 1.178410
Train - Epoch 126, Batch: 0, Loss: 1.184898
Train - Epoch 127, Batch: 0, Loss: 1.190505
Train - Epoch 128, Batch: 0, Loss: 1.176675
Train - Epoch 129, Batch: 0, Loss: 1.186470
Train - Epoch 130, Batch: 0, Loss: 1.179185
Train - Epoch 131, Batch: 0, Loss: 1.176277
Train - Epoch 132, Batch: 0, Loss: 1.166051
Train - Epoch 133, Batch: 0, Loss: 1.168692
Train - Epoch 134, Batch: 0, Loss: 1.174824
Train - Epoch 135, Batch: 0, Loss: 1.180861
Train - Epoch 136, Batch: 0, Loss: 1.170262
Train - Epoch 137, Batch: 0, Loss: 1.168528
Train - Epoch 138, Batch: 0, Loss: 1.162417
Train - Epoch 139, Batch: 0, Loss: 1.148601
Train - Epoch 140, Batch: 0, Loss: 1.164450
Train - Epoch 141, Batch: 0, Loss: 1.169014
Train - Epoch 142, Batch: 0, Loss: 1.145160
Train - Epoch 143, Batch: 0, Loss: 1.157411
Train - Epoch 144, Batch: 0, Loss: 1.169448
Train - Epoch 145, Batch: 0, Loss: 1.161636
Train - Epoch 146, Batch: 0, Loss: 1.161825
Train - Epoch 147, Batch: 0, Loss: 1.143781
Train - Epoch 148, Batch: 0, Loss: 1.153429
Train - Epoch 149, Batch: 0, Loss: 1.142673
Train - Epoch 150, Batch: 0, Loss: 1.156889
Train - Epoch 151, Batch: 0, Loss: 1.157113
Train - Epoch 152, Batch: 0, Loss: 1.159523
Train - Epoch 153, Batch: 0, Loss: 1.158246
Train - Epoch 154, Batch: 0, Loss: 1.155054
Train - Epoch 155, Batch: 0, Loss: 1.146004
Train - Epoch 156, Batch: 0, Loss: 1.148589
Train - Epoch 157, Batch: 0, Loss: 1.146018
Train - Epoch 158, Batch: 0, Loss: 1.151950
Train - Epoch 159, Batch: 0, Loss: 1.159386
Train - Epoch 160, Batch: 0, Loss: 1.147592
Train - Epoch 161, Batch: 0, Loss: 1.125971
Train - Epoch 162, Batch: 0, Loss: 1.149510
Train - Epoch 163, Batch: 0, Loss: 1.143028
Train - Epoch 164, Batch: 0, Loss: 1.154487
Train - Epoch 165, Batch: 0, Loss: 1.133703
Train - Epoch 166, Batch: 0, Loss: 1.145122
Train - Epoch 167, Batch: 0, Loss: 1.140454
Train - Epoch 168, Batch: 0, Loss: 1.125014
Train - Epoch 169, Batch: 0, Loss: 1.133412
Train - Epoch 170, Batch: 0, Loss: 1.141004
Train - Epoch 171, Batch: 0, Loss: 1.141410
Train - Epoch 172, Batch: 0, Loss: 1.137119
Train - Epoch 173, Batch: 0, Loss: 1.134098
Train - Epoch 174, Batch: 0, Loss: 1.134669
Train - Epoch 175, Batch: 0, Loss: 1.129889
Train - Epoch 176, Batch: 0, Loss: 1.139657
Train - Epoch 177, Batch: 0, Loss: 1.131296
Train - Epoch 178, Batch: 0, Loss: 1.128862
Train - Epoch 179, Batch: 0, Loss: 1.131232
Train - Epoch 180, Batch: 0, Loss: 1.133800
Train - Epoch 181, Batch: 0, Loss: 1.142241
Train - Epoch 182, Batch: 0, Loss: 1.131183
Train - Epoch 183, Batch: 0, Loss: 1.126142
Train - Epoch 184, Batch: 0, Loss: 1.103235
Train - Epoch 185, Batch: 0, Loss: 1.129839
Train - Epoch 186, Batch: 0, Loss: 1.142995
Train - Epoch 187, Batch: 0, Loss: 1.124043
Train - Epoch 188, Batch: 0, Loss: 1.127485
Train - Epoch 189, Batch: 0, Loss: 1.148225
Train - Epoch 190, Batch: 0, Loss: 1.130792
Train - Epoch 191, Batch: 0, Loss: 1.127499
Train - Epoch 192, Batch: 0, Loss: 1.117713
Train - Epoch 193, Batch: 0, Loss: 1.131256
Train - Epoch 194, Batch: 0, Loss: 1.114338
Train - Epoch 195, Batch: 0, Loss: 1.115890
Train - Epoch 196, Batch: 0, Loss: 1.118206
Train - Epoch 197, Batch: 0, Loss: 1.106314
Train - Epoch 198, Batch: 0, Loss: 1.122897
Train - Epoch 199, Batch: 0, Loss: 1.117889
Train - Epoch 200, Batch: 0, Loss: 1.126811
Train - Epoch 201, Batch: 0, Loss: 1.131288
Train - Epoch 202, Batch: 0, Loss: 1.125034
Train - Epoch 203, Batch: 0, Loss: 1.116012
Train - Epoch 204, Batch: 0, Loss: 1.132937
Train - Epoch 205, Batch: 0, Loss: 1.116080
Train - Epoch 206, Batch: 0, Loss: 1.108658
Train - Epoch 207, Batch: 0, Loss: 1.121688
Train - Epoch 208, Batch: 0, Loss: 1.109458
Train - Epoch 209, Batch: 0, Loss: 1.102504
Train - Epoch 210, Batch: 0, Loss: 1.109863
Train - Epoch 211, Batch: 0, Loss: 1.121520
Train - Epoch 212, Batch: 0, Loss: 1.103244
Train - Epoch 213, Batch: 0, Loss: 1.090746
Train - Epoch 214, Batch: 0, Loss: 1.109442
Train - Epoch 215, Batch: 0, Loss: 1.107546
Train - Epoch 216, Batch: 0, Loss: 1.109864
Train - Epoch 217, Batch: 0, Loss: 1.123763
Train - Epoch 218, Batch: 0, Loss: 1.108577
Train - Epoch 219, Batch: 0, Loss: 1.098541
Train - Epoch 220, Batch: 0, Loss: 1.114961
Train - Epoch 221, Batch: 0, Loss: 1.107253
Train - Epoch 222, Batch: 0, Loss: 1.132951
Train - Epoch 223, Batch: 0, Loss: 1.106659
Train - Epoch 224, Batch: 0, Loss: 1.109045
Train - Epoch 225, Batch: 0, Loss: 1.108912
Train - Epoch 226, Batch: 0, Loss: 1.102228
Train - Epoch 227, Batch: 0, Loss: 1.092223
Train - Epoch 228, Batch: 0, Loss: 1.113949
Train - Epoch 229, Batch: 0, Loss: 1.084255
Train - Epoch 230, Batch: 0, Loss: 1.107059
Train - Epoch 231, Batch: 0, Loss: 1.099432
Train - Epoch 232, Batch: 0, Loss: 1.114434
Train - Epoch 233, Batch: 0, Loss: 1.099928
Train - Epoch 234, Batch: 0, Loss: 1.108482
Train - Epoch 235, Batch: 0, Loss: 1.103703
Train - Epoch 236, Batch: 0, Loss: 1.112049
Train - Epoch 237, Batch: 0, Loss: 1.095268
Train - Epoch 238, Batch: 0, Loss: 1.107917
Train - Epoch 239, Batch: 0, Loss: 1.090064
Train - Epoch 240, Batch: 0, Loss: 1.107324
Train - Epoch 241, Batch: 0, Loss: 1.099601
Train - Epoch 242, Batch: 0, Loss: 1.101022
Train - Epoch 243, Batch: 0, Loss: 1.089057
Train - Epoch 244, Batch: 0, Loss: 1.099699
Train - Epoch 245, Batch: 0, Loss: 1.093032
Train - Epoch 246, Batch: 0, Loss: 1.096723
Train - Epoch 247, Batch: 0, Loss: 1.091816
Train - Epoch 248, Batch: 0, Loss: 1.085417
Train - Epoch 249, Batch: 0, Loss: 1.093982
Train - Epoch 250, Batch: 0, Loss: 1.091506
Train - Epoch 251, Batch: 0, Loss: 1.083442
Train - Epoch 252, Batch: 0, Loss: 1.092661
Train - Epoch 253, Batch: 0, Loss: 1.083501
Train - Epoch 254, Batch: 0, Loss: 1.083764
Train - Epoch 255, Batch: 0, Loss: 1.102188
Train - Epoch 256, Batch: 0, Loss: 1.084049
Train - Epoch 257, Batch: 0, Loss: 1.098033
Train - Epoch 258, Batch: 0, Loss: 1.082603
Train - Epoch 259, Batch: 0, Loss: 1.092792
Train - Epoch 260, Batch: 0, Loss: 1.095644
Train - Epoch 261, Batch: 0, Loss: 1.088550
Train - Epoch 262, Batch: 0, Loss: 1.098585
Train - Epoch 263, Batch: 0, Loss: 1.107603
Train - Epoch 264, Batch: 0, Loss: 1.078723
Train - Epoch 265, Batch: 0, Loss: 1.091174
Train - Epoch 266, Batch: 0, Loss: 1.080192
Train - Epoch 267, Batch: 0, Loss: 1.102751
Train - Epoch 268, Batch: 0, Loss: 1.085796
Train - Epoch 269, Batch: 0, Loss: 1.080587
Train - Epoch 270, Batch: 0, Loss: 1.096761
Train - Epoch 271, Batch: 0, Loss: 1.086134
Train - Epoch 272, Batch: 0, Loss: 1.090230
Train - Epoch 273, Batch: 0, Loss: 1.083620
Train - Epoch 274, Batch: 0, Loss: 1.086088
Train - Epoch 275, Batch: 0, Loss: 1.083173
Train - Epoch 276, Batch: 0, Loss: 1.089655
Train - Epoch 277, Batch: 0, Loss: 1.095010
Train - Epoch 278, Batch: 0, Loss: 1.087949
Train - Epoch 279, Batch: 0, Loss: 1.090143
Train - Epoch 280, Batch: 0, Loss: 1.110564
Train - Epoch 281, Batch: 0, Loss: 1.085866
Train - Epoch 282, Batch: 0, Loss: 1.075526
Train - Epoch 283, Batch: 0, Loss: 1.084045
Train - Epoch 284, Batch: 0, Loss: 1.086705
Train - Epoch 285, Batch: 0, Loss: 1.080664
Train - Epoch 286, Batch: 0, Loss: 1.084439
Train - Epoch 287, Batch: 0, Loss: 1.091380
Train - Epoch 288, Batch: 0, Loss: 1.071933
Train - Epoch 289, Batch: 0, Loss: 1.069336
Train - Epoch 290, Batch: 0, Loss: 1.089106
Train - Epoch 291, Batch: 0, Loss: 1.076031
Train - Epoch 292, Batch: 0, Loss: 1.084352
Train - Epoch 293, Batch: 0, Loss: 1.105548
Train - Epoch 294, Batch: 0, Loss: 1.094071
Train - Epoch 295, Batch: 0, Loss: 1.094215
Train - Epoch 296, Batch: 0, Loss: 1.078272
Train - Epoch 297, Batch: 0, Loss: 1.098601
Train - Epoch 298, Batch: 0, Loss: 1.073629
Train - Epoch 299, Batch: 0, Loss: 1.075102
Train - Epoch 300, Batch: 0, Loss: 1.081752
Train - Epoch 301, Batch: 0, Loss: 1.077716
Train - Epoch 302, Batch: 0, Loss: 1.081456
Train - Epoch 303, Batch: 0, Loss: 1.079133
Train - Epoch 304, Batch: 0, Loss: 1.085631
Train - Epoch 305, Batch: 0, Loss: 1.071516
Train - Epoch 306, Batch: 0, Loss: 1.081715
Train - Epoch 307, Batch: 0, Loss: 1.068169
Train - Epoch 308, Batch: 0, Loss: 1.077371
Train - Epoch 309, Batch: 0, Loss: 1.084752
Train - Epoch 310, Batch: 0, Loss: 1.068713
Train - Epoch 311, Batch: 0, Loss: 1.077595
Train - Epoch 312, Batch: 0, Loss: 1.070009
Train - Epoch 313, Batch: 0, Loss: 1.078050
Train - Epoch 314, Batch: 0, Loss: 1.085031
Train - Epoch 315, Batch: 0, Loss: 1.059949
Train - Epoch 316, Batch: 0, Loss: 1.067020
Train - Epoch 317, Batch: 0, Loss: 1.080175
Train - Epoch 318, Batch: 0, Loss: 1.084133
Train - Epoch 319, Batch: 0, Loss: 1.078916
Train - Epoch 320, Batch: 0, Loss: 1.075588
Train - Epoch 321, Batch: 0, Loss: 1.077281
Train - Epoch 322, Batch: 0, Loss: 1.060942
Train - Epoch 323, Batch: 0, Loss: 1.051027
Train - Epoch 324, Batch: 0, Loss: 1.059406
Train - Epoch 325, Batch: 0, Loss: 1.060069
Train - Epoch 326, Batch: 0, Loss: 1.056586
Train - Epoch 327, Batch: 0, Loss: 1.068192
Train - Epoch 328, Batch: 0, Loss: 1.068263
Train - Epoch 329, Batch: 0, Loss: 1.078392
Train - Epoch 330, Batch: 0, Loss: 1.087194
Train - Epoch 331, Batch: 0, Loss: 1.056151
Train - Epoch 332, Batch: 0, Loss: 1.075077
Train - Epoch 333, Batch: 0, Loss: 1.082351
Train - Epoch 334, Batch: 0, Loss: 1.055387
Train - Epoch 335, Batch: 0, Loss: 1.064561
Train - Epoch 336, Batch: 0, Loss: 1.079516
Train - Epoch 337, Batch: 0, Loss: 1.058660
Train - Epoch 338, Batch: 0, Loss: 1.073649
Train - Epoch 339, Batch: 0, Loss: 1.063734
Train - Epoch 340, Batch: 0, Loss: 1.063612
Train - Epoch 341, Batch: 0, Loss: 1.057751
Train - Epoch 342, Batch: 0, Loss: 1.063736
Train - Epoch 343, Batch: 0, Loss: 1.058939
Train - Epoch 344, Batch: 0, Loss: 1.071215
Train - Epoch 345, Batch: 0, Loss: 1.058098
Train - Epoch 346, Batch: 0, Loss: 1.067304
Train - Epoch 347, Batch: 0, Loss: 1.062896
Train - Epoch 348, Batch: 0, Loss: 1.062089
Train - Epoch 349, Batch: 0, Loss: 1.049992
Train - Epoch 350, Batch: 0, Loss: 1.067126
Train - Epoch 351, Batch: 0, Loss: 1.068408
Train - Epoch 352, Batch: 0, Loss: 1.070395
Train - Epoch 353, Batch: 0, Loss: 1.066698
Train - Epoch 354, Batch: 0, Loss: 1.070723
Train - Epoch 355, Batch: 0, Loss: 1.075931
Train - Epoch 356, Batch: 0, Loss: 1.074043
Train - Epoch 357, Batch: 0, Loss: 1.060784
Train - Epoch 358, Batch: 0, Loss: 1.048308
Train - Epoch 359, Batch: 0, Loss: 1.060826
Train - Epoch 360, Batch: 0, Loss: 1.070943
Train - Epoch 361, Batch: 0, Loss: 1.046084
Train - Epoch 362, Batch: 0, Loss: 1.062471
Train - Epoch 363, Batch: 0, Loss: 1.055421
Train - Epoch 364, Batch: 0, Loss: 1.055749
Train - Epoch 365, Batch: 0, Loss: 1.054183
Train - Epoch 366, Batch: 0, Loss: 1.045870
Train - Epoch 367, Batch: 0, Loss: 1.064159
Train - Epoch 368, Batch: 0, Loss: 1.046806
Train - Epoch 369, Batch: 0, Loss: 1.071460
Train - Epoch 370, Batch: 0, Loss: 1.060201
Train - Epoch 371, Batch: 0, Loss: 1.062766
Train - Epoch 372, Batch: 0, Loss: 1.055486
Train - Epoch 373, Batch: 0, Loss: 1.051030/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 374, Batch: 0, Loss: 1.046212
Train - Epoch 375, Batch: 0, Loss: 1.053771
Train - Epoch 376, Batch: 0, Loss: 1.051809
Train - Epoch 377, Batch: 0, Loss: 1.060040
Train - Epoch 378, Batch: 0, Loss: 1.060203
Train - Epoch 379, Batch: 0, Loss: 1.041049
Train - Epoch 380, Batch: 0, Loss: 1.051877
Train - Epoch 381, Batch: 0, Loss: 1.054565
Train - Epoch 382, Batch: 0, Loss: 1.054371
Train - Epoch 383, Batch: 0, Loss: 1.064826
Train - Epoch 384, Batch: 0, Loss: 1.049227
Train - Epoch 385, Batch: 0, Loss: 1.052470
Train - Epoch 386, Batch: 0, Loss: 1.068763
Train - Epoch 387, Batch: 0, Loss: 1.033339
Train - Epoch 388, Batch: 0, Loss: 1.052964
Train - Epoch 389, Batch: 0, Loss: 1.061393
Train - Epoch 390, Batch: 0, Loss: 1.057858
Train - Epoch 391, Batch: 0, Loss: 1.048550
Train - Epoch 392, Batch: 0, Loss: 1.050240
Train - Epoch 393, Batch: 0, Loss: 1.072192
Train - Epoch 394, Batch: 0, Loss: 1.065429
Train - Epoch 395, Batch: 0, Loss: 1.059111
Train - Epoch 396, Batch: 0, Loss: 1.051732
Train - Epoch 397, Batch: 0, Loss: 1.067698
Train - Epoch 398, Batch: 0, Loss: 1.063500
Train - Epoch 399, Batch: 0, Loss: 1.061807
Train - Epoch 400, Batch: 0, Loss: 1.050522
Train - Epoch 401, Batch: 0, Loss: 1.052087
Train - Epoch 402, Batch: 0, Loss: 1.065605
Train - Epoch 403, Batch: 0, Loss: 1.055931
Train - Epoch 404, Batch: 0, Loss: 1.047251
Train - Epoch 405, Batch: 0, Loss: 1.043763
Train - Epoch 406, Batch: 0, Loss: 1.056816
Train - Epoch 407, Batch: 0, Loss: 1.059140
Train - Epoch 408, Batch: 0, Loss: 1.038849
Train - Epoch 409, Batch: 0, Loss: 1.056001
Train - Epoch 410, Batch: 0, Loss: 1.066324
Train - Epoch 411, Batch: 0, Loss: 1.050510
Train - Epoch 412, Batch: 0, Loss: 1.038843
Train - Epoch 413, Batch: 0, Loss: 1.036106
Train - Epoch 414, Batch: 0, Loss: 1.048075
Train - Epoch 415, Batch: 0, Loss: 1.052258
Train - Epoch 416, Batch: 0, Loss: 1.054219
Train - Epoch 417, Batch: 0, Loss: 1.059383
Train - Epoch 418, Batch: 0, Loss: 1.064725
Train - Epoch 419, Batch: 0, Loss: 1.055907
Train - Epoch 420, Batch: 0, Loss: 1.060435
Train - Epoch 421, Batch: 0, Loss: 1.054534
Train - Epoch 422, Batch: 0, Loss: 1.058668
Train - Epoch 423, Batch: 0, Loss: 1.054337
Train - Epoch 424, Batch: 0, Loss: 1.051206
Train - Epoch 425, Batch: 0, Loss: 1.057456
Train - Epoch 426, Batch: 0, Loss: 1.037313
Train - Epoch 427, Batch: 0, Loss: 1.036077
Train - Epoch 428, Batch: 0, Loss: 1.061724
Train - Epoch 429, Batch: 0, Loss: 1.060673
Train - Epoch 430, Batch: 0, Loss: 1.048608
Train - Epoch 431, Batch: 0, Loss: 1.052030
Train - Epoch 432, Batch: 0, Loss: 1.046476
Train - Epoch 433, Batch: 0, Loss: 1.040565
Train - Epoch 434, Batch: 0, Loss: 1.038390
Train - Epoch 435, Batch: 0, Loss: 1.046787
Train - Epoch 436, Batch: 0, Loss: 1.035620
Train - Epoch 437, Batch: 0, Loss: 1.051730
Train - Epoch 438, Batch: 0, Loss: 1.054752
Train - Epoch 439, Batch: 0, Loss: 1.055804
Train - Epoch 440, Batch: 0, Loss: 1.052024
Train - Epoch 441, Batch: 0, Loss: 1.052625
Train - Epoch 442, Batch: 0, Loss: 1.047052
Train - Epoch 443, Batch: 0, Loss: 1.047150
Train - Epoch 444, Batch: 0, Loss: 1.033300
Train - Epoch 445, Batch: 0, Loss: 1.030094
Train - Epoch 446, Batch: 0, Loss: 1.038546
Train - Epoch 447, Batch: 0, Loss: 1.039673
Train - Epoch 448, Batch: 0, Loss: 1.040885
Train - Epoch 449, Batch: 0, Loss: 1.047638
Train - Epoch 450, Batch: 0, Loss: 1.055899
Train - Epoch 451, Batch: 0, Loss: 1.033139
Train - Epoch 452, Batch: 0, Loss: 1.053270
Train - Epoch 453, Batch: 0, Loss: 1.044826
Train - Epoch 454, Batch: 0, Loss: 1.040530
Train - Epoch 455, Batch: 0, Loss: 1.046455
Train - Epoch 456, Batch: 0, Loss: 1.044408
Train - Epoch 457, Batch: 0, Loss: 1.039041
Train - Epoch 458, Batch: 0, Loss: 1.062584
Train - Epoch 459, Batch: 0, Loss: 1.052152
Train - Epoch 460, Batch: 0, Loss: 1.055894
Train - Epoch 461, Batch: 0, Loss: 1.040303
Train - Epoch 462, Batch: 0, Loss: 1.033585
Train - Epoch 463, Batch: 0, Loss: 1.053586
Train - Epoch 464, Batch: 0, Loss: 1.052102
Train - Epoch 465, Batch: 0, Loss: 1.044327
Train - Epoch 466, Batch: 0, Loss: 1.038205
Train - Epoch 467, Batch: 0, Loss: 1.050307
Train - Epoch 468, Batch: 0, Loss: 1.048811
Train - Epoch 469, Batch: 0, Loss: 1.049204
Train - Epoch 470, Batch: 0, Loss: 1.058539
Train - Epoch 471, Batch: 0, Loss: 1.041910
Train - Epoch 472, Batch: 0, Loss: 1.048782
Train - Epoch 473, Batch: 0, Loss: 1.046301
Train - Epoch 474, Batch: 0, Loss: 1.032317
Train - Epoch 475, Batch: 0, Loss: 1.047949
Train - Epoch 476, Batch: 0, Loss: 1.036182
Train - Epoch 477, Batch: 0, Loss: 1.044119
Train - Epoch 478, Batch: 0, Loss: 1.032828
Train - Epoch 479, Batch: 0, Loss: 1.048507
Train - Epoch 480, Batch: 0, Loss: 1.045776
Train - Epoch 481, Batch: 0, Loss: 1.051049
Train - Epoch 482, Batch: 0, Loss: 1.049409
Train - Epoch 483, Batch: 0, Loss: 1.033462
Train - Epoch 484, Batch: 0, Loss: 1.050438
Train - Epoch 485, Batch: 0, Loss: 1.035796
Train - Epoch 486, Batch: 0, Loss: 1.046430
Train - Epoch 487, Batch: 0, Loss: 1.025409
Train - Epoch 488, Batch: 0, Loss: 1.036211
Train - Epoch 489, Batch: 0, Loss: 1.052941
Train - Epoch 490, Batch: 0, Loss: 1.056764
Train - Epoch 491, Batch: 0, Loss: 1.018039
Train - Epoch 492, Batch: 0, Loss: 1.052938
Train - Epoch 493, Batch: 0, Loss: 1.055098
Train - Epoch 494, Batch: 0, Loss: 1.037412
Train - Epoch 495, Batch: 0, Loss: 1.046209
Train - Epoch 496, Batch: 0, Loss: 1.056001
Train - Epoch 497, Batch: 0, Loss: 1.029124
Train - Epoch 498, Batch: 0, Loss: 1.020743
Train - Epoch 499, Batch: 0, Loss: 1.037144
training_time:: 108.47036385536194
training time full:: 108.47044587135315
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632300
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 500
max_epoch:: 500
batch_size:: 10000
epoch:: 0
tensor([    3, 32773,     7,    13,    17,    22,    24,    25,    32, 32801,
        32802,    35, 32803,    36, 32806, 32810, 32812, 32815,    49,    53,
           54, 32823,    61,    67, 32835,    68,    71, 32840, 32841, 32842,
        32843, 32848,    81, 32850, 32852,    85,    86,    91, 32860, 32862,
           95,    98,    99,   100,   102,   104,   105,   107, 32876,   117])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
epoch:: 60
epoch:: 61
epoch:: 62
epoch:: 63
epoch:: 64
epoch:: 65
epoch:: 66
epoch:: 67
epoch:: 68
epoch:: 69
epoch:: 70
epoch:: 71
epoch:: 72
epoch:: 73
epoch:: 74
epoch:: 75
epoch:: 76
epoch:: 77
epoch:: 78
epoch:: 79
epoch:: 80
epoch:: 81
epoch:: 82
epoch:: 83
epoch:: 84
epoch:: 85
epoch:: 86
epoch:: 87
epoch:: 88
epoch:: 89
epoch:: 90
epoch:: 91
epoch:: 92
epoch:: 93
epoch:: 94
epoch:: 95
epoch:: 96
epoch:: 97
epoch:: 98
epoch:: 99
epoch:: 100
epoch:: 101
epoch:: 102
epoch:: 103
epoch:: 104
epoch:: 105
epoch:: 106
epoch:: 107
epoch:: 108
epoch:: 109
epoch:: 110
epoch:: 111
epoch:: 112
epoch:: 113
epoch:: 114
epoch:: 115
epoch:: 116
epoch:: 117
epoch:: 118
epoch:: 119
epoch:: 120
epoch:: 121
epoch:: 122
epoch:: 123
epoch:: 124
epoch:: 125
epoch:: 126
epoch:: 127
epoch:: 128
epoch:: 129
epoch:: 130
epoch:: 131
epoch:: 132
epoch:: 133
epoch:: 134
epoch:: 135
epoch:: 136
epoch:: 137
epoch:: 138
epoch:: 139
epoch:: 140
epoch:: 141
epoch:: 142
epoch:: 143
epoch:: 144
epoch:: 145
epoch:: 146
epoch:: 147
epoch:: 148
epoch:: 149
epoch:: 150
epoch:: 151
epoch:: 152
epoch:: 153
epoch:: 154
epoch:: 155
epoch:: 156
epoch:: 157
epoch:: 158
epoch:: 159
epoch:: 160
epoch:: 161
epoch:: 162
epoch:: 163
epoch:: 164
epoch:: 165
epoch:: 166
epoch:: 167
epoch:: 168
epoch:: 169
epoch:: 170
epoch:: 171
epoch:: 172
epoch:: 173
epoch:: 174
epoch:: 175
epoch:: 176
epoch:: 177
epoch:: 178
epoch:: 179
epoch:: 180
epoch:: 181
epoch:: 182
epoch:: 183
epoch:: 184
epoch:: 185
epoch:: 186
epoch:: 187
epoch:: 188
epoch:: 189
epoch:: 190
epoch:: 191
epoch:: 192
epoch:: 193
epoch:: 194
epoch:: 195
epoch:: 196
epoch:: 197
epoch:: 198
epoch:: 199
epoch:: 200
epoch:: 201
epoch:: 202
epoch:: 203
epoch:: 204
epoch:: 205
epoch:: 206
epoch:: 207
epoch:: 208
epoch:: 209
epoch:: 210
epoch:: 211
epoch:: 212
epoch:: 213
epoch:: 214
epoch:: 215
epoch:: 216
epoch:: 217
epoch:: 218
epoch:: 219
epoch:: 220
epoch:: 221
epoch:: 222
epoch:: 223
epoch:: 224
epoch:: 225
epoch:: 226
epoch:: 227
epoch:: 228
epoch:: 229
epoch:: 230
epoch:: 231
epoch:: 232
epoch:: 233
epoch:: 234
epoch:: 235
epoch:: 236
epoch:: 237
epoch:: 238
epoch:: 239
epoch:: 240
epoch:: 241
epoch:: 242
epoch:: 243
epoch:: 244
epoch:: 245
epoch:: 246
epoch:: 247
epoch:: 248
epoch:: 249
epoch:: 250
epoch:: 251
epoch:: 252
epoch:: 253
epoch:: 254
epoch:: 255
epoch:: 256
epoch:: 257
epoch:: 258
epoch:: 259
epoch:: 260
epoch:: 261
epoch:: 262
epoch:: 263
epoch:: 264
epoch:: 265
epoch:: 266
epoch:: 267
epoch:: 268
epoch:: 269
epoch:: 270
epoch:: 271
epoch:: 272
epoch:: 273
epoch:: 274
epoch:: 275
epoch:: 276
epoch:: 277
epoch:: 278
epoch:: 279
epoch:: 280
epoch:: 281
epoch:: 282
epoch:: 283
epoch:: 284
epoch:: 285
epoch:: 286
epoch:: 287
epoch:: 288
epoch:: 289
epoch:: 290
epoch:: 291
epoch:: 292
epoch:: 293
epoch:: 294
epoch:: 295
epoch:: 296
epoch:: 297
epoch:: 298
epoch:: 299
epoch:: 300
epoch:: 301
epoch:: 302
epoch:: 303
epoch:: 304
epoch:: 305
epoch:: 306
epoch:: 307
epoch:: 308
epoch:: 309
epoch:: 310
epoch:: 311
epoch:: 312
epoch:: 313
epoch:: 314
epoch:: 315
epoch:: 316
epoch:: 317
epoch:: 318
epoch:: 319
epoch:: 320
epoch:: 321
epoch:: 322
epoch:: 323
epoch:: 324
epoch:: 325
epoch:: 326
epoch:: 327
epoch:: 328
epoch:: 329
epoch:: 330
epoch:: 331
epoch:: 332
epoch:: 333
epoch:: 334
epoch:: 335
epoch:: 336
epoch:: 337
epoch:: 338
epoch:: 339
epoch:: 340
epoch:: 341
epoch:: 342
epoch:: 343
epoch:: 344
epoch:: 345
epoch:: 346
epoch:: 347
epoch:: 348
epoch:: 349
epoch:: 350
epoch:: 351
epoch:: 352
epoch:: 353
epoch:: 354
epoch:: 355
epoch:: 356
epoch:: 357
epoch:: 358
epoch:: 359
epoch:: 360
epoch:: 361
epoch:: 362
epoch:: 363
epoch:: 364
epoch:: 365
epoch:: 366
epoch:: 367
epoch:: 368
epoch:: 369
epoch:: 370
epoch:: 371
epoch:: 372
epoch:: 373
epoch:: 374
epoch:: 375
epoch:: 376
epoch:: 377
epoch:: 378
epoch:: 379
epoch:: 380
epoch:: 381
epoch:: 382
epoch:: 383
epoch:: 384
epoch:: 385
epoch:: 386
epoch:: 387
epoch:: 388
epoch:: 389
epoch:: 390
epoch:: 391
epoch:: 392
epoch:: 393
epoch:: 394
epoch:: 395
epoch:: 396
epoch:: 397
epoch:: 398
epoch:: 399
epoch:: 400
epoch:: 401
epoch:: 402
epoch:: 403
epoch:: 404
epoch:: 405
epoch:: 406
epoch:: 407
epoch:: 408
epoch:: 409
epoch:: 410
epoch:: 411
epoch:: 412
epoch:: 413
epoch:: 414
epoch:: 415
epoch:: 416
epoch:: 417
epoch:: 418
epoch:: 419
epoch:: 420
epoch:: 421
epoch:: 422
epoch:: 423
epoch:: 424
epoch:: 425
epoch:: 426
epoch:: 427
epoch:: 428
epoch:: 429
epoch:: 430
epoch:: 431
epoch:: 432
epoch:: 433
epoch:: 434
epoch:: 435
epoch:: 436
epoch:: 437
epoch:: 438
epoch:: 439
epoch:: 440
epoch:: 441
epoch:: 442
epoch:: 443
epoch:: 444
epoch:: 445
epoch:: 446
epoch:: 447
epoch:: 448
epoch:: 449
epoch:: 450
epoch:: 451
epoch:: 452
epoch:: 453
epoch:: 454
epoch:: 455
epoch:: 456
epoch:: 457
epoch:: 458
epoch:: 459
epoch:: 460
epoch:: 461
epoch:: 462
epoch:: 463
epoch:: 464
epoch:: 465
epoch:: 466
epoch:: 467
epoch:: 468
epoch:: 469
epoch:: 470
epoch:: 471
epoch:: 472
epoch:: 473
epoch:: 474
epoch:: 475
epoch:: 476
epoch:: 477
epoch:: 478
epoch:: 479
epoch:: 480
epoch:: 481
epoch:: 482
epoch:: 483
epoch:: 484
epoch:: 485
epoch:: 486
epoch:: 487
epoch:: 488
epoch:: 489
epoch:: 490
epoch:: 491
epoch:: 492
epoch:: 493
epoch:: 494
epoch:: 495
epoch:: 496
epoch:: 497
epoch:: 498
epoch:: 499
training time is 80.15260100364685
overhead:: 0
overhead2:: 2.6684682369232178
overhead3:: 0
time_baseline:: 80.15264415740967
curr_diff: 0 tensor(0.1926, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1926, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632800
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 500
max_epoch:: 500
overhead:: 0
overhead2:: 0.08727288246154785
overhead3:: 0.2606015205383301
overhead4:: 9.567119359970093
overhead5:: 0
memory usage:: 5637668864
time_provenance:: 17.997981548309326
curr_diff: 0 tensor(0.0460, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0460, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.2276, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.2276, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632900
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 500
max_epoch:: 500
overhead:: 0
overhead2:: 0.08802175521850586
overhead3:: 0.272174596786499
overhead4:: 9.778321266174316
overhead5:: 0
memory usage:: 5691932672
time_provenance:: 18.293248891830444
curr_diff: 0 tensor(0.0461, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0461, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.2277, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.2277, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632900
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 500
max_epoch:: 500
overhead:: 0
overhead2:: 0.1025705337524414
overhead3:: 0.2600722312927246
overhead4:: 10.06112289428711
overhead5:: 0
memory usage:: 5628325888
time_provenance:: 18.6626558303833
curr_diff: 0 tensor(0.0463, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0463, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.2278, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.2278, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632400
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 500
max_epoch:: 500
overhead:: 0
overhead2:: 0.17158007621765137
overhead3:: 0.4433286190032959
overhead4:: 17.493264198303223
overhead5:: 0
memory usage:: 5631410176
time_provenance:: 28.02214026451111
curr_diff: 0 tensor(0.0046, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0046, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1959, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1959, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632700
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 500
max_epoch:: 500
overhead:: 0
overhead2:: 0.17139315605163574
overhead3:: 0.45317602157592773
overhead4:: 17.392029523849487
overhead5:: 0
memory usage:: 5646413824
time_provenance:: 27.976971864700317
curr_diff: 0 tensor(0.0049, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0049, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1961, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1961, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632800
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 500
max_epoch:: 500
overhead:: 0
overhead2:: 0.18105173110961914
overhead3:: 0.46488285064697266
overhead4:: 17.67793917655945
overhead5:: 0
memory usage:: 5643669504
time_provenance:: 28.40132784843445
curr_diff: 0 tensor(0.0049, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0049, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1961, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1961, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632800
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 500
max_epoch:: 500
overhead:: 0
overhead2:: 0.4406604766845703
overhead3:: 1.044468641281128
overhead4:: 40.45740270614624
overhead5:: 0
memory usage:: 5646475264
time_provenance:: 57.406880140304565
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1928, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1928, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632700
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 500
max_epoch:: 500
overhead:: 0
overhead2:: 0.41576647758483887
overhead3:: 1.042264461517334
overhead4:: 40.60780310630798
overhead5:: 0
memory usage:: 5632757760
time_provenance:: 57.53173518180847
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1928, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1928, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632700
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 500
max_epoch:: 500
overhead:: 0
overhead2:: 0.4316141605377197
overhead3:: 1.0135822296142578
overhead4:: 41.54596185684204
overhead5:: 0
memory usage:: 5626736640
time_provenance:: 58.5177903175354
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1928, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1928, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632700
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 0 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 500
max_epoch:: 500
overhead:: 0
overhead2:: 0.8324906826019287
overhead3:: 2.0620944499969482
overhead4:: 81.77126049995422
overhead5:: 0
memory usage:: 5628018688
time_provenance:: 106.9422562122345
curr_diff: 0 tensor(5.0058e-13, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.0058e-13, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1926, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1926, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632800
repetition 1
python3 benchmark_exp_lr.py 0.001 10000 500 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05] Logistic_regression cifar10_2 1 0.0001 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 2.308726
Train - Epoch 1, Batch: 0, Loss: 2.248612
Train - Epoch 2, Batch: 0, Loss: 2.191609
Train - Epoch 3, Batch: 0, Loss: 2.141415
Train - Epoch 4, Batch: 0, Loss: 2.093496
Train - Epoch 5, Batch: 0, Loss: 2.050774
Train - Epoch 6, Batch: 0, Loss: 2.009743
Train - Epoch 7, Batch: 0, Loss: 1.971480
Train - Epoch 8, Batch: 0, Loss: 1.932838
Train - Epoch 9, Batch: 0, Loss: 1.901654
Train - Epoch 10, Batch: 0, Loss: 1.870619
Train - Epoch 11, Batch: 0, Loss: 1.844631
Train - Epoch 12, Batch: 0, Loss: 1.811897
Train - Epoch 13, Batch: 0, Loss: 1.788259
Train - Epoch 14, Batch: 0, Loss: 1.756591
Train - Epoch 15, Batch: 0, Loss: 1.730412
Train - Epoch 16, Batch: 0, Loss: 1.720713
Train - Epoch 17, Batch: 0, Loss: 1.690595
Train - Epoch 18, Batch: 0, Loss: 1.681559
Train - Epoch 19, Batch: 0, Loss: 1.659192
Train - Epoch 20, Batch: 0, Loss: 1.639420
Train - Epoch 21, Batch: 0, Loss: 1.628845
Train - Epoch 22, Batch: 0, Loss: 1.608991
Train - Epoch 23, Batch: 0, Loss: 1.596786
Train - Epoch 24, Batch: 0, Loss: 1.579818
Train - Epoch 25, Batch: 0, Loss: 1.563445
Train - Epoch 26, Batch: 0, Loss: 1.553271
Train - Epoch 27, Batch: 0, Loss: 1.548088
Train - Epoch 28, Batch: 0, Loss: 1.527104
Train - Epoch 29, Batch: 0, Loss: 1.520085
Train - Epoch 30, Batch: 0, Loss: 1.505406
Train - Epoch 31, Batch: 0, Loss: 1.496848
Train - Epoch 32, Batch: 0, Loss: 1.492644
Train - Epoch 33, Batch: 0, Loss: 1.480261
Train - Epoch 34, Batch: 0, Loss: 1.478603
Train - Epoch 35, Batch: 0, Loss: 1.465053
Train - Epoch 36, Batch: 0, Loss: 1.440108
Train - Epoch 37, Batch: 0, Loss: 1.446677
Train - Epoch 38, Batch: 0, Loss: 1.435086
Train - Epoch 39, Batch: 0, Loss: 1.430358
Train - Epoch 40, Batch: 0, Loss: 1.421093
Train - Epoch 41, Batch: 0, Loss: 1.413972
Train - Epoch 42, Batch: 0, Loss: 1.418759
Train - Epoch 43, Batch: 0, Loss: 1.404883
Train - Epoch 44, Batch: 0, Loss: 1.393158
Train - Epoch 45, Batch: 0, Loss: 1.385806
Train - Epoch 46, Batch: 0, Loss: 1.382409
Train - Epoch 47, Batch: 0, Loss: 1.385299
Train - Epoch 48, Batch: 0, Loss: 1.369748
Train - Epoch 49, Batch: 0, Loss: 1.361034
Train - Epoch 50, Batch: 0, Loss: 1.352480
Train - Epoch 51, Batch: 0, Loss: 1.360844
Train - Epoch 52, Batch: 0, Loss: 1.354410
Train - Epoch 53, Batch: 0, Loss: 1.345367
Train - Epoch 54, Batch: 0, Loss: 1.333270
Train - Epoch 55, Batch: 0, Loss: 1.337416
Train - Epoch 56, Batch: 0, Loss: 1.333608
Train - Epoch 57, Batch: 0, Loss: 1.335620
Train - Epoch 58, Batch: 0, Loss: 1.329848
Train - Epoch 59, Batch: 0, Loss: 1.321390
Train - Epoch 60, Batch: 0, Loss: 1.310134
Train - Epoch 61, Batch: 0, Loss: 1.316559
Train - Epoch 62, Batch: 0, Loss: 1.313991
Train - Epoch 63, Batch: 0, Loss: 1.304086
Train - Epoch 64, Batch: 0, Loss: 1.300350
Train - Epoch 65, Batch: 0, Loss: 1.297972
Train - Epoch 66, Batch: 0, Loss: 1.298693
Train - Epoch 67, Batch: 0, Loss: 1.288926
Train - Epoch 68, Batch: 0, Loss: 1.291679
Train - Epoch 69, Batch: 0, Loss: 1.292806
Train - Epoch 70, Batch: 0, Loss: 1.280989
Train - Epoch 71, Batch: 0, Loss: 1.270869
Train - Epoch 72, Batch: 0, Loss: 1.277626
Train - Epoch 73, Batch: 0, Loss: 1.275475
Train - Epoch 74, Batch: 0, Loss: 1.264067
Train - Epoch 75, Batch: 0, Loss: 1.263988
Train - Epoch 76, Batch: 0, Loss: 1.267580
Train - Epoch 77, Batch: 0, Loss: 1.264067
Train - Epoch 78, Batch: 0, Loss: 1.267535
Train - Epoch 79, Batch: 0, Loss: 1.260966
Train - Epoch 80, Batch: 0, Loss: 1.251425
Train - Epoch 81, Batch: 0, Loss: 1.265408
Train - Epoch 82, Batch: 0, Loss: 1.248558
Train - Epoch 83, Batch: 0, Loss: 1.245198
Train - Epoch 84, Batch: 0, Loss: 1.234673
Train - Epoch 85, Batch: 0, Loss: 1.248195
Train - Epoch 86, Batch: 0, Loss: 1.253954
Train - Epoch 87, Batch: 0, Loss: 1.245723
Train - Epoch 88, Batch: 0, Loss: 1.231680
Train - Epoch 89, Batch: 0, Loss: 1.233376
Train - Epoch 90, Batch: 0, Loss: 1.233006
Train - Epoch 91, Batch: 0, Loss: 1.235272
Train - Epoch 92, Batch: 0, Loss: 1.232329
Train - Epoch 93, Batch: 0, Loss: 1.230168
Train - Epoch 94, Batch: 0, Loss: 1.233585
Train - Epoch 95, Batch: 0, Loss: 1.228308
Train - Epoch 96, Batch: 0, Loss: 1.213822
Train - Epoch 97, Batch: 0, Loss: 1.218107
Train - Epoch 98, Batch: 0, Loss: 1.220142
Train - Epoch 99, Batch: 0, Loss: 1.209007
Train - Epoch 100, Batch: 0, Loss: 1.209401
Train - Epoch 101, Batch: 0, Loss: 1.216762
Train - Epoch 102, Batch: 0, Loss: 1.213583
Train - Epoch 103, Batch: 0, Loss: 1.217169
Train - Epoch 104, Batch: 0, Loss: 1.222117
Train - Epoch 105, Batch: 0, Loss: 1.209660
Train - Epoch 106, Batch: 0, Loss: 1.211783
Train - Epoch 107, Batch: 0, Loss: 1.207261
Train - Epoch 108, Batch: 0, Loss: 1.207669
Train - Epoch 109, Batch: 0, Loss: 1.189089
Train - Epoch 110, Batch: 0, Loss: 1.194331
Train - Epoch 111, Batch: 0, Loss: 1.196088
Train - Epoch 112, Batch: 0, Loss: 1.203973
Train - Epoch 113, Batch: 0, Loss: 1.188281
Train - Epoch 114, Batch: 0, Loss: 1.204779
Train - Epoch 115, Batch: 0, Loss: 1.191143
Train - Epoch 116, Batch: 0, Loss: 1.176838
Train - Epoch 117, Batch: 0, Loss: 1.186492
Train - Epoch 118, Batch: 0, Loss: 1.188601
Train - Epoch 119, Batch: 0, Loss: 1.187345
Train - Epoch 120, Batch: 0, Loss: 1.181957
Train - Epoch 121, Batch: 0, Loss: 1.185637
Train - Epoch 122, Batch: 0, Loss: 1.188970
Train - Epoch 123, Batch: 0, Loss: 1.195221
Train - Epoch 124, Batch: 0, Loss: 1.171614
Train - Epoch 125, Batch: 0, Loss: 1.173581
Train - Epoch 126, Batch: 0, Loss: 1.199969
Train - Epoch 127, Batch: 0, Loss: 1.176959
Train - Epoch 128, Batch: 0, Loss: 1.175093
Train - Epoch 129, Batch: 0, Loss: 1.160630
Train - Epoch 130, Batch: 0, Loss: 1.171956
Train - Epoch 131, Batch: 0, Loss: 1.162879
Train - Epoch 132, Batch: 0, Loss: 1.164053
Train - Epoch 133, Batch: 0, Loss: 1.170254
Train - Epoch 134, Batch: 0, Loss: 1.165424
Train - Epoch 135, Batch: 0, Loss: 1.167010
Train - Epoch 136, Batch: 0, Loss: 1.172758
Train - Epoch 137, Batch: 0, Loss: 1.159233
Train - Epoch 138, Batch: 0, Loss: 1.161407
Train - Epoch 139, Batch: 0, Loss: 1.178279
Train - Epoch 140, Batch: 0, Loss: 1.161784
Train - Epoch 141, Batch: 0, Loss: 1.163089
Train - Epoch 142, Batch: 0, Loss: 1.175721
Train - Epoch 143, Batch: 0, Loss: 1.165373
Train - Epoch 144, Batch: 0, Loss: 1.156398
Train - Epoch 145, Batch: 0, Loss: 1.155316
Train - Epoch 146, Batch: 0, Loss: 1.166505
Train - Epoch 147, Batch: 0, Loss: 1.158481
Train - Epoch 148, Batch: 0, Loss: 1.164664
Train - Epoch 149, Batch: 0, Loss: 1.166914
Train - Epoch 150, Batch: 0, Loss: 1.152282
Train - Epoch 151, Batch: 0, Loss: 1.158766
Train - Epoch 152, Batch: 0, Loss: 1.139229
Train - Epoch 153, Batch: 0, Loss: 1.153689
Train - Epoch 154, Batch: 0, Loss: 1.154192
Train - Epoch 155, Batch: 0, Loss: 1.154517
Train - Epoch 156, Batch: 0, Loss: 1.139348
Train - Epoch 157, Batch: 0, Loss: 1.164567
Train - Epoch 158, Batch: 0, Loss: 1.144031
Train - Epoch 159, Batch: 0, Loss: 1.161654
Train - Epoch 160, Batch: 0, Loss: 1.148800
Train - Epoch 161, Batch: 0, Loss: 1.144224
Train - Epoch 162, Batch: 0, Loss: 1.153064
Train - Epoch 163, Batch: 0, Loss: 1.152443
Train - Epoch 164, Batch: 0, Loss: 1.142734
Train - Epoch 165, Batch: 0, Loss: 1.145322
Train - Epoch 166, Batch: 0, Loss: 1.151462
Train - Epoch 167, Batch: 0, Loss: 1.132946
Train - Epoch 168, Batch: 0, Loss: 1.141116
Train - Epoch 169, Batch: 0, Loss: 1.127207
Train - Epoch 170, Batch: 0, Loss: 1.144811
Train - Epoch 171, Batch: 0, Loss: 1.131446
Train - Epoch 172, Batch: 0, Loss: 1.135986
Train - Epoch 173, Batch: 0, Loss: 1.129275
Train - Epoch 174, Batch: 0, Loss: 1.131967
Train - Epoch 175, Batch: 0, Loss: 1.143734
Train - Epoch 176, Batch: 0, Loss: 1.135967
Train - Epoch 177, Batch: 0, Loss: 1.144231
Train - Epoch 178, Batch: 0, Loss: 1.144948
Train - Epoch 179, Batch: 0, Loss: 1.132570
Train - Epoch 180, Batch: 0, Loss: 1.131163
Train - Epoch 181, Batch: 0, Loss: 1.126933
Train - Epoch 182, Batch: 0, Loss: 1.124056
Train - Epoch 183, Batch: 0, Loss: 1.129311
Train - Epoch 184, Batch: 0, Loss: 1.137309
Train - Epoch 185, Batch: 0, Loss: 1.127326
Train - Epoch 186, Batch: 0, Loss: 1.152144
Train - Epoch 187, Batch: 0, Loss: 1.121618
Train - Epoch 188, Batch: 0, Loss: 1.128558
Train - Epoch 189, Batch: 0, Loss: 1.126805
Train - Epoch 190, Batch: 0, Loss: 1.131722
Train - Epoch 191, Batch: 0, Loss: 1.132701
Train - Epoch 192, Batch: 0, Loss: 1.130677
Train - Epoch 193, Batch: 0, Loss: 1.113784
Train - Epoch 194, Batch: 0, Loss: 1.130755
Train - Epoch 195, Batch: 0, Loss: 1.121294
Train - Epoch 196, Batch: 0, Loss: 1.126159
Train - Epoch 197, Batch: 0, Loss: 1.124925
Train - Epoch 198, Batch: 0, Loss: 1.114171
Train - Epoch 199, Batch: 0, Loss: 1.107757
Train - Epoch 200, Batch: 0, Loss: 1.119161
Train - Epoch 201, Batch: 0, Loss: 1.110100
Train - Epoch 202, Batch: 0, Loss: 1.118234
Train - Epoch 203, Batch: 0, Loss: 1.118110
Train - Epoch 204, Batch: 0, Loss: 1.123637
Train - Epoch 205, Batch: 0, Loss: 1.125083
Train - Epoch 206, Batch: 0, Loss: 1.132765
Train - Epoch 207, Batch: 0, Loss: 1.103555
Train - Epoch 208, Batch: 0, Loss: 1.109836
Train - Epoch 209, Batch: 0, Loss: 1.098349
Train - Epoch 210, Batch: 0, Loss: 1.110895
Train - Epoch 211, Batch: 0, Loss: 1.115532
Train - Epoch 212, Batch: 0, Loss: 1.108904
Train - Epoch 213, Batch: 0, Loss: 1.114768
Train - Epoch 214, Batch: 0, Loss: 1.117767
Train - Epoch 215, Batch: 0, Loss: 1.112029
Train - Epoch 216, Batch: 0, Loss: 1.109250
Train - Epoch 217, Batch: 0, Loss: 1.113999
Train - Epoch 218, Batch: 0, Loss: 1.105260
Train - Epoch 219, Batch: 0, Loss: 1.097280
Train - Epoch 220, Batch: 0, Loss: 1.103437
Train - Epoch 221, Batch: 0, Loss: 1.114109
Train - Epoch 222, Batch: 0, Loss: 1.107312
Train - Epoch 223, Batch: 0, Loss: 1.110280
Train - Epoch 224, Batch: 0, Loss: 1.095831
Train - Epoch 225, Batch: 0, Loss: 1.116807
Train - Epoch 226, Batch: 0, Loss: 1.104501
Train - Epoch 227, Batch: 0, Loss: 1.110659
Train - Epoch 228, Batch: 0, Loss: 1.089536
Train - Epoch 229, Batch: 0, Loss: 1.105380
Train - Epoch 230, Batch: 0, Loss: 1.099617
Train - Epoch 231, Batch: 0, Loss: 1.105929
Train - Epoch 232, Batch: 0, Loss: 1.099186
Train - Epoch 233, Batch: 0, Loss: 1.081565
Train - Epoch 234, Batch: 0, Loss: 1.097898
Train - Epoch 235, Batch: 0, Loss: 1.103936
Train - Epoch 236, Batch: 0, Loss: 1.105239
Train - Epoch 237, Batch: 0, Loss: 1.115823
Train - Epoch 238, Batch: 0, Loss: 1.097435
Train - Epoch 239, Batch: 0, Loss: 1.100927
Train - Epoch 240, Batch: 0, Loss: 1.080976
Train - Epoch 241, Batch: 0, Loss: 1.115245
Train - Epoch 242, Batch: 0, Loss: 1.110010
Train - Epoch 243, Batch: 0, Loss: 1.097920
Train - Epoch 244, Batch: 0, Loss: 1.102320
Train - Epoch 245, Batch: 0, Loss: 1.098998
Train - Epoch 246, Batch: 0, Loss: 1.092369
Train - Epoch 247, Batch: 0, Loss: 1.089939
Train - Epoch 248, Batch: 0, Loss: 1.097323
Train - Epoch 249, Batch: 0, Loss: 1.092194
Train - Epoch 250, Batch: 0, Loss: 1.087354
Train - Epoch 251, Batch: 0, Loss: 1.090098
Train - Epoch 252, Batch: 0, Loss: 1.088580
Train - Epoch 253, Batch: 0, Loss: 1.100719
Train - Epoch 254, Batch: 0, Loss: 1.083770
Train - Epoch 255, Batch: 0, Loss: 1.092189
Train - Epoch 256, Batch: 0, Loss: 1.095036
Train - Epoch 257, Batch: 0, Loss: 1.096188
Train - Epoch 258, Batch: 0, Loss: 1.095545
Train - Epoch 259, Batch: 0, Loss: 1.083904
Train - Epoch 260, Batch: 0, Loss: 1.085430
Train - Epoch 261, Batch: 0, Loss: 1.076604
Train - Epoch 262, Batch: 0, Loss: 1.092092
Train - Epoch 263, Batch: 0, Loss: 1.085186
Train - Epoch 264, Batch: 0, Loss: 1.090071
Train - Epoch 265, Batch: 0, Loss: 1.081068
Train - Epoch 266, Batch: 0, Loss: 1.088675
Train - Epoch 267, Batch: 0, Loss: 1.081848
Train - Epoch 268, Batch: 0, Loss: 1.088993
Train - Epoch 269, Batch: 0, Loss: 1.089121
Train - Epoch 270, Batch: 0, Loss: 1.082364
Train - Epoch 271, Batch: 0, Loss: 1.094360
Train - Epoch 272, Batch: 0, Loss: 1.094087
Train - Epoch 273, Batch: 0, Loss: 1.086110
Train - Epoch 274, Batch: 0, Loss: 1.080656
Train - Epoch 275, Batch: 0, Loss: 1.080831
Train - Epoch 276, Batch: 0, Loss: 1.090323
Train - Epoch 277, Batch: 0, Loss: 1.082109
Train - Epoch 278, Batch: 0, Loss: 1.087613
Train - Epoch 279, Batch: 0, Loss: 1.078107
Train - Epoch 280, Batch: 0, Loss: 1.087733
Train - Epoch 281, Batch: 0, Loss: 1.089473
Train - Epoch 282, Batch: 0, Loss: 1.079598
Train - Epoch 283, Batch: 0, Loss: 1.084506
Train - Epoch 284, Batch: 0, Loss: 1.093284
Train - Epoch 285, Batch: 0, Loss: 1.076451
Train - Epoch 286, Batch: 0, Loss: 1.082597
Train - Epoch 287, Batch: 0, Loss: 1.076026
Train - Epoch 288, Batch: 0, Loss: 1.078748
Train - Epoch 289, Batch: 0, Loss: 1.077088
Train - Epoch 290, Batch: 0, Loss: 1.077550
Train - Epoch 291, Batch: 0, Loss: 1.092627
Train - Epoch 292, Batch: 0, Loss: 1.078810
Train - Epoch 293, Batch: 0, Loss: 1.067889
Train - Epoch 294, Batch: 0, Loss: 1.082354
Train - Epoch 295, Batch: 0, Loss: 1.077636
Train - Epoch 296, Batch: 0, Loss: 1.082625
Train - Epoch 297, Batch: 0, Loss: 1.078447
Train - Epoch 298, Batch: 0, Loss: 1.081280
Train - Epoch 299, Batch: 0, Loss: 1.063832
Train - Epoch 300, Batch: 0, Loss: 1.074541
Train - Epoch 301, Batch: 0, Loss: 1.080941
Train - Epoch 302, Batch: 0, Loss: 1.074248
Train - Epoch 303, Batch: 0, Loss: 1.087666
Train - Epoch 304, Batch: 0, Loss: 1.065061
Train - Epoch 305, Batch: 0, Loss: 1.089432
Train - Epoch 306, Batch: 0, Loss: 1.089580
Train - Epoch 307, Batch: 0, Loss: 1.063447
Train - Epoch 308, Batch: 0, Loss: 1.062792
Train - Epoch 309, Batch: 0, Loss: 1.070019
Train - Epoch 310, Batch: 0, Loss: 1.066720
Train - Epoch 311, Batch: 0, Loss: 1.066929
Train - Epoch 312, Batch: 0, Loss: 1.091015
Train - Epoch 313, Batch: 0, Loss: 1.080498
Train - Epoch 314, Batch: 0, Loss: 1.066181
Train - Epoch 315, Batch: 0, Loss: 1.073892
Train - Epoch 316, Batch: 0, Loss: 1.082280
Train - Epoch 317, Batch: 0, Loss: 1.067767
Train - Epoch 318, Batch: 0, Loss: 1.075822
Train - Epoch 319, Batch: 0, Loss: 1.068311
Train - Epoch 320, Batch: 0, Loss: 1.085123
Train - Epoch 321, Batch: 0, Loss: 1.069819
Train - Epoch 322, Batch: 0, Loss: 1.078824
Train - Epoch 323, Batch: 0, Loss: 1.074205
Train - Epoch 324, Batch: 0, Loss: 1.058137
Train - Epoch 325, Batch: 0, Loss: 1.072974
Train - Epoch 326, Batch: 0, Loss: 1.072147
Train - Epoch 327, Batch: 0, Loss: 1.078222
Train - Epoch 328, Batch: 0, Loss: 1.048091
Train - Epoch 329, Batch: 0, Loss: 1.076578
Train - Epoch 330, Batch: 0, Loss: 1.057745
Train - Epoch 331, Batch: 0, Loss: 1.080144
Train - Epoch 332, Batch: 0, Loss: 1.076625
Train - Epoch 333, Batch: 0, Loss: 1.066276
Train - Epoch 334, Batch: 0, Loss: 1.061584
Train - Epoch 335, Batch: 0, Loss: 1.059055
Train - Epoch 336, Batch: 0, Loss: 1.067666
Train - Epoch 337, Batch: 0, Loss: 1.073945
Train - Epoch 338, Batch: 0, Loss: 1.070850
Train - Epoch 339, Batch: 0, Loss: 1.060860
Train - Epoch 340, Batch: 0, Loss: 1.070399
Train - Epoch 341, Batch: 0, Loss: 1.066647
Train - Epoch 342, Batch: 0, Loss: 1.058502
Train - Epoch 343, Batch: 0, Loss: 1.058456
Train - Epoch 344, Batch: 0, Loss: 1.077168
Train - Epoch 345, Batch: 0, Loss: 1.075518
Train - Epoch 346, Batch: 0, Loss: 1.070291
Train - Epoch 347, Batch: 0, Loss: 1.060029
Train - Epoch 348, Batch: 0, Loss: 1.063152
Train - Epoch 349, Batch: 0, Loss: 1.070937
Train - Epoch 350, Batch: 0, Loss: 1.076467
Train - Epoch 351, Batch: 0, Loss: 1.058360
Train - Epoch 352, Batch: 0, Loss: 1.068379
Train - Epoch 353, Batch: 0, Loss: 1.060945
Train - Epoch 354, Batch: 0, Loss: 1.045272
Train - Epoch 355, Batch: 0, Loss: 1.063404
Train - Epoch 356, Batch: 0, Loss: 1.062533
Train - Epoch 357, Batch: 0, Loss: 1.060526
Train - Epoch 358, Batch: 0, Loss: 1.071340
Train - Epoch 359, Batch: 0, Loss: 1.060037
Train - Epoch 360, Batch: 0, Loss: 1.063839
Train - Epoch 361, Batch: 0, Loss: 1.065783
Train - Epoch 362, Batch: 0, Loss: 1.076213
Train - Epoch 363, Batch: 0, Loss: 1.053271
Train - Epoch 364, Batch: 0, Loss: 1.043872
Train - Epoch 365, Batch: 0, Loss: 1.060104
Train - Epoch 366, Batch: 0, Loss: 1.049301
Train - Epoch 367, Batch: 0, Loss: 1.045481
Train - Epoch 368, Batch: 0, Loss: 1.060817
Train - Epoch 369, Batch: 0, Loss: 1.057353
Train - Epoch 370, Batch: 0, Loss: 1.069020
Train - Epoch 371, Batch: 0, Loss: 1.052302
Train - Epoch 372, Batch: 0, Loss: 1.059609
Train - Epoch 373, Batch: 0, Loss: 1.048631/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 374, Batch: 0, Loss: 1.068445
Train - Epoch 375, Batch: 0, Loss: 1.066577
Train - Epoch 376, Batch: 0, Loss: 1.045889
Train - Epoch 377, Batch: 0, Loss: 1.067304
Train - Epoch 378, Batch: 0, Loss: 1.064802
Train - Epoch 379, Batch: 0, Loss: 1.058178
Train - Epoch 380, Batch: 0, Loss: 1.041275
Train - Epoch 381, Batch: 0, Loss: 1.055045
Train - Epoch 382, Batch: 0, Loss: 1.058589
Train - Epoch 383, Batch: 0, Loss: 1.058068
Train - Epoch 384, Batch: 0, Loss: 1.051765
Train - Epoch 385, Batch: 0, Loss: 1.062172
Train - Epoch 386, Batch: 0, Loss: 1.078028
Train - Epoch 387, Batch: 0, Loss: 1.049994
Train - Epoch 388, Batch: 0, Loss: 1.062060
Train - Epoch 389, Batch: 0, Loss: 1.059079
Train - Epoch 390, Batch: 0, Loss: 1.056492
Train - Epoch 391, Batch: 0, Loss: 1.059861
Train - Epoch 392, Batch: 0, Loss: 1.059400
Train - Epoch 393, Batch: 0, Loss: 1.060722
Train - Epoch 394, Batch: 0, Loss: 1.069965
Train - Epoch 395, Batch: 0, Loss: 1.054911
Train - Epoch 396, Batch: 0, Loss: 1.051765
Train - Epoch 397, Batch: 0, Loss: 1.046906
Train - Epoch 398, Batch: 0, Loss: 1.069070
Train - Epoch 399, Batch: 0, Loss: 1.040818
Train - Epoch 400, Batch: 0, Loss: 1.056773
Train - Epoch 401, Batch: 0, Loss: 1.057886
Train - Epoch 402, Batch: 0, Loss: 1.053951
Train - Epoch 403, Batch: 0, Loss: 1.061164
Train - Epoch 404, Batch: 0, Loss: 1.052354
Train - Epoch 405, Batch: 0, Loss: 1.053165
Train - Epoch 406, Batch: 0, Loss: 1.051863
Train - Epoch 407, Batch: 0, Loss: 1.056891
Train - Epoch 408, Batch: 0, Loss: 1.049314
Train - Epoch 409, Batch: 0, Loss: 1.052231
Train - Epoch 410, Batch: 0, Loss: 1.044954
Train - Epoch 411, Batch: 0, Loss: 1.040474
Train - Epoch 412, Batch: 0, Loss: 1.060237
Train - Epoch 413, Batch: 0, Loss: 1.069579
Train - Epoch 414, Batch: 0, Loss: 1.044229
Train - Epoch 415, Batch: 0, Loss: 1.045862
Train - Epoch 416, Batch: 0, Loss: 1.067279
Train - Epoch 417, Batch: 0, Loss: 1.052578
Train - Epoch 418, Batch: 0, Loss: 1.054672
Train - Epoch 419, Batch: 0, Loss: 1.051333
Train - Epoch 420, Batch: 0, Loss: 1.038322
Train - Epoch 421, Batch: 0, Loss: 1.042809
Train - Epoch 422, Batch: 0, Loss: 1.054987
Train - Epoch 423, Batch: 0, Loss: 1.064173
Train - Epoch 424, Batch: 0, Loss: 1.041395
Train - Epoch 425, Batch: 0, Loss: 1.045653
Train - Epoch 426, Batch: 0, Loss: 1.053501
Train - Epoch 427, Batch: 0, Loss: 1.049427
Train - Epoch 428, Batch: 0, Loss: 1.045458
Train - Epoch 429, Batch: 0, Loss: 1.060874
Train - Epoch 430, Batch: 0, Loss: 1.041998
Train - Epoch 431, Batch: 0, Loss: 1.043349
Train - Epoch 432, Batch: 0, Loss: 1.041937
Train - Epoch 433, Batch: 0, Loss: 1.044319
Train - Epoch 434, Batch: 0, Loss: 1.051879
Train - Epoch 435, Batch: 0, Loss: 1.055040
Train - Epoch 436, Batch: 0, Loss: 1.049016
Train - Epoch 437, Batch: 0, Loss: 1.050481
Train - Epoch 438, Batch: 0, Loss: 1.032517
Train - Epoch 439, Batch: 0, Loss: 1.057181
Train - Epoch 440, Batch: 0, Loss: 1.044366
Train - Epoch 441, Batch: 0, Loss: 1.049668
Train - Epoch 442, Batch: 0, Loss: 1.048013
Train - Epoch 443, Batch: 0, Loss: 1.046425
Train - Epoch 444, Batch: 0, Loss: 1.051177
Train - Epoch 445, Batch: 0, Loss: 1.041858
Train - Epoch 446, Batch: 0, Loss: 1.036550
Train - Epoch 447, Batch: 0, Loss: 1.052500
Train - Epoch 448, Batch: 0, Loss: 1.045955
Train - Epoch 449, Batch: 0, Loss: 1.045556
Train - Epoch 450, Batch: 0, Loss: 1.039698
Train - Epoch 451, Batch: 0, Loss: 1.060229
Train - Epoch 452, Batch: 0, Loss: 1.046293
Train - Epoch 453, Batch: 0, Loss: 1.053187
Train - Epoch 454, Batch: 0, Loss: 1.026724
Train - Epoch 455, Batch: 0, Loss: 1.055625
Train - Epoch 456, Batch: 0, Loss: 1.048748
Train - Epoch 457, Batch: 0, Loss: 1.057155
Train - Epoch 458, Batch: 0, Loss: 1.043434
Train - Epoch 459, Batch: 0, Loss: 1.047403
Train - Epoch 460, Batch: 0, Loss: 1.042763
Train - Epoch 461, Batch: 0, Loss: 1.044520
Train - Epoch 462, Batch: 0, Loss: 1.034464
Train - Epoch 463, Batch: 0, Loss: 1.049053
Train - Epoch 464, Batch: 0, Loss: 1.040060
Train - Epoch 465, Batch: 0, Loss: 1.052838
Train - Epoch 466, Batch: 0, Loss: 1.026593
Train - Epoch 467, Batch: 0, Loss: 1.040141
Train - Epoch 468, Batch: 0, Loss: 1.036491
Train - Epoch 469, Batch: 0, Loss: 1.030291
Train - Epoch 470, Batch: 0, Loss: 1.047131
Train - Epoch 471, Batch: 0, Loss: 1.043853
Train - Epoch 472, Batch: 0, Loss: 1.047727
Train - Epoch 473, Batch: 0, Loss: 1.055399
Train - Epoch 474, Batch: 0, Loss: 1.046468
Train - Epoch 475, Batch: 0, Loss: 1.034990
Train - Epoch 476, Batch: 0, Loss: 1.043903
Train - Epoch 477, Batch: 0, Loss: 1.050337
Train - Epoch 478, Batch: 0, Loss: 1.038991
Train - Epoch 479, Batch: 0, Loss: 1.031548
Train - Epoch 480, Batch: 0, Loss: 1.038267
Train - Epoch 481, Batch: 0, Loss: 1.057026
Train - Epoch 482, Batch: 0, Loss: 1.033337
Train - Epoch 483, Batch: 0, Loss: 1.030143
Train - Epoch 484, Batch: 0, Loss: 1.038974
Train - Epoch 485, Batch: 0, Loss: 1.040397
Train - Epoch 486, Batch: 0, Loss: 1.049465
Train - Epoch 487, Batch: 0, Loss: 1.041826
Train - Epoch 488, Batch: 0, Loss: 1.027542
Train - Epoch 489, Batch: 0, Loss: 1.021737
Train - Epoch 490, Batch: 0, Loss: 1.038968
Train - Epoch 491, Batch: 0, Loss: 1.028579
Train - Epoch 492, Batch: 0, Loss: 1.034745
Train - Epoch 493, Batch: 0, Loss: 1.018918
Train - Epoch 494, Batch: 0, Loss: 1.056404
Train - Epoch 495, Batch: 0, Loss: 1.040969
Train - Epoch 496, Batch: 0, Loss: 1.032762
Train - Epoch 497, Batch: 0, Loss: 1.040658
Train - Epoch 498, Batch: 0, Loss: 1.035821
Train - Epoch 499, Batch: 0, Loss: 1.042054
training_time:: 107.47892880439758
training time full:: 107.4790096282959
provenance prepare time:: 0.0
here
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.634000
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 500
max_epoch:: 500
batch_size:: 10000
epoch:: 0
tensor([    0, 32776, 32777,    12, 32780,    15,    16, 32792, 32800,    38,
           42, 32817,    50,    49,    52, 32818,    55, 32825, 32830, 32836,
        32837,    70, 32840, 32841,    78,    79,    91, 32860,    95,    96,
        32866,   100,   103, 32872,   106, 32876, 32880,   116, 32892,   126,
          128,   132,   135, 32904,   141,   145, 32913,   147,   149, 32920])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
epoch:: 60
epoch:: 61
epoch:: 62
epoch:: 63
epoch:: 64
epoch:: 65
epoch:: 66
epoch:: 67
epoch:: 68
epoch:: 69
epoch:: 70
epoch:: 71
epoch:: 72
epoch:: 73
epoch:: 74
epoch:: 75
epoch:: 76
epoch:: 77
epoch:: 78
epoch:: 79
epoch:: 80
epoch:: 81
epoch:: 82
epoch:: 83
epoch:: 84
epoch:: 85
epoch:: 86
epoch:: 87
epoch:: 88
epoch:: 89
epoch:: 90
epoch:: 91
epoch:: 92
epoch:: 93
epoch:: 94
epoch:: 95
epoch:: 96
epoch:: 97
epoch:: 98
epoch:: 99
epoch:: 100
epoch:: 101
epoch:: 102
epoch:: 103
epoch:: 104
epoch:: 105
epoch:: 106
epoch:: 107
epoch:: 108
epoch:: 109
epoch:: 110
epoch:: 111
epoch:: 112
epoch:: 113
epoch:: 114
epoch:: 115
epoch:: 116
epoch:: 117
epoch:: 118
epoch:: 119
epoch:: 120
epoch:: 121
epoch:: 122
epoch:: 123
epoch:: 124
epoch:: 125
epoch:: 126
epoch:: 127
epoch:: 128
epoch:: 129
epoch:: 130
epoch:: 131
epoch:: 132
epoch:: 133
epoch:: 134
epoch:: 135
epoch:: 136
epoch:: 137
epoch:: 138
epoch:: 139
epoch:: 140
epoch:: 141
epoch:: 142
epoch:: 143
epoch:: 144
epoch:: 145
epoch:: 146
epoch:: 147
epoch:: 148
epoch:: 149
epoch:: 150
epoch:: 151
epoch:: 152
epoch:: 153
epoch:: 154
epoch:: 155
epoch:: 156
epoch:: 157
epoch:: 158
epoch:: 159
epoch:: 160
epoch:: 161
epoch:: 162
epoch:: 163
epoch:: 164
epoch:: 165
epoch:: 166
epoch:: 167
epoch:: 168
epoch:: 169
epoch:: 170
epoch:: 171
epoch:: 172
epoch:: 173
epoch:: 174
epoch:: 175
epoch:: 176
epoch:: 177
epoch:: 178
epoch:: 179
epoch:: 180
epoch:: 181
epoch:: 182
epoch:: 183
epoch:: 184
epoch:: 185
epoch:: 186
epoch:: 187
epoch:: 188
epoch:: 189
epoch:: 190
epoch:: 191
epoch:: 192
epoch:: 193
epoch:: 194
epoch:: 195
epoch:: 196
epoch:: 197
epoch:: 198
epoch:: 199
epoch:: 200
epoch:: 201
epoch:: 202
epoch:: 203
epoch:: 204
epoch:: 205
epoch:: 206
epoch:: 207
epoch:: 208
epoch:: 209
epoch:: 210
epoch:: 211
epoch:: 212
epoch:: 213
epoch:: 214
epoch:: 215
epoch:: 216
epoch:: 217
epoch:: 218
epoch:: 219
epoch:: 220
epoch:: 221
epoch:: 222
epoch:: 223
epoch:: 224
epoch:: 225
epoch:: 226
epoch:: 227
epoch:: 228
epoch:: 229
epoch:: 230
epoch:: 231
epoch:: 232
epoch:: 233
epoch:: 234
epoch:: 235
epoch:: 236
epoch:: 237
epoch:: 238
epoch:: 239
epoch:: 240
epoch:: 241
epoch:: 242
epoch:: 243
epoch:: 244
epoch:: 245
epoch:: 246
epoch:: 247
epoch:: 248
epoch:: 249
epoch:: 250
epoch:: 251
epoch:: 252
epoch:: 253
epoch:: 254
epoch:: 255
epoch:: 256
epoch:: 257
epoch:: 258
epoch:: 259
epoch:: 260
epoch:: 261
epoch:: 262
epoch:: 263
epoch:: 264
epoch:: 265
epoch:: 266
epoch:: 267
epoch:: 268
epoch:: 269
epoch:: 270
epoch:: 271
epoch:: 272
epoch:: 273
epoch:: 274
epoch:: 275
epoch:: 276
epoch:: 277
epoch:: 278
epoch:: 279
epoch:: 280
epoch:: 281
epoch:: 282
epoch:: 283
epoch:: 284
epoch:: 285
epoch:: 286
epoch:: 287
epoch:: 288
epoch:: 289
epoch:: 290
epoch:: 291
epoch:: 292
epoch:: 293
epoch:: 294
epoch:: 295
epoch:: 296
epoch:: 297
epoch:: 298
epoch:: 299
epoch:: 300
epoch:: 301
epoch:: 302
epoch:: 303
epoch:: 304
epoch:: 305
epoch:: 306
epoch:: 307
epoch:: 308
epoch:: 309
epoch:: 310
epoch:: 311
epoch:: 312
epoch:: 313
epoch:: 314
epoch:: 315
epoch:: 316
epoch:: 317
epoch:: 318
epoch:: 319
epoch:: 320
epoch:: 321
epoch:: 322
epoch:: 323
epoch:: 324
epoch:: 325
epoch:: 326
epoch:: 327
epoch:: 328
epoch:: 329
epoch:: 330
epoch:: 331
epoch:: 332
epoch:: 333
epoch:: 334
epoch:: 335
epoch:: 336
epoch:: 337
epoch:: 338
epoch:: 339
epoch:: 340
epoch:: 341
epoch:: 342
epoch:: 343
epoch:: 344
epoch:: 345
epoch:: 346
epoch:: 347
epoch:: 348
epoch:: 349
epoch:: 350
epoch:: 351
epoch:: 352
epoch:: 353
epoch:: 354
epoch:: 355
epoch:: 356
epoch:: 357
epoch:: 358
epoch:: 359
epoch:: 360
epoch:: 361
epoch:: 362
epoch:: 363
epoch:: 364
epoch:: 365
epoch:: 366
epoch:: 367
epoch:: 368
epoch:: 369
epoch:: 370
epoch:: 371
epoch:: 372
epoch:: 373
epoch:: 374
epoch:: 375
epoch:: 376
epoch:: 377
epoch:: 378
epoch:: 379
epoch:: 380
epoch:: 381
epoch:: 382
epoch:: 383
epoch:: 384
epoch:: 385
epoch:: 386
epoch:: 387
epoch:: 388
epoch:: 389
epoch:: 390
epoch:: 391
epoch:: 392
epoch:: 393
epoch:: 394
epoch:: 395
epoch:: 396
epoch:: 397
epoch:: 398
epoch:: 399
epoch:: 400
epoch:: 401
epoch:: 402
epoch:: 403
epoch:: 404
epoch:: 405
epoch:: 406
epoch:: 407
epoch:: 408
epoch:: 409
epoch:: 410
epoch:: 411
epoch:: 412
epoch:: 413
epoch:: 414
epoch:: 415
epoch:: 416
epoch:: 417
epoch:: 418
epoch:: 419
epoch:: 420
epoch:: 421
epoch:: 422
epoch:: 423
epoch:: 424
epoch:: 425
epoch:: 426
epoch:: 427
epoch:: 428
epoch:: 429
epoch:: 430
epoch:: 431
epoch:: 432
epoch:: 433
epoch:: 434
epoch:: 435
epoch:: 436
epoch:: 437
epoch:: 438
epoch:: 439
epoch:: 440
epoch:: 441
epoch:: 442
epoch:: 443
epoch:: 444
epoch:: 445
epoch:: 446
epoch:: 447
epoch:: 448
epoch:: 449
epoch:: 450
epoch:: 451
epoch:: 452
epoch:: 453
epoch:: 454
epoch:: 455
epoch:: 456
epoch:: 457
epoch:: 458
epoch:: 459
epoch:: 460
epoch:: 461
epoch:: 462
epoch:: 463
epoch:: 464
epoch:: 465
epoch:: 466
epoch:: 467
epoch:: 468
epoch:: 469
epoch:: 470
epoch:: 471
epoch:: 472
epoch:: 473
epoch:: 474
epoch:: 475
epoch:: 476
epoch:: 477
epoch:: 478
epoch:: 479
epoch:: 480
epoch:: 481
epoch:: 482
epoch:: 483
epoch:: 484
epoch:: 485
epoch:: 486
epoch:: 487
epoch:: 488
epoch:: 489
epoch:: 490
epoch:: 491
epoch:: 492
epoch:: 493
epoch:: 494
epoch:: 495
epoch:: 496
epoch:: 497
epoch:: 498
epoch:: 499
training time is 80.0802960395813
overhead:: 0
overhead2:: 2.6525254249572754
overhead3:: 0
time_baseline:: 80.08034038543701
curr_diff: 0 tensor(0.1926, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1926, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632300
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 500
max_epoch:: 500
overhead:: 0
overhead2:: 0.08954453468322754
overhead3:: 0.25112223625183105
overhead4:: 9.47815990447998
overhead5:: 0
memory usage:: 5625483264
time_provenance:: 17.945900201797485
curr_diff: 0 tensor(0.0462, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0462, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.2277, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.2277, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632400
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 500
max_epoch:: 500
overhead:: 0
overhead2:: 0.09233736991882324
overhead3:: 0.2585020065307617
overhead4:: 9.947449922561646
overhead5:: 0
memory usage:: 5630578688
time_provenance:: 18.448503017425537
curr_diff: 0 tensor(0.0462, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0462, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.2277, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.2277, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633200
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 500
max_epoch:: 500
overhead:: 0
overhead2:: 0.09702920913696289
overhead3:: 0.27278685569763184
overhead4:: 10.263230562210083
overhead5:: 0
memory usage:: 5626630144
time_provenance:: 18.86147975921631
curr_diff: 0 tensor(0.0463, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0463, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.2278, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.2278, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632400
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 500
max_epoch:: 500
overhead:: 0
overhead2:: 0.17517900466918945
overhead3:: 0.4370748996734619
overhead4:: 17.131795406341553
overhead5:: 0
memory usage:: 5629927424
time_provenance:: 27.691681385040283
curr_diff: 0 tensor(0.0047, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0047, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1959, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1959, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632600
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 500
max_epoch:: 500
overhead:: 0
overhead2:: 0.17775416374206543
overhead3:: 0.45798802375793457
overhead4:: 17.427701950073242
overhead5:: 0
memory usage:: 5625876480
time_provenance:: 28.053895473480225
curr_diff: 0 tensor(0.0047, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0047, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1960, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1960, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632600
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 500
max_epoch:: 500
overhead:: 0
overhead2:: 0.18522095680236816
overhead3:: 0.4565708637237549
overhead4:: 17.837382555007935
overhead5:: 0
memory usage:: 5641318400
time_provenance:: 28.491307497024536
curr_diff: 0 tensor(0.0047, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0047, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1960, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1960, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632600
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 500
max_epoch:: 500
overhead:: 0
overhead2:: 0.4162874221801758
overhead3:: 1.0452804565429688
overhead4:: 40.49426507949829
overhead5:: 0
memory usage:: 5630656512
time_provenance:: 57.41628408432007
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1928, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1928, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632200
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 500
max_epoch:: 500
overhead:: 0
overhead2:: 0.422806978225708
overhead3:: 1.009800910949707
overhead4:: 41.05305480957031
overhead5:: 0
memory usage:: 5636960256
time_provenance:: 57.973844051361084
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1928, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1928, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632200
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 500
max_epoch:: 500
overhead:: 0
overhead2:: 0.44681382179260254
overhead3:: 0.9706535339355469
overhead4:: 41.19442963600159
overhead5:: 0
memory usage:: 5647130624
time_provenance:: 58.15818381309509
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1928, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1928, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632200
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 1 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 500
max_epoch:: 500
overhead:: 0
overhead2:: 0.8316512107849121
overhead3:: 2.0949745178222656
overhead4:: 82.0977430343628
overhead5:: 0
memory usage:: 5633622016
time_provenance:: 107.27850842475891
curr_diff: 0 tensor(5.0042e-13, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.0042e-13, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1926, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1926, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632300
repetition 2
python3 benchmark_exp_lr.py 0.001 10000 500 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05] Logistic_regression cifar10_2 2 0.0001 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 2.300285
Train - Epoch 1, Batch: 0, Loss: 2.235759
Train - Epoch 2, Batch: 0, Loss: 2.180606
Train - Epoch 3, Batch: 0, Loss: 2.130487
Train - Epoch 4, Batch: 0, Loss: 2.083498
Train - Epoch 5, Batch: 0, Loss: 2.046258
Train - Epoch 6, Batch: 0, Loss: 1.999348
Train - Epoch 7, Batch: 0, Loss: 1.963446
Train - Epoch 8, Batch: 0, Loss: 1.924986
Train - Epoch 9, Batch: 0, Loss: 1.893679
Train - Epoch 10, Batch: 0, Loss: 1.859641
Train - Epoch 11, Batch: 0, Loss: 1.828835
Train - Epoch 12, Batch: 0, Loss: 1.809404
Train - Epoch 13, Batch: 0, Loss: 1.786210
Train - Epoch 14, Batch: 0, Loss: 1.761143
Train - Epoch 15, Batch: 0, Loss: 1.733163
Train - Epoch 16, Batch: 0, Loss: 1.705821
Train - Epoch 17, Batch: 0, Loss: 1.692781
Train - Epoch 18, Batch: 0, Loss: 1.668178
Train - Epoch 19, Batch: 0, Loss: 1.656175
Train - Epoch 20, Batch: 0, Loss: 1.641812
Train - Epoch 21, Batch: 0, Loss: 1.619158
Train - Epoch 22, Batch: 0, Loss: 1.604091
Train - Epoch 23, Batch: 0, Loss: 1.583574
Train - Epoch 24, Batch: 0, Loss: 1.580171
Train - Epoch 25, Batch: 0, Loss: 1.565772
Train - Epoch 26, Batch: 0, Loss: 1.557162
Train - Epoch 27, Batch: 0, Loss: 1.544078
Train - Epoch 28, Batch: 0, Loss: 1.522805
Train - Epoch 29, Batch: 0, Loss: 1.523363
Train - Epoch 30, Batch: 0, Loss: 1.497650
Train - Epoch 31, Batch: 0, Loss: 1.496046
Train - Epoch 32, Batch: 0, Loss: 1.479995
Train - Epoch 33, Batch: 0, Loss: 1.480317
Train - Epoch 34, Batch: 0, Loss: 1.454088
Train - Epoch 35, Batch: 0, Loss: 1.461177
Train - Epoch 36, Batch: 0, Loss: 1.446211
Train - Epoch 37, Batch: 0, Loss: 1.452245
Train - Epoch 38, Batch: 0, Loss: 1.427127
Train - Epoch 39, Batch: 0, Loss: 1.421757
Train - Epoch 40, Batch: 0, Loss: 1.419689
Train - Epoch 41, Batch: 0, Loss: 1.416154
Train - Epoch 42, Batch: 0, Loss: 1.410089
Train - Epoch 43, Batch: 0, Loss: 1.394433
Train - Epoch 44, Batch: 0, Loss: 1.398370
Train - Epoch 45, Batch: 0, Loss: 1.389774
Train - Epoch 46, Batch: 0, Loss: 1.377269
Train - Epoch 47, Batch: 0, Loss: 1.378955
Train - Epoch 48, Batch: 0, Loss: 1.369719
Train - Epoch 49, Batch: 0, Loss: 1.362650
Train - Epoch 50, Batch: 0, Loss: 1.350843
Train - Epoch 51, Batch: 0, Loss: 1.355548
Train - Epoch 52, Batch: 0, Loss: 1.357219
Train - Epoch 53, Batch: 0, Loss: 1.357897
Train - Epoch 54, Batch: 0, Loss: 1.342059
Train - Epoch 55, Batch: 0, Loss: 1.334488
Train - Epoch 56, Batch: 0, Loss: 1.336459
Train - Epoch 57, Batch: 0, Loss: 1.332326
Train - Epoch 58, Batch: 0, Loss: 1.311085
Train - Epoch 59, Batch: 0, Loss: 1.318241
Train - Epoch 60, Batch: 0, Loss: 1.318356
Train - Epoch 61, Batch: 0, Loss: 1.319149
Train - Epoch 62, Batch: 0, Loss: 1.308253
Train - Epoch 63, Batch: 0, Loss: 1.313358
Train - Epoch 64, Batch: 0, Loss: 1.302829
Train - Epoch 65, Batch: 0, Loss: 1.300547
Train - Epoch 66, Batch: 0, Loss: 1.292364
Train - Epoch 67, Batch: 0, Loss: 1.295253
Train - Epoch 68, Batch: 0, Loss: 1.282851
Train - Epoch 69, Batch: 0, Loss: 1.293712
Train - Epoch 70, Batch: 0, Loss: 1.264533
Train - Epoch 71, Batch: 0, Loss: 1.278304
Train - Epoch 72, Batch: 0, Loss: 1.270474
Train - Epoch 73, Batch: 0, Loss: 1.279147
Train - Epoch 74, Batch: 0, Loss: 1.287702
Train - Epoch 75, Batch: 0, Loss: 1.259468
Train - Epoch 76, Batch: 0, Loss: 1.259383
Train - Epoch 77, Batch: 0, Loss: 1.254419
Train - Epoch 78, Batch: 0, Loss: 1.266939
Train - Epoch 79, Batch: 0, Loss: 1.255698
Train - Epoch 80, Batch: 0, Loss: 1.250539
Train - Epoch 81, Batch: 0, Loss: 1.264468
Train - Epoch 82, Batch: 0, Loss: 1.258635
Train - Epoch 83, Batch: 0, Loss: 1.246415
Train - Epoch 84, Batch: 0, Loss: 1.251203
Train - Epoch 85, Batch: 0, Loss: 1.250579
Train - Epoch 86, Batch: 0, Loss: 1.249492
Train - Epoch 87, Batch: 0, Loss: 1.238360
Train - Epoch 88, Batch: 0, Loss: 1.241053
Train - Epoch 89, Batch: 0, Loss: 1.239082
Train - Epoch 90, Batch: 0, Loss: 1.221857
Train - Epoch 91, Batch: 0, Loss: 1.233418
Train - Epoch 92, Batch: 0, Loss: 1.209430
Train - Epoch 93, Batch: 0, Loss: 1.233286
Train - Epoch 94, Batch: 0, Loss: 1.218882
Train - Epoch 95, Batch: 0, Loss: 1.247571
Train - Epoch 96, Batch: 0, Loss: 1.217475
Train - Epoch 97, Batch: 0, Loss: 1.234190
Train - Epoch 98, Batch: 0, Loss: 1.202500
Train - Epoch 99, Batch: 0, Loss: 1.206712
Train - Epoch 100, Batch: 0, Loss: 1.222181
Train - Epoch 101, Batch: 0, Loss: 1.205661
Train - Epoch 102, Batch: 0, Loss: 1.231879
Train - Epoch 103, Batch: 0, Loss: 1.202043
Train - Epoch 104, Batch: 0, Loss: 1.210234
Train - Epoch 105, Batch: 0, Loss: 1.204188
Train - Epoch 106, Batch: 0, Loss: 1.206418
Train - Epoch 107, Batch: 0, Loss: 1.192887
Train - Epoch 108, Batch: 0, Loss: 1.201953
Train - Epoch 109, Batch: 0, Loss: 1.215830
Train - Epoch 110, Batch: 0, Loss: 1.190604
Train - Epoch 111, Batch: 0, Loss: 1.195518
Train - Epoch 112, Batch: 0, Loss: 1.195456
Train - Epoch 113, Batch: 0, Loss: 1.195061
Train - Epoch 114, Batch: 0, Loss: 1.182936
Train - Epoch 115, Batch: 0, Loss: 1.193664
Train - Epoch 116, Batch: 0, Loss: 1.188483
Train - Epoch 117, Batch: 0, Loss: 1.181708
Train - Epoch 118, Batch: 0, Loss: 1.175286
Train - Epoch 119, Batch: 0, Loss: 1.182183
Train - Epoch 120, Batch: 0, Loss: 1.178342
Train - Epoch 121, Batch: 0, Loss: 1.179869
Train - Epoch 122, Batch: 0, Loss: 1.177875
Train - Epoch 123, Batch: 0, Loss: 1.184225
Train - Epoch 124, Batch: 0, Loss: 1.168719
Train - Epoch 125, Batch: 0, Loss: 1.176252
Train - Epoch 126, Batch: 0, Loss: 1.173350
Train - Epoch 127, Batch: 0, Loss: 1.185412
Train - Epoch 128, Batch: 0, Loss: 1.169636
Train - Epoch 129, Batch: 0, Loss: 1.172424
Train - Epoch 130, Batch: 0, Loss: 1.166694
Train - Epoch 131, Batch: 0, Loss: 1.162330
Train - Epoch 132, Batch: 0, Loss: 1.159717
Train - Epoch 133, Batch: 0, Loss: 1.165662
Train - Epoch 134, Batch: 0, Loss: 1.186145
Train - Epoch 135, Batch: 0, Loss: 1.155767
Train - Epoch 136, Batch: 0, Loss: 1.170972
Train - Epoch 137, Batch: 0, Loss: 1.165259
Train - Epoch 138, Batch: 0, Loss: 1.166607
Train - Epoch 139, Batch: 0, Loss: 1.158077
Train - Epoch 140, Batch: 0, Loss: 1.156455
Train - Epoch 141, Batch: 0, Loss: 1.174215
Train - Epoch 142, Batch: 0, Loss: 1.164725
Train - Epoch 143, Batch: 0, Loss: 1.163101
Train - Epoch 144, Batch: 0, Loss: 1.164706
Train - Epoch 145, Batch: 0, Loss: 1.170360
Train - Epoch 146, Batch: 0, Loss: 1.151260
Train - Epoch 147, Batch: 0, Loss: 1.158118
Train - Epoch 148, Batch: 0, Loss: 1.153642
Train - Epoch 149, Batch: 0, Loss: 1.155688
Train - Epoch 150, Batch: 0, Loss: 1.160493
Train - Epoch 151, Batch: 0, Loss: 1.144602
Train - Epoch 152, Batch: 0, Loss: 1.141068
Train - Epoch 153, Batch: 0, Loss: 1.156372
Train - Epoch 154, Batch: 0, Loss: 1.162981
Train - Epoch 155, Batch: 0, Loss: 1.151547
Train - Epoch 156, Batch: 0, Loss: 1.148136
Train - Epoch 157, Batch: 0, Loss: 1.152938
Train - Epoch 158, Batch: 0, Loss: 1.156197
Train - Epoch 159, Batch: 0, Loss: 1.139000
Train - Epoch 160, Batch: 0, Loss: 1.144779
Train - Epoch 161, Batch: 0, Loss: 1.137436
Train - Epoch 162, Batch: 0, Loss: 1.141725
Train - Epoch 163, Batch: 0, Loss: 1.144560
Train - Epoch 164, Batch: 0, Loss: 1.146927
Train - Epoch 165, Batch: 0, Loss: 1.135272
Train - Epoch 166, Batch: 0, Loss: 1.129252
Train - Epoch 167, Batch: 0, Loss: 1.133709
Train - Epoch 168, Batch: 0, Loss: 1.128491
Train - Epoch 169, Batch: 0, Loss: 1.122603
Train - Epoch 170, Batch: 0, Loss: 1.138491
Train - Epoch 171, Batch: 0, Loss: 1.137134
Train - Epoch 172, Batch: 0, Loss: 1.136637
Train - Epoch 173, Batch: 0, Loss: 1.123537
Train - Epoch 174, Batch: 0, Loss: 1.124377
Train - Epoch 175, Batch: 0, Loss: 1.139137
Train - Epoch 176, Batch: 0, Loss: 1.144847
Train - Epoch 177, Batch: 0, Loss: 1.123734
Train - Epoch 178, Batch: 0, Loss: 1.126867
Train - Epoch 179, Batch: 0, Loss: 1.131375
Train - Epoch 180, Batch: 0, Loss: 1.129520
Train - Epoch 181, Batch: 0, Loss: 1.131771
Train - Epoch 182, Batch: 0, Loss: 1.115590
Train - Epoch 183, Batch: 0, Loss: 1.122360
Train - Epoch 184, Batch: 0, Loss: 1.139314
Train - Epoch 185, Batch: 0, Loss: 1.138201
Train - Epoch 186, Batch: 0, Loss: 1.130090
Train - Epoch 187, Batch: 0, Loss: 1.120273
Train - Epoch 188, Batch: 0, Loss: 1.130029
Train - Epoch 189, Batch: 0, Loss: 1.127233
Train - Epoch 190, Batch: 0, Loss: 1.123577
Train - Epoch 191, Batch: 0, Loss: 1.129359
Train - Epoch 192, Batch: 0, Loss: 1.119990
Train - Epoch 193, Batch: 0, Loss: 1.126807
Train - Epoch 194, Batch: 0, Loss: 1.128396
Train - Epoch 195, Batch: 0, Loss: 1.129542
Train - Epoch 196, Batch: 0, Loss: 1.112391
Train - Epoch 197, Batch: 0, Loss: 1.121861
Train - Epoch 198, Batch: 0, Loss: 1.118497
Train - Epoch 199, Batch: 0, Loss: 1.119147
Train - Epoch 200, Batch: 0, Loss: 1.111611
Train - Epoch 201, Batch: 0, Loss: 1.123968
Train - Epoch 202, Batch: 0, Loss: 1.109013
Train - Epoch 203, Batch: 0, Loss: 1.099794
Train - Epoch 204, Batch: 0, Loss: 1.115634
Train - Epoch 205, Batch: 0, Loss: 1.101899
Train - Epoch 206, Batch: 0, Loss: 1.113549
Train - Epoch 207, Batch: 0, Loss: 1.116007
Train - Epoch 208, Batch: 0, Loss: 1.111201
Train - Epoch 209, Batch: 0, Loss: 1.123304
Train - Epoch 210, Batch: 0, Loss: 1.120185
Train - Epoch 211, Batch: 0, Loss: 1.127081
Train - Epoch 212, Batch: 0, Loss: 1.109620
Train - Epoch 213, Batch: 0, Loss: 1.106690
Train - Epoch 214, Batch: 0, Loss: 1.119679
Train - Epoch 215, Batch: 0, Loss: 1.111197
Train - Epoch 216, Batch: 0, Loss: 1.116872
Train - Epoch 217, Batch: 0, Loss: 1.104190
Train - Epoch 218, Batch: 0, Loss: 1.104235
Train - Epoch 219, Batch: 0, Loss: 1.108207
Train - Epoch 220, Batch: 0, Loss: 1.120408
Train - Epoch 221, Batch: 0, Loss: 1.105928
Train - Epoch 222, Batch: 0, Loss: 1.110891
Train - Epoch 223, Batch: 0, Loss: 1.104928
Train - Epoch 224, Batch: 0, Loss: 1.104555
Train - Epoch 225, Batch: 0, Loss: 1.096772
Train - Epoch 226, Batch: 0, Loss: 1.114159
Train - Epoch 227, Batch: 0, Loss: 1.103015
Train - Epoch 228, Batch: 0, Loss: 1.099639
Train - Epoch 229, Batch: 0, Loss: 1.084067
Train - Epoch 230, Batch: 0, Loss: 1.107472
Train - Epoch 231, Batch: 0, Loss: 1.094719
Train - Epoch 232, Batch: 0, Loss: 1.105393
Train - Epoch 233, Batch: 0, Loss: 1.095455
Train - Epoch 234, Batch: 0, Loss: 1.102793
Train - Epoch 235, Batch: 0, Loss: 1.108155
Train - Epoch 236, Batch: 0, Loss: 1.112240
Train - Epoch 237, Batch: 0, Loss: 1.111027
Train - Epoch 238, Batch: 0, Loss: 1.106780
Train - Epoch 239, Batch: 0, Loss: 1.097529
Train - Epoch 240, Batch: 0, Loss: 1.093203
Train - Epoch 241, Batch: 0, Loss: 1.102755
Train - Epoch 242, Batch: 0, Loss: 1.114245
Train - Epoch 243, Batch: 0, Loss: 1.088828
Train - Epoch 244, Batch: 0, Loss: 1.100986
Train - Epoch 245, Batch: 0, Loss: 1.087918
Train - Epoch 246, Batch: 0, Loss: 1.080120
Train - Epoch 247, Batch: 0, Loss: 1.086840
Train - Epoch 248, Batch: 0, Loss: 1.090170
Train - Epoch 249, Batch: 0, Loss: 1.095068
Train - Epoch 250, Batch: 0, Loss: 1.103850
Train - Epoch 251, Batch: 0, Loss: 1.107126
Train - Epoch 252, Batch: 0, Loss: 1.073375
Train - Epoch 253, Batch: 0, Loss: 1.089442
Train - Epoch 254, Batch: 0, Loss: 1.088234
Train - Epoch 255, Batch: 0, Loss: 1.097843
Train - Epoch 256, Batch: 0, Loss: 1.099678
Train - Epoch 257, Batch: 0, Loss: 1.086897
Train - Epoch 258, Batch: 0, Loss: 1.091619
Train - Epoch 259, Batch: 0, Loss: 1.091345
Train - Epoch 260, Batch: 0, Loss: 1.090205
Train - Epoch 261, Batch: 0, Loss: 1.091505
Train - Epoch 262, Batch: 0, Loss: 1.101513
Train - Epoch 263, Batch: 0, Loss: 1.081113
Train - Epoch 264, Batch: 0, Loss: 1.083077
Train - Epoch 265, Batch: 0, Loss: 1.084976
Train - Epoch 266, Batch: 0, Loss: 1.095752
Train - Epoch 267, Batch: 0, Loss: 1.103974
Train - Epoch 268, Batch: 0, Loss: 1.089207
Train - Epoch 269, Batch: 0, Loss: 1.091287
Train - Epoch 270, Batch: 0, Loss: 1.095199
Train - Epoch 271, Batch: 0, Loss: 1.071734
Train - Epoch 272, Batch: 0, Loss: 1.091428
Train - Epoch 273, Batch: 0, Loss: 1.088964
Train - Epoch 274, Batch: 0, Loss: 1.071887
Train - Epoch 275, Batch: 0, Loss: 1.070620
Train - Epoch 276, Batch: 0, Loss: 1.080932
Train - Epoch 277, Batch: 0, Loss: 1.087661
Train - Epoch 278, Batch: 0, Loss: 1.078771
Train - Epoch 279, Batch: 0, Loss: 1.091946
Train - Epoch 280, Batch: 0, Loss: 1.083610
Train - Epoch 281, Batch: 0, Loss: 1.087401
Train - Epoch 282, Batch: 0, Loss: 1.088714
Train - Epoch 283, Batch: 0, Loss: 1.084845
Train - Epoch 284, Batch: 0, Loss: 1.084769
Train - Epoch 285, Batch: 0, Loss: 1.079556
Train - Epoch 286, Batch: 0, Loss: 1.082843
Train - Epoch 287, Batch: 0, Loss: 1.083671
Train - Epoch 288, Batch: 0, Loss: 1.081937
Train - Epoch 289, Batch: 0, Loss: 1.078945
Train - Epoch 290, Batch: 0, Loss: 1.077369
Train - Epoch 291, Batch: 0, Loss: 1.073616
Train - Epoch 292, Batch: 0, Loss: 1.075615
Train - Epoch 293, Batch: 0, Loss: 1.083050
Train - Epoch 294, Batch: 0, Loss: 1.080354
Train - Epoch 295, Batch: 0, Loss: 1.085537
Train - Epoch 296, Batch: 0, Loss: 1.080190
Train - Epoch 297, Batch: 0, Loss: 1.067361
Train - Epoch 298, Batch: 0, Loss: 1.067944
Train - Epoch 299, Batch: 0, Loss: 1.082203
Train - Epoch 300, Batch: 0, Loss: 1.086190
Train - Epoch 301, Batch: 0, Loss: 1.079189
Train - Epoch 302, Batch: 0, Loss: 1.075068
Train - Epoch 303, Batch: 0, Loss: 1.077842
Train - Epoch 304, Batch: 0, Loss: 1.088589
Train - Epoch 305, Batch: 0, Loss: 1.068888
Train - Epoch 306, Batch: 0, Loss: 1.079244
Train - Epoch 307, Batch: 0, Loss: 1.055722
Train - Epoch 308, Batch: 0, Loss: 1.083405
Train - Epoch 309, Batch: 0, Loss: 1.070855
Train - Epoch 310, Batch: 0, Loss: 1.076178
Train - Epoch 311, Batch: 0, Loss: 1.074171
Train - Epoch 312, Batch: 0, Loss: 1.098908
Train - Epoch 313, Batch: 0, Loss: 1.081679
Train - Epoch 314, Batch: 0, Loss: 1.085053
Train - Epoch 315, Batch: 0, Loss: 1.062602
Train - Epoch 316, Batch: 0, Loss: 1.078360
Train - Epoch 317, Batch: 0, Loss: 1.060440
Train - Epoch 318, Batch: 0, Loss: 1.078704
Train - Epoch 319, Batch: 0, Loss: 1.084068
Train - Epoch 320, Batch: 0, Loss: 1.077757
Train - Epoch 321, Batch: 0, Loss: 1.079176
Train - Epoch 322, Batch: 0, Loss: 1.070740
Train - Epoch 323, Batch: 0, Loss: 1.075128
Train - Epoch 324, Batch: 0, Loss: 1.074450
Train - Epoch 325, Batch: 0, Loss: 1.076534
Train - Epoch 326, Batch: 0, Loss: 1.067679
Train - Epoch 327, Batch: 0, Loss: 1.073901
Train - Epoch 328, Batch: 0, Loss: 1.053788
Train - Epoch 329, Batch: 0, Loss: 1.065412
Train - Epoch 330, Batch: 0, Loss: 1.076532
Train - Epoch 331, Batch: 0, Loss: 1.070829
Train - Epoch 332, Batch: 0, Loss: 1.068526
Train - Epoch 333, Batch: 0, Loss: 1.059774
Train - Epoch 334, Batch: 0, Loss: 1.044226
Train - Epoch 335, Batch: 0, Loss: 1.061692
Train - Epoch 336, Batch: 0, Loss: 1.065746
Train - Epoch 337, Batch: 0, Loss: 1.078636
Train - Epoch 338, Batch: 0, Loss: 1.063532
Train - Epoch 339, Batch: 0, Loss: 1.072997
Train - Epoch 340, Batch: 0, Loss: 1.071859
Train - Epoch 341, Batch: 0, Loss: 1.061212
Train - Epoch 342, Batch: 0, Loss: 1.077194
Train - Epoch 343, Batch: 0, Loss: 1.054460
Train - Epoch 344, Batch: 0, Loss: 1.065032
Train - Epoch 345, Batch: 0, Loss: 1.058977
Train - Epoch 346, Batch: 0, Loss: 1.070052
Train - Epoch 347, Batch: 0, Loss: 1.070521
Train - Epoch 348, Batch: 0, Loss: 1.073859
Train - Epoch 349, Batch: 0, Loss: 1.067817
Train - Epoch 350, Batch: 0, Loss: 1.056589
Train - Epoch 351, Batch: 0, Loss: 1.061706
Train - Epoch 352, Batch: 0, Loss: 1.079252
Train - Epoch 353, Batch: 0, Loss: 1.075432
Train - Epoch 354, Batch: 0, Loss: 1.073375
Train - Epoch 355, Batch: 0, Loss: 1.075393
Train - Epoch 356, Batch: 0, Loss: 1.077075
Train - Epoch 357, Batch: 0, Loss: 1.063781
Train - Epoch 358, Batch: 0, Loss: 1.079352
Train - Epoch 359, Batch: 0, Loss: 1.052906
Train - Epoch 360, Batch: 0, Loss: 1.057463
Train - Epoch 361, Batch: 0, Loss: 1.051217
Train - Epoch 362, Batch: 0, Loss: 1.068840
Train - Epoch 363, Batch: 0, Loss: 1.072092
Train - Epoch 364, Batch: 0, Loss: 1.050751
Train - Epoch 365, Batch: 0, Loss: 1.074849
Train - Epoch 366, Batch: 0, Loss: 1.053553
Train - Epoch 367, Batch: 0, Loss: 1.047558
Train - Epoch 368, Batch: 0, Loss: 1.058456
Train - Epoch 369, Batch: 0, Loss: 1.061588
Train - Epoch 370, Batch: 0, Loss: 1.054803
Train - Epoch 371, Batch: 0, Loss: 1.053395
Train - Epoch 372, Batch: 0, Loss: 1.071942
Train - Epoch 373, Batch: 0, Loss: 1.055373/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 374, Batch: 0, Loss: 1.075925
Train - Epoch 375, Batch: 0, Loss: 1.058458
Train - Epoch 376, Batch: 0, Loss: 1.053190
Train - Epoch 377, Batch: 0, Loss: 1.068161
Train - Epoch 378, Batch: 0, Loss: 1.074086
Train - Epoch 379, Batch: 0, Loss: 1.059269
Train - Epoch 380, Batch: 0, Loss: 1.057494
Train - Epoch 381, Batch: 0, Loss: 1.056439
Train - Epoch 382, Batch: 0, Loss: 1.060857
Train - Epoch 383, Batch: 0, Loss: 1.065684
Train - Epoch 384, Batch: 0, Loss: 1.046904
Train - Epoch 385, Batch: 0, Loss: 1.068133
Train - Epoch 386, Batch: 0, Loss: 1.068454
Train - Epoch 387, Batch: 0, Loss: 1.055796
Train - Epoch 388, Batch: 0, Loss: 1.050716
Train - Epoch 389, Batch: 0, Loss: 1.061784
Train - Epoch 390, Batch: 0, Loss: 1.054138
Train - Epoch 391, Batch: 0, Loss: 1.045224
Train - Epoch 392, Batch: 0, Loss: 1.061370
Train - Epoch 393, Batch: 0, Loss: 1.056998
Train - Epoch 394, Batch: 0, Loss: 1.079241
Train - Epoch 395, Batch: 0, Loss: 1.057223
Train - Epoch 396, Batch: 0, Loss: 1.053217
Train - Epoch 397, Batch: 0, Loss: 1.045181
Train - Epoch 398, Batch: 0, Loss: 1.059311
Train - Epoch 399, Batch: 0, Loss: 1.050912
Train - Epoch 400, Batch: 0, Loss: 1.057811
Train - Epoch 401, Batch: 0, Loss: 1.063413
Train - Epoch 402, Batch: 0, Loss: 1.069250
Train - Epoch 403, Batch: 0, Loss: 1.056348
Train - Epoch 404, Batch: 0, Loss: 1.056655
Train - Epoch 405, Batch: 0, Loss: 1.061587
Train - Epoch 406, Batch: 0, Loss: 1.064981
Train - Epoch 407, Batch: 0, Loss: 1.029126
Train - Epoch 408, Batch: 0, Loss: 1.058317
Train - Epoch 409, Batch: 0, Loss: 1.051806
Train - Epoch 410, Batch: 0, Loss: 1.055466
Train - Epoch 411, Batch: 0, Loss: 1.059995
Train - Epoch 412, Batch: 0, Loss: 1.056591
Train - Epoch 413, Batch: 0, Loss: 1.045239
Train - Epoch 414, Batch: 0, Loss: 1.040410
Train - Epoch 415, Batch: 0, Loss: 1.063464
Train - Epoch 416, Batch: 0, Loss: 1.054409
Train - Epoch 417, Batch: 0, Loss: 1.056516
Train - Epoch 418, Batch: 0, Loss: 1.054193
Train - Epoch 419, Batch: 0, Loss: 1.044140
Train - Epoch 420, Batch: 0, Loss: 1.043579
Train - Epoch 421, Batch: 0, Loss: 1.041075
Train - Epoch 422, Batch: 0, Loss: 1.047457
Train - Epoch 423, Batch: 0, Loss: 1.063755
Train - Epoch 424, Batch: 0, Loss: 1.059717
Train - Epoch 425, Batch: 0, Loss: 1.051817
Train - Epoch 426, Batch: 0, Loss: 1.043833
Train - Epoch 427, Batch: 0, Loss: 1.063483
Train - Epoch 428, Batch: 0, Loss: 1.055972
Train - Epoch 429, Batch: 0, Loss: 1.039789
Train - Epoch 430, Batch: 0, Loss: 1.053233
Train - Epoch 431, Batch: 0, Loss: 1.039117
Train - Epoch 432, Batch: 0, Loss: 1.025100
Train - Epoch 433, Batch: 0, Loss: 1.055880
Train - Epoch 434, Batch: 0, Loss: 1.060629
Train - Epoch 435, Batch: 0, Loss: 1.031987
Train - Epoch 436, Batch: 0, Loss: 1.039967
Train - Epoch 437, Batch: 0, Loss: 1.056035
Train - Epoch 438, Batch: 0, Loss: 1.038860
Train - Epoch 439, Batch: 0, Loss: 1.061747
Train - Epoch 440, Batch: 0, Loss: 1.047266
Train - Epoch 441, Batch: 0, Loss: 1.051343
Train - Epoch 442, Batch: 0, Loss: 1.046301
Train - Epoch 443, Batch: 0, Loss: 1.059829
Train - Epoch 444, Batch: 0, Loss: 1.048293
Train - Epoch 445, Batch: 0, Loss: 1.049856
Train - Epoch 446, Batch: 0, Loss: 1.034201
Train - Epoch 447, Batch: 0, Loss: 1.043059
Train - Epoch 448, Batch: 0, Loss: 1.047534
Train - Epoch 449, Batch: 0, Loss: 1.040619
Train - Epoch 450, Batch: 0, Loss: 1.036145
Train - Epoch 451, Batch: 0, Loss: 1.039461
Train - Epoch 452, Batch: 0, Loss: 1.024326
Train - Epoch 453, Batch: 0, Loss: 1.045194
Train - Epoch 454, Batch: 0, Loss: 1.046854
Train - Epoch 455, Batch: 0, Loss: 1.037200
Train - Epoch 456, Batch: 0, Loss: 1.035548
Train - Epoch 457, Batch: 0, Loss: 1.033427
Train - Epoch 458, Batch: 0, Loss: 1.040790
Train - Epoch 459, Batch: 0, Loss: 1.054482
Train - Epoch 460, Batch: 0, Loss: 1.039673
Train - Epoch 461, Batch: 0, Loss: 1.044647
Train - Epoch 462, Batch: 0, Loss: 1.029669
Train - Epoch 463, Batch: 0, Loss: 1.035035
Train - Epoch 464, Batch: 0, Loss: 1.047077
Train - Epoch 465, Batch: 0, Loss: 1.067767
Train - Epoch 466, Batch: 0, Loss: 1.035003
Train - Epoch 467, Batch: 0, Loss: 1.045501
Train - Epoch 468, Batch: 0, Loss: 1.047440
Train - Epoch 469, Batch: 0, Loss: 1.052096
Train - Epoch 470, Batch: 0, Loss: 1.031339
Train - Epoch 471, Batch: 0, Loss: 1.044672
Train - Epoch 472, Batch: 0, Loss: 1.045526
Train - Epoch 473, Batch: 0, Loss: 1.038124
Train - Epoch 474, Batch: 0, Loss: 1.032368
Train - Epoch 475, Batch: 0, Loss: 1.043210
Train - Epoch 476, Batch: 0, Loss: 1.051373
Train - Epoch 477, Batch: 0, Loss: 1.042836
Train - Epoch 478, Batch: 0, Loss: 1.055557
Train - Epoch 479, Batch: 0, Loss: 1.029611
Train - Epoch 480, Batch: 0, Loss: 1.069000
Train - Epoch 481, Batch: 0, Loss: 1.049232
Train - Epoch 482, Batch: 0, Loss: 1.043511
Train - Epoch 483, Batch: 0, Loss: 1.049180
Train - Epoch 484, Batch: 0, Loss: 1.038140
Train - Epoch 485, Batch: 0, Loss: 1.035915
Train - Epoch 486, Batch: 0, Loss: 1.035133
Train - Epoch 487, Batch: 0, Loss: 1.022876
Train - Epoch 488, Batch: 0, Loss: 1.037432
Train - Epoch 489, Batch: 0, Loss: 1.031474
Train - Epoch 490, Batch: 0, Loss: 1.041215
Train - Epoch 491, Batch: 0, Loss: 1.040860
Train - Epoch 492, Batch: 0, Loss: 1.027126
Train - Epoch 493, Batch: 0, Loss: 1.033638
Train - Epoch 494, Batch: 0, Loss: 1.043807
Train - Epoch 495, Batch: 0, Loss: 1.040824
Train - Epoch 496, Batch: 0, Loss: 1.042752
Train - Epoch 497, Batch: 0, Loss: 1.040996
Train - Epoch 498, Batch: 0, Loss: 1.023282
Train - Epoch 499, Batch: 0, Loss: 1.028125
training_time:: 106.67527484893799
training time full:: 106.67534899711609
provenance prepare time:: 2.384185791015625e-07
here
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.631900
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 500
max_epoch:: 500
batch_size:: 10000
epoch:: 0
tensor([    1,     6, 32775, 32776, 32781, 32783,    31,    33, 32802,    43,
        32811,    45,    46, 32815, 32820,    53,    54, 32823,    55, 32826,
        32829, 32831, 32834,    75, 32844, 32856,    88,    94, 32864,    98,
          104, 32874,   107, 32876,   112,   113,   118, 32886, 32893, 32896,
          131,   136, 32908,   144, 32913,   158,   159, 32927,   164, 32933])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
epoch:: 60
epoch:: 61
epoch:: 62
epoch:: 63
epoch:: 64
epoch:: 65
epoch:: 66
epoch:: 67
epoch:: 68
epoch:: 69
epoch:: 70
epoch:: 71
epoch:: 72
epoch:: 73
epoch:: 74
epoch:: 75
epoch:: 76
epoch:: 77
epoch:: 78
epoch:: 79
epoch:: 80
epoch:: 81
epoch:: 82
epoch:: 83
epoch:: 84
epoch:: 85
epoch:: 86
epoch:: 87
epoch:: 88
epoch:: 89
epoch:: 90
epoch:: 91
epoch:: 92
epoch:: 93
epoch:: 94
epoch:: 95
epoch:: 96
epoch:: 97
epoch:: 98
epoch:: 99
epoch:: 100
epoch:: 101
epoch:: 102
epoch:: 103
epoch:: 104
epoch:: 105
epoch:: 106
epoch:: 107
epoch:: 108
epoch:: 109
epoch:: 110
epoch:: 111
epoch:: 112
epoch:: 113
epoch:: 114
epoch:: 115
epoch:: 116
epoch:: 117
epoch:: 118
epoch:: 119
epoch:: 120
epoch:: 121
epoch:: 122
epoch:: 123
epoch:: 124
epoch:: 125
epoch:: 126
epoch:: 127
epoch:: 128
epoch:: 129
epoch:: 130
epoch:: 131
epoch:: 132
epoch:: 133
epoch:: 134
epoch:: 135
epoch:: 136
epoch:: 137
epoch:: 138
epoch:: 139
epoch:: 140
epoch:: 141
epoch:: 142
epoch:: 143
epoch:: 144
epoch:: 145
epoch:: 146
epoch:: 147
epoch:: 148
epoch:: 149
epoch:: 150
epoch:: 151
epoch:: 152
epoch:: 153
epoch:: 154
epoch:: 155
epoch:: 156
epoch:: 157
epoch:: 158
epoch:: 159
epoch:: 160
epoch:: 161
epoch:: 162
epoch:: 163
epoch:: 164
epoch:: 165
epoch:: 166
epoch:: 167
epoch:: 168
epoch:: 169
epoch:: 170
epoch:: 171
epoch:: 172
epoch:: 173
epoch:: 174
epoch:: 175
epoch:: 176
epoch:: 177
epoch:: 178
epoch:: 179
epoch:: 180
epoch:: 181
epoch:: 182
epoch:: 183
epoch:: 184
epoch:: 185
epoch:: 186
epoch:: 187
epoch:: 188
epoch:: 189
epoch:: 190
epoch:: 191
epoch:: 192
epoch:: 193
epoch:: 194
epoch:: 195
epoch:: 196
epoch:: 197
epoch:: 198
epoch:: 199
epoch:: 200
epoch:: 201
epoch:: 202
epoch:: 203
epoch:: 204
epoch:: 205
epoch:: 206
epoch:: 207
epoch:: 208
epoch:: 209
epoch:: 210
epoch:: 211
epoch:: 212
epoch:: 213
epoch:: 214
epoch:: 215
epoch:: 216
epoch:: 217
epoch:: 218
epoch:: 219
epoch:: 220
epoch:: 221
epoch:: 222
epoch:: 223
epoch:: 224
epoch:: 225
epoch:: 226
epoch:: 227
epoch:: 228
epoch:: 229
epoch:: 230
epoch:: 231
epoch:: 232
epoch:: 233
epoch:: 234
epoch:: 235
epoch:: 236
epoch:: 237
epoch:: 238
epoch:: 239
epoch:: 240
epoch:: 241
epoch:: 242
epoch:: 243
epoch:: 244
epoch:: 245
epoch:: 246
epoch:: 247
epoch:: 248
epoch:: 249
epoch:: 250
epoch:: 251
epoch:: 252
epoch:: 253
epoch:: 254
epoch:: 255
epoch:: 256
epoch:: 257
epoch:: 258
epoch:: 259
epoch:: 260
epoch:: 261
epoch:: 262
epoch:: 263
epoch:: 264
epoch:: 265
epoch:: 266
epoch:: 267
epoch:: 268
epoch:: 269
epoch:: 270
epoch:: 271
epoch:: 272
epoch:: 273
epoch:: 274
epoch:: 275
epoch:: 276
epoch:: 277
epoch:: 278
epoch:: 279
epoch:: 280
epoch:: 281
epoch:: 282
epoch:: 283
epoch:: 284
epoch:: 285
epoch:: 286
epoch:: 287
epoch:: 288
epoch:: 289
epoch:: 290
epoch:: 291
epoch:: 292
epoch:: 293
epoch:: 294
epoch:: 295
epoch:: 296
epoch:: 297
epoch:: 298
epoch:: 299
epoch:: 300
epoch:: 301
epoch:: 302
epoch:: 303
epoch:: 304
epoch:: 305
epoch:: 306
epoch:: 307
epoch:: 308
epoch:: 309
epoch:: 310
epoch:: 311
epoch:: 312
epoch:: 313
epoch:: 314
epoch:: 315
epoch:: 316
epoch:: 317
epoch:: 318
epoch:: 319
epoch:: 320
epoch:: 321
epoch:: 322
epoch:: 323
epoch:: 324
epoch:: 325
epoch:: 326
epoch:: 327
epoch:: 328
epoch:: 329
epoch:: 330
epoch:: 331
epoch:: 332
epoch:: 333
epoch:: 334
epoch:: 335
epoch:: 336
epoch:: 337
epoch:: 338
epoch:: 339
epoch:: 340
epoch:: 341
epoch:: 342
epoch:: 343
epoch:: 344
epoch:: 345
epoch:: 346
epoch:: 347
epoch:: 348
epoch:: 349
epoch:: 350
epoch:: 351
epoch:: 352
epoch:: 353
epoch:: 354
epoch:: 355
epoch:: 356
epoch:: 357
epoch:: 358
epoch:: 359
epoch:: 360
epoch:: 361
epoch:: 362
epoch:: 363
epoch:: 364
epoch:: 365
epoch:: 366
epoch:: 367
epoch:: 368
epoch:: 369
epoch:: 370
epoch:: 371
epoch:: 372
epoch:: 373
epoch:: 374
epoch:: 375
epoch:: 376
epoch:: 377
epoch:: 378
epoch:: 379
epoch:: 380
epoch:: 381
epoch:: 382
epoch:: 383
epoch:: 384
epoch:: 385
epoch:: 386
epoch:: 387
epoch:: 388
epoch:: 389
epoch:: 390
epoch:: 391
epoch:: 392
epoch:: 393
epoch:: 394
epoch:: 395
epoch:: 396
epoch:: 397
epoch:: 398
epoch:: 399
epoch:: 400
epoch:: 401
epoch:: 402
epoch:: 403
epoch:: 404
epoch:: 405
epoch:: 406
epoch:: 407
epoch:: 408
epoch:: 409
epoch:: 410
epoch:: 411
epoch:: 412
epoch:: 413
epoch:: 414
epoch:: 415
epoch:: 416
epoch:: 417
epoch:: 418
epoch:: 419
epoch:: 420
epoch:: 421
epoch:: 422
epoch:: 423
epoch:: 424
epoch:: 425
epoch:: 426
epoch:: 427
epoch:: 428
epoch:: 429
epoch:: 430
epoch:: 431
epoch:: 432
epoch:: 433
epoch:: 434
epoch:: 435
epoch:: 436
epoch:: 437
epoch:: 438
epoch:: 439
epoch:: 440
epoch:: 441
epoch:: 442
epoch:: 443
epoch:: 444
epoch:: 445
epoch:: 446
epoch:: 447
epoch:: 448
epoch:: 449
epoch:: 450
epoch:: 451
epoch:: 452
epoch:: 453
epoch:: 454
epoch:: 455
epoch:: 456
epoch:: 457
epoch:: 458
epoch:: 459
epoch:: 460
epoch:: 461
epoch:: 462
epoch:: 463
epoch:: 464
epoch:: 465
epoch:: 466
epoch:: 467
epoch:: 468
epoch:: 469
epoch:: 470
epoch:: 471
epoch:: 472
epoch:: 473
epoch:: 474
epoch:: 475
epoch:: 476
epoch:: 477
epoch:: 478
epoch:: 479
epoch:: 480
epoch:: 481
epoch:: 482
epoch:: 483
epoch:: 484
epoch:: 485
epoch:: 486
epoch:: 487
epoch:: 488
epoch:: 489
epoch:: 490
epoch:: 491
epoch:: 492
epoch:: 493
epoch:: 494
epoch:: 495
epoch:: 496
epoch:: 497
epoch:: 498
epoch:: 499
training time is 80.47002243995667
overhead:: 0
overhead2:: 2.6795778274536133
overhead3:: 0
time_baseline:: 80.47006559371948
curr_diff: 0 tensor(0.1926, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1926, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632100
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 500
max_epoch:: 500
overhead:: 0
overhead2:: 0.08996915817260742
overhead3:: 0.2565493583679199
overhead4:: 9.487671136856079
overhead5:: 0
memory usage:: 5646565376
time_provenance:: 17.93430805206299
curr_diff: 0 tensor(0.0462, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0462, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.2278, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.2278, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632100
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 500
max_epoch:: 500
overhead:: 0
overhead2:: 0.09293198585510254
overhead3:: 0.2632594108581543
overhead4:: 9.82503366470337
overhead5:: 0
memory usage:: 5630963712
time_provenance:: 18.305343627929688
curr_diff: 0 tensor(0.0462, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0462, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.2278, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.2278, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632100
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 500
max_epoch:: 500
overhead:: 0
overhead2:: 0.10130620002746582
overhead3:: 0.27074337005615234
overhead4:: 10.22381329536438
overhead5:: 0
memory usage:: 5635588096
time_provenance:: 18.83301830291748
curr_diff: 0 tensor(0.0463, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0463, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.2279, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.2279, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632100
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 500
max_epoch:: 500
overhead:: 0
overhead2:: 0.18586111068725586
overhead3:: 0.4339771270751953
overhead4:: 17.341529846191406
overhead5:: 0
memory usage:: 5695561728
time_provenance:: 27.876131057739258
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1959, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1959, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632000
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 500
max_epoch:: 500
overhead:: 0
overhead2:: 0.19180893898010254
overhead3:: 0.43548035621643066
overhead4:: 17.084964752197266
overhead5:: 0
memory usage:: 5655027712
time_provenance:: 27.74198293685913
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1959, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1959, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632000
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 500
max_epoch:: 500
overhead:: 0
overhead2:: 0.1809251308441162
overhead3:: 0.4593691825866699
overhead4:: 17.628355503082275
overhead5:: 0
memory usage:: 5627625472
time_provenance:: 28.306265354156494
curr_diff: 0 tensor(0.0049, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0049, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1961, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1961, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632300
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 500
max_epoch:: 500
overhead:: 0
overhead2:: 0.4205348491668701
overhead3:: 1.0460107326507568
overhead4:: 40.79415678977966
overhead5:: 0
memory usage:: 5626052608
time_provenance:: 57.71586298942566
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1928, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1928, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632000
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 500
max_epoch:: 500
overhead:: 0
overhead2:: 0.41654348373413086
overhead3:: 1.0441083908081055
overhead4:: 41.152352809906006
overhead5:: 0
memory usage:: 5640175616
time_provenance:: 58.094801902770996
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1928, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1928, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632000
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 500
max_epoch:: 500
overhead:: 0
overhead2:: 0.43682265281677246
overhead3:: 1.0011687278747559
overhead4:: 40.42982745170593
overhead5:: 0
memory usage:: 5637439488
time_provenance:: 57.37658429145813
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1928, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1928, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632000
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 2 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 500
max_epoch:: 500
overhead:: 0
overhead2:: 0.8320865631103516
overhead3:: 2.0787417888641357
overhead4:: 81.3585901260376
overhead5:: 0
memory usage:: 5636784128
time_provenance:: 106.54142355918884
curr_diff: 0 tensor(4.9965e-13, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.9965e-13, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1926, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1926, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632100
repetition 3
python3 benchmark_exp_lr.py 0.001 10000 500 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05] Logistic_regression cifar10_2 3 0.0001 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 2.310333
Train - Epoch 1, Batch: 0, Loss: 2.249572
Train - Epoch 2, Batch: 0, Loss: 2.193284
Train - Epoch 3, Batch: 0, Loss: 2.143152
Train - Epoch 4, Batch: 0, Loss: 2.096220
Train - Epoch 5, Batch: 0, Loss: 2.051053
Train - Epoch 6, Batch: 0, Loss: 2.008712
Train - Epoch 7, Batch: 0, Loss: 1.972420
Train - Epoch 8, Batch: 0, Loss: 1.935610
Train - Epoch 9, Batch: 0, Loss: 1.903063
Train - Epoch 10, Batch: 0, Loss: 1.871927
Train - Epoch 11, Batch: 0, Loss: 1.841944
Train - Epoch 12, Batch: 0, Loss: 1.814243
Train - Epoch 13, Batch: 0, Loss: 1.788948
Train - Epoch 14, Batch: 0, Loss: 1.755219
Train - Epoch 15, Batch: 0, Loss: 1.737936
Train - Epoch 16, Batch: 0, Loss: 1.719449
Train - Epoch 17, Batch: 0, Loss: 1.691886
Train - Epoch 18, Batch: 0, Loss: 1.675781
Train - Epoch 19, Batch: 0, Loss: 1.662484
Train - Epoch 20, Batch: 0, Loss: 1.644467
Train - Epoch 21, Batch: 0, Loss: 1.621012
Train - Epoch 22, Batch: 0, Loss: 1.616756
Train - Epoch 23, Batch: 0, Loss: 1.591674
Train - Epoch 24, Batch: 0, Loss: 1.580711
Train - Epoch 25, Batch: 0, Loss: 1.569030
Train - Epoch 26, Batch: 0, Loss: 1.544979
Train - Epoch 27, Batch: 0, Loss: 1.546253
Train - Epoch 28, Batch: 0, Loss: 1.531030
Train - Epoch 29, Batch: 0, Loss: 1.527791
Train - Epoch 30, Batch: 0, Loss: 1.508076
Train - Epoch 31, Batch: 0, Loss: 1.501910
Train - Epoch 32, Batch: 0, Loss: 1.489081
Train - Epoch 33, Batch: 0, Loss: 1.480048
Train - Epoch 34, Batch: 0, Loss: 1.483916
Train - Epoch 35, Batch: 0, Loss: 1.458900
Train - Epoch 36, Batch: 0, Loss: 1.460705
Train - Epoch 37, Batch: 0, Loss: 1.443609
Train - Epoch 38, Batch: 0, Loss: 1.440513
Train - Epoch 39, Batch: 0, Loss: 1.435247
Train - Epoch 40, Batch: 0, Loss: 1.418730
Train - Epoch 41, Batch: 0, Loss: 1.419088
Train - Epoch 42, Batch: 0, Loss: 1.401781
Train - Epoch 43, Batch: 0, Loss: 1.409531
Train - Epoch 44, Batch: 0, Loss: 1.406656
Train - Epoch 45, Batch: 0, Loss: 1.384003
Train - Epoch 46, Batch: 0, Loss: 1.381633
Train - Epoch 47, Batch: 0, Loss: 1.372308
Train - Epoch 48, Batch: 0, Loss: 1.376110
Train - Epoch 49, Batch: 0, Loss: 1.364694
Train - Epoch 50, Batch: 0, Loss: 1.357306
Train - Epoch 51, Batch: 0, Loss: 1.356557
Train - Epoch 52, Batch: 0, Loss: 1.350568
Train - Epoch 53, Batch: 0, Loss: 1.369241
Train - Epoch 54, Batch: 0, Loss: 1.339067
Train - Epoch 55, Batch: 0, Loss: 1.344793
Train - Epoch 56, Batch: 0, Loss: 1.337271
Train - Epoch 57, Batch: 0, Loss: 1.324584
Train - Epoch 58, Batch: 0, Loss: 1.324313
Train - Epoch 59, Batch: 0, Loss: 1.318275
Train - Epoch 60, Batch: 0, Loss: 1.313232
Train - Epoch 61, Batch: 0, Loss: 1.309935
Train - Epoch 62, Batch: 0, Loss: 1.315561
Train - Epoch 63, Batch: 0, Loss: 1.306026
Train - Epoch 64, Batch: 0, Loss: 1.301966
Train - Epoch 65, Batch: 0, Loss: 1.307806
Train - Epoch 66, Batch: 0, Loss: 1.291097
Train - Epoch 67, Batch: 0, Loss: 1.292955
Train - Epoch 68, Batch: 0, Loss: 1.286263
Train - Epoch 69, Batch: 0, Loss: 1.289627
Train - Epoch 70, Batch: 0, Loss: 1.289459
Train - Epoch 71, Batch: 0, Loss: 1.271002
Train - Epoch 72, Batch: 0, Loss: 1.268928
Train - Epoch 73, Batch: 0, Loss: 1.260605
Train - Epoch 74, Batch: 0, Loss: 1.269325
Train - Epoch 75, Batch: 0, Loss: 1.274673
Train - Epoch 76, Batch: 0, Loss: 1.272500
Train - Epoch 77, Batch: 0, Loss: 1.266593
Train - Epoch 78, Batch: 0, Loss: 1.266499
Train - Epoch 79, Batch: 0, Loss: 1.271081
Train - Epoch 80, Batch: 0, Loss: 1.251846
Train - Epoch 81, Batch: 0, Loss: 1.244060
Train - Epoch 82, Batch: 0, Loss: 1.253759
Train - Epoch 83, Batch: 0, Loss: 1.243436
Train - Epoch 84, Batch: 0, Loss: 1.244597
Train - Epoch 85, Batch: 0, Loss: 1.250816
Train - Epoch 86, Batch: 0, Loss: 1.256214
Train - Epoch 87, Batch: 0, Loss: 1.246719
Train - Epoch 88, Batch: 0, Loss: 1.229017
Train - Epoch 89, Batch: 0, Loss: 1.238250
Train - Epoch 90, Batch: 0, Loss: 1.230602
Train - Epoch 91, Batch: 0, Loss: 1.230040
Train - Epoch 92, Batch: 0, Loss: 1.241353
Train - Epoch 93, Batch: 0, Loss: 1.218250
Train - Epoch 94, Batch: 0, Loss: 1.225056
Train - Epoch 95, Batch: 0, Loss: 1.241273
Train - Epoch 96, Batch: 0, Loss: 1.232800
Train - Epoch 97, Batch: 0, Loss: 1.220450
Train - Epoch 98, Batch: 0, Loss: 1.223376
Train - Epoch 99, Batch: 0, Loss: 1.227230
Train - Epoch 100, Batch: 0, Loss: 1.208474
Train - Epoch 101, Batch: 0, Loss: 1.200688
Train - Epoch 102, Batch: 0, Loss: 1.215603
Train - Epoch 103, Batch: 0, Loss: 1.211771
Train - Epoch 104, Batch: 0, Loss: 1.202832
Train - Epoch 105, Batch: 0, Loss: 1.196114
Train - Epoch 106, Batch: 0, Loss: 1.213553
Train - Epoch 107, Batch: 0, Loss: 1.206635
Train - Epoch 108, Batch: 0, Loss: 1.203623
Train - Epoch 109, Batch: 0, Loss: 1.189505
Train - Epoch 110, Batch: 0, Loss: 1.198541
Train - Epoch 111, Batch: 0, Loss: 1.198587
Train - Epoch 112, Batch: 0, Loss: 1.187104
Train - Epoch 113, Batch: 0, Loss: 1.188495
Train - Epoch 114, Batch: 0, Loss: 1.193500
Train - Epoch 115, Batch: 0, Loss: 1.197168
Train - Epoch 116, Batch: 0, Loss: 1.182444
Train - Epoch 117, Batch: 0, Loss: 1.193342
Train - Epoch 118, Batch: 0, Loss: 1.187948
Train - Epoch 119, Batch: 0, Loss: 1.170960
Train - Epoch 120, Batch: 0, Loss: 1.187278
Train - Epoch 121, Batch: 0, Loss: 1.192462
Train - Epoch 122, Batch: 0, Loss: 1.196708
Train - Epoch 123, Batch: 0, Loss: 1.189227
Train - Epoch 124, Batch: 0, Loss: 1.172572
Train - Epoch 125, Batch: 0, Loss: 1.186696
Train - Epoch 126, Batch: 0, Loss: 1.185605
Train - Epoch 127, Batch: 0, Loss: 1.196624
Train - Epoch 128, Batch: 0, Loss: 1.185686
Train - Epoch 129, Batch: 0, Loss: 1.158773
Train - Epoch 130, Batch: 0, Loss: 1.162672
Train - Epoch 131, Batch: 0, Loss: 1.166204
Train - Epoch 132, Batch: 0, Loss: 1.178583
Train - Epoch 133, Batch: 0, Loss: 1.165947
Train - Epoch 134, Batch: 0, Loss: 1.157753
Train - Epoch 135, Batch: 0, Loss: 1.169634
Train - Epoch 136, Batch: 0, Loss: 1.161125
Train - Epoch 137, Batch: 0, Loss: 1.173590
Train - Epoch 138, Batch: 0, Loss: 1.159801
Train - Epoch 139, Batch: 0, Loss: 1.154727
Train - Epoch 140, Batch: 0, Loss: 1.169779
Train - Epoch 141, Batch: 0, Loss: 1.174735
Train - Epoch 142, Batch: 0, Loss: 1.153080
Train - Epoch 143, Batch: 0, Loss: 1.163037
Train - Epoch 144, Batch: 0, Loss: 1.155049
Train - Epoch 145, Batch: 0, Loss: 1.164374
Train - Epoch 146, Batch: 0, Loss: 1.163115
Train - Epoch 147, Batch: 0, Loss: 1.152955
Train - Epoch 148, Batch: 0, Loss: 1.163426
Train - Epoch 149, Batch: 0, Loss: 1.155488
Train - Epoch 150, Batch: 0, Loss: 1.152605
Train - Epoch 151, Batch: 0, Loss: 1.158193
Train - Epoch 152, Batch: 0, Loss: 1.142752
Train - Epoch 153, Batch: 0, Loss: 1.144489
Train - Epoch 154, Batch: 0, Loss: 1.156852
Train - Epoch 155, Batch: 0, Loss: 1.155375
Train - Epoch 156, Batch: 0, Loss: 1.154106
Train - Epoch 157, Batch: 0, Loss: 1.148861
Train - Epoch 158, Batch: 0, Loss: 1.140329
Train - Epoch 159, Batch: 0, Loss: 1.144152
Train - Epoch 160, Batch: 0, Loss: 1.143475
Train - Epoch 161, Batch: 0, Loss: 1.139369
Train - Epoch 162, Batch: 0, Loss: 1.139755
Train - Epoch 163, Batch: 0, Loss: 1.144054
Train - Epoch 164, Batch: 0, Loss: 1.155834
Train - Epoch 165, Batch: 0, Loss: 1.145425
Train - Epoch 166, Batch: 0, Loss: 1.149508
Train - Epoch 167, Batch: 0, Loss: 1.144296
Train - Epoch 168, Batch: 0, Loss: 1.145362
Train - Epoch 169, Batch: 0, Loss: 1.136253
Train - Epoch 170, Batch: 0, Loss: 1.147524
Train - Epoch 171, Batch: 0, Loss: 1.139148
Train - Epoch 172, Batch: 0, Loss: 1.130196
Train - Epoch 173, Batch: 0, Loss: 1.143253
Train - Epoch 174, Batch: 0, Loss: 1.132846
Train - Epoch 175, Batch: 0, Loss: 1.133597
Train - Epoch 176, Batch: 0, Loss: 1.126584
Train - Epoch 177, Batch: 0, Loss: 1.118464
Train - Epoch 178, Batch: 0, Loss: 1.126883
Train - Epoch 179, Batch: 0, Loss: 1.122754
Train - Epoch 180, Batch: 0, Loss: 1.132262
Train - Epoch 181, Batch: 0, Loss: 1.113534
Train - Epoch 182, Batch: 0, Loss: 1.118628
Train - Epoch 183, Batch: 0, Loss: 1.133681
Train - Epoch 184, Batch: 0, Loss: 1.139956
Train - Epoch 185, Batch: 0, Loss: 1.110786
Train - Epoch 186, Batch: 0, Loss: 1.123573
Train - Epoch 187, Batch: 0, Loss: 1.125805
Train - Epoch 188, Batch: 0, Loss: 1.116127
Train - Epoch 189, Batch: 0, Loss: 1.134238
Train - Epoch 190, Batch: 0, Loss: 1.123719
Train - Epoch 191, Batch: 0, Loss: 1.123723
Train - Epoch 192, Batch: 0, Loss: 1.125436
Train - Epoch 193, Batch: 0, Loss: 1.126998
Train - Epoch 194, Batch: 0, Loss: 1.125938
Train - Epoch 195, Batch: 0, Loss: 1.126166
Train - Epoch 196, Batch: 0, Loss: 1.125619
Train - Epoch 197, Batch: 0, Loss: 1.113581
Train - Epoch 198, Batch: 0, Loss: 1.124562
Train - Epoch 199, Batch: 0, Loss: 1.118871
Train - Epoch 200, Batch: 0, Loss: 1.113437
Train - Epoch 201, Batch: 0, Loss: 1.111794
Train - Epoch 202, Batch: 0, Loss: 1.117080
Train - Epoch 203, Batch: 0, Loss: 1.130472
Train - Epoch 204, Batch: 0, Loss: 1.126320
Train - Epoch 205, Batch: 0, Loss: 1.108348
Train - Epoch 206, Batch: 0, Loss: 1.129741
Train - Epoch 207, Batch: 0, Loss: 1.117493
Train - Epoch 208, Batch: 0, Loss: 1.118700
Train - Epoch 209, Batch: 0, Loss: 1.111300
Train - Epoch 210, Batch: 0, Loss: 1.115302
Train - Epoch 211, Batch: 0, Loss: 1.107999
Train - Epoch 212, Batch: 0, Loss: 1.123513
Train - Epoch 213, Batch: 0, Loss: 1.109525
Train - Epoch 214, Batch: 0, Loss: 1.111682
Train - Epoch 215, Batch: 0, Loss: 1.123084
Train - Epoch 216, Batch: 0, Loss: 1.111563
Train - Epoch 217, Batch: 0, Loss: 1.105883
Train - Epoch 218, Batch: 0, Loss: 1.113633
Train - Epoch 219, Batch: 0, Loss: 1.114349
Train - Epoch 220, Batch: 0, Loss: 1.111465
Train - Epoch 221, Batch: 0, Loss: 1.105401
Train - Epoch 222, Batch: 0, Loss: 1.096600
Train - Epoch 223, Batch: 0, Loss: 1.109067
Train - Epoch 224, Batch: 0, Loss: 1.101528
Train - Epoch 225, Batch: 0, Loss: 1.110009
Train - Epoch 226, Batch: 0, Loss: 1.109628
Train - Epoch 227, Batch: 0, Loss: 1.111357
Train - Epoch 228, Batch: 0, Loss: 1.098250
Train - Epoch 229, Batch: 0, Loss: 1.111473
Train - Epoch 230, Batch: 0, Loss: 1.091385
Train - Epoch 231, Batch: 0, Loss: 1.090780
Train - Epoch 232, Batch: 0, Loss: 1.096636
Train - Epoch 233, Batch: 0, Loss: 1.097548
Train - Epoch 234, Batch: 0, Loss: 1.090663
Train - Epoch 235, Batch: 0, Loss: 1.098518
Train - Epoch 236, Batch: 0, Loss: 1.099380
Train - Epoch 237, Batch: 0, Loss: 1.106180
Train - Epoch 238, Batch: 0, Loss: 1.097964
Train - Epoch 239, Batch: 0, Loss: 1.097903
Train - Epoch 240, Batch: 0, Loss: 1.111549
Train - Epoch 241, Batch: 0, Loss: 1.099894
Train - Epoch 242, Batch: 0, Loss: 1.094318
Train - Epoch 243, Batch: 0, Loss: 1.116382
Train - Epoch 244, Batch: 0, Loss: 1.096343
Train - Epoch 245, Batch: 0, Loss: 1.086582
Train - Epoch 246, Batch: 0, Loss: 1.093729
Train - Epoch 247, Batch: 0, Loss: 1.090977
Train - Epoch 248, Batch: 0, Loss: 1.089737
Train - Epoch 249, Batch: 0, Loss: 1.084479
Train - Epoch 250, Batch: 0, Loss: 1.085178
Train - Epoch 251, Batch: 0, Loss: 1.106675
Train - Epoch 252, Batch: 0, Loss: 1.086154
Train - Epoch 253, Batch: 0, Loss: 1.100786
Train - Epoch 254, Batch: 0, Loss: 1.092786
Train - Epoch 255, Batch: 0, Loss: 1.107433
Train - Epoch 256, Batch: 0, Loss: 1.086180
Train - Epoch 257, Batch: 0, Loss: 1.085816
Train - Epoch 258, Batch: 0, Loss: 1.081259
Train - Epoch 259, Batch: 0, Loss: 1.082519
Train - Epoch 260, Batch: 0, Loss: 1.093607
Train - Epoch 261, Batch: 0, Loss: 1.085796
Train - Epoch 262, Batch: 0, Loss: 1.088567
Train - Epoch 263, Batch: 0, Loss: 1.084944
Train - Epoch 264, Batch: 0, Loss: 1.072794
Train - Epoch 265, Batch: 0, Loss: 1.081936
Train - Epoch 266, Batch: 0, Loss: 1.084378
Train - Epoch 267, Batch: 0, Loss: 1.091255
Train - Epoch 268, Batch: 0, Loss: 1.087756
Train - Epoch 269, Batch: 0, Loss: 1.080251
Train - Epoch 270, Batch: 0, Loss: 1.074614
Train - Epoch 271, Batch: 0, Loss: 1.087442
Train - Epoch 272, Batch: 0, Loss: 1.089135
Train - Epoch 273, Batch: 0, Loss: 1.092288
Train - Epoch 274, Batch: 0, Loss: 1.094270
Train - Epoch 275, Batch: 0, Loss: 1.079423
Train - Epoch 276, Batch: 0, Loss: 1.079746
Train - Epoch 277, Batch: 0, Loss: 1.081009
Train - Epoch 278, Batch: 0, Loss: 1.066380
Train - Epoch 279, Batch: 0, Loss: 1.092428
Train - Epoch 280, Batch: 0, Loss: 1.080976
Train - Epoch 281, Batch: 0, Loss: 1.091621
Train - Epoch 282, Batch: 0, Loss: 1.087418
Train - Epoch 283, Batch: 0, Loss: 1.104385
Train - Epoch 284, Batch: 0, Loss: 1.085245
Train - Epoch 285, Batch: 0, Loss: 1.072245
Train - Epoch 286, Batch: 0, Loss: 1.091050
Train - Epoch 287, Batch: 0, Loss: 1.089805
Train - Epoch 288, Batch: 0, Loss: 1.074390
Train - Epoch 289, Batch: 0, Loss: 1.086839
Train - Epoch 290, Batch: 0, Loss: 1.078828
Train - Epoch 291, Batch: 0, Loss: 1.083179
Train - Epoch 292, Batch: 0, Loss: 1.061300
Train - Epoch 293, Batch: 0, Loss: 1.083255
Train - Epoch 294, Batch: 0, Loss: 1.074887
Train - Epoch 295, Batch: 0, Loss: 1.078541
Train - Epoch 296, Batch: 0, Loss: 1.079832
Train - Epoch 297, Batch: 0, Loss: 1.076074
Train - Epoch 298, Batch: 0, Loss: 1.068586
Train - Epoch 299, Batch: 0, Loss: 1.072526
Train - Epoch 300, Batch: 0, Loss: 1.077821
Train - Epoch 301, Batch: 0, Loss: 1.092028
Train - Epoch 302, Batch: 0, Loss: 1.080375
Train - Epoch 303, Batch: 0, Loss: 1.078033
Train - Epoch 304, Batch: 0, Loss: 1.081934
Train - Epoch 305, Batch: 0, Loss: 1.066958
Train - Epoch 306, Batch: 0, Loss: 1.072326
Train - Epoch 307, Batch: 0, Loss: 1.081730
Train - Epoch 308, Batch: 0, Loss: 1.079607
Train - Epoch 309, Batch: 0, Loss: 1.062867
Train - Epoch 310, Batch: 0, Loss: 1.075716
Train - Epoch 311, Batch: 0, Loss: 1.062413
Train - Epoch 312, Batch: 0, Loss: 1.063175
Train - Epoch 313, Batch: 0, Loss: 1.066834
Train - Epoch 314, Batch: 0, Loss: 1.080329
Train - Epoch 315, Batch: 0, Loss: 1.081173
Train - Epoch 316, Batch: 0, Loss: 1.071521
Train - Epoch 317, Batch: 0, Loss: 1.076297
Train - Epoch 318, Batch: 0, Loss: 1.085102
Train - Epoch 319, Batch: 0, Loss: 1.078074
Train - Epoch 320, Batch: 0, Loss: 1.066505
Train - Epoch 321, Batch: 0, Loss: 1.083744
Train - Epoch 322, Batch: 0, Loss: 1.058021
Train - Epoch 323, Batch: 0, Loss: 1.072703
Train - Epoch 324, Batch: 0, Loss: 1.071952
Train - Epoch 325, Batch: 0, Loss: 1.074143
Train - Epoch 326, Batch: 0, Loss: 1.053995
Train - Epoch 327, Batch: 0, Loss: 1.064065
Train - Epoch 328, Batch: 0, Loss: 1.067911
Train - Epoch 329, Batch: 0, Loss: 1.062313
Train - Epoch 330, Batch: 0, Loss: 1.069982
Train - Epoch 331, Batch: 0, Loss: 1.081480
Train - Epoch 332, Batch: 0, Loss: 1.064562
Train - Epoch 333, Batch: 0, Loss: 1.081647
Train - Epoch 334, Batch: 0, Loss: 1.088369
Train - Epoch 335, Batch: 0, Loss: 1.087123
Train - Epoch 336, Batch: 0, Loss: 1.083375
Train - Epoch 337, Batch: 0, Loss: 1.070833
Train - Epoch 338, Batch: 0, Loss: 1.071967
Train - Epoch 339, Batch: 0, Loss: 1.056487
Train - Epoch 340, Batch: 0, Loss: 1.080510
Train - Epoch 341, Batch: 0, Loss: 1.068635
Train - Epoch 342, Batch: 0, Loss: 1.058647
Train - Epoch 343, Batch: 0, Loss: 1.061599
Train - Epoch 344, Batch: 0, Loss: 1.059725
Train - Epoch 345, Batch: 0, Loss: 1.071313
Train - Epoch 346, Batch: 0, Loss: 1.071860
Train - Epoch 347, Batch: 0, Loss: 1.063377
Train - Epoch 348, Batch: 0, Loss: 1.073017
Train - Epoch 349, Batch: 0, Loss: 1.070320
Train - Epoch 350, Batch: 0, Loss: 1.061121
Train - Epoch 351, Batch: 0, Loss: 1.070549
Train - Epoch 352, Batch: 0, Loss: 1.055341
Train - Epoch 353, Batch: 0, Loss: 1.062188
Train - Epoch 354, Batch: 0, Loss: 1.067996
Train - Epoch 355, Batch: 0, Loss: 1.058984
Train - Epoch 356, Batch: 0, Loss: 1.048309
Train - Epoch 357, Batch: 0, Loss: 1.062759
Train - Epoch 358, Batch: 0, Loss: 1.068503
Train - Epoch 359, Batch: 0, Loss: 1.067737
Train - Epoch 360, Batch: 0, Loss: 1.061469
Train - Epoch 361, Batch: 0, Loss: 1.056171
Train - Epoch 362, Batch: 0, Loss: 1.070100
Train - Epoch 363, Batch: 0, Loss: 1.062534
Train - Epoch 364, Batch: 0, Loss: 1.079027
Train - Epoch 365, Batch: 0, Loss: 1.061747
Train - Epoch 366, Batch: 0, Loss: 1.067864
Train - Epoch 367, Batch: 0, Loss: 1.069267
Train - Epoch 368, Batch: 0, Loss: 1.061821
Train - Epoch 369, Batch: 0, Loss: 1.074433
Train - Epoch 370, Batch: 0, Loss: 1.074059
Train - Epoch 371, Batch: 0, Loss: 1.060344
Train - Epoch 372, Batch: 0, Loss: 1.058537
Train - Epoch 373, Batch: 0, Loss: 1.061148/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 374, Batch: 0, Loss: 1.058154
Train - Epoch 375, Batch: 0, Loss: 1.056905
Train - Epoch 376, Batch: 0, Loss: 1.063271
Train - Epoch 377, Batch: 0, Loss: 1.067885
Train - Epoch 378, Batch: 0, Loss: 1.061919
Train - Epoch 379, Batch: 0, Loss: 1.045837
Train - Epoch 380, Batch: 0, Loss: 1.041212
Train - Epoch 381, Batch: 0, Loss: 1.057042
Train - Epoch 382, Batch: 0, Loss: 1.058040
Train - Epoch 383, Batch: 0, Loss: 1.058092
Train - Epoch 384, Batch: 0, Loss: 1.060903
Train - Epoch 385, Batch: 0, Loss: 1.055112
Train - Epoch 386, Batch: 0, Loss: 1.058394
Train - Epoch 387, Batch: 0, Loss: 1.061447
Train - Epoch 388, Batch: 0, Loss: 1.054509
Train - Epoch 389, Batch: 0, Loss: 1.071620
Train - Epoch 390, Batch: 0, Loss: 1.051654
Train - Epoch 391, Batch: 0, Loss: 1.059083
Train - Epoch 392, Batch: 0, Loss: 1.058198
Train - Epoch 393, Batch: 0, Loss: 1.058255
Train - Epoch 394, Batch: 0, Loss: 1.064094
Train - Epoch 395, Batch: 0, Loss: 1.063447
Train - Epoch 396, Batch: 0, Loss: 1.047759
Train - Epoch 397, Batch: 0, Loss: 1.073185
Train - Epoch 398, Batch: 0, Loss: 1.051543
Train - Epoch 399, Batch: 0, Loss: 1.068114
Train - Epoch 400, Batch: 0, Loss: 1.066929
Train - Epoch 401, Batch: 0, Loss: 1.043085
Train - Epoch 402, Batch: 0, Loss: 1.043993
Train - Epoch 403, Batch: 0, Loss: 1.063820
Train - Epoch 404, Batch: 0, Loss: 1.072392
Train - Epoch 405, Batch: 0, Loss: 1.047536
Train - Epoch 406, Batch: 0, Loss: 1.053335
Train - Epoch 407, Batch: 0, Loss: 1.055348
Train - Epoch 408, Batch: 0, Loss: 1.051922
Train - Epoch 409, Batch: 0, Loss: 1.055648
Train - Epoch 410, Batch: 0, Loss: 1.039792
Train - Epoch 411, Batch: 0, Loss: 1.062465
Train - Epoch 412, Batch: 0, Loss: 1.050754
Train - Epoch 413, Batch: 0, Loss: 1.050984
Train - Epoch 414, Batch: 0, Loss: 1.058714
Train - Epoch 415, Batch: 0, Loss: 1.043426
Train - Epoch 416, Batch: 0, Loss: 1.043295
Train - Epoch 417, Batch: 0, Loss: 1.049722
Train - Epoch 418, Batch: 0, Loss: 1.051319
Train - Epoch 419, Batch: 0, Loss: 1.040610
Train - Epoch 420, Batch: 0, Loss: 1.049512
Train - Epoch 421, Batch: 0, Loss: 1.054584
Train - Epoch 422, Batch: 0, Loss: 1.055981
Train - Epoch 423, Batch: 0, Loss: 1.041920
Train - Epoch 424, Batch: 0, Loss: 1.047433
Train - Epoch 425, Batch: 0, Loss: 1.046702
Train - Epoch 426, Batch: 0, Loss: 1.033197
Train - Epoch 427, Batch: 0, Loss: 1.042085
Train - Epoch 428, Batch: 0, Loss: 1.043212
Train - Epoch 429, Batch: 0, Loss: 1.049067
Train - Epoch 430, Batch: 0, Loss: 1.052512
Train - Epoch 431, Batch: 0, Loss: 1.053586
Train - Epoch 432, Batch: 0, Loss: 1.062263
Train - Epoch 433, Batch: 0, Loss: 1.056148
Train - Epoch 434, Batch: 0, Loss: 1.046202
Train - Epoch 435, Batch: 0, Loss: 1.062750
Train - Epoch 436, Batch: 0, Loss: 1.039161
Train - Epoch 437, Batch: 0, Loss: 1.051147
Train - Epoch 438, Batch: 0, Loss: 1.050638
Train - Epoch 439, Batch: 0, Loss: 1.037099
Train - Epoch 440, Batch: 0, Loss: 1.034312
Train - Epoch 441, Batch: 0, Loss: 1.057606
Train - Epoch 442, Batch: 0, Loss: 1.072385
Train - Epoch 443, Batch: 0, Loss: 1.033433
Train - Epoch 444, Batch: 0, Loss: 1.058170
Train - Epoch 445, Batch: 0, Loss: 1.042442
Train - Epoch 446, Batch: 0, Loss: 1.040848
Train - Epoch 447, Batch: 0, Loss: 1.046882
Train - Epoch 448, Batch: 0, Loss: 1.057745
Train - Epoch 449, Batch: 0, Loss: 1.037618
Train - Epoch 450, Batch: 0, Loss: 1.037167
Train - Epoch 451, Batch: 0, Loss: 1.049508
Train - Epoch 452, Batch: 0, Loss: 1.046900
Train - Epoch 453, Batch: 0, Loss: 1.035914
Train - Epoch 454, Batch: 0, Loss: 1.054699
Train - Epoch 455, Batch: 0, Loss: 1.048191
Train - Epoch 456, Batch: 0, Loss: 1.042581
Train - Epoch 457, Batch: 0, Loss: 1.057675
Train - Epoch 458, Batch: 0, Loss: 1.044325
Train - Epoch 459, Batch: 0, Loss: 1.046209
Train - Epoch 460, Batch: 0, Loss: 1.049513
Train - Epoch 461, Batch: 0, Loss: 1.048215
Train - Epoch 462, Batch: 0, Loss: 1.045279
Train - Epoch 463, Batch: 0, Loss: 1.037427
Train - Epoch 464, Batch: 0, Loss: 1.041299
Train - Epoch 465, Batch: 0, Loss: 1.047741
Train - Epoch 466, Batch: 0, Loss: 1.051201
Train - Epoch 467, Batch: 0, Loss: 1.048771
Train - Epoch 468, Batch: 0, Loss: 1.059781
Train - Epoch 469, Batch: 0, Loss: 1.050530
Train - Epoch 470, Batch: 0, Loss: 1.036373
Train - Epoch 471, Batch: 0, Loss: 1.041162
Train - Epoch 472, Batch: 0, Loss: 1.041414
Train - Epoch 473, Batch: 0, Loss: 1.030775
Train - Epoch 474, Batch: 0, Loss: 1.041001
Train - Epoch 475, Batch: 0, Loss: 1.044172
Train - Epoch 476, Batch: 0, Loss: 1.045575
Train - Epoch 477, Batch: 0, Loss: 1.038448
Train - Epoch 478, Batch: 0, Loss: 1.025452
Train - Epoch 479, Batch: 0, Loss: 1.033093
Train - Epoch 480, Batch: 0, Loss: 1.035625
Train - Epoch 481, Batch: 0, Loss: 1.037998
Train - Epoch 482, Batch: 0, Loss: 1.059933
Train - Epoch 483, Batch: 0, Loss: 1.037993
Train - Epoch 484, Batch: 0, Loss: 1.036395
Train - Epoch 485, Batch: 0, Loss: 1.037933
Train - Epoch 486, Batch: 0, Loss: 1.032838
Train - Epoch 487, Batch: 0, Loss: 1.032544
Train - Epoch 488, Batch: 0, Loss: 1.021406
Train - Epoch 489, Batch: 0, Loss: 1.036501
Train - Epoch 490, Batch: 0, Loss: 1.041884
Train - Epoch 491, Batch: 0, Loss: 1.046314
Train - Epoch 492, Batch: 0, Loss: 1.036549
Train - Epoch 493, Batch: 0, Loss: 1.043747
Train - Epoch 494, Batch: 0, Loss: 1.042354
Train - Epoch 495, Batch: 0, Loss: 1.049702
Train - Epoch 496, Batch: 0, Loss: 1.034448
Train - Epoch 497, Batch: 0, Loss: 1.040795
Train - Epoch 498, Batch: 0, Loss: 1.040201
Train - Epoch 499, Batch: 0, Loss: 1.049521
training_time:: 108.08599138259888
training time full:: 108.08605980873108
provenance prepare time:: 0.0
here
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632600
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 500
max_epoch:: 500
batch_size:: 10000
epoch:: 0
tensor([32768,     4, 32774,    10, 32780,    13,    15, 32784,    20,    28,
        32798,    31,    45, 32816,    50, 32820,    54, 32831,    65, 32834,
        32836,    69,    75, 32847,    81,    82, 32849,    84,    85, 32853,
        32855, 32861, 32864,    97, 32865, 32866, 32869,   110,   111, 32881,
        32882, 32883,   118,   127,   129, 32899, 32901,   136, 32909, 32914])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
epoch:: 60
epoch:: 61
epoch:: 62
epoch:: 63
epoch:: 64
epoch:: 65
epoch:: 66
epoch:: 67
epoch:: 68
epoch:: 69
epoch:: 70
epoch:: 71
epoch:: 72
epoch:: 73
epoch:: 74
epoch:: 75
epoch:: 76
epoch:: 77
epoch:: 78
epoch:: 79
epoch:: 80
epoch:: 81
epoch:: 82
epoch:: 83
epoch:: 84
epoch:: 85
epoch:: 86
epoch:: 87
epoch:: 88
epoch:: 89
epoch:: 90
epoch:: 91
epoch:: 92
epoch:: 93
epoch:: 94
epoch:: 95
epoch:: 96
epoch:: 97
epoch:: 98
epoch:: 99
epoch:: 100
epoch:: 101
epoch:: 102
epoch:: 103
epoch:: 104
epoch:: 105
epoch:: 106
epoch:: 107
epoch:: 108
epoch:: 109
epoch:: 110
epoch:: 111
epoch:: 112
epoch:: 113
epoch:: 114
epoch:: 115
epoch:: 116
epoch:: 117
epoch:: 118
epoch:: 119
epoch:: 120
epoch:: 121
epoch:: 122
epoch:: 123
epoch:: 124
epoch:: 125
epoch:: 126
epoch:: 127
epoch:: 128
epoch:: 129
epoch:: 130
epoch:: 131
epoch:: 132
epoch:: 133
epoch:: 134
epoch:: 135
epoch:: 136
epoch:: 137
epoch:: 138
epoch:: 139
epoch:: 140
epoch:: 141
epoch:: 142
epoch:: 143
epoch:: 144
epoch:: 145
epoch:: 146
epoch:: 147
epoch:: 148
epoch:: 149
epoch:: 150
epoch:: 151
epoch:: 152
epoch:: 153
epoch:: 154
epoch:: 155
epoch:: 156
epoch:: 157
epoch:: 158
epoch:: 159
epoch:: 160
epoch:: 161
epoch:: 162
epoch:: 163
epoch:: 164
epoch:: 165
epoch:: 166
epoch:: 167
epoch:: 168
epoch:: 169
epoch:: 170
epoch:: 171
epoch:: 172
epoch:: 173
epoch:: 174
epoch:: 175
epoch:: 176
epoch:: 177
epoch:: 178
epoch:: 179
epoch:: 180
epoch:: 181
epoch:: 182
epoch:: 183
epoch:: 184
epoch:: 185
epoch:: 186
epoch:: 187
epoch:: 188
epoch:: 189
epoch:: 190
epoch:: 191
epoch:: 192
epoch:: 193
epoch:: 194
epoch:: 195
epoch:: 196
epoch:: 197
epoch:: 198
epoch:: 199
epoch:: 200
epoch:: 201
epoch:: 202
epoch:: 203
epoch:: 204
epoch:: 205
epoch:: 206
epoch:: 207
epoch:: 208
epoch:: 209
epoch:: 210
epoch:: 211
epoch:: 212
epoch:: 213
epoch:: 214
epoch:: 215
epoch:: 216
epoch:: 217
epoch:: 218
epoch:: 219
epoch:: 220
epoch:: 221
epoch:: 222
epoch:: 223
epoch:: 224
epoch:: 225
epoch:: 226
epoch:: 227
epoch:: 228
epoch:: 229
epoch:: 230
epoch:: 231
epoch:: 232
epoch:: 233
epoch:: 234
epoch:: 235
epoch:: 236
epoch:: 237
epoch:: 238
epoch:: 239
epoch:: 240
epoch:: 241
epoch:: 242
epoch:: 243
epoch:: 244
epoch:: 245
epoch:: 246
epoch:: 247
epoch:: 248
epoch:: 249
epoch:: 250
epoch:: 251
epoch:: 252
epoch:: 253
epoch:: 254
epoch:: 255
epoch:: 256
epoch:: 257
epoch:: 258
epoch:: 259
epoch:: 260
epoch:: 261
epoch:: 262
epoch:: 263
epoch:: 264
epoch:: 265
epoch:: 266
epoch:: 267
epoch:: 268
epoch:: 269
epoch:: 270
epoch:: 271
epoch:: 272
epoch:: 273
epoch:: 274
epoch:: 275
epoch:: 276
epoch:: 277
epoch:: 278
epoch:: 279
epoch:: 280
epoch:: 281
epoch:: 282
epoch:: 283
epoch:: 284
epoch:: 285
epoch:: 286
epoch:: 287
epoch:: 288
epoch:: 289
epoch:: 290
epoch:: 291
epoch:: 292
epoch:: 293
epoch:: 294
epoch:: 295
epoch:: 296
epoch:: 297
epoch:: 298
epoch:: 299
epoch:: 300
epoch:: 301
epoch:: 302
epoch:: 303
epoch:: 304
epoch:: 305
epoch:: 306
epoch:: 307
epoch:: 308
epoch:: 309
epoch:: 310
epoch:: 311
epoch:: 312
epoch:: 313
epoch:: 314
epoch:: 315
epoch:: 316
epoch:: 317
epoch:: 318
epoch:: 319
epoch:: 320
epoch:: 321
epoch:: 322
epoch:: 323
epoch:: 324
epoch:: 325
epoch:: 326
epoch:: 327
epoch:: 328
epoch:: 329
epoch:: 330
epoch:: 331
epoch:: 332
epoch:: 333
epoch:: 334
epoch:: 335
epoch:: 336
epoch:: 337
epoch:: 338
epoch:: 339
epoch:: 340
epoch:: 341
epoch:: 342
epoch:: 343
epoch:: 344
epoch:: 345
epoch:: 346
epoch:: 347
epoch:: 348
epoch:: 349
epoch:: 350
epoch:: 351
epoch:: 352
epoch:: 353
epoch:: 354
epoch:: 355
epoch:: 356
epoch:: 357
epoch:: 358
epoch:: 359
epoch:: 360
epoch:: 361
epoch:: 362
epoch:: 363
epoch:: 364
epoch:: 365
epoch:: 366
epoch:: 367
epoch:: 368
epoch:: 369
epoch:: 370
epoch:: 371
epoch:: 372
epoch:: 373
epoch:: 374
epoch:: 375
epoch:: 376
epoch:: 377
epoch:: 378
epoch:: 379
epoch:: 380
epoch:: 381
epoch:: 382
epoch:: 383
epoch:: 384
epoch:: 385
epoch:: 386
epoch:: 387
epoch:: 388
epoch:: 389
epoch:: 390
epoch:: 391
epoch:: 392
epoch:: 393
epoch:: 394
epoch:: 395
epoch:: 396
epoch:: 397
epoch:: 398
epoch:: 399
epoch:: 400
epoch:: 401
epoch:: 402
epoch:: 403
epoch:: 404
epoch:: 405
epoch:: 406
epoch:: 407
epoch:: 408
epoch:: 409
epoch:: 410
epoch:: 411
epoch:: 412
epoch:: 413
epoch:: 414
epoch:: 415
epoch:: 416
epoch:: 417
epoch:: 418
epoch:: 419
epoch:: 420
epoch:: 421
epoch:: 422
epoch:: 423
epoch:: 424
epoch:: 425
epoch:: 426
epoch:: 427
epoch:: 428
epoch:: 429
epoch:: 430
epoch:: 431
epoch:: 432
epoch:: 433
epoch:: 434
epoch:: 435
epoch:: 436
epoch:: 437
epoch:: 438
epoch:: 439
epoch:: 440
epoch:: 441
epoch:: 442
epoch:: 443
epoch:: 444
epoch:: 445
epoch:: 446
epoch:: 447
epoch:: 448
epoch:: 449
epoch:: 450
epoch:: 451
epoch:: 452
epoch:: 453
epoch:: 454
epoch:: 455
epoch:: 456
epoch:: 457
epoch:: 458
epoch:: 459
epoch:: 460
epoch:: 461
epoch:: 462
epoch:: 463
epoch:: 464
epoch:: 465
epoch:: 466
epoch:: 467
epoch:: 468
epoch:: 469
epoch:: 470
epoch:: 471
epoch:: 472
epoch:: 473
epoch:: 474
epoch:: 475
epoch:: 476
epoch:: 477
epoch:: 478
epoch:: 479
epoch:: 480
epoch:: 481
epoch:: 482
epoch:: 483
epoch:: 484
epoch:: 485
epoch:: 486
epoch:: 487
epoch:: 488
epoch:: 489
epoch:: 490
epoch:: 491
epoch:: 492
epoch:: 493
epoch:: 494
epoch:: 495
epoch:: 496
epoch:: 497
epoch:: 498
epoch:: 499
training time is 80.49788403511047
overhead:: 0
overhead2:: 2.6667778491973877
overhead3:: 0
time_baseline:: 80.49792909622192
curr_diff: 0 tensor(0.1927, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1927, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632400
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 500
max_epoch:: 500
overhead:: 0
overhead2:: 0.08681011199951172
overhead3:: 0.26082825660705566
overhead4:: 9.502930879592896
overhead5:: 0
memory usage:: 5635850240
time_provenance:: 17.958839893341064
curr_diff: 0 tensor(0.0462, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0462, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.2279, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.2279, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632700
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 500
max_epoch:: 500
overhead:: 0
overhead2:: 0.09256339073181152
overhead3:: 0.2608299255371094
overhead4:: 9.631040334701538
overhead5:: 0
memory usage:: 5649412096
time_provenance:: 18.13361096382141
curr_diff: 0 tensor(0.0463, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0463, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.2279, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.2279, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632800
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 500
max_epoch:: 500
overhead:: 0
overhead2:: 0.09922957420349121
overhead3:: 0.2664985656738281
overhead4:: 10.249356508255005
overhead5:: 0
memory usage:: 5643862016
time_provenance:: 18.836977005004883
curr_diff: 0 tensor(0.0463, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0463, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.2279, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.2279, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632700
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 500
max_epoch:: 500
overhead:: 0
overhead2:: 0.1862940788269043
overhead3:: 0.4276876449584961
overhead4:: 17.153045177459717
overhead5:: 0
memory usage:: 5650501632
time_provenance:: 27.687653064727783
curr_diff: 0 tensor(0.0047, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0047, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1961, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1961, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632300
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 500
max_epoch:: 500
overhead:: 0
overhead2:: 0.17510032653808594
overhead3:: 0.4498324394226074
overhead4:: 17.25040292739868
overhead5:: 0
memory usage:: 5634347008
time_provenance:: 27.828570127487183
curr_diff: 0 tensor(0.0047, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0047, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1961, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1961, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632300
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 500
max_epoch:: 500
overhead:: 0
overhead2:: 0.1735522747039795
overhead3:: 0.464461088180542
overhead4:: 17.83353877067566
overhead5:: 0
memory usage:: 5631717376
time_provenance:: 28.520813703536987
curr_diff: 0 tensor(0.0047, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0047, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1961, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1961, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632300
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 500
max_epoch:: 500
overhead:: 0
overhead2:: 0.441791296005249
overhead3:: 0.9834330081939697
overhead4:: 40.31507134437561
overhead5:: 0
memory usage:: 5634019328
time_provenance:: 57.197502851486206
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1929, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1929, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632400
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 500
max_epoch:: 500
overhead:: 0
overhead2:: 0.4302978515625
overhead3:: 0.9943521022796631
overhead4:: 41.49886679649353
overhead5:: 0
memory usage:: 5650001920
time_provenance:: 58.44270086288452
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1929, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1929, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632400
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 500
max_epoch:: 500
overhead:: 0
overhead2:: 0.44594478607177734
overhead3:: 0.993250846862793
overhead4:: 40.06718635559082
overhead5:: 0
memory usage:: 5693931520
time_provenance:: 57.02771878242493
curr_diff: 0 tensor(0.0006, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0006, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1929, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1929, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632400
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 3 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 500
max_epoch:: 500
overhead:: 0
overhead2:: 0.8467593193054199
overhead3:: 2.048856496810913
overhead4:: 82.22150087356567
overhead5:: 0
memory usage:: 5615747072
time_provenance:: 107.40204381942749
curr_diff: 0 tensor(4.9979e-13, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.9979e-13, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1927, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1927, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.632400
repetition 4
python3 benchmark_exp_lr.py 0.001 10000 500 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05] Logistic_regression cifar10_2 4 0.0001 1 1
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:1
Train - Epoch 0, Batch: 0, Loss: 2.305848
Train - Epoch 1, Batch: 0, Loss: 2.246217
Train - Epoch 2, Batch: 0, Loss: 2.190833
Train - Epoch 3, Batch: 0, Loss: 2.140787
Train - Epoch 4, Batch: 0, Loss: 2.092540
Train - Epoch 5, Batch: 0, Loss: 2.047296
Train - Epoch 6, Batch: 0, Loss: 2.007720
Train - Epoch 7, Batch: 0, Loss: 1.969791
Train - Epoch 8, Batch: 0, Loss: 1.931986
Train - Epoch 9, Batch: 0, Loss: 1.897220
Train - Epoch 10, Batch: 0, Loss: 1.860564
Train - Epoch 11, Batch: 0, Loss: 1.837518
Train - Epoch 12, Batch: 0, Loss: 1.805253
Train - Epoch 13, Batch: 0, Loss: 1.785647
Train - Epoch 14, Batch: 0, Loss: 1.764370
Train - Epoch 15, Batch: 0, Loss: 1.734472
Train - Epoch 16, Batch: 0, Loss: 1.708581
Train - Epoch 17, Batch: 0, Loss: 1.697059
Train - Epoch 18, Batch: 0, Loss: 1.679622
Train - Epoch 19, Batch: 0, Loss: 1.656333
Train - Epoch 20, Batch: 0, Loss: 1.646618
Train - Epoch 21, Batch: 0, Loss: 1.620547
Train - Epoch 22, Batch: 0, Loss: 1.605265
Train - Epoch 23, Batch: 0, Loss: 1.591497
Train - Epoch 24, Batch: 0, Loss: 1.576134
Train - Epoch 25, Batch: 0, Loss: 1.571694
Train - Epoch 26, Batch: 0, Loss: 1.552090
Train - Epoch 27, Batch: 0, Loss: 1.537613
Train - Epoch 28, Batch: 0, Loss: 1.524379
Train - Epoch 29, Batch: 0, Loss: 1.519961
Train - Epoch 30, Batch: 0, Loss: 1.502306
Train - Epoch 31, Batch: 0, Loss: 1.496133
Train - Epoch 32, Batch: 0, Loss: 1.477611
Train - Epoch 33, Batch: 0, Loss: 1.482974
Train - Epoch 34, Batch: 0, Loss: 1.465551
Train - Epoch 35, Batch: 0, Loss: 1.461548
Train - Epoch 36, Batch: 0, Loss: 1.445136
Train - Epoch 37, Batch: 0, Loss: 1.438747
Train - Epoch 38, Batch: 0, Loss: 1.440512
Train - Epoch 39, Batch: 0, Loss: 1.423664
Train - Epoch 40, Batch: 0, Loss: 1.419730
Train - Epoch 41, Batch: 0, Loss: 1.416594
Train - Epoch 42, Batch: 0, Loss: 1.400193
Train - Epoch 43, Batch: 0, Loss: 1.404378
Train - Epoch 44, Batch: 0, Loss: 1.397532
Train - Epoch 45, Batch: 0, Loss: 1.386798
Train - Epoch 46, Batch: 0, Loss: 1.387519
Train - Epoch 47, Batch: 0, Loss: 1.380778
Train - Epoch 48, Batch: 0, Loss: 1.377281
Train - Epoch 49, Batch: 0, Loss: 1.378041
Train - Epoch 50, Batch: 0, Loss: 1.351476
Train - Epoch 51, Batch: 0, Loss: 1.349912
Train - Epoch 52, Batch: 0, Loss: 1.347875
Train - Epoch 53, Batch: 0, Loss: 1.346698
Train - Epoch 54, Batch: 0, Loss: 1.346450
Train - Epoch 55, Batch: 0, Loss: 1.340716
Train - Epoch 56, Batch: 0, Loss: 1.347757
Train - Epoch 57, Batch: 0, Loss: 1.330888
Train - Epoch 58, Batch: 0, Loss: 1.324214
Train - Epoch 59, Batch: 0, Loss: 1.313431
Train - Epoch 60, Batch: 0, Loss: 1.320527
Train - Epoch 61, Batch: 0, Loss: 1.317566
Train - Epoch 62, Batch: 0, Loss: 1.307447
Train - Epoch 63, Batch: 0, Loss: 1.309019
Train - Epoch 64, Batch: 0, Loss: 1.303463
Train - Epoch 65, Batch: 0, Loss: 1.295043
Train - Epoch 66, Batch: 0, Loss: 1.304324
Train - Epoch 67, Batch: 0, Loss: 1.290283
Train - Epoch 68, Batch: 0, Loss: 1.280944
Train - Epoch 69, Batch: 0, Loss: 1.269081
Train - Epoch 70, Batch: 0, Loss: 1.276801
Train - Epoch 71, Batch: 0, Loss: 1.291434
Train - Epoch 72, Batch: 0, Loss: 1.272812
Train - Epoch 73, Batch: 0, Loss: 1.277529
Train - Epoch 74, Batch: 0, Loss: 1.263270
Train - Epoch 75, Batch: 0, Loss: 1.272156
Train - Epoch 76, Batch: 0, Loss: 1.262218
Train - Epoch 77, Batch: 0, Loss: 1.275654
Train - Epoch 78, Batch: 0, Loss: 1.262333
Train - Epoch 79, Batch: 0, Loss: 1.253708
Train - Epoch 80, Batch: 0, Loss: 1.255407
Train - Epoch 81, Batch: 0, Loss: 1.244294
Train - Epoch 82, Batch: 0, Loss: 1.247109
Train - Epoch 83, Batch: 0, Loss: 1.247869
Train - Epoch 84, Batch: 0, Loss: 1.248205
Train - Epoch 85, Batch: 0, Loss: 1.255938
Train - Epoch 86, Batch: 0, Loss: 1.239418
Train - Epoch 87, Batch: 0, Loss: 1.247155
Train - Epoch 88, Batch: 0, Loss: 1.244623
Train - Epoch 89, Batch: 0, Loss: 1.237642
Train - Epoch 90, Batch: 0, Loss: 1.217754
Train - Epoch 91, Batch: 0, Loss: 1.231653
Train - Epoch 92, Batch: 0, Loss: 1.218326
Train - Epoch 93, Batch: 0, Loss: 1.230177
Train - Epoch 94, Batch: 0, Loss: 1.212806
Train - Epoch 95, Batch: 0, Loss: 1.229086
Train - Epoch 96, Batch: 0, Loss: 1.221094
Train - Epoch 97, Batch: 0, Loss: 1.214143
Train - Epoch 98, Batch: 0, Loss: 1.217954
Train - Epoch 99, Batch: 0, Loss: 1.219070
Train - Epoch 100, Batch: 0, Loss: 1.216391
Train - Epoch 101, Batch: 0, Loss: 1.200751
Train - Epoch 102, Batch: 0, Loss: 1.204805
Train - Epoch 103, Batch: 0, Loss: 1.207946
Train - Epoch 104, Batch: 0, Loss: 1.212578
Train - Epoch 105, Batch: 0, Loss: 1.223755
Train - Epoch 106, Batch: 0, Loss: 1.202972
Train - Epoch 107, Batch: 0, Loss: 1.208360
Train - Epoch 108, Batch: 0, Loss: 1.208316
Train - Epoch 109, Batch: 0, Loss: 1.196576
Train - Epoch 110, Batch: 0, Loss: 1.195406
Train - Epoch 111, Batch: 0, Loss: 1.191132
Train - Epoch 112, Batch: 0, Loss: 1.197874
Train - Epoch 113, Batch: 0, Loss: 1.182825
Train - Epoch 114, Batch: 0, Loss: 1.191407
Train - Epoch 115, Batch: 0, Loss: 1.198644
Train - Epoch 116, Batch: 0, Loss: 1.186404
Train - Epoch 117, Batch: 0, Loss: 1.190583
Train - Epoch 118, Batch: 0, Loss: 1.186929
Train - Epoch 119, Batch: 0, Loss: 1.192304
Train - Epoch 120, Batch: 0, Loss: 1.177718
Train - Epoch 121, Batch: 0, Loss: 1.189101
Train - Epoch 122, Batch: 0, Loss: 1.176579
Train - Epoch 123, Batch: 0, Loss: 1.191694
Train - Epoch 124, Batch: 0, Loss: 1.177759
Train - Epoch 125, Batch: 0, Loss: 1.175686
Train - Epoch 126, Batch: 0, Loss: 1.173796
Train - Epoch 127, Batch: 0, Loss: 1.160392
Train - Epoch 128, Batch: 0, Loss: 1.177790
Train - Epoch 129, Batch: 0, Loss: 1.179084
Train - Epoch 130, Batch: 0, Loss: 1.172197
Train - Epoch 131, Batch: 0, Loss: 1.170318
Train - Epoch 132, Batch: 0, Loss: 1.177591
Train - Epoch 133, Batch: 0, Loss: 1.180669
Train - Epoch 134, Batch: 0, Loss: 1.173338
Train - Epoch 135, Batch: 0, Loss: 1.172148
Train - Epoch 136, Batch: 0, Loss: 1.155833
Train - Epoch 137, Batch: 0, Loss: 1.160513
Train - Epoch 138, Batch: 0, Loss: 1.151408
Train - Epoch 139, Batch: 0, Loss: 1.174750
Train - Epoch 140, Batch: 0, Loss: 1.165103
Train - Epoch 141, Batch: 0, Loss: 1.167521
Train - Epoch 142, Batch: 0, Loss: 1.163325
Train - Epoch 143, Batch: 0, Loss: 1.167098
Train - Epoch 144, Batch: 0, Loss: 1.158148
Train - Epoch 145, Batch: 0, Loss: 1.156312
Train - Epoch 146, Batch: 0, Loss: 1.159296
Train - Epoch 147, Batch: 0, Loss: 1.156279
Train - Epoch 148, Batch: 0, Loss: 1.159456
Train - Epoch 149, Batch: 0, Loss: 1.155348
Train - Epoch 150, Batch: 0, Loss: 1.143290
Train - Epoch 151, Batch: 0, Loss: 1.137143
Train - Epoch 152, Batch: 0, Loss: 1.150760
Train - Epoch 153, Batch: 0, Loss: 1.147978
Train - Epoch 154, Batch: 0, Loss: 1.145234
Train - Epoch 155, Batch: 0, Loss: 1.143801
Train - Epoch 156, Batch: 0, Loss: 1.141169
Train - Epoch 157, Batch: 0, Loss: 1.146389
Train - Epoch 158, Batch: 0, Loss: 1.144772
Train - Epoch 159, Batch: 0, Loss: 1.146340
Train - Epoch 160, Batch: 0, Loss: 1.147112
Train - Epoch 161, Batch: 0, Loss: 1.146095
Train - Epoch 162, Batch: 0, Loss: 1.160160
Train - Epoch 163, Batch: 0, Loss: 1.129670
Train - Epoch 164, Batch: 0, Loss: 1.138036
Train - Epoch 165, Batch: 0, Loss: 1.129970
Train - Epoch 166, Batch: 0, Loss: 1.137549
Train - Epoch 167, Batch: 0, Loss: 1.134634
Train - Epoch 168, Batch: 0, Loss: 1.144870
Train - Epoch 169, Batch: 0, Loss: 1.135782
Train - Epoch 170, Batch: 0, Loss: 1.129062
Train - Epoch 171, Batch: 0, Loss: 1.136267
Train - Epoch 172, Batch: 0, Loss: 1.138728
Train - Epoch 173, Batch: 0, Loss: 1.137046
Train - Epoch 174, Batch: 0, Loss: 1.128819
Train - Epoch 175, Batch: 0, Loss: 1.132221
Train - Epoch 176, Batch: 0, Loss: 1.139501
Train - Epoch 177, Batch: 0, Loss: 1.139161
Train - Epoch 178, Batch: 0, Loss: 1.132783
Train - Epoch 179, Batch: 0, Loss: 1.122365
Train - Epoch 180, Batch: 0, Loss: 1.134824
Train - Epoch 181, Batch: 0, Loss: 1.120875
Train - Epoch 182, Batch: 0, Loss: 1.133440
Train - Epoch 183, Batch: 0, Loss: 1.137716
Train - Epoch 184, Batch: 0, Loss: 1.128459
Train - Epoch 185, Batch: 0, Loss: 1.102730
Train - Epoch 186, Batch: 0, Loss: 1.134009
Train - Epoch 187, Batch: 0, Loss: 1.130359
Train - Epoch 188, Batch: 0, Loss: 1.140239
Train - Epoch 189, Batch: 0, Loss: 1.119116
Train - Epoch 190, Batch: 0, Loss: 1.126023
Train - Epoch 191, Batch: 0, Loss: 1.129499
Train - Epoch 192, Batch: 0, Loss: 1.127545
Train - Epoch 193, Batch: 0, Loss: 1.119592
Train - Epoch 194, Batch: 0, Loss: 1.111668
Train - Epoch 195, Batch: 0, Loss: 1.118323
Train - Epoch 196, Batch: 0, Loss: 1.142006
Train - Epoch 197, Batch: 0, Loss: 1.120048
Train - Epoch 198, Batch: 0, Loss: 1.116333
Train - Epoch 199, Batch: 0, Loss: 1.115694
Train - Epoch 200, Batch: 0, Loss: 1.122381
Train - Epoch 201, Batch: 0, Loss: 1.117022
Train - Epoch 202, Batch: 0, Loss: 1.118914
Train - Epoch 203, Batch: 0, Loss: 1.098683
Train - Epoch 204, Batch: 0, Loss: 1.124048
Train - Epoch 205, Batch: 0, Loss: 1.117945
Train - Epoch 206, Batch: 0, Loss: 1.116620
Train - Epoch 207, Batch: 0, Loss: 1.110081
Train - Epoch 208, Batch: 0, Loss: 1.127414
Train - Epoch 209, Batch: 0, Loss: 1.108692
Train - Epoch 210, Batch: 0, Loss: 1.101576
Train - Epoch 211, Batch: 0, Loss: 1.114531
Train - Epoch 212, Batch: 0, Loss: 1.130119
Train - Epoch 213, Batch: 0, Loss: 1.120483
Train - Epoch 214, Batch: 0, Loss: 1.117343
Train - Epoch 215, Batch: 0, Loss: 1.103866
Train - Epoch 216, Batch: 0, Loss: 1.113630
Train - Epoch 217, Batch: 0, Loss: 1.101630
Train - Epoch 218, Batch: 0, Loss: 1.114212
Train - Epoch 219, Batch: 0, Loss: 1.108755
Train - Epoch 220, Batch: 0, Loss: 1.121595
Train - Epoch 221, Batch: 0, Loss: 1.109813
Train - Epoch 222, Batch: 0, Loss: 1.098755
Train - Epoch 223, Batch: 0, Loss: 1.111515
Train - Epoch 224, Batch: 0, Loss: 1.106590
Train - Epoch 225, Batch: 0, Loss: 1.110217
Train - Epoch 226, Batch: 0, Loss: 1.099904
Train - Epoch 227, Batch: 0, Loss: 1.100157
Train - Epoch 228, Batch: 0, Loss: 1.114486
Train - Epoch 229, Batch: 0, Loss: 1.109612
Train - Epoch 230, Batch: 0, Loss: 1.095456
Train - Epoch 231, Batch: 0, Loss: 1.095399
Train - Epoch 232, Batch: 0, Loss: 1.102666
Train - Epoch 233, Batch: 0, Loss: 1.107866
Train - Epoch 234, Batch: 0, Loss: 1.111163
Train - Epoch 235, Batch: 0, Loss: 1.110316
Train - Epoch 236, Batch: 0, Loss: 1.106040
Train - Epoch 237, Batch: 0, Loss: 1.103684
Train - Epoch 238, Batch: 0, Loss: 1.099297
Train - Epoch 239, Batch: 0, Loss: 1.102333
Train - Epoch 240, Batch: 0, Loss: 1.098986
Train - Epoch 241, Batch: 0, Loss: 1.094022
Train - Epoch 242, Batch: 0, Loss: 1.109683
Train - Epoch 243, Batch: 0, Loss: 1.082420
Train - Epoch 244, Batch: 0, Loss: 1.093654
Train - Epoch 245, Batch: 0, Loss: 1.097220
Train - Epoch 246, Batch: 0, Loss: 1.101833
Train - Epoch 247, Batch: 0, Loss: 1.094475
Train - Epoch 248, Batch: 0, Loss: 1.098255
Train - Epoch 249, Batch: 0, Loss: 1.077711
Train - Epoch 250, Batch: 0, Loss: 1.084429
Train - Epoch 251, Batch: 0, Loss: 1.103591
Train - Epoch 252, Batch: 0, Loss: 1.084934
Train - Epoch 253, Batch: 0, Loss: 1.088420
Train - Epoch 254, Batch: 0, Loss: 1.106303
Train - Epoch 255, Batch: 0, Loss: 1.097061
Train - Epoch 256, Batch: 0, Loss: 1.084823
Train - Epoch 257, Batch: 0, Loss: 1.094735
Train - Epoch 258, Batch: 0, Loss: 1.109134
Train - Epoch 259, Batch: 0, Loss: 1.089925
Train - Epoch 260, Batch: 0, Loss: 1.093940
Train - Epoch 261, Batch: 0, Loss: 1.085375
Train - Epoch 262, Batch: 0, Loss: 1.096157
Train - Epoch 263, Batch: 0, Loss: 1.087181
Train - Epoch 264, Batch: 0, Loss: 1.084620
Train - Epoch 265, Batch: 0, Loss: 1.089809
Train - Epoch 266, Batch: 0, Loss: 1.084392
Train - Epoch 267, Batch: 0, Loss: 1.088805
Train - Epoch 268, Batch: 0, Loss: 1.093519
Train - Epoch 269, Batch: 0, Loss: 1.096960
Train - Epoch 270, Batch: 0, Loss: 1.096153
Train - Epoch 271, Batch: 0, Loss: 1.092445
Train - Epoch 272, Batch: 0, Loss: 1.092897
Train - Epoch 273, Batch: 0, Loss: 1.093571
Train - Epoch 274, Batch: 0, Loss: 1.096147
Train - Epoch 275, Batch: 0, Loss: 1.074795
Train - Epoch 276, Batch: 0, Loss: 1.090833
Train - Epoch 277, Batch: 0, Loss: 1.064211
Train - Epoch 278, Batch: 0, Loss: 1.067594
Train - Epoch 279, Batch: 0, Loss: 1.092950
Train - Epoch 280, Batch: 0, Loss: 1.085749
Train - Epoch 281, Batch: 0, Loss: 1.090587
Train - Epoch 282, Batch: 0, Loss: 1.084552
Train - Epoch 283, Batch: 0, Loss: 1.083207
Train - Epoch 284, Batch: 0, Loss: 1.077557
Train - Epoch 285, Batch: 0, Loss: 1.067069
Train - Epoch 286, Batch: 0, Loss: 1.083447
Train - Epoch 287, Batch: 0, Loss: 1.065491
Train - Epoch 288, Batch: 0, Loss: 1.084165
Train - Epoch 289, Batch: 0, Loss: 1.080639
Train - Epoch 290, Batch: 0, Loss: 1.082793
Train - Epoch 291, Batch: 0, Loss: 1.078016
Train - Epoch 292, Batch: 0, Loss: 1.070008
Train - Epoch 293, Batch: 0, Loss: 1.080165
Train - Epoch 294, Batch: 0, Loss: 1.081419
Train - Epoch 295, Batch: 0, Loss: 1.090111
Train - Epoch 296, Batch: 0, Loss: 1.076768
Train - Epoch 297, Batch: 0, Loss: 1.057520
Train - Epoch 298, Batch: 0, Loss: 1.079341
Train - Epoch 299, Batch: 0, Loss: 1.069674
Train - Epoch 300, Batch: 0, Loss: 1.080546
Train - Epoch 301, Batch: 0, Loss: 1.100403
Train - Epoch 302, Batch: 0, Loss: 1.077448
Train - Epoch 303, Batch: 0, Loss: 1.078951
Train - Epoch 304, Batch: 0, Loss: 1.079448
Train - Epoch 305, Batch: 0, Loss: 1.074921
Train - Epoch 306, Batch: 0, Loss: 1.067264
Train - Epoch 307, Batch: 0, Loss: 1.084773
Train - Epoch 308, Batch: 0, Loss: 1.078001
Train - Epoch 309, Batch: 0, Loss: 1.087291
Train - Epoch 310, Batch: 0, Loss: 1.075723
Train - Epoch 311, Batch: 0, Loss: 1.077460
Train - Epoch 312, Batch: 0, Loss: 1.080123
Train - Epoch 313, Batch: 0, Loss: 1.072658
Train - Epoch 314, Batch: 0, Loss: 1.061284
Train - Epoch 315, Batch: 0, Loss: 1.072343
Train - Epoch 316, Batch: 0, Loss: 1.059230
Train - Epoch 317, Batch: 0, Loss: 1.078963
Train - Epoch 318, Batch: 0, Loss: 1.068407
Train - Epoch 319, Batch: 0, Loss: 1.079070
Train - Epoch 320, Batch: 0, Loss: 1.065277
Train - Epoch 321, Batch: 0, Loss: 1.078543
Train - Epoch 322, Batch: 0, Loss: 1.079742
Train - Epoch 323, Batch: 0, Loss: 1.080374
Train - Epoch 324, Batch: 0, Loss: 1.057698
Train - Epoch 325, Batch: 0, Loss: 1.072987
Train - Epoch 326, Batch: 0, Loss: 1.064843
Train - Epoch 327, Batch: 0, Loss: 1.067361
Train - Epoch 328, Batch: 0, Loss: 1.068727
Train - Epoch 329, Batch: 0, Loss: 1.060570
Train - Epoch 330, Batch: 0, Loss: 1.075438
Train - Epoch 331, Batch: 0, Loss: 1.078515
Train - Epoch 332, Batch: 0, Loss: 1.072920
Train - Epoch 333, Batch: 0, Loss: 1.072710
Train - Epoch 334, Batch: 0, Loss: 1.069636
Train - Epoch 335, Batch: 0, Loss: 1.072090
Train - Epoch 336, Batch: 0, Loss: 1.072048
Train - Epoch 337, Batch: 0, Loss: 1.072510
Train - Epoch 338, Batch: 0, Loss: 1.076509
Train - Epoch 339, Batch: 0, Loss: 1.073605
Train - Epoch 340, Batch: 0, Loss: 1.073738
Train - Epoch 341, Batch: 0, Loss: 1.063789
Train - Epoch 342, Batch: 0, Loss: 1.069989
Train - Epoch 343, Batch: 0, Loss: 1.069621
Train - Epoch 344, Batch: 0, Loss: 1.073458
Train - Epoch 345, Batch: 0, Loss: 1.066322
Train - Epoch 346, Batch: 0, Loss: 1.066004
Train - Epoch 347, Batch: 0, Loss: 1.074934
Train - Epoch 348, Batch: 0, Loss: 1.091052
Train - Epoch 349, Batch: 0, Loss: 1.080668
Train - Epoch 350, Batch: 0, Loss: 1.065281
Train - Epoch 351, Batch: 0, Loss: 1.056816
Train - Epoch 352, Batch: 0, Loss: 1.059063
Train - Epoch 353, Batch: 0, Loss: 1.054007
Train - Epoch 354, Batch: 0, Loss: 1.073165
Train - Epoch 355, Batch: 0, Loss: 1.062799
Train - Epoch 356, Batch: 0, Loss: 1.067759
Train - Epoch 357, Batch: 0, Loss: 1.057206
Train - Epoch 358, Batch: 0, Loss: 1.064527
Train - Epoch 359, Batch: 0, Loss: 1.060038
Train - Epoch 360, Batch: 0, Loss: 1.058430
Train - Epoch 361, Batch: 0, Loss: 1.065583
Train - Epoch 362, Batch: 0, Loss: 1.060428
Train - Epoch 363, Batch: 0, Loss: 1.058157
Train - Epoch 364, Batch: 0, Loss: 1.047711
Train - Epoch 365, Batch: 0, Loss: 1.065792
Train - Epoch 366, Batch: 0, Loss: 1.065888
Train - Epoch 367, Batch: 0, Loss: 1.058067
Train - Epoch 368, Batch: 0, Loss: 1.060259
Train - Epoch 369, Batch: 0, Loss: 1.069274
Train - Epoch 370, Batch: 0, Loss: 1.060855
Train - Epoch 371, Batch: 0, Loss: 1.057092
Train - Epoch 372, Batch: 0, Loss: 1.081021
Train - Epoch 373, Batch: 0, Loss: 1.059490/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:75: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)

Train - Epoch 374, Batch: 0, Loss: 1.053396
Train - Epoch 375, Batch: 0, Loss: 1.056409
Train - Epoch 376, Batch: 0, Loss: 1.064179
Train - Epoch 377, Batch: 0, Loss: 1.065997
Train - Epoch 378, Batch: 0, Loss: 1.054632
Train - Epoch 379, Batch: 0, Loss: 1.062726
Train - Epoch 380, Batch: 0, Loss: 1.064343
Train - Epoch 381, Batch: 0, Loss: 1.058811
Train - Epoch 382, Batch: 0, Loss: 1.059192
Train - Epoch 383, Batch: 0, Loss: 1.054845
Train - Epoch 384, Batch: 0, Loss: 1.054896
Train - Epoch 385, Batch: 0, Loss: 1.060759
Train - Epoch 386, Batch: 0, Loss: 1.064177
Train - Epoch 387, Batch: 0, Loss: 1.064028
Train - Epoch 388, Batch: 0, Loss: 1.061790
Train - Epoch 389, Batch: 0, Loss: 1.052187
Train - Epoch 390, Batch: 0, Loss: 1.066434
Train - Epoch 391, Batch: 0, Loss: 1.048858
Train - Epoch 392, Batch: 0, Loss: 1.051812
Train - Epoch 393, Batch: 0, Loss: 1.058128
Train - Epoch 394, Batch: 0, Loss: 1.054854
Train - Epoch 395, Batch: 0, Loss: 1.040544
Train - Epoch 396, Batch: 0, Loss: 1.055738
Train - Epoch 397, Batch: 0, Loss: 1.042387
Train - Epoch 398, Batch: 0, Loss: 1.054892
Train - Epoch 399, Batch: 0, Loss: 1.061924
Train - Epoch 400, Batch: 0, Loss: 1.053221
Train - Epoch 401, Batch: 0, Loss: 1.049626
Train - Epoch 402, Batch: 0, Loss: 1.057298
Train - Epoch 403, Batch: 0, Loss: 1.052832
Train - Epoch 404, Batch: 0, Loss: 1.064514
Train - Epoch 405, Batch: 0, Loss: 1.058135
Train - Epoch 406, Batch: 0, Loss: 1.061558
Train - Epoch 407, Batch: 0, Loss: 1.063980
Train - Epoch 408, Batch: 0, Loss: 1.057286
Train - Epoch 409, Batch: 0, Loss: 1.065878
Train - Epoch 410, Batch: 0, Loss: 1.077545
Train - Epoch 411, Batch: 0, Loss: 1.050522
Train - Epoch 412, Batch: 0, Loss: 1.048913
Train - Epoch 413, Batch: 0, Loss: 1.055906
Train - Epoch 414, Batch: 0, Loss: 1.050567
Train - Epoch 415, Batch: 0, Loss: 1.058066
Train - Epoch 416, Batch: 0, Loss: 1.061008
Train - Epoch 417, Batch: 0, Loss: 1.058044
Train - Epoch 418, Batch: 0, Loss: 1.042721
Train - Epoch 419, Batch: 0, Loss: 1.048079
Train - Epoch 420, Batch: 0, Loss: 1.052778
Train - Epoch 421, Batch: 0, Loss: 1.039641
Train - Epoch 422, Batch: 0, Loss: 1.048318
Train - Epoch 423, Batch: 0, Loss: 1.076046
Train - Epoch 424, Batch: 0, Loss: 1.050490
Train - Epoch 425, Batch: 0, Loss: 1.054741
Train - Epoch 426, Batch: 0, Loss: 1.051821
Train - Epoch 427, Batch: 0, Loss: 1.049799
Train - Epoch 428, Batch: 0, Loss: 1.058956
Train - Epoch 429, Batch: 0, Loss: 1.060713
Train - Epoch 430, Batch: 0, Loss: 1.067003
Train - Epoch 431, Batch: 0, Loss: 1.042268
Train - Epoch 432, Batch: 0, Loss: 1.056181
Train - Epoch 433, Batch: 0, Loss: 1.050250
Train - Epoch 434, Batch: 0, Loss: 1.057103
Train - Epoch 435, Batch: 0, Loss: 1.035024
Train - Epoch 436, Batch: 0, Loss: 1.046725
Train - Epoch 437, Batch: 0, Loss: 1.055903
Train - Epoch 438, Batch: 0, Loss: 1.042292
Train - Epoch 439, Batch: 0, Loss: 1.057633
Train - Epoch 440, Batch: 0, Loss: 1.055269
Train - Epoch 441, Batch: 0, Loss: 1.046263
Train - Epoch 442, Batch: 0, Loss: 1.026985
Train - Epoch 443, Batch: 0, Loss: 1.053773
Train - Epoch 444, Batch: 0, Loss: 1.044010
Train - Epoch 445, Batch: 0, Loss: 1.050361
Train - Epoch 446, Batch: 0, Loss: 1.041527
Train - Epoch 447, Batch: 0, Loss: 1.032718
Train - Epoch 448, Batch: 0, Loss: 1.026755
Train - Epoch 449, Batch: 0, Loss: 1.039187
Train - Epoch 450, Batch: 0, Loss: 1.053513
Train - Epoch 451, Batch: 0, Loss: 1.039657
Train - Epoch 452, Batch: 0, Loss: 1.057074
Train - Epoch 453, Batch: 0, Loss: 1.039542
Train - Epoch 454, Batch: 0, Loss: 1.036514
Train - Epoch 455, Batch: 0, Loss: 1.045413
Train - Epoch 456, Batch: 0, Loss: 1.056073
Train - Epoch 457, Batch: 0, Loss: 1.056945
Train - Epoch 458, Batch: 0, Loss: 1.047380
Train - Epoch 459, Batch: 0, Loss: 1.042638
Train - Epoch 460, Batch: 0, Loss: 1.051658
Train - Epoch 461, Batch: 0, Loss: 1.047355
Train - Epoch 462, Batch: 0, Loss: 1.053966
Train - Epoch 463, Batch: 0, Loss: 1.040983
Train - Epoch 464, Batch: 0, Loss: 1.042434
Train - Epoch 465, Batch: 0, Loss: 1.043407
Train - Epoch 466, Batch: 0, Loss: 1.057754
Train - Epoch 467, Batch: 0, Loss: 1.065174
Train - Epoch 468, Batch: 0, Loss: 1.032355
Train - Epoch 469, Batch: 0, Loss: 1.054750
Train - Epoch 470, Batch: 0, Loss: 1.038619
Train - Epoch 471, Batch: 0, Loss: 1.044721
Train - Epoch 472, Batch: 0, Loss: 1.025944
Train - Epoch 473, Batch: 0, Loss: 1.037868
Train - Epoch 474, Batch: 0, Loss: 1.040655
Train - Epoch 475, Batch: 0, Loss: 1.052351
Train - Epoch 476, Batch: 0, Loss: 1.040126
Train - Epoch 477, Batch: 0, Loss: 1.044396
Train - Epoch 478, Batch: 0, Loss: 1.048456
Train - Epoch 479, Batch: 0, Loss: 1.060755
Train - Epoch 480, Batch: 0, Loss: 1.046416
Train - Epoch 481, Batch: 0, Loss: 1.058409
Train - Epoch 482, Batch: 0, Loss: 1.040102
Train - Epoch 483, Batch: 0, Loss: 1.041398
Train - Epoch 484, Batch: 0, Loss: 1.041652
Train - Epoch 485, Batch: 0, Loss: 1.060229
Train - Epoch 486, Batch: 0, Loss: 1.039273
Train - Epoch 487, Batch: 0, Loss: 1.041815
Train - Epoch 488, Batch: 0, Loss: 1.039145
Train - Epoch 489, Batch: 0, Loss: 1.047232
Train - Epoch 490, Batch: 0, Loss: 1.036321
Train - Epoch 491, Batch: 0, Loss: 1.047036
Train - Epoch 492, Batch: 0, Loss: 1.040589
Train - Epoch 493, Batch: 0, Loss: 1.038135
Train - Epoch 494, Batch: 0, Loss: 1.024299
Train - Epoch 495, Batch: 0, Loss: 1.034741
Train - Epoch 496, Batch: 0, Loss: 1.028485
Train - Epoch 497, Batch: 0, Loss: 1.045890
Train - Epoch 498, Batch: 0, Loss: 1.033372
Train - Epoch 499, Batch: 0, Loss: 1.049836
training_time:: 107.62511539459229
training time full:: 107.62518692016602
provenance prepare time:: 0.0
here
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633000
baseline::
python3 incremental_updates_base_line_lr.py 0
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 500
max_epoch:: 500
batch_size:: 10000
epoch:: 0
tensor([    0, 32774,     6, 32778, 32780,    13,    15,    18,    19, 32788,
        32790, 32793, 32794, 32796, 32798,    38,    40,    42,    44, 32814,
           47,    46, 32817, 32818, 32819,    49,    52, 32816,    57, 32825,
           60, 32829,    62,    61,    65, 32835,    69,    72,    80,    84,
        32852, 32859,    91, 32861,    95,    98, 32868,   101,   100,   102])
epoch:: 1
epoch:: 2
epoch:: 3
epoch:: 4
epoch:: 5
epoch:: 6
epoch:: 7
epoch:: 8
epoch:: 9
epoch:: 10
epoch:: 11
epoch:: 12
epoch:: 13
epoch:: 14
epoch:: 15
epoch:: 16
epoch:: 17
epoch:: 18
epoch:: 19
epoch:: 20
epoch:: 21
epoch:: 22
epoch:: 23
epoch:: 24
epoch:: 25
epoch:: 26
epoch:: 27
epoch:: 28
epoch:: 29
epoch:: 30
epoch:: 31
epoch:: 32
epoch:: 33
epoch:: 34
epoch:: 35
epoch:: 36
epoch:: 37
epoch:: 38
epoch:: 39
epoch:: 40
epoch:: 41
epoch:: 42
epoch:: 43
epoch:: 44
epoch:: 45
epoch:: 46
epoch:: 47
epoch:: 48
epoch:: 49
epoch:: 50
epoch:: 51
epoch:: 52
epoch:: 53
epoch:: 54
epoch:: 55
epoch:: 56
epoch:: 57
epoch:: 58
epoch:: 59
epoch:: 60
epoch:: 61
epoch:: 62
epoch:: 63
epoch:: 64
epoch:: 65
epoch:: 66
epoch:: 67
epoch:: 68
epoch:: 69
epoch:: 70
epoch:: 71
epoch:: 72
epoch:: 73
epoch:: 74
epoch:: 75
epoch:: 76
epoch:: 77
epoch:: 78
epoch:: 79
epoch:: 80
epoch:: 81
epoch:: 82
epoch:: 83
epoch:: 84
epoch:: 85
epoch:: 86
epoch:: 87
epoch:: 88
epoch:: 89
epoch:: 90
epoch:: 91
epoch:: 92
epoch:: 93
epoch:: 94
epoch:: 95
epoch:: 96
epoch:: 97
epoch:: 98
epoch:: 99
epoch:: 100
epoch:: 101
epoch:: 102
epoch:: 103
epoch:: 104
epoch:: 105
epoch:: 106
epoch:: 107
epoch:: 108
epoch:: 109
epoch:: 110
epoch:: 111
epoch:: 112
epoch:: 113
epoch:: 114
epoch:: 115
epoch:: 116
epoch:: 117
epoch:: 118
epoch:: 119
epoch:: 120
epoch:: 121
epoch:: 122
epoch:: 123
epoch:: 124
epoch:: 125
epoch:: 126
epoch:: 127
epoch:: 128
epoch:: 129
epoch:: 130
epoch:: 131
epoch:: 132
epoch:: 133
epoch:: 134
epoch:: 135
epoch:: 136
epoch:: 137
epoch:: 138
epoch:: 139
epoch:: 140
epoch:: 141
epoch:: 142
epoch:: 143
epoch:: 144
epoch:: 145
epoch:: 146
epoch:: 147
epoch:: 148
epoch:: 149
epoch:: 150
epoch:: 151
epoch:: 152
epoch:: 153
epoch:: 154
epoch:: 155
epoch:: 156
epoch:: 157
epoch:: 158
epoch:: 159
epoch:: 160
epoch:: 161
epoch:: 162
epoch:: 163
epoch:: 164
epoch:: 165
epoch:: 166
epoch:: 167
epoch:: 168
epoch:: 169
epoch:: 170
epoch:: 171
epoch:: 172
epoch:: 173
epoch:: 174
epoch:: 175
epoch:: 176
epoch:: 177
epoch:: 178
epoch:: 179
epoch:: 180
epoch:: 181
epoch:: 182
epoch:: 183
epoch:: 184
epoch:: 185
epoch:: 186
epoch:: 187
epoch:: 188
epoch:: 189
epoch:: 190
epoch:: 191
epoch:: 192
epoch:: 193
epoch:: 194
epoch:: 195
epoch:: 196
epoch:: 197
epoch:: 198
epoch:: 199
epoch:: 200
epoch:: 201
epoch:: 202
epoch:: 203
epoch:: 204
epoch:: 205
epoch:: 206
epoch:: 207
epoch:: 208
epoch:: 209
epoch:: 210
epoch:: 211
epoch:: 212
epoch:: 213
epoch:: 214
epoch:: 215
epoch:: 216
epoch:: 217
epoch:: 218
epoch:: 219
epoch:: 220
epoch:: 221
epoch:: 222
epoch:: 223
epoch:: 224
epoch:: 225
epoch:: 226
epoch:: 227
epoch:: 228
epoch:: 229
epoch:: 230
epoch:: 231
epoch:: 232
epoch:: 233
epoch:: 234
epoch:: 235
epoch:: 236
epoch:: 237
epoch:: 238
epoch:: 239
epoch:: 240
epoch:: 241
epoch:: 242
epoch:: 243
epoch:: 244
epoch:: 245
epoch:: 246
epoch:: 247
epoch:: 248
epoch:: 249
epoch:: 250
epoch:: 251
epoch:: 252
epoch:: 253
epoch:: 254
epoch:: 255
epoch:: 256
epoch:: 257
epoch:: 258
epoch:: 259
epoch:: 260
epoch:: 261
epoch:: 262
epoch:: 263
epoch:: 264
epoch:: 265
epoch:: 266
epoch:: 267
epoch:: 268
epoch:: 269
epoch:: 270
epoch:: 271
epoch:: 272
epoch:: 273
epoch:: 274
epoch:: 275
epoch:: 276
epoch:: 277
epoch:: 278
epoch:: 279
epoch:: 280
epoch:: 281
epoch:: 282
epoch:: 283
epoch:: 284
epoch:: 285
epoch:: 286
epoch:: 287
epoch:: 288
epoch:: 289
epoch:: 290
epoch:: 291
epoch:: 292
epoch:: 293
epoch:: 294
epoch:: 295
epoch:: 296
epoch:: 297
epoch:: 298
epoch:: 299
epoch:: 300
epoch:: 301
epoch:: 302
epoch:: 303
epoch:: 304
epoch:: 305
epoch:: 306
epoch:: 307
epoch:: 308
epoch:: 309
epoch:: 310
epoch:: 311
epoch:: 312
epoch:: 313
epoch:: 314
epoch:: 315
epoch:: 316
epoch:: 317
epoch:: 318
epoch:: 319
epoch:: 320
epoch:: 321
epoch:: 322
epoch:: 323
epoch:: 324
epoch:: 325
epoch:: 326
epoch:: 327
epoch:: 328
epoch:: 329
epoch:: 330
epoch:: 331
epoch:: 332
epoch:: 333
epoch:: 334
epoch:: 335
epoch:: 336
epoch:: 337
epoch:: 338
epoch:: 339
epoch:: 340
epoch:: 341
epoch:: 342
epoch:: 343
epoch:: 344
epoch:: 345
epoch:: 346
epoch:: 347
epoch:: 348
epoch:: 349
epoch:: 350
epoch:: 351
epoch:: 352
epoch:: 353
epoch:: 354
epoch:: 355
epoch:: 356
epoch:: 357
epoch:: 358
epoch:: 359
epoch:: 360
epoch:: 361
epoch:: 362
epoch:: 363
epoch:: 364
epoch:: 365
epoch:: 366
epoch:: 367
epoch:: 368
epoch:: 369
epoch:: 370
epoch:: 371
epoch:: 372
epoch:: 373
epoch:: 374
epoch:: 375
epoch:: 376
epoch:: 377
epoch:: 378
epoch:: 379
epoch:: 380
epoch:: 381
epoch:: 382
epoch:: 383
epoch:: 384
epoch:: 385
epoch:: 386
epoch:: 387
epoch:: 388
epoch:: 389
epoch:: 390
epoch:: 391
epoch:: 392
epoch:: 393
epoch:: 394
epoch:: 395
epoch:: 396
epoch:: 397
epoch:: 398
epoch:: 399
epoch:: 400
epoch:: 401
epoch:: 402
epoch:: 403
epoch:: 404
epoch:: 405
epoch:: 406
epoch:: 407
epoch:: 408
epoch:: 409
epoch:: 410
epoch:: 411
epoch:: 412
epoch:: 413
epoch:: 414
epoch:: 415
epoch:: 416
epoch:: 417
epoch:: 418
epoch:: 419
epoch:: 420
epoch:: 421
epoch:: 422
epoch:: 423
epoch:: 424
epoch:: 425
epoch:: 426
epoch:: 427
epoch:: 428
epoch:: 429
epoch:: 430
epoch:: 431
epoch:: 432
epoch:: 433
epoch:: 434
epoch:: 435
epoch:: 436
epoch:: 437
epoch:: 438
epoch:: 439
epoch:: 440
epoch:: 441
epoch:: 442
epoch:: 443
epoch:: 444
epoch:: 445
epoch:: 446
epoch:: 447
epoch:: 448
epoch:: 449
epoch:: 450
epoch:: 451
epoch:: 452
epoch:: 453
epoch:: 454
epoch:: 455
epoch:: 456
epoch:: 457
epoch:: 458
epoch:: 459
epoch:: 460
epoch:: 461
epoch:: 462
epoch:: 463
epoch:: 464
epoch:: 465
epoch:: 466
epoch:: 467
epoch:: 468
epoch:: 469
epoch:: 470
epoch:: 471
epoch:: 472
epoch:: 473
epoch:: 474
epoch:: 475
epoch:: 476
epoch:: 477
epoch:: 478
epoch:: 479
epoch:: 480
epoch:: 481
epoch:: 482
epoch:: 483
epoch:: 484
epoch:: 485
epoch:: 486
epoch:: 487
epoch:: 488
epoch:: 489
epoch:: 490
epoch:: 491
epoch:: 492
epoch:: 493
epoch:: 494
epoch:: 495
epoch:: 496
epoch:: 497
epoch:: 498
epoch:: 499
training time is 80.27076983451843
overhead:: 0
overhead2:: 2.6626739501953125
overhead3:: 0
time_baseline:: 80.2708146572113
curr_diff: 0 tensor(0.1926, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1926, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633200
period:: 10
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 10 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 500
max_epoch:: 500
overhead:: 0
overhead2:: 0.08464527130126953
overhead3:: 0.26685380935668945
overhead4:: 9.763242721557617
overhead5:: 0
memory usage:: 5632401408
time_provenance:: 18.188971042633057
curr_diff: 0 tensor(0.0462, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0462, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.2278, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.2278, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633500
period:: 10
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 10 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 500
max_epoch:: 500
overhead:: 0
overhead2:: 0.09473514556884766
overhead3:: 0.26045870780944824
overhead4:: 9.858166456222534
overhead5:: 0
memory usage:: 5639475200
time_provenance:: 18.36968183517456
curr_diff: 0 tensor(0.0462, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0462, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.2278, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.2278, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633500
period:: 10
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 10 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 500
max_epoch:: 500
overhead:: 0
overhead2:: 0.0960843563079834
overhead3:: 0.2685511112213135
overhead4:: 10.104350090026855
overhead5:: 0
memory usage:: 5645193216
time_provenance:: 18.697660207748413
curr_diff: 0 tensor(0.0462, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0462, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.2278, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.2278, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633500
period:: 5
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 5 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 500
max_epoch:: 500
overhead:: 0
overhead2:: 0.17261099815368652
overhead3:: 0.4507160186767578
overhead4:: 17.075709104537964
overhead5:: 0
memory usage:: 5629108224
time_provenance:: 27.58632731437683
curr_diff: 0 tensor(0.0048, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0048, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1960, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1960, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633000
period:: 5
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 5 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 500
max_epoch:: 500
overhead:: 0
overhead2:: 0.1700434684753418
overhead3:: 0.4464728832244873
overhead4:: 17.431405544281006
overhead5:: 0
memory usage:: 5646733312
time_provenance:: 28.066457986831665
curr_diff: 0 tensor(0.0048, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0048, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1960, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1960, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633000
period:: 5
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 500
max_epoch:: 500
overhead:: 0
overhead2:: 0.18008971214294434
overhead3:: 0.4560661315917969
overhead4:: 17.507110357284546
overhead5:: 0
memory usage:: 5636763648
time_provenance:: 28.19748306274414
curr_diff: 0 tensor(0.0046, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0046, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1960, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1960, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633000
period:: 2
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 2 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 500
max_epoch:: 500
overhead:: 0
overhead2:: 0.4280869960784912
overhead3:: 1.002237319946289
overhead4:: 40.9122154712677
overhead5:: 0
memory usage:: 5643046912
time_provenance:: 57.813780784606934
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1928, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1928, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633200
period:: 2
init_iters:: 20
incremental updates::
python3 incremental_updates_provenance3_lr.py 20 2 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 500
max_epoch:: 500
overhead:: 0
overhead2:: 0.41330456733703613
overhead3:: 1.03477144241333
overhead4:: 40.73448348045349
overhead5:: 0
memory usage:: 5643436032
time_provenance:: 57.64823508262634
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1928, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1928, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633200
period:: 2
init_iters:: 30
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 2 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 500
max_epoch:: 500
overhead:: 0
overhead2:: 0.42911195755004883
overhead3:: 1.024949312210083
overhead4:: 40.96576189994812
overhead5:: 0
memory usage:: 5627613184
time_provenance:: 57.931581258773804
curr_diff: 0 tensor(0.0007, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0007, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1928, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1928, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633200
period:: 1
init_iters:: 10
incremental updates::
python3 incremental_updates_provenance3_lr.py 10 1 4 0.01 6000
/home/wuyinjun/ML_provenance/src/sensitivity_analysis_SGD/Models/DNN_single.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  out = self.fc2(out1)
max_epoch:: 500
delta_size:: 500
max_epoch:: 500
overhead:: 0
overhead2:: 0.8322954177856445
overhead3:: 2.072225332260132
overhead4:: 81.74013876914978
overhead5:: 0
memory usage:: 5632761856
time_provenance:: 106.9212338924408
curr_diff: 0 tensor(5.0025e-13, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.0025e-13, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.1926, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.1926, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(0.9998, dtype=torch.float64, grad_fn=<DivBackward0>)
start test::
Test Avg. Loss: 0.000107, Accuracy: 0.633200
