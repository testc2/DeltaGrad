period::5
init_iters::30
varied deletion rate::
varied number of samples::
deletion rate:: 0.00002
python3 generate_rand_ids 0.00002  MNIST5 1
tensor([19336])
batch size:: 60000
repetition 1
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 108.31935715675354
training time full:: 108.329434633255
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 1
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 106.36869764328003
time_baseline:: 106.63812923431396
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 120
delta_size:: 1
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 54.7057991027832
curr_diff: 0 tensor(9.5201e-07, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.5201e-07, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
repetition 2
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 110.37836766242981
training time full:: 110.39014625549316
provenance prepare time:: 2.384185791015625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 1
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 108.18848633766174
time_baseline:: 108.46046829223633
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 120
delta_size:: 1
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 53.6483097076416
curr_diff: 0 tensor(9.5201e-07, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.5201e-07, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
repetition 3
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 109.02108144760132
training time full:: 109.03311944007874
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 1
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 105.26238417625427
time_baseline:: 105.53214693069458
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 120
delta_size:: 1
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 53.71364068984985
curr_diff: 0 tensor(9.5201e-07, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.5201e-07, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
repetition 4
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 109.46363234519958
training time full:: 109.47536993026733
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 1
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 106.25507950782776
time_baseline:: 106.53802251815796
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 120
delta_size:: 1
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 54.33122277259827
curr_diff: 0 tensor(9.5201e-07, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.5201e-07, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
repetition 5
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 109.40935778617859
training time full:: 109.4210958480835
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 1
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 105.8725688457489
time_baseline:: 106.14244627952576
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 120
delta_size:: 1
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 53.95805096626282
curr_diff: 0 tensor(9.5201e-07, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.5201e-07, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
batch size:: 30000
repetition 1
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 53.14327597618103
training time full:: 53.15037775039673
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 1
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.91468405723572
time_baseline:: 49.05433988571167
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 60
delta_size:: 1
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.345584630966187
curr_diff: 0 tensor(1.2770e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2770e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
repetition 2
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.438161849975586
training time full:: 52.44413661956787
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 1
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.6002459526062
time_baseline:: 48.73736619949341
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 60
delta_size:: 1
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.40451169013977
curr_diff: 0 tensor(1.2770e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2770e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
repetition 3
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 51.898661613464355
training time full:: 51.90492820739746
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 1
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.445547103881836
time_baseline:: 48.58156871795654
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 60
delta_size:: 1
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.323307037353516
curr_diff: 0 tensor(1.2770e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2770e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
repetition 4
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 53.01452398300171
training time full:: 53.02083683013916
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 1
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 47.98455739021301
time_baseline:: 48.11845827102661
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 60
delta_size:: 1
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.130934715270996
curr_diff: 0 tensor(1.2770e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2770e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
repetition 5
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.81183671951294
training time full:: 52.81823134422302
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 1
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.16903305053711
time_baseline:: 48.30342483520508
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 60
delta_size:: 1
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.35671591758728
curr_diff: 0 tensor(1.2770e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2770e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
batch size:: 16384
repetition 1
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.422756910324097
training time full:: 27.425287008285522
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.405959844589233
time_baseline:: 24.476279497146606
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 12.876620292663574
curr_diff: 0 tensor(1.2409e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2409e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 2
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.321136236190796
training time full:: 27.323286533355713
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.642716646194458
time_baseline:: 24.71428155899048
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 13.096258640289307
curr_diff: 0 tensor(1.2409e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2409e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 3
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.20640993118286
training time full:: 27.208566665649414
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.363459825515747
time_baseline:: 24.43728470802307
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 12.854122877120972
curr_diff: 0 tensor(1.2409e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2409e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 4
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.236714363098145
training time full:: 27.238845109939575
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.321152687072754
time_baseline:: 24.391592025756836
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 13.060303211212158
curr_diff: 0 tensor(1.2409e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2409e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 5
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.34406542778015
training time full:: 27.34626579284668
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.415921449661255
time_baseline:: 24.487112760543823
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 32
delta_size:: 1
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 13.183577299118042
curr_diff: 0 tensor(1.2409e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.2409e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
batch size:: 4096
repetition 1
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.8796775341033936
training time full:: 5.879716157913208
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 4.990745544433594
time_baseline:: 5.008067846298218
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.147199869155884
curr_diff: 0 tensor(2.7568e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.7568e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
repetition 2
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.86673903465271
training time full:: 5.86677622795105
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.067443132400513
time_baseline:: 5.0851287841796875
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.1618683338165283
curr_diff: 0 tensor(2.7568e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.7568e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
repetition 3
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.828330993652344
training time full:: 5.828370094299316
provenance prepare time:: 2.384185791015625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.0562779903411865
time_baseline:: 5.074018239974976
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.078145742416382
curr_diff: 0 tensor(2.7568e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.7568e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
repetition 4
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.777572870254517
training time full:: 5.777610540390015
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.269436597824097
time_baseline:: 5.286624908447266
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.114753484725952
curr_diff: 0 tensor(2.7568e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.7568e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
repetition 5
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.932586193084717
training time full:: 5.932624101638794
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 4.973837852478027
time_baseline:: 4.9914000034332275
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 8
delta_size:: 1
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.161067485809326
curr_diff: 0 tensor(2.7568e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.7568e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
batch size:: 1024
repetition 1
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.7174999713897705
training time full:: 1.7175347805023193
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 1
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4426195621490479
time_baseline:: 1.4477300643920898
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 2
delta_size:: 1
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.0244088172912598
curr_diff: 0 tensor(9.9019e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.9019e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
repetition 2
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.6626923084259033
training time full:: 1.6627287864685059
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 1
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.462836503982544
time_baseline:: 1.4676415920257568
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 2
delta_size:: 1
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.0352702140808105
curr_diff: 0 tensor(9.9019e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.9019e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
repetition 3
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.6778545379638672
training time full:: 1.677891731262207
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 1
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.449211597442627
time_baseline:: 1.4543039798736572
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 2
delta_size:: 1
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.018855333328247
curr_diff: 0 tensor(9.9019e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.9019e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
repetition 4
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.7088489532470703
training time full:: 1.7088866233825684
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 1
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4290564060211182
time_baseline:: 1.433915615081787
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 2
delta_size:: 1
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.0753004550933838
curr_diff: 0 tensor(9.9019e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.9019e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
repetition 5
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.6494123935699463
training time full:: 1.6494500637054443
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 1
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4569430351257324
time_baseline:: 1.461918830871582
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 2
delta_size:: 1
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.0129880905151367
curr_diff: 0 tensor(9.9019e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.9019e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
deletion rate:: 0.00005
python3 generate_rand_ids 0.00005  MNIST5 0
tensor([19336, 19819, 54120])
batch size:: 60000
repetition 1
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 108.89015603065491
training time full:: 108.90028595924377
provenance prepare time:: 2.384185791015625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 3
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 106.04564142227173
time_baseline:: 106.32639956474304
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 120
delta_size:: 3
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 53.393173694610596
curr_diff: 0 tensor(2.5021e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.5021e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
repetition 2
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 109.85588264465332
training time full:: 109.86869239807129
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 3
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 106.4728331565857
time_baseline:: 106.74254035949707
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 120
delta_size:: 3
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 53.56696820259094
curr_diff: 0 tensor(2.5021e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.5021e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
repetition 3
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 109.79750514030457
training time full:: 109.80907082557678
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 3
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 105.68328523635864
time_baseline:: 105.95354890823364
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 120
delta_size:: 3
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 53.60429525375366
curr_diff: 0 tensor(2.5021e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.5021e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
repetition 4
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 109.01705622673035
training time full:: 109.02864933013916
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 3
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 107.6895227432251
time_baseline:: 107.96536350250244
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 120
delta_size:: 3
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 53.396811723709106
curr_diff: 0 tensor(2.5021e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.5021e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
repetition 5
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 108.87708330154419
training time full:: 108.88882565498352
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 3
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 105.90391635894775
time_baseline:: 106.18016576766968
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 120
delta_size:: 3
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 53.55224061012268
curr_diff: 0 tensor(2.5021e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.5021e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
batch size:: 30000
repetition 1
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 53.06090521812439
training time full:: 53.06802535057068
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 47.67151713371277
time_baseline:: 47.807454109191895
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 26.188270092010498
curr_diff: 0 tensor(6.9315e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.9315e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
repetition 2
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.935906171798706
training time full:: 52.94226789474487
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.231289863586426
time_baseline:: 48.36696457862854
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.566181898117065
curr_diff: 0 tensor(6.9315e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.9315e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
repetition 3
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.72602677345276
training time full:: 52.7321879863739
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 47.941463232040405
time_baseline:: 48.07883977890015
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.771451950073242
curr_diff: 0 tensor(6.9315e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.9315e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
repetition 4
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.42971992492676
training time full:: 52.435797929763794
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.29030442237854
time_baseline:: 48.4260094165802
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.963507175445557
curr_diff: 0 tensor(6.9315e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.9315e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
repetition 5
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.091591119766235
training time full:: 52.09773802757263
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 49.1960129737854
time_baseline:: 49.33132004737854
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 60
delta_size:: 3
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.59975528717041
curr_diff: 0 tensor(6.9315e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.9315e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
batch size:: 16384
repetition 1
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.420380353927612
training time full:: 27.422874450683594
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.252057790756226
time_baseline:: 24.32626700401306
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 13.031420946121216
curr_diff: 0 tensor(1.1148e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1148e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 2
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.327789783477783
training time full:: 27.329963445663452
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.22024965286255
time_baseline:: 24.291805028915405
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 13.017010927200317
curr_diff: 0 tensor(1.1148e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1148e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 3
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.568923473358154
training time full:: 27.571075201034546
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.373293161392212
time_baseline:: 24.44448447227478
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 12.98890471458435
curr_diff: 0 tensor(1.1148e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1148e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 4
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 26.94189691543579
training time full:: 26.944013833999634
provenance prepare time:: 2.6226043701171875e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.431569814682007
time_baseline:: 24.50297451019287
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 12.96239948272705
curr_diff: 0 tensor(1.1148e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1148e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 5
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.19036364555359
training time full:: 27.19256591796875
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.206692934036255
time_baseline:: 24.27819037437439
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 32
delta_size:: 3
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 13.120162963867188
curr_diff: 0 tensor(1.1148e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.1148e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
batch size:: 4096
repetition 1
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.8451972007751465
training time full:: 5.845238447189331
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 3
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 4.982267141342163
time_baseline:: 4.9997358322143555
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 8
delta_size:: 3
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.067826986312866
curr_diff: 0 tensor(4.6019e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.6019e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
repetition 2
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.860522270202637
training time full:: 5.860559463500977
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 3
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.235752105712891
time_baseline:: 5.253827333450317
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 8
delta_size:: 3
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.1030712127685547
curr_diff: 0 tensor(4.6019e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.6019e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
repetition 3
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.923101186752319
training time full:: 5.923141002655029
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 3
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.289060592651367
time_baseline:: 5.306484699249268
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 8
delta_size:: 3
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.1590280532836914
curr_diff: 0 tensor(4.6019e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.6019e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
repetition 4
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.887789487838745
training time full:: 5.887828350067139
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 3
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.166095018386841
time_baseline:: 5.1845481395721436
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 8
delta_size:: 3
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.077338695526123
curr_diff: 0 tensor(4.6019e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.6019e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
repetition 5
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.875681161880493
training time full:: 5.875718593597412
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 3
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.122070074081421
time_baseline:: 5.140064716339111
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 8
delta_size:: 3
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.2502899169921875
curr_diff: 0 tensor(4.6019e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.6019e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0008, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0008, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
batch size:: 1024
repetition 1
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.674532175064087
training time full:: 1.6745669841766357
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 3
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4333879947662354
time_baseline:: 1.4382338523864746
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 2
delta_size:: 3
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.030493974685669
curr_diff: 0 tensor(2.8993e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.8993e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
repetition 2
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.6872646808624268
training time full:: 1.6873037815093994
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 3
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4796831607818604
time_baseline:: 1.484431505203247
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 2
delta_size:: 3
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.0182476043701172
curr_diff: 0 tensor(2.8993e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.8993e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
repetition 3
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.638002634048462
training time full:: 1.6380400657653809
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 3
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4101529121398926
time_baseline:: 1.4150586128234863
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 2
delta_size:: 3
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.0162293910980225
curr_diff: 0 tensor(2.8993e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.8993e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
repetition 4
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.696545124053955
training time full:: 1.6965923309326172
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 3
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4384539127349854
time_baseline:: 1.4433567523956299
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 2
delta_size:: 3
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.044536828994751
curr_diff: 0 tensor(2.8993e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.8993e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
repetition 5
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.6742980480194092
training time full:: 1.6743347644805908
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 3
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.409780502319336
time_baseline:: 1.4147169589996338
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 2
delta_size:: 3
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.0274937152862549
curr_diff: 0 tensor(2.8993e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.8993e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
deletion rate:: 0.0001
python3 generate_rand_ids 0.0001  MNIST5 0
tensor([16624, 32528, 54120, 19336, 11177, 19819])
batch size:: 60000
repetition 1
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 107.33278799057007
training time full:: 107.34292483329773
provenance prepare time:: 2.384185791015625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 6
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 106.44941425323486
time_baseline:: 106.72709321975708
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 120
delta_size:: 6
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 53.847877979278564
curr_diff: 0 tensor(3.4349e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.4349e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
repetition 2
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 108.50918817520142
training time full:: 108.52103328704834
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 6
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 107.18462491035461
time_baseline:: 107.45617938041687
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 120
delta_size:: 6
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 53.89612412452698
curr_diff: 0 tensor(3.4349e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.4349e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
repetition 3
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 109.00879263877869
training time full:: 109.02033066749573
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 6
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 106.1033992767334
time_baseline:: 106.37519598007202
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 120
delta_size:: 6
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 53.981926679611206
curr_diff: 0 tensor(3.4349e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.4349e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
repetition 4
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 109.6142475605011
training time full:: 109.62582850456238
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 6
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 105.68057250976562
time_baseline:: 105.95124650001526
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 120
delta_size:: 6
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 54.110742807388306
curr_diff: 0 tensor(3.4349e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.4349e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
repetition 5
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 110.55641984939575
training time full:: 110.56814289093018
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 6
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 107.15960216522217
time_baseline:: 107.4346387386322
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 120
delta_size:: 6
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 54.24654173851013
curr_diff: 0 tensor(3.4349e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.4349e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
batch size:: 30000
repetition 1
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.37079882621765
training time full:: 52.37801504135132
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.205689430236816
time_baseline:: 48.34239602088928
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.47594666481018
curr_diff: 0 tensor(1.0051e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0051e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
repetition 2
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.98797035217285
training time full:: 52.99426770210266
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.30145502090454
time_baseline:: 48.443166732788086
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.67639470100403
curr_diff: 0 tensor(1.0051e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0051e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
repetition 3
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 53.37554740905762
training time full:: 53.38170886039734
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.463884592056274
time_baseline:: 48.59969925880432
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.280534029006958
curr_diff: 0 tensor(1.0051e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0051e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
repetition 4
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.70733594894409
training time full:: 52.7135169506073
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.56332206726074
time_baseline:: 48.7008912563324
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.53280019760132
curr_diff: 0 tensor(1.0051e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0051e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
repetition 5
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.66874361038208
training time full:: 52.67499232292175
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.21060013771057
time_baseline:: 48.35137462615967
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 60
delta_size:: 6
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.644951343536377
curr_diff: 0 tensor(1.0051e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.0051e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
batch size:: 16384
repetition 1
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.472397327423096
training time full:: 27.474815845489502
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.429235458374023
time_baseline:: 24.50075054168701
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 13.239405870437622
curr_diff: 0 tensor(2.2792e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.2792e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 2
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.38450789451599
training time full:: 27.386720657348633
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.342928886413574
time_baseline:: 24.413785219192505
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 13.01329493522644
curr_diff: 0 tensor(2.2792e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.2792e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 3
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.484364986419678
training time full:: 27.48651361465454
provenance prepare time:: 2.384185791015625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.305663108825684
time_baseline:: 24.378448486328125
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 13.12034821510315
curr_diff: 0 tensor(2.2792e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.2792e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 4
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.050975561141968
training time full:: 27.053125858306885
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.511151552200317
time_baseline:: 24.59285879135132
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 13.154550552368164
curr_diff: 0 tensor(2.2792e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.2792e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 5
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 26.966834545135498
training time full:: 26.96899151802063
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.513866662979126
time_baseline:: 24.585431814193726
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 32
delta_size:: 6
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 13.130040168762207
curr_diff: 0 tensor(2.2792e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.2792e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
batch size:: 4096
repetition 1
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.802419900894165
training time full:: 5.802457332611084
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 6
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.00279426574707
time_baseline:: 5.020516872406006
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 8
delta_size:: 6
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.1291422843933105
curr_diff: 0 tensor(9.2816e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.2816e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
repetition 2
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.762554883956909
training time full:: 5.762596607208252
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 6
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.053258180618286
time_baseline:: 5.071054458618164
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 8
delta_size:: 6
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.1609694957733154
curr_diff: 0 tensor(9.2816e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.2816e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
repetition 3
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.9039506912231445
training time full:: 5.903989315032959
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 6
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.1329405307769775
time_baseline:: 5.150521993637085
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 8
delta_size:: 6
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.2432479858398438
curr_diff: 0 tensor(9.2816e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.2816e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
repetition 4
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.885781764984131
training time full:: 5.885819673538208
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 6
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.194157600402832
time_baseline:: 5.21432638168335
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 8
delta_size:: 6
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.1540510654449463
curr_diff: 0 tensor(9.2816e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.2816e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
repetition 5
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.820261001586914
training time full:: 5.82029914855957
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 6
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.150237560272217
time_baseline:: 5.169007062911987
curr_diff: 0 tensor(0.0012, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0012, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 8
delta_size:: 6
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.1513757705688477
curr_diff: 0 tensor(9.2816e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.2816e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
batch size:: 1024
repetition 1
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.73966646194458
training time full:: 1.7397041320800781
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 6
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.438025951385498
time_baseline:: 1.4428644180297852
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 2
delta_size:: 6
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.055335521697998
curr_diff: 0 tensor(5.0013e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.0013e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
repetition 2
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.6451127529144287
training time full:: 1.6451468467712402
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 6
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4237642288208008
time_baseline:: 1.4285714626312256
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 2
delta_size:: 6
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.042710542678833
curr_diff: 0 tensor(5.0013e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.0013e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
repetition 3
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.7022647857666016
training time full:: 1.7023048400878906
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 6
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4269285202026367
time_baseline:: 1.4318008422851562
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 2
delta_size:: 6
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.1092243194580078
curr_diff: 0 tensor(5.0013e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.0013e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
repetition 4
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.6460657119750977
training time full:: 1.6461024284362793
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 6
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4519810676574707
time_baseline:: 1.4568073749542236
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 2
delta_size:: 6
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.0233440399169922
curr_diff: 0 tensor(5.0013e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.0013e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
repetition 5
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.7089762687683105
training time full:: 1.7090117931365967
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 6
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.425220012664795
time_baseline:: 1.4299523830413818
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 2
delta_size:: 6
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.0391523838043213
curr_diff: 0 tensor(5.0013e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(5.0013e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0013, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0013, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
deletion rate:: 0.0002
python3 generate_rand_ids 0.0002  MNIST5 0
tensor([57409, 54177, 32646, 19336, 54120, 11177, 19819, 11369, 55786, 18890,
        16624, 32528])
batch size:: 60000
repetition 1
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 108.57481098175049
training time full:: 108.58494019508362
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 12
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 106.16837763786316
time_baseline:: 106.43648910522461
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 120
delta_size:: 12
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 54.26393485069275
curr_diff: 0 tensor(4.8601e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.8601e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
repetition 2
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 108.38416028022766
training time full:: 108.39572811126709
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 12
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 107.08670616149902
time_baseline:: 107.35666966438293
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 120
delta_size:: 12
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 54.567190170288086
curr_diff: 0 tensor(4.8601e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.8601e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
repetition 3
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 109.3070023059845
training time full:: 109.31859278678894
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 12
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 106.70955848693848
time_baseline:: 106.98122310638428
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 120
delta_size:: 12
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 53.88329482078552
curr_diff: 0 tensor(4.8601e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.8601e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
repetition 4
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 109.82163882255554
training time full:: 109.83329319953918
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 12
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 107.10045552253723
time_baseline:: 107.37078475952148
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 120
delta_size:: 12
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 54.56321454048157
curr_diff: 0 tensor(4.8601e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.8601e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
repetition 5
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 109.44805407524109
training time full:: 109.4600121974945
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 12
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 107.06134009361267
time_baseline:: 107.32960486412048
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 120
delta_size:: 12
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 53.69914674758911
curr_diff: 0 tensor(4.8601e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.8601e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
batch size:: 30000
repetition 1
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.71099877357483
training time full:: 52.71796727180481
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 12
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.50620746612549
time_baseline:: 48.64104986190796
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 60
delta_size:: 12
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.402891874313354
curr_diff: 0 tensor(1.3959e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3959e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
repetition 2
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.308688163757324
training time full:: 52.31484770774841
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 12
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.14164757728577
time_baseline:: 48.27742075920105
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 60
delta_size:: 12
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.255171298980713
curr_diff: 0 tensor(1.3959e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3959e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
repetition 3
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 51.97111797332764
training time full:: 51.97752571105957
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 12
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 47.61036539077759
time_baseline:: 47.746225357055664
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 60
delta_size:: 12
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.946362495422363
curr_diff: 0 tensor(1.3959e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3959e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
repetition 4
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 53.55656933784485
training time full:: 53.562830686569214
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 12
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.79379987716675
time_baseline:: 48.9285831451416
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 60
delta_size:: 12
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.70814561843872
curr_diff: 0 tensor(1.3959e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3959e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
repetition 5
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.33097052574158
training time full:: 52.3371217250824
provenance prepare time:: 2.384185791015625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 12
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.67988848686218
time_baseline:: 48.81761121749878
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 60
delta_size:: 12
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.794479608535767
curr_diff: 0 tensor(1.3959e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.3959e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
batch size:: 16384
repetition 1
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.60832643508911
training time full:: 27.610870838165283
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.51875638961792
time_baseline:: 24.595772981643677
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 13.107954502105713
curr_diff: 0 tensor(2.7752e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.7752e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 2
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.47739267349243
training time full:: 27.479623556137085
provenance prepare time:: 2.6226043701171875e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.25078296661377
time_baseline:: 24.3243727684021
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 13.162484884262085
curr_diff: 0 tensor(2.7752e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.7752e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 3
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.19074845314026
training time full:: 27.192960739135742
provenance prepare time:: 2.384185791015625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.1089768409729
time_baseline:: 24.180182933807373
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 12.916563034057617
curr_diff: 0 tensor(2.7752e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.7752e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 4
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.096314668655396
training time full:: 27.09848141670227
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.35110878944397
time_baseline:: 24.4223575592041
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 13.08996033668518
curr_diff: 0 tensor(2.7752e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.7752e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 5
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.27239489555359
training time full:: 27.27459716796875
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.446767568588257
time_baseline:: 24.51811981201172
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 32
delta_size:: 12
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 13.237263202667236
curr_diff: 0 tensor(2.7752e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.7752e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
batch size:: 4096
repetition 1
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.819061994552612
training time full:: 5.819098234176636
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 12
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.03459906578064
time_baseline:: 5.052696466445923
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 8
delta_size:: 12
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.126499652862549
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
repetition 2
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.843355655670166
training time full:: 5.8433918952941895
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 12
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 4.968511581420898
time_baseline:: 4.985961437225342
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 8
delta_size:: 12
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.245330333709717
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
repetition 3
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.786824464797974
training time full:: 5.786861181259155
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 12
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.1233909130096436
time_baseline:: 5.141057729721069
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 8
delta_size:: 12
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.1192002296447754
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
repetition 4
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.819582462310791
training time full:: 5.819622993469238
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 12
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.146536588668823
time_baseline:: 5.166660308837891
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 8
delta_size:: 12
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.160959482192993
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
repetition 5
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.78396201133728
training time full:: 5.784007787704468
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 12
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.179413080215454
time_baseline:: 5.1971681118011475
curr_diff: 0 tensor(0.0017, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0017, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 8
delta_size:: 12
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.275535821914673
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874900
batch size:: 1024
repetition 1
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.6745500564575195
training time full:: 1.674586534500122
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 12
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.5216896533966064
time_baseline:: 1.5266358852386475
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 2
delta_size:: 12
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.02783203125
curr_diff: 0 tensor(9.5186e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.5186e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
repetition 2
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.6710736751556396
training time full:: 1.6711130142211914
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 12
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4239962100982666
time_baseline:: 1.4287095069885254
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 2
delta_size:: 12
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.0281879901885986
curr_diff: 0 tensor(9.5186e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.5186e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
repetition 3
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.7294423580169678
training time full:: 1.7294809818267822
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 12
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.438598871231079
time_baseline:: 1.443507194519043
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 2
delta_size:: 12
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.065443992614746
curr_diff: 0 tensor(9.5186e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.5186e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
repetition 4
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.7040131092071533
training time full:: 1.7040534019470215
provenance prepare time:: 2.384185791015625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 12
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4161086082458496
time_baseline:: 1.4208662509918213
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 2
delta_size:: 12
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.0457067489624023
curr_diff: 0 tensor(9.5186e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.5186e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
repetition 5
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.6678330898284912
training time full:: 1.66786789894104
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 12
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4238004684448242
time_baseline:: 1.429004430770874
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 2
delta_size:: 12
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.0344805717468262
curr_diff: 0 tensor(9.5186e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(9.5186e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0018, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0018, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
deletion rate:: 0.0005
python3 generate_rand_ids 0.0005  MNIST5 0
tensor([21504, 57409, 32646, 19336, 52744, 18890, 38154, 36362, 32528, 28500,
        35092, 56151, 40156, 14046,  4153, 54177, 54120, 11177, 11369, 19819,
        55786, 49259,  3565, 53753, 16624, 50100, 10295, 33720, 13817, 57722])
batch size:: 60000
repetition 1
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 109.49087905883789
training time full:: 109.50090169906616
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 30
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 106.41002750396729
time_baseline:: 106.67611694335938
curr_diff: 0 tensor(0.0028, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0028, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 120
delta_size:: 30
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 54.8550283908844
curr_diff: 0 tensor(8.4827e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.4827e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0028, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0028, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
repetition 2
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 109.54658007621765
training time full:: 109.5585412979126
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 30
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 106.67403364181519
time_baseline:: 106.94460773468018
curr_diff: 0 tensor(0.0028, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0028, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 120
delta_size:: 30
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 53.8433678150177
curr_diff: 0 tensor(8.4827e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.4827e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0028, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0028, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
repetition 3
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 109.49119091033936
training time full:: 109.50267601013184
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 30
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 106.53023409843445
time_baseline:: 106.79960298538208
curr_diff: 0 tensor(0.0028, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0028, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 120
delta_size:: 30
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 54.10782790184021
curr_diff: 0 tensor(8.4827e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.4827e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0028, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0028, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
repetition 4
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 108.91551113128662
training time full:: 108.92711901664734
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 30
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 108.36041617393494
time_baseline:: 108.63296866416931
curr_diff: 0 tensor(0.0028, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0028, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 120
delta_size:: 30
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 54.307586669921875
curr_diff: 0 tensor(8.4827e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.4827e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0028, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0028, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
repetition 5
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 109.46101498603821
training time full:: 109.47256183624268
provenance prepare time:: 2.384185791015625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 30
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 106.47099709510803
time_baseline:: 106.7401282787323
curr_diff: 0 tensor(0.0028, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0028, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 120
delta_size:: 30
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 53.938382387161255
curr_diff: 0 tensor(8.4827e-06, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.4827e-06, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0028, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0028, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875500
batch size:: 30000
repetition 1
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.90527057647705
training time full:: 52.912323236465454
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.26299476623535
time_baseline:: 48.39935064315796
curr_diff: 0 tensor(0.0028, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0028, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.883519649505615
curr_diff: 0 tensor(3.3456e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.3456e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0028, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0028, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
repetition 2
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 53.08345937728882
training time full:: 53.08963632583618
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 47.57313871383667
time_baseline:: 47.70841407775879
curr_diff: 0 tensor(0.0028, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0028, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.96851372718811
curr_diff: 0 tensor(3.3456e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.3456e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0028, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0028, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
repetition 3
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 53.1009418964386
training time full:: 53.10703921318054
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 47.88905119895935
time_baseline:: 48.02322816848755
curr_diff: 0 tensor(0.0028, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0028, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.333179712295532
curr_diff: 0 tensor(3.3456e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.3456e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0028, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0028, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
repetition 4
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.53157877922058
training time full:: 52.53774952888489
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.58789801597595
time_baseline:: 48.724034786224365
curr_diff: 0 tensor(0.0028, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0028, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.83455729484558
curr_diff: 0 tensor(3.3456e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.3456e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0028, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0028, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
repetition 5
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.640106201171875
training time full:: 52.64638829231262
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 47.780182123184204
time_baseline:: 47.91740107536316
curr_diff: 0 tensor(0.0028, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0028, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 60
delta_size:: 30
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.554747819900513
curr_diff: 0 tensor(3.3456e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(3.3456e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0028, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0028, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875600
batch size:: 16384
repetition 1
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.498322010040283
training time full:: 27.500800848007202
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.094268321990967
time_baseline:: 24.164363145828247
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 13.246100902557373
curr_diff: 0 tensor(4.1512e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.1512e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 2
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.746338605880737
training time full:: 27.74860906600952
provenance prepare time:: 2.86102294921875e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.53921341896057
time_baseline:: 24.610769510269165
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 13.111153602600098
curr_diff: 0 tensor(4.1512e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.1512e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 3
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.471632719039917
training time full:: 27.47385025024414
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.116286516189575
time_baseline:: 24.18752694129944
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 13.13647198677063
curr_diff: 0 tensor(4.1512e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.1512e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 4
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.236989736557007
training time full:: 27.239074230194092
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.710479497909546
time_baseline:: 24.780818462371826
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 13.17231035232544
curr_diff: 0 tensor(4.1512e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.1512e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 5
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.205142974853516
training time full:: 27.207285165786743
provenance prepare time:: 2.384185791015625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.171066999435425
time_baseline:: 24.241392135620117
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 32
delta_size:: 30
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 12.979621648788452
curr_diff: 0 tensor(4.1512e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.1512e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
batch size:: 4096
repetition 1
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.8617470264434814
training time full:: 5.861788272857666
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 30
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.168749809265137
time_baseline:: 5.1866514682769775
curr_diff: 0 tensor(0.0028, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0028, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 8
delta_size:: 30
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.2572531700134277
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
repetition 2
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.8565404415130615
training time full:: 5.85657811164856
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 30
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.088795185089111
time_baseline:: 5.106377840042114
curr_diff: 0 tensor(0.0028, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0028, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 8
delta_size:: 30
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.230757713317871
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
repetition 3
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.88292121887207
training time full:: 5.882958889007568
provenance prepare time:: 1.1920928955078125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 30
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.306572914123535
time_baseline:: 5.323857545852661
curr_diff: 0 tensor(0.0028, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0028, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 8
delta_size:: 30
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.195127487182617
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
repetition 4
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.788221836090088
training time full:: 5.788259744644165
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 30
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 4.952073097229004
time_baseline:: 4.969618320465088
curr_diff: 0 tensor(0.0028, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0028, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 8
delta_size:: 30
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.204608678817749
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
repetition 5
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.827892065048218
training time full:: 5.82793402671814
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 30
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.031905651092529
time_baseline:: 5.049567937850952
curr_diff: 0 tensor(0.0028, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0028, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 8
delta_size:: 30
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.1928741931915283
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0029, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0029, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.875000
batch size:: 1024
repetition 1
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.6886651515960693
training time full:: 1.6886999607086182
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 30
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4397187232971191
time_baseline:: 1.4446816444396973
curr_diff: 0 tensor(0.0028, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0028, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.872900
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 2
delta_size:: 30
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.0663471221923828
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0028, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0028, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.872900
repetition 2
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.6408112049102783
training time full:: 1.6408493518829346
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 30
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4182891845703125
time_baseline:: 1.4231173992156982
curr_diff: 0 tensor(0.0028, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0028, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.872900
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 2
delta_size:: 30
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.0458195209503174
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0028, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0028, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.872900
repetition 3
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.6812975406646729
training time full:: 1.681333303451538
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 30
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4369208812713623
time_baseline:: 1.442079782485962
curr_diff: 0 tensor(0.0028, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0028, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.872900
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 2
delta_size:: 30
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.0613391399383545
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0028, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0028, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.872900
repetition 4
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.6628108024597168
training time full:: 1.662846565246582
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 30
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4166436195373535
time_baseline:: 1.4215075969696045
curr_diff: 0 tensor(0.0028, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0028, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.872900
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 2
delta_size:: 30
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.0506458282470703
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0028, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0028, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.872900
repetition 5
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.761103868484497
training time full:: 1.7611393928527832
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 30
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4205923080444336
time_baseline:: 1.4260852336883545
curr_diff: 0 tensor(0.0028, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0028, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.872900
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 2
delta_size:: 30
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.0614221096038818
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0028, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0028, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.872900
deletion rate:: 0.001
python3 generate_rand_ids 0.001  MNIST5 0
tensor([21504, 25987, 40580, 48133, 32646, 19336, 52744, 38154, 36362, 54024,
        13817, 51081, 14346, 32528, 18448, 47506, 35092, 29856, 54177, 50467,
        26020,  9510, 11177, 50100,  2998, 10295, 33720,  4153, 20662, 28214,
        32703, 57408, 57409, 58825, 18890, 33610, 18891, 18250, 45900, 49097,
        22482, 20435, 28500, 57940, 56151, 40156, 14046, 15710, 54120, 11369,
        55786, 19819, 49259,  3565, 16624, 33778,  7411, 53753, 57722, 48893])
batch size:: 60000
repetition 1
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 107.74237322807312
training time full:: 107.75241756439209
provenance prepare time:: 2.384185791015625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 60
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 105.3226809501648
time_baseline:: 105.59119653701782
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875300
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 120
delta_size:: 60
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 54.20480251312256
curr_diff: 0 tensor(1.4618e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4618e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875300
repetition 2
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 110.31275582313538
training time full:: 110.32438659667969
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 60
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 105.97254371643066
time_baseline:: 106.2395350933075
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875300
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 120
delta_size:: 60
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 53.975239515304565
curr_diff: 0 tensor(1.4618e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4618e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875300
repetition 3
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 110.30477929115295
training time full:: 110.31644606590271
provenance prepare time:: 2.384185791015625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 60
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 106.48356413841248
time_baseline:: 106.7545862197876
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875300
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 120
delta_size:: 60
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 54.624096632003784
curr_diff: 0 tensor(1.4618e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4618e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875300
repetition 4
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 109.20494723320007
training time full:: 109.21664381027222
provenance prepare time:: 2.384185791015625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 60
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 106.67029738426208
time_baseline:: 106.94026589393616
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875300
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 120
delta_size:: 60
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 54.74505925178528
curr_diff: 0 tensor(1.4618e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4618e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875300
repetition 5
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 108.51273107528687
training time full:: 108.52436566352844
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 60
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 105.82665085792542
time_baseline:: 106.09827160835266
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875300
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 120
delta_size:: 60
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 54.26227951049805
curr_diff: 0 tensor(1.4618e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(1.4618e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875300
batch size:: 30000
repetition 1
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 53.175331115722656
training time full:: 53.18248391151428
provenance prepare time:: 2.384185791015625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.92814016342163
time_baseline:: 49.06415390968323
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875400
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.545934677124023
curr_diff: 0 tensor(4.9526e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.9526e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875200
repetition 2
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.72680711746216
training time full:: 52.73302221298218
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.27325129508972
time_baseline:: 48.41401410102844
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875400
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.63208508491516
curr_diff: 0 tensor(4.9526e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.9526e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875200
repetition 3
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.122390270233154
training time full:: 52.12855935096741
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.32459807395935
time_baseline:: 48.46051096916199
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875400
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.72812819480896
curr_diff: 0 tensor(4.9526e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.9526e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875200
repetition 4
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.91325926780701
training time full:: 52.91927695274353
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.97294473648071
time_baseline:: 49.11113977432251
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875400
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.64541983604431
curr_diff: 0 tensor(4.9526e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.9526e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875200
repetition 5
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 53.00813388824463
training time full:: 53.01424503326416
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.156416177749634
time_baseline:: 48.2915358543396
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875400
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 60
delta_size:: 60
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.9851291179657
curr_diff: 0 tensor(4.9526e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.9526e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875200
batch size:: 16384
repetition 1
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.487794637680054
training time full:: 27.490257263183594
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.135594129562378
time_baseline:: 24.207313060760498
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 13.205661058425903
curr_diff: 0 tensor(6.4925e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.4925e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0046, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0046, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 2
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.44296360015869
training time full:: 27.44536328315735
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.426079273223877
time_baseline:: 24.49750328063965
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 13.152499437332153
curr_diff: 0 tensor(6.4925e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.4925e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0046, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0046, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 3
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.377830266952515
training time full:: 27.380032539367676
provenance prepare time:: 2.6226043701171875e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.411754369735718
time_baseline:: 24.48494029045105
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 13.177397966384888
curr_diff: 0 tensor(6.4925e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.4925e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0046, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0046, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 4
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.31291890144348
training time full:: 27.315096616744995
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.172324419021606
time_baseline:: 24.242369651794434
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 13.209697961807251
curr_diff: 0 tensor(6.4925e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.4925e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0046, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0046, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
repetition 5
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.290385246276855
training time full:: 27.292495727539062
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.15295171737671
time_baseline:: 24.223891258239746
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 32
delta_size:: 60
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 13.384182929992676
curr_diff: 0 tensor(6.4925e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(6.4925e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0046, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0046, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876500
batch size:: 4096
repetition 1
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.835982322692871
training time full:: 5.836019515991211
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 60
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.1103901863098145
time_baseline:: 5.1282057762146
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874700
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 8
delta_size:: 60
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.240792989730835
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874800
repetition 2
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.804845333099365
training time full:: 5.804885149002075
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 60
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.079390048980713
time_baseline:: 5.097198247909546
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874700
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 8
delta_size:: 60
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.1932504177093506
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874800
repetition 3
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.811419725418091
training time full:: 5.811460494995117
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 60
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.0987749099731445
time_baseline:: 5.116703987121582
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874700
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 8
delta_size:: 60
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.1910924911499023
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874800
repetition 4
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.774246454238892
training time full:: 5.774283409118652
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 60
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 4.996922731399536
time_baseline:: 5.016878843307495
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874700
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 8
delta_size:: 60
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.2925710678100586
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874800
repetition 5
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.827126502990723
training time full:: 5.827165365219116
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 60
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.0586464405059814
time_baseline:: 5.076457977294922
curr_diff: 0 tensor(0.0043, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0043, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874700
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 8
delta_size:: 60
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.189304828643799
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874800
batch size:: 1024
repetition 1
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.666468620300293
training time full:: 1.6665043830871582
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 60
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4511573314666748
time_baseline:: 1.4561140537261963
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 2
delta_size:: 60
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.1039314270019531
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0047, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0047, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
repetition 2
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.7211880683898926
training time full:: 1.721235752105713
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 60
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4226980209350586
time_baseline:: 1.427605152130127
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 2
delta_size:: 60
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.0596587657928467
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0047, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0047, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
repetition 3
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.6533291339874268
training time full:: 1.6533653736114502
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 60
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4581835269927979
time_baseline:: 1.4628965854644775
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 2
delta_size:: 60
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.080552577972412
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0047, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0047, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
repetition 4
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.7017107009887695
training time full:: 1.7017455101013184
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 60
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4319186210632324
time_baseline:: 1.4367344379425049
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 2
delta_size:: 60
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.058828353881836
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0047, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0047, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
repetition 5
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.6517040729522705
training time full:: 1.651740550994873
provenance prepare time:: 2.384185791015625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 60
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4223213195800781
time_baseline:: 1.4270334243774414
curr_diff: 0 tensor(0.0045, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0045, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 2
delta_size:: 60
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.1080315113067627
curr_diff: 0 tensor(0.0004, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0004, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0047, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0047, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873000
deletion rate:: 0.002
python3 generate_rand_ids 0.002  MNIST5 0
tensor([21504, 48133, 52744, 54024, 38154, 36362, 14346, 32528, 18448, 35092,
        51222, 11292, 38941, 50467, 39204,  9510, 42280, 14634, 42495, 28214,
        10295, 24631,  4153, 57408, 57409,  4417, 49986, 21569, 33610, 18250,
        45900, 16975,  1873, 28500, 57940, 59989, 56151,  3163, 15710, 11358,
        26214, 54120, 11369, 19819, 49259, 42347, 12653, 22383, 57722, 11387,
        47227, 58493, 26241, 30082, 25987, 40580, 12797, 32646, 19336, 51081,
         7308, 13817, 51084, 53902, 47506,   915, 48275,  7581, 29856, 54177,
        39072, 26020, 33445, 11177,   427, 23213, 39090, 50100, 30389,  2998,
        20662, 33720, 46261, 17597, 32703, 53183,  8384, 58049, 58825, 18890,
        18891, 49097, 22731, 36816, 22482, 20435, 19666, 19412, 10456, 40156,
        14046, 25314,  2023, 55785, 55786, 11242,  1258,  3565, 42989, 15871,
        16624, 55024, 33778,  7411, 36849, 17396, 53753,  5114, 48893, 25343])
batch size:: 60000
repetition 1
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 110.18948936462402
training time full:: 110.19969606399536
provenance prepare time:: 2.6226043701171875e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 120
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 105.96050024032593
time_baseline:: 106.22911524772644
curr_diff: 0 tensor(0.0059, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0059, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875100
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 120
delta_size:: 120
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 54.015551805496216
curr_diff: 0 tensor(2.1614e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1614e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0059, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0059, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875100
repetition 2
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 108.68239092826843
training time full:: 108.69352340698242
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 120
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 105.53932046890259
time_baseline:: 105.81056356430054
curr_diff: 0 tensor(0.0059, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0059, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875100
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 120
delta_size:: 120
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 53.68734002113342
curr_diff: 0 tensor(2.1614e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1614e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0059, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0059, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875100
repetition 3
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 109.4592764377594
training time full:: 109.47128033638
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 120
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 105.64956593513489
time_baseline:: 105.9247772693634
curr_diff: 0 tensor(0.0059, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0059, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875100
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 120
delta_size:: 120
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 53.83098578453064
curr_diff: 0 tensor(2.1614e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1614e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0059, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0059, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875100
repetition 4
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 109.60631537437439
training time full:: 109.61778712272644
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 120
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 105.63283634185791
time_baseline:: 105.91335368156433
curr_diff: 0 tensor(0.0059, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0059, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875100
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 120
delta_size:: 120
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 54.165552616119385
curr_diff: 0 tensor(2.1614e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1614e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0059, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0059, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875100
repetition 5
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 108.82797169685364
training time full:: 108.84014010429382
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 120
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 106.5621235370636
time_baseline:: 106.83336591720581
curr_diff: 0 tensor(0.0059, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0059, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875100
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 120
delta_size:: 120
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 54.19423580169678
curr_diff: 0 tensor(2.1614e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(2.1614e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0059, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0059, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875100
batch size:: 30000
repetition 1
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.752142906188965
training time full:: 52.75924468040466
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 120
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.21972680091858
time_baseline:: 48.356178760528564
curr_diff: 0 tensor(0.0059, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0059, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875000
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 60
delta_size:: 120
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.603342533111572
curr_diff: 0 tensor(8.5681e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.5681e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0060, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0060, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875100
repetition 2
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.15540838241577
training time full:: 52.16154479980469
provenance prepare time:: 2.6226043701171875e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 120
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 47.643627405166626
time_baseline:: 47.78033781051636
curr_diff: 0 tensor(0.0059, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0059, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875000
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 60
delta_size:: 120
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.686543226242065
curr_diff: 0 tensor(8.5681e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.5681e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0060, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0060, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875100
repetition 3
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 53.06503438949585
training time full:: 53.07127642631531
provenance prepare time:: 2.384185791015625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 120
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.23300123214722
time_baseline:: 48.368281841278076
curr_diff: 0 tensor(0.0059, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0059, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875000
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 60
delta_size:: 120
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.917349576950073
curr_diff: 0 tensor(8.5681e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.5681e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0060, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0060, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875100
repetition 4
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.92475748062134
training time full:: 52.930949449539185
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 120
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.11725640296936
time_baseline:: 48.252153635025024
curr_diff: 0 tensor(0.0059, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0059, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875000
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 60
delta_size:: 120
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.420238733291626
curr_diff: 0 tensor(8.5681e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.5681e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0060, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0060, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875100
repetition 5
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.44703388214111
training time full:: 52.453290939331055
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 120
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.83191680908203
time_baseline:: 48.96809148788452
curr_diff: 0 tensor(0.0059, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0059, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875000
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 60
delta_size:: 120
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.538848400115967
curr_diff: 0 tensor(8.5681e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(8.5681e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0060, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0060, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875100
batch size:: 16384
repetition 1
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.32319188117981
training time full:: 27.325759410858154
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.29181146621704
time_baseline:: 24.364370107650757
curr_diff: 0 tensor(0.0062, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0062, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876200
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 13.281894207000732
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0063, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0063, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876200
repetition 2
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.233101844787598
training time full:: 27.23526096343994
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.382153749465942
time_baseline:: 24.452872276306152
curr_diff: 0 tensor(0.0062, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0062, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876200
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 13.161898612976074
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0063, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0063, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876200
repetition 3
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.09039044380188
training time full:: 27.092589616775513
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.568502187728882
time_baseline:: 24.63971710205078
curr_diff: 0 tensor(0.0062, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0062, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876200
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 13.348625421524048
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0063, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0063, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876200
repetition 4
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.38507103919983
training time full:: 27.387195587158203
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.288708448410034
time_baseline:: 24.362009048461914
curr_diff: 0 tensor(0.0062, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0062, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876200
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 13.41202974319458
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0063, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0063, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876200
repetition 5
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 26.925021409988403
training time full:: 26.927133798599243
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.214177131652832
time_baseline:: 24.28628635406494
curr_diff: 0 tensor(0.0062, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0062, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876200
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 32
delta_size:: 120
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 13.24541425704956
curr_diff: 0 tensor(0.0001, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0001, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0063, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0063, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876200
batch size:: 4096
repetition 1
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.7889134883880615
training time full:: 5.788951635360718
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 120
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.045157432556152
time_baseline:: 5.063141345977783
curr_diff: 0 tensor(0.0060, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0060, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874700
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 8
delta_size:: 120
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.17448353767395
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0061, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0061, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874700
repetition 2
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.880213737487793
training time full:: 5.880251169204712
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 120
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.022126197814941
time_baseline:: 5.039459943771362
curr_diff: 0 tensor(0.0060, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0060, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874700
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 8
delta_size:: 120
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.2210042476654053
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0061, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0061, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874700
repetition 3
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.9037861824035645
training time full:: 5.9038262367248535
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 120
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.384746551513672
time_baseline:: 5.402616262435913
curr_diff: 0 tensor(0.0060, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0060, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874700
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 8
delta_size:: 120
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.1775248050689697
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0061, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0061, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874700
repetition 4
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.81958794593811
training time full:: 5.819626092910767
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 120
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.006952524185181
time_baseline:: 5.024561882019043
curr_diff: 0 tensor(0.0060, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0060, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874700
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 8
delta_size:: 120
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.125209093093872
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0061, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0061, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874700
repetition 5
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.933974027633667
training time full:: 5.934013366699219
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 120
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 4.981600522994995
time_baseline:: 4.999805927276611
curr_diff: 0 tensor(0.0060, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0060, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874700
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 8
delta_size:: 120
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.2367634773254395
curr_diff: 0 tensor(0.0003, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0003, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0061, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0061, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874700
batch size:: 1024
repetition 1
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.6679103374481201
training time full:: 1.66794753074646
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 120
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.446249008178711
time_baseline:: 1.4510498046875
curr_diff: 0 tensor(0.0060, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0060, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873100
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 2
delta_size:: 120
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.1173286437988281
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0061, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0061, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873100
repetition 2
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.6859664916992188
training time full:: 1.6860055923461914
provenance prepare time:: 1.1920928955078125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 120
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4388716220855713
time_baseline:: 1.443744421005249
curr_diff: 0 tensor(0.0060, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0060, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873100
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 2
delta_size:: 120
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.074882984161377
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0061, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0061, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873100
repetition 3
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.653738260269165
training time full:: 1.6537723541259766
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 120
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.413684606552124
time_baseline:: 1.41843843460083
curr_diff: 0 tensor(0.0060, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0060, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873100
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 2
delta_size:: 120
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.084167242050171
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0061, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0061, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873100
repetition 4
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.6708261966705322
training time full:: 1.6708619594573975
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 120
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4339666366577148
time_baseline:: 1.4388480186462402
curr_diff: 0 tensor(0.0060, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0060, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873100
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 2
delta_size:: 120
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.1218481063842773
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0061, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0061, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873100
repetition 5
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.7084550857543945
training time full:: 1.7084922790527344
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 120
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.4495244026184082
time_baseline:: 1.4543969631195068
curr_diff: 0 tensor(0.0060, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0060, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873100
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 2
delta_size:: 120
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.0763473510742188
curr_diff: 0 tensor(0.0005, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0005, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0061, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0061, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873100
deletion rate:: 0.01
python3 generate_rand_ids 0.01  MNIST5 0
tensor([34818,     6, 14345, 14346, 38921, 34828, 18448, 55315, 51222,  8214,
        38941, 45090, 36900, 20517, 59431, 49194, 49204, 32822, 10295, 24631,
         4153, 20538, 47165, 57408, 57409, 49218,  8263, 10315, 30795, 10317,
        34895, 53331, 55381, 55385, 30818, 36964, 49259, 28780, 28784,  2161,
        41075, 47227, 53378, 59528, 32905, 37005, 18573,  2194, 39067, 39072,
        53420,  2225, 39090, 51380, 20662, 43192,  8384, 49344, 57538, 28867,
        22731, 59595, 28879, 20689, 41170, 10456, 45275, 16624, 10480, 57588,
        43257, 10498, 47370, 53522, 35092, 31003, 49441,  8483, 39204, 45349,
        18729, 14634, 59689, 31029, 20789,  4405, 57663,  8511,  4417,  2371,
        14659, 49479, 12617, 12619, 39251, 37206, 59737,  4442, 22875, 37210,
        29026, 12647, 39272, 51563, 12653,  4462, 26997, 57722, 12667, 31102,
        24963, 35209, 55691, 41361, 47506, 27027, 20885,  6551, 27040, 24993,
        29090, 47523, 18851, 53666, 31138,   427, 12720, 10678, 35256, 29120,
        55745, 31174,  6599, 45512, 39368, 18890, 18891, 45515, 16848, 25040,
          465, 18899, 27093, 23001, 31208, 55785, 55786, 14830, 41458, 53753,
        12797, 10752,   515, 12811,  8721, 51733, 59926, 45590, 39448, 27162,
        45595, 41506, 16945,  4662, 37434,  8765, 47682, 35396, 25161, 59977,
        29259, 10828, 39499, 59982, 16975,  2638, 39504, 47698, 57940, 59989,
        43610, 47707,  6752, 41569, 57963, 49771,  8815, 39541, 47742, 47750,
        51851, 53902, 39567, 45722, 10909, 25250, 33445, 23213, 12982, 58049,
        49860, 19141,  6861, 33488, 21207, 23255, 31447,  2779, 31455, 19168,
        25314, 39653, 35558,  8936, 56043, 39668, 39671, 33528,   765, 51966,
        25343, 27398, 54024, 17162, 13067, 45841, 23314, 27410, 56083, 37654,
        27422, 19233, 58146, 41763, 41769, 47914, 13100, 41774,  4912,  9009,
        19257, 33595, 49986, 33610, 41802, 45900, 56151, 17240, 15193, 58205,
        43871, 47968,   868,  9062, 54120, 21355, 25453, 54127, 17271,  7035,
        27516,   891, 48002, 41863, 19336, 11144, 33680,   915, 13203, 48023,
        17305, 50073, 54177, 21409, 45987, 13221,  7078, 11177, 48044, 43953,
        29617, 50100, 33717,  2998, 56247, 33720, 11194,   960, 15298,  7108,
        19412, 39901, 41951,  5088,  7140, 50149, 11242, 37871, 33778, 17396,
         3060,  5114,  1020, 21504, 48133, 58380, 48146, 56342, 11292, 54309,
        11309, 29746, 27699, 58420, 19512, 46136, 46139, 21569,  7233, 35909,
        33863,  1095, 29772, 44113, 11353, 50266,  3163, 13403, 29788, 11358,
        31843, 11369, 19575, 52345, 11387, 58493, 29822, 23682, 50306, 54407,
         1162,  7307,  7308, 25743, 40081, 48275, 44181,  7322,  1180, 29856,
        44193, 21666, 17571, 46261, 15541,  7356, 17597, 42173, 36032,  3265,
        29893, 52421, 31942,  9416, 58577, 19666,  7386, 40156, 36060, 52445,
        36072,  1258, 46316,  7411, 32001, 21763, 32009, 38154, 27917, 56589,
        44301, 29971, 15637, 54551, 11553, 50467,  9510, 21798, 42280,  5417,
         1323, 50478, 23854, 40245, 48437, 32057, 50497, 58700, 23886,  3406,
        25946, 56668, 15710,  1374, 42334, 50528, 21854, 34152, 19819, 42347,
        52588, 44401, 56697, 19834, 32121, 38269, 36221, 30082, 25987, 54658,
        32133, 36236,  7565, 34197, 58776,  7581, 58785, 52643, 26020, 15780,
         5541, 46505, 13743, 42424,  1469, 32194, 26052, 24008, 58825, 28104,
         9675,  3534, 38354, 52691, 17877, 42453, 34261, 28127,  7655,  7656,
         3561, 50664, 46568,  3565, 42477, 56815, 42482, 40437,  7670, 13817,
        42495, 15871,  5631, 50689, 44548,  9733, 52744, 36362, 46614, 36375,
        42519, 17950, 48672, 48674, 26154, 28214,  3641, 11841, 42562, 40535,
        54871, 40538, 40540, 53242, 13924,  7781, 26214, 52843, 32363, 52857,
        26241, 40580, 50820, 40586, 56971, 18063, 16021, 42651, 50847, 11936,
        36515, 42660, 13988, 22182, 48807, 38569,  1706, 18092, 32431, 11951,
        57011, 30389, 38595,  1731, 32456, 36555, 50894, 32467, 50909, 14046,
        55010, 34531, 52962, 52967, 42729, 46828, 55024, 57072, 14065, 26355,
        44788, 40691, 22259, 22267, 48893, 34567, 32528,  7960, 44826, 46877,
        26402, 42786, 44839,  5944, 48954, 16190, 36672,  5959, 18250, 57165,
         1873, 28500, 26452, 42840,  8028, 40796, 38750, 34654, 34667, 22383,
        30576, 26482,  1907, 18292, 46965, 22399, 32646, 51081, 51084, 38807,
        51103,  4001, 59298, 38824, 18345, 53167, 24498,  8115, 16317, 32703,
        53183, 49097, 14281, 34761, 10188, 10189, 36816, 22482, 20435, 20436,
        51159, 22487,  2023, 40939, 14316, 42989, 36849, 38898, 22522, 12285])
batch size:: 60000
repetition 1
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 107.8674168586731
training time full:: 107.87768530845642
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 600
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 106.09819078445435
time_baseline:: 106.36406064033508
curr_diff: 0 tensor(0.0136, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0136, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875400
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 120
delta_size:: 600
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 54.75546383857727
curr_diff: 0 tensor(4.0478e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.0478e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0136, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0136, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875400
repetition 2
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 109.98485684394836
training time full:: 109.99615836143494
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 600
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 105.01941156387329
time_baseline:: 105.28820061683655
curr_diff: 0 tensor(0.0136, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0136, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875400
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 120
delta_size:: 600
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 53.88546323776245
curr_diff: 0 tensor(4.0478e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.0478e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0136, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0136, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875400
repetition 3
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 108.976243019104
training time full:: 108.98805713653564
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 600
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 105.92497110366821
time_baseline:: 106.19290971755981
curr_diff: 0 tensor(0.0136, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0136, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875400
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 120
delta_size:: 600
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 54.408639669418335
curr_diff: 0 tensor(4.0478e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.0478e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0136, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0136, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875400
repetition 4
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 109.26096510887146
training time full:: 109.2727153301239
provenance prepare time:: 2.384185791015625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 600
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 104.43349099159241
time_baseline:: 104.71239686012268
curr_diff: 0 tensor(0.0136, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0136, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875400
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 120
delta_size:: 600
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 55.03082060813904
curr_diff: 0 tensor(4.0478e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.0478e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0136, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0136, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875400
repetition 5
python3 benchmark_exp_lr.py 0.001 60000 120 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.347891
Test Avg. Loss: 0.000223, Accuracy: 0.191500
Train - Epoch 1, Batch: 0, Loss: 2.237525
Test Avg. Loss: 0.000213, Accuracy: 0.423400
Train - Epoch 2, Batch: 0, Loss: 2.137838
Test Avg. Loss: 0.000203, Accuracy: 0.562700
Train - Epoch 3, Batch: 0, Loss: 2.046036
Test Avg. Loss: 0.000195, Accuracy: 0.630900
Train - Epoch 4, Batch: 0, Loss: 1.961015
Test Avg. Loss: 0.000187, Accuracy: 0.667900
Train - Epoch 5, Batch: 0, Loss: 1.882185
Test Avg. Loss: 0.000179, Accuracy: 0.694500
Train - Epoch 6, Batch: 0, Loss: 1.809117
Test Avg. Loss: 0.000173, Accuracy: 0.716700
Train - Epoch 7, Batch: 0, Loss: 1.741420
Test Avg. Loss: 0.000166, Accuracy: 0.736000
Train - Epoch 8, Batch: 0, Loss: 1.678718
Test Avg. Loss: 0.000160, Accuracy: 0.753200
Train - Epoch 9, Batch: 0, Loss: 1.620640
Test Avg. Loss: 0.000155, Accuracy: 0.765300
Train - Epoch 10, Batch: 0, Loss: 1.566828
Test Avg. Loss: 0.000150, Accuracy: 0.774200
Train - Epoch 11, Batch: 0, Loss: 1.516939
Test Avg. Loss: 0.000145, Accuracy: 0.782200
Train - Epoch 12, Batch: 0, Loss: 1.470647
Test Avg. Loss: 0.000141, Accuracy: 0.788600
Train - Epoch 13, Batch: 0, Loss: 1.427651
Test Avg. Loss: 0.000137, Accuracy: 0.793200
Train - Epoch 14, Batch: 0, Loss: 1.387668
Test Avg. Loss: 0.000133, Accuracy: 0.798300
Train - Epoch 15, Batch: 0, Loss: 1.350441
Test Avg. Loss: 0.000129, Accuracy: 0.803600
Train - Epoch 16, Batch: 0, Loss: 1.315732
Test Avg. Loss: 0.000126, Accuracy: 0.806900
Train - Epoch 17, Batch: 0, Loss: 1.283325
Test Avg. Loss: 0.000123, Accuracy: 0.810800
Train - Epoch 18, Batch: 0, Loss: 1.253021
Test Avg. Loss: 0.000120, Accuracy: 0.814100
Train - Epoch 19, Batch: 0, Loss: 1.224644
Test Avg. Loss: 0.000117, Accuracy: 0.815100
Train - Epoch 20, Batch: 0, Loss: 1.198029
Test Avg. Loss: 0.000115, Accuracy: 0.816700
Train - Epoch 21, Batch: 0, Loss: 1.173031
Test Avg. Loss: 0.000113, Accuracy: 0.819300
Train - Epoch 22, Batch: 0, Loss: 1.149517
Test Avg. Loss: 0.000110, Accuracy: 0.821200
Train - Epoch 23, Batch: 0, Loss: 1.127366
Test Avg. Loss: 0.000108, Accuracy: 0.822400
Train - Epoch 24, Batch: 0, Loss: 1.106469
Test Avg. Loss: 0.000106, Accuracy: 0.823000
Train - Epoch 25, Batch: 0, Loss: 1.086727
Test Avg. Loss: 0.000104, Accuracy: 0.824600
Train - Epoch 26, Batch: 0, Loss: 1.068051
Test Avg. Loss: 0.000103, Accuracy: 0.825600
Train - Epoch 27, Batch: 0, Loss: 1.050361
Test Avg. Loss: 0.000101, Accuracy: 0.827300
Train - Epoch 28, Batch: 0, Loss: 1.033582
Test Avg. Loss: 0.000099, Accuracy: 0.828600
Train - Epoch 29, Batch: 0, Loss: 1.017647
Test Avg. Loss: 0.000098, Accuracy: 0.830500
Train - Epoch 30, Batch: 0, Loss: 1.002496
Test Avg. Loss: 0.000096, Accuracy: 0.831400
Train - Epoch 31, Batch: 0, Loss: 0.988072
Test Avg. Loss: 0.000095, Accuracy: 0.832700
Train - Epoch 32, Batch: 0, Loss: 0.974326
Test Avg. Loss: 0.000094, Accuracy: 0.834000
Train - Epoch 33, Batch: 0, Loss: 0.961211
Test Avg. Loss: 0.000092, Accuracy: 0.835600
Train - Epoch 34, Batch: 0, Loss: 0.948685
Test Avg. Loss: 0.000091, Accuracy: 0.836600
Train - Epoch 35, Batch: 0, Loss: 0.936709
Test Avg. Loss: 0.000090, Accuracy: 0.837700
Train - Epoch 36, Batch: 0, Loss: 0.925248
Test Avg. Loss: 0.000089, Accuracy: 0.838900
Train - Epoch 37, Batch: 0, Loss: 0.914269
Test Avg. Loss: 0.000088, Accuracy: 0.839500
Train - Epoch 38, Batch: 0, Loss: 0.903742
Test Avg. Loss: 0.000087, Accuracy: 0.840400
Train - Epoch 39, Batch: 0, Loss: 0.893639
Test Avg. Loss: 0.000086, Accuracy: 0.841100
Train - Epoch 40, Batch: 0, Loss: 0.883935
Test Avg. Loss: 0.000085, Accuracy: 0.842200
Train - Epoch 41, Batch: 0, Loss: 0.874607
Test Avg. Loss: 0.000084, Accuracy: 0.842900
Train - Epoch 42, Batch: 0, Loss: 0.865633
Test Avg. Loss: 0.000083, Accuracy: 0.843700
Train - Epoch 43, Batch: 0, Loss: 0.856992
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 44, Batch: 0, Loss: 0.848667
Test Avg. Loss: 0.000081, Accuracy: 0.845200
Train - Epoch 45, Batch: 0, Loss: 0.840639
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 46, Batch: 0, Loss: 0.832893
Test Avg. Loss: 0.000080, Accuracy: 0.846400
Train - Epoch 47, Batch: 0, Loss: 0.825413
Test Avg. Loss: 0.000079, Accuracy: 0.847100
Train - Epoch 48, Batch: 0, Loss: 0.818187
Test Avg. Loss: 0.000079, Accuracy: 0.848600
Train - Epoch 49, Batch: 0, Loss: 0.811200
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 50, Batch: 0, Loss: 0.804442
Test Avg. Loss: 0.000077, Accuracy: 0.850200
Train - Epoch 51, Batch: 0, Loss: 0.797900
Test Avg. Loss: 0.000077, Accuracy: 0.851100
Train - Epoch 52, Batch: 0, Loss: 0.791564
Test Avg. Loss: 0.000076, Accuracy: 0.852200
Train - Epoch 53, Batch: 0, Loss: 0.785424
Test Avg. Loss: 0.000075, Accuracy: 0.852800
Train - Epoch 54, Batch: 0, Loss: 0.779471
Test Avg. Loss: 0.000075, Accuracy: 0.853400
Train - Epoch 55, Batch: 0, Loss: 0.773696
Test Avg. Loss: 0.000074, Accuracy: 0.854000
Train - Epoch 56, Batch: 0, Loss: 0.768091
Test Avg. Loss: 0.000074, Accuracy: 0.854400
Train - Epoch 57, Batch: 0, Loss: 0.762649
Test Avg. Loss: 0.000073, Accuracy: 0.854900
Train - Epoch 58, Batch: 0, Loss: 0.757362
Test Avg. Loss: 0.000073, Accuracy: 0.855100
Train - Epoch 59, Batch: 0, Loss: 0.752223
Test Avg. Loss: 0.000072, Accuracy: 0.855100
Train - Epoch 60, Batch: 0, Loss: 0.747226
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 61, Batch: 0, Loss: 0.742365
Test Avg. Loss: 0.000071, Accuracy: 0.856400
Train - Epoch 62, Batch: 0, Loss: 0.737634
Test Avg. Loss: 0.000071, Accuracy: 0.857100
Train - Epoch 63, Batch: 0, Loss: 0.733029
Test Avg. Loss: 0.000070, Accuracy: 0.857100
Train - Epoch 64, Batch: 0, Loss: 0.728543
Test Avg. Loss: 0.000070, Accuracy: 0.857900
Train - Epoch 65, Batch: 0, Loss: 0.724172
Test Avg. Loss: 0.000069, Accuracy: 0.858300
Train - Epoch 66, Batch: 0, Loss: 0.719911
Test Avg. Loss: 0.000069, Accuracy: 0.858500
Train - Epoch 67, Batch: 0, Loss: 0.715757
Test Avg. Loss: 0.000069, Accuracy: 0.858700
Train - Epoch 68, Batch: 0, Loss: 0.711705
Test Avg. Loss: 0.000068, Accuracy: 0.859200
Train - Epoch 69, Batch: 0, Loss: 0.707751
Test Avg. Loss: 0.000068, Accuracy: 0.859400
Train - Epoch 70, Batch: 0, Loss: 0.703891
Test Avg. Loss: 0.000067, Accuracy: 0.859600
Train - Epoch 71, Batch: 0, Loss: 0.700123
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 72, Batch: 0, Loss: 0.696442
Test Avg. Loss: 0.000067, Accuracy: 0.860700
Train - Epoch 73, Batch: 0, Loss: 0.692846
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 74, Batch: 0, Loss: 0.689331
Test Avg. Loss: 0.000066, Accuracy: 0.861300
Train - Epoch 75, Batch: 0, Loss: 0.685896
Test Avg. Loss: 0.000066, Accuracy: 0.861800
Train - Epoch 76, Batch: 0, Loss: 0.682535
Test Avg. Loss: 0.000065, Accuracy: 0.862000
Train - Epoch 77, Batch: 0, Loss: 0.679249
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 78, Batch: 0, Loss: 0.676033
Test Avg. Loss: 0.000065, Accuracy: 0.862500
Train - Epoch 79, Batch: 0, Loss: 0.672885
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 80, Batch: 0, Loss: 0.669804
Test Avg. Loss: 0.000064, Accuracy: 0.863400
Train - Epoch 81, Batch: 0, Loss: 0.666787
Test Avg. Loss: 0.000064, Accuracy: 0.863900
Train - Epoch 82, Batch: 0, Loss: 0.663832
Test Avg. Loss: 0.000064, Accuracy: 0.864500
Train - Epoch 83, Batch: 0, Loss: 0.660937
Test Avg. Loss: 0.000063, Accuracy: 0.864900
Train - Epoch 84, Batch: 0, Loss: 0.658099
Test Avg. Loss: 0.000063, Accuracy: 0.865500
Train - Epoch 85, Batch: 0, Loss: 0.655319
Test Avg. Loss: 0.000063, Accuracy: 0.866000
Train - Epoch 86, Batch: 0, Loss: 0.652592
Test Avg. Loss: 0.000062, Accuracy: 0.866500
Train - Epoch 87, Batch: 0, Loss: 0.649919
Test Avg. Loss: 0.000062, Accuracy: 0.866800
Train - Epoch 88, Batch: 0, Loss: 0.647297
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 89, Batch: 0, Loss: 0.644724
Test Avg. Loss: 0.000062, Accuracy: 0.867200
Train - Epoch 90, Batch: 0, Loss: 0.642200
Test Avg. Loss: 0.000061, Accuracy: 0.867400
Train - Epoch 91, Batch: 0, Loss: 0.639723
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 92, Batch: 0, Loss: 0.637291
Test Avg. Loss: 0.000061, Accuracy: 0.868300
Train - Epoch 93, Batch: 0, Loss: 0.634903
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 94, Batch: 0, Loss: 0.632559
Test Avg. Loss: 0.000061, Accuracy: 0.869000
Train - Epoch 95, Batch: 0, Loss: 0.630256
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 96, Batch: 0, Loss: 0.627993
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 97, Batch: 0, Loss: 0.625770
Test Avg. Loss: 0.000060, Accuracy: 0.870000
Train - Epoch 98, Batch: 0, Loss: 0.623586
Test Avg. Loss: 0.000060, Accuracy: 0.870500
Train - Epoch 99, Batch: 0, Loss: 0.621439
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 100, Batch: 0, Loss: 0.619328
Test Avg. Loss: 0.000059, Accuracy: 0.870800
Train - Epoch 101, Batch: 0, Loss: 0.617252
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 102, Batch: 0, Loss: 0.615211
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 103, Batch: 0, Loss: 0.613204
Test Avg. Loss: 0.000059, Accuracy: 0.872000
Train - Epoch 104, Batch: 0, Loss: 0.611229
Test Avg. Loss: 0.000058, Accuracy: 0.872200
Train - Epoch 105, Batch: 0, Loss: 0.609286
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 106, Batch: 0, Loss: 0.607374
Test Avg. Loss: 0.000058, Accuracy: 0.872800
Train - Epoch 107, Batch: 0, Loss: 0.605492
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 108, Batch: 0, Loss: 0.603640
Test Avg. Loss: 0.000058, Accuracy: 0.872700
Train - Epoch 109, Batch: 0, Loss: 0.601817
Test Avg. Loss: 0.000058, Accuracy: 0.873200
Train - Epoch 110, Batch: 0, Loss: 0.600021
Test Avg. Loss: 0.000057, Accuracy: 0.873100
Train - Epoch 111, Batch: 0, Loss: 0.598254
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 112, Batch: 0, Loss: 0.596513
Test Avg. Loss: 0.000057, Accuracy: 0.873300
Train - Epoch 113, Batch: 0, Loss: 0.594798
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 114, Batch: 0, Loss: 0.593108
Test Avg. Loss: 0.000057, Accuracy: 0.873800
Train - Epoch 115, Batch: 0, Loss: 0.591444
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 116, Batch: 0, Loss: 0.589803
Test Avg. Loss: 0.000056, Accuracy: 0.874600
Train - Epoch 117, Batch: 0, Loss: 0.588187
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 118, Batch: 0, Loss: 0.586594
Test Avg. Loss: 0.000056, Accuracy: 0.875200
Train - Epoch 119, Batch: 0, Loss: 0.585023
Test Avg. Loss: 0.000056, Accuracy: 0.875500
training_time:: 109.30382561683655
training time full:: 109.31567215919495
provenance prepare time:: 2.384185791015625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 120
delta_size:: 600
max_epoch:: 120
batch_size:: 60000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
epoch:: 60
	calling Sampler:__iter__
epoch:: 61
	calling Sampler:__iter__
epoch:: 62
	calling Sampler:__iter__
epoch:: 63
	calling Sampler:__iter__
epoch:: 64
	calling Sampler:__iter__
epoch:: 65
	calling Sampler:__iter__
epoch:: 66
	calling Sampler:__iter__
epoch:: 67
	calling Sampler:__iter__
epoch:: 68
	calling Sampler:__iter__
epoch:: 69
	calling Sampler:__iter__
epoch:: 70
	calling Sampler:__iter__
epoch:: 71
	calling Sampler:__iter__
epoch:: 72
	calling Sampler:__iter__
epoch:: 73
	calling Sampler:__iter__
epoch:: 74
	calling Sampler:__iter__
epoch:: 75
	calling Sampler:__iter__
epoch:: 76
	calling Sampler:__iter__
epoch:: 77
	calling Sampler:__iter__
epoch:: 78
	calling Sampler:__iter__
epoch:: 79
	calling Sampler:__iter__
epoch:: 80
	calling Sampler:__iter__
epoch:: 81
	calling Sampler:__iter__
epoch:: 82
	calling Sampler:__iter__
epoch:: 83
	calling Sampler:__iter__
epoch:: 84
	calling Sampler:__iter__
epoch:: 85
	calling Sampler:__iter__
epoch:: 86
	calling Sampler:__iter__
epoch:: 87
	calling Sampler:__iter__
epoch:: 88
	calling Sampler:__iter__
epoch:: 89
	calling Sampler:__iter__
epoch:: 90
	calling Sampler:__iter__
epoch:: 91
	calling Sampler:__iter__
epoch:: 92
	calling Sampler:__iter__
epoch:: 93
	calling Sampler:__iter__
epoch:: 94
	calling Sampler:__iter__
epoch:: 95
	calling Sampler:__iter__
epoch:: 96
	calling Sampler:__iter__
epoch:: 97
	calling Sampler:__iter__
epoch:: 98
	calling Sampler:__iter__
epoch:: 99
	calling Sampler:__iter__
epoch:: 100
	calling Sampler:__iter__
epoch:: 101
	calling Sampler:__iter__
epoch:: 102
	calling Sampler:__iter__
epoch:: 103
	calling Sampler:__iter__
epoch:: 104
	calling Sampler:__iter__
epoch:: 105
	calling Sampler:__iter__
epoch:: 106
	calling Sampler:__iter__
epoch:: 107
	calling Sampler:__iter__
epoch:: 108
	calling Sampler:__iter__
epoch:: 109
	calling Sampler:__iter__
epoch:: 110
	calling Sampler:__iter__
epoch:: 111
	calling Sampler:__iter__
epoch:: 112
	calling Sampler:__iter__
epoch:: 113
	calling Sampler:__iter__
epoch:: 114
	calling Sampler:__iter__
epoch:: 115
	calling Sampler:__iter__
epoch:: 116
	calling Sampler:__iter__
epoch:: 117
	calling Sampler:__iter__
epoch:: 118
	calling Sampler:__iter__
epoch:: 119
	calling Sampler:__iter__
training time is 105.18490815162659
time_baseline:: 105.45078611373901
curr_diff: 0 tensor(0.0136, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0136, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875400
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 120
delta_size:: 600
max_epoch:: 120
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
epoch  60
	calling Sampler:__iter__
epoch  61
	calling Sampler:__iter__
epoch  62
	calling Sampler:__iter__
epoch  63
	calling Sampler:__iter__
epoch  64
	calling Sampler:__iter__
epoch  65
	calling Sampler:__iter__
epoch  66
	calling Sampler:__iter__
epoch  67
	calling Sampler:__iter__
epoch  68
	calling Sampler:__iter__
epoch  69
	calling Sampler:__iter__
epoch  70
	calling Sampler:__iter__
epoch  71
	calling Sampler:__iter__
epoch  72
	calling Sampler:__iter__
epoch  73
	calling Sampler:__iter__
epoch  74
	calling Sampler:__iter__
epoch  75
	calling Sampler:__iter__
epoch  76
	calling Sampler:__iter__
epoch  77
	calling Sampler:__iter__
epoch  78
	calling Sampler:__iter__
epoch  79
	calling Sampler:__iter__
epoch  80
	calling Sampler:__iter__
epoch  81
	calling Sampler:__iter__
epoch  82
	calling Sampler:__iter__
epoch  83
	calling Sampler:__iter__
epoch  84
	calling Sampler:__iter__
epoch  85
	calling Sampler:__iter__
epoch  86
	calling Sampler:__iter__
epoch  87
	calling Sampler:__iter__
epoch  88
	calling Sampler:__iter__
epoch  89
	calling Sampler:__iter__
epoch  90
	calling Sampler:__iter__
epoch  91
	calling Sampler:__iter__
epoch  92
	calling Sampler:__iter__
epoch  93
	calling Sampler:__iter__
epoch  94
	calling Sampler:__iter__
epoch  95
	calling Sampler:__iter__
epoch  96
	calling Sampler:__iter__
epoch  97
	calling Sampler:__iter__
epoch  98
	calling Sampler:__iter__
epoch  99
	calling Sampler:__iter__
epoch  100
	calling Sampler:__iter__
epoch  101
	calling Sampler:__iter__
epoch  102
	calling Sampler:__iter__
epoch  103
	calling Sampler:__iter__
epoch  104
	calling Sampler:__iter__
epoch  105
	calling Sampler:__iter__
epoch  106
	calling Sampler:__iter__
epoch  107
	calling Sampler:__iter__
epoch  108
	calling Sampler:__iter__
epoch  109
	calling Sampler:__iter__
epoch  110
	calling Sampler:__iter__
epoch  111
	calling Sampler:__iter__
epoch  112
	calling Sampler:__iter__
epoch  113
	calling Sampler:__iter__
epoch  114
	calling Sampler:__iter__
epoch  115
	calling Sampler:__iter__
epoch  116
	calling Sampler:__iter__
epoch  117
	calling Sampler:__iter__
epoch  118
	calling Sampler:__iter__
epoch  119
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 55.23694396018982
curr_diff: 0 tensor(4.0478e-05, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(4.0478e-05, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0136, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0136, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875400
batch size:: 30000
repetition 1
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.51433229446411
training time full:: 52.52139711380005
provenance prepare time:: 2.6226043701171875e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 47.50655007362366
time_baseline:: 47.64136028289795
curr_diff: 0 tensor(0.0136, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0136, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875400
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 26.384432554244995
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0137, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0137, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875400
repetition 2
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.37207221984863
training time full:: 52.37793755531311
provenance prepare time:: 2.6226043701171875e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.25570344924927
time_baseline:: 48.392688035964966
curr_diff: 0 tensor(0.0136, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0136, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875400
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 26.083811044692993
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0137, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0137, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875400
repetition 3
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.721330881118774
training time full:: 52.727306842803955
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.300204277038574
time_baseline:: 48.43264126777649
curr_diff: 0 tensor(0.0136, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0136, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875400
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 26.016589641571045
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0137, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0137, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875400
repetition 4
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.15366172790527
training time full:: 52.159422397613525
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 48.40828013420105
time_baseline:: 48.541950702667236
curr_diff: 0 tensor(0.0136, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0136, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875400
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 26.01425814628601
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0137, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0137, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875400
repetition 5
python3 benchmark_exp_lr.py 0.001 30000 60 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348821
Test Avg. Loss: 0.000213, Accuracy: 0.424000
Train - Epoch 1, Batch: 0, Loss: 2.138115
Test Avg. Loss: 0.000195, Accuracy: 0.630200
Train - Epoch 2, Batch: 0, Loss: 1.961489
Test Avg. Loss: 0.000179, Accuracy: 0.694000
Train - Epoch 3, Batch: 0, Loss: 1.808869
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 4, Batch: 0, Loss: 1.679231
Test Avg. Loss: 0.000155, Accuracy: 0.765900
Train - Epoch 5, Batch: 0, Loss: 1.566318
Test Avg. Loss: 0.000145, Accuracy: 0.782500
Train - Epoch 6, Batch: 0, Loss: 1.469925
Test Avg. Loss: 0.000137, Accuracy: 0.793100
Train - Epoch 7, Batch: 0, Loss: 1.387169
Test Avg. Loss: 0.000129, Accuracy: 0.803000
Train - Epoch 8, Batch: 0, Loss: 1.317201
Test Avg. Loss: 0.000123, Accuracy: 0.811000
Train - Epoch 9, Batch: 0, Loss: 1.254191
Test Avg. Loss: 0.000117, Accuracy: 0.815300
Train - Epoch 10, Batch: 0, Loss: 1.198045
Test Avg. Loss: 0.000113, Accuracy: 0.819200
Train - Epoch 11, Batch: 0, Loss: 1.154045
Test Avg. Loss: 0.000108, Accuracy: 0.822600
Train - Epoch 12, Batch: 0, Loss: 1.107446
Test Avg. Loss: 0.000104, Accuracy: 0.824100
Train - Epoch 13, Batch: 0, Loss: 1.063643
Test Avg. Loss: 0.000101, Accuracy: 0.827800
Train - Epoch 14, Batch: 0, Loss: 1.034337
Test Avg. Loss: 0.000098, Accuracy: 0.830700
Train - Epoch 15, Batch: 0, Loss: 0.998310
Test Avg. Loss: 0.000095, Accuracy: 0.832800
Train - Epoch 16, Batch: 0, Loss: 0.976412
Test Avg. Loss: 0.000092, Accuracy: 0.835500
Train - Epoch 17, Batch: 0, Loss: 0.946828
Test Avg. Loss: 0.000090, Accuracy: 0.837500
Train - Epoch 18, Batch: 0, Loss: 0.928421
Test Avg. Loss: 0.000088, Accuracy: 0.839200
Train - Epoch 19, Batch: 0, Loss: 0.905205
Test Avg. Loss: 0.000086, Accuracy: 0.841200
Train - Epoch 20, Batch: 0, Loss: 0.879086
Test Avg. Loss: 0.000084, Accuracy: 0.843000
Train - Epoch 21, Batch: 0, Loss: 0.863342
Test Avg. Loss: 0.000082, Accuracy: 0.844200
Train - Epoch 22, Batch: 0, Loss: 0.849513
Test Avg. Loss: 0.000081, Accuracy: 0.845800
Train - Epoch 23, Batch: 0, Loss: 0.835842
Test Avg. Loss: 0.000079, Accuracy: 0.846900
Train - Epoch 24, Batch: 0, Loss: 0.813785
Test Avg. Loss: 0.000078, Accuracy: 0.849200
Train - Epoch 25, Batch: 0, Loss: 0.804097
Test Avg. Loss: 0.000077, Accuracy: 0.851300
Train - Epoch 26, Batch: 0, Loss: 0.788671
Test Avg. Loss: 0.000075, Accuracy: 0.852700
Train - Epoch 27, Batch: 0, Loss: 0.782948
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 28, Batch: 0, Loss: 0.766154
Test Avg. Loss: 0.000073, Accuracy: 0.854800
Train - Epoch 29, Batch: 0, Loss: 0.758710
Test Avg. Loss: 0.000072, Accuracy: 0.855200
Train - Epoch 30, Batch: 0, Loss: 0.746625
Test Avg. Loss: 0.000071, Accuracy: 0.856500
Train - Epoch 31, Batch: 0, Loss: 0.737564
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 32, Batch: 0, Loss: 0.726335
Test Avg. Loss: 0.000069, Accuracy: 0.858200
Train - Epoch 33, Batch: 0, Loss: 0.721642
Test Avg. Loss: 0.000069, Accuracy: 0.858600
Train - Epoch 34, Batch: 0, Loss: 0.713078
Test Avg. Loss: 0.000068, Accuracy: 0.859300
Train - Epoch 35, Batch: 0, Loss: 0.700682
Test Avg. Loss: 0.000067, Accuracy: 0.860200
Train - Epoch 36, Batch: 0, Loss: 0.700812
Test Avg. Loss: 0.000066, Accuracy: 0.860900
Train - Epoch 37, Batch: 0, Loss: 0.689859
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 38, Batch: 0, Loss: 0.679378
Test Avg. Loss: 0.000065, Accuracy: 0.862200
Train - Epoch 39, Batch: 0, Loss: 0.676990
Test Avg. Loss: 0.000064, Accuracy: 0.863000
Train - Epoch 40, Batch: 0, Loss: 0.669911
Test Avg. Loss: 0.000064, Accuracy: 0.864000
Train - Epoch 41, Batch: 0, Loss: 0.663914
Test Avg. Loss: 0.000063, Accuracy: 0.865200
Train - Epoch 42, Batch: 0, Loss: 0.657617
Test Avg. Loss: 0.000063, Accuracy: 0.866100
Train - Epoch 43, Batch: 0, Loss: 0.652909
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 44, Batch: 0, Loss: 0.644696
Test Avg. Loss: 0.000062, Accuracy: 0.867300
Train - Epoch 45, Batch: 0, Loss: 0.640895
Test Avg. Loss: 0.000061, Accuracy: 0.867600
Train - Epoch 46, Batch: 0, Loss: 0.639250
Test Avg. Loss: 0.000061, Accuracy: 0.868800
Train - Epoch 47, Batch: 0, Loss: 0.629198
Test Avg. Loss: 0.000060, Accuracy: 0.869300
Train - Epoch 48, Batch: 0, Loss: 0.627732
Test Avg. Loss: 0.000060, Accuracy: 0.869800
Train - Epoch 49, Batch: 0, Loss: 0.624117
Test Avg. Loss: 0.000059, Accuracy: 0.870600
Train - Epoch 50, Batch: 0, Loss: 0.620291
Test Avg. Loss: 0.000059, Accuracy: 0.871300
Train - Epoch 51, Batch: 0, Loss: 0.618690
Test Avg. Loss: 0.000059, Accuracy: 0.871900
Train - Epoch 52, Batch: 0, Loss: 0.615736
Test Avg. Loss: 0.000058, Accuracy: 0.872500
Train - Epoch 53, Batch: 0, Loss: 0.603859
Test Avg. Loss: 0.000058, Accuracy: 0.872600
Train - Epoch 54, Batch: 0, Loss: 0.605914
Test Avg. Loss: 0.000058, Accuracy: 0.872900
Train - Epoch 55, Batch: 0, Loss: 0.600248
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 56, Batch: 0, Loss: 0.598377
Test Avg. Loss: 0.000057, Accuracy: 0.873700
Train - Epoch 57, Batch: 0, Loss: 0.596536
Test Avg. Loss: 0.000057, Accuracy: 0.874200
Train - Epoch 58, Batch: 0, Loss: 0.594489
Test Avg. Loss: 0.000056, Accuracy: 0.874800
Train - Epoch 59, Batch: 0, Loss: 0.587038
Test Avg. Loss: 0.000056, Accuracy: 0.875600
training_time:: 52.35370850563049
training time full:: 52.35958743095398
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
batch_size:: 30000
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
epoch:: 32
	calling Sampler:__iter__
epoch:: 33
	calling Sampler:__iter__
epoch:: 34
	calling Sampler:__iter__
epoch:: 35
	calling Sampler:__iter__
epoch:: 36
	calling Sampler:__iter__
epoch:: 37
	calling Sampler:__iter__
epoch:: 38
	calling Sampler:__iter__
epoch:: 39
	calling Sampler:__iter__
epoch:: 40
	calling Sampler:__iter__
epoch:: 41
	calling Sampler:__iter__
epoch:: 42
	calling Sampler:__iter__
epoch:: 43
	calling Sampler:__iter__
epoch:: 44
	calling Sampler:__iter__
epoch:: 45
	calling Sampler:__iter__
epoch:: 46
	calling Sampler:__iter__
epoch:: 47
	calling Sampler:__iter__
epoch:: 48
	calling Sampler:__iter__
epoch:: 49
	calling Sampler:__iter__
epoch:: 50
	calling Sampler:__iter__
epoch:: 51
	calling Sampler:__iter__
epoch:: 52
	calling Sampler:__iter__
epoch:: 53
	calling Sampler:__iter__
epoch:: 54
	calling Sampler:__iter__
epoch:: 55
	calling Sampler:__iter__
epoch:: 56
	calling Sampler:__iter__
epoch:: 57
	calling Sampler:__iter__
epoch:: 58
	calling Sampler:__iter__
epoch:: 59
	calling Sampler:__iter__
training time is 47.713289976119995
time_baseline:: 47.85547709465027
curr_diff: 0 tensor(0.0136, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0136, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875400
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 60
delta_size:: 600
max_epoch:: 60
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
epoch  32
	calling Sampler:__iter__
epoch  33
	calling Sampler:__iter__
epoch  34
	calling Sampler:__iter__
epoch  35
	calling Sampler:__iter__
epoch  36
	calling Sampler:__iter__
epoch  37
	calling Sampler:__iter__
epoch  38
	calling Sampler:__iter__
epoch  39
	calling Sampler:__iter__
epoch  40
	calling Sampler:__iter__
epoch  41
	calling Sampler:__iter__
epoch  42
	calling Sampler:__iter__
epoch  43
	calling Sampler:__iter__
epoch  44
	calling Sampler:__iter__
epoch  45
	calling Sampler:__iter__
epoch  46
	calling Sampler:__iter__
epoch  47
	calling Sampler:__iter__
epoch  48
	calling Sampler:__iter__
epoch  49
	calling Sampler:__iter__
epoch  50
	calling Sampler:__iter__
epoch  51
	calling Sampler:__iter__
epoch  52
	calling Sampler:__iter__
epoch  53
	calling Sampler:__iter__
epoch  54
	calling Sampler:__iter__
epoch  55
	calling Sampler:__iter__
epoch  56
	calling Sampler:__iter__
epoch  57
	calling Sampler:__iter__
epoch  58
	calling Sampler:__iter__
epoch  59
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 25.914283990859985
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0137, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0137, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000056, Accuracy: 0.875400
batch size:: 16384
repetition 1
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.459442853927612
training time full:: 27.461918830871582
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.488155364990234
time_baseline:: 24.558720111846924
curr_diff: 0 tensor(0.0142, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0142, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876200
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 13.449721336364746
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0144, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0144, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876200
repetition 2
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.373223781585693
training time full:: 27.3753981590271
provenance prepare time:: 2.6226043701171875e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.178951025009155
time_baseline:: 24.248942375183105
curr_diff: 0 tensor(0.0142, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0142, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876200
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 13.351994037628174
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0144, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0144, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876200
repetition 3
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.27281355857849
training time full:: 27.27494740486145
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 23.910284996032715
time_baseline:: 23.98137879371643
curr_diff: 0 tensor(0.0142, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0142, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876200
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 13.347068548202515
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0144, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0144, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876200
repetition 4
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.486093997955322
training time full:: 27.488253593444824
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.106354236602783
time_baseline:: 24.17923355102539
curr_diff: 0 tensor(0.0142, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0142, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876200
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 13.382828950881958
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0144, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0144, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876200
repetition 5
python3 benchmark_exp_lr.py 0.001 16384 32 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.348648
Test Avg. Loss: 0.000195, Accuracy: 0.633000
Train - Epoch 1, Batch: 0, Loss: 1.960936
Test Avg. Loss: 0.000166, Accuracy: 0.735800
Train - Epoch 2, Batch: 0, Loss: 1.678961
Test Avg. Loss: 0.000145, Accuracy: 0.780600
Train - Epoch 3, Batch: 0, Loss: 1.470068
Test Avg. Loss: 0.000129, Accuracy: 0.804500
Train - Epoch 4, Batch: 0, Loss: 1.315870
Test Avg. Loss: 0.000117, Accuracy: 0.815700
Train - Epoch 5, Batch: 0, Loss: 1.202495
Test Avg. Loss: 0.000108, Accuracy: 0.822000
Train - Epoch 6, Batch: 0, Loss: 1.104932
Test Avg. Loss: 0.000101, Accuracy: 0.828100
Train - Epoch 7, Batch: 0, Loss: 1.034821
Test Avg. Loss: 0.000095, Accuracy: 0.831900
Train - Epoch 8, Batch: 0, Loss: 0.974025
Test Avg. Loss: 0.000090, Accuracy: 0.837400
Train - Epoch 9, Batch: 0, Loss: 0.925199
Test Avg. Loss: 0.000086, Accuracy: 0.841000
Train - Epoch 10, Batch: 0, Loss: 0.882047
Test Avg. Loss: 0.000082, Accuracy: 0.844300
Train - Epoch 11, Batch: 0, Loss: 0.855393
Test Avg. Loss: 0.000079, Accuracy: 0.847600
Train - Epoch 12, Batch: 0, Loss: 0.821111
Test Avg. Loss: 0.000077, Accuracy: 0.851400
Train - Epoch 13, Batch: 0, Loss: 0.788666
Test Avg. Loss: 0.000074, Accuracy: 0.853900
Train - Epoch 14, Batch: 0, Loss: 0.764943
Test Avg. Loss: 0.000072, Accuracy: 0.855300
Train - Epoch 15, Batch: 0, Loss: 0.744872
Test Avg. Loss: 0.000070, Accuracy: 0.857200
Train - Epoch 16, Batch: 0, Loss: 0.738664
Test Avg. Loss: 0.000069, Accuracy: 0.858900
Train - Epoch 17, Batch: 0, Loss: 0.709542
Test Avg. Loss: 0.000067, Accuracy: 0.860300
Train - Epoch 18, Batch: 0, Loss: 0.703773
Test Avg. Loss: 0.000066, Accuracy: 0.861900
Train - Epoch 19, Batch: 0, Loss: 0.685781
Test Avg. Loss: 0.000064, Accuracy: 0.863200
Train - Epoch 20, Batch: 0, Loss: 0.663260
Test Avg. Loss: 0.000063, Accuracy: 0.864700
Train - Epoch 21, Batch: 0, Loss: 0.653552
Test Avg. Loss: 0.000062, Accuracy: 0.866900
Train - Epoch 22, Batch: 0, Loss: 0.644926
Test Avg. Loss: 0.000061, Accuracy: 0.868200
Train - Epoch 23, Batch: 0, Loss: 0.645986
Test Avg. Loss: 0.000060, Accuracy: 0.869200
Train - Epoch 24, Batch: 0, Loss: 0.622019
Test Avg. Loss: 0.000059, Accuracy: 0.870700
Train - Epoch 25, Batch: 0, Loss: 0.617705
Test Avg. Loss: 0.000059, Accuracy: 0.871800
Train - Epoch 26, Batch: 0, Loss: 0.607360
Test Avg. Loss: 0.000058, Accuracy: 0.872000
Train - Epoch 27, Batch: 0, Loss: 0.607074
Test Avg. Loss: 0.000057, Accuracy: 0.873200
Train - Epoch 28, Batch: 0, Loss: 0.596077
Test Avg. Loss: 0.000057, Accuracy: 0.874000
Train - Epoch 29, Batch: 0, Loss: 0.587701
Test Avg. Loss: 0.000056, Accuracy: 0.875500
Train - Epoch 30, Batch: 0, Loss: 0.578453
Test Avg. Loss: 0.000055, Accuracy: 0.875500
Train - Epoch 31, Batch: 0, Loss: 0.580322
Test Avg. Loss: 0.000055, Accuracy: 0.876500
training_time:: 27.39392614364624
training time full:: 27.39605164527893
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
batch_size:: 16384
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
epoch:: 8
	calling Sampler:__iter__
epoch:: 9
	calling Sampler:__iter__
epoch:: 10
	calling Sampler:__iter__
epoch:: 11
	calling Sampler:__iter__
epoch:: 12
	calling Sampler:__iter__
epoch:: 13
	calling Sampler:__iter__
epoch:: 14
	calling Sampler:__iter__
epoch:: 15
	calling Sampler:__iter__
epoch:: 16
	calling Sampler:__iter__
epoch:: 17
	calling Sampler:__iter__
epoch:: 18
	calling Sampler:__iter__
epoch:: 19
	calling Sampler:__iter__
epoch:: 20
	calling Sampler:__iter__
epoch:: 21
	calling Sampler:__iter__
epoch:: 22
	calling Sampler:__iter__
epoch:: 23
	calling Sampler:__iter__
epoch:: 24
	calling Sampler:__iter__
epoch:: 25
	calling Sampler:__iter__
epoch:: 26
	calling Sampler:__iter__
epoch:: 27
	calling Sampler:__iter__
epoch:: 28
	calling Sampler:__iter__
epoch:: 29
	calling Sampler:__iter__
epoch:: 30
	calling Sampler:__iter__
epoch:: 31
	calling Sampler:__iter__
training time is 24.164857387542725
time_baseline:: 24.234817266464233
curr_diff: 0 tensor(0.0142, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0142, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876200
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 32
delta_size:: 600
max_epoch:: 32
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
epoch  8
	calling Sampler:__iter__
epoch  9
	calling Sampler:__iter__
epoch  10
	calling Sampler:__iter__
epoch  11
	calling Sampler:__iter__
epoch  12
	calling Sampler:__iter__
epoch  13
	calling Sampler:__iter__
epoch  14
	calling Sampler:__iter__
epoch  15
	calling Sampler:__iter__
epoch  16
	calling Sampler:__iter__
epoch  17
	calling Sampler:__iter__
epoch  18
	calling Sampler:__iter__
epoch  19
	calling Sampler:__iter__
epoch  20
	calling Sampler:__iter__
epoch  21
	calling Sampler:__iter__
epoch  22
	calling Sampler:__iter__
epoch  23
	calling Sampler:__iter__
epoch  24
	calling Sampler:__iter__
epoch  25
	calling Sampler:__iter__
epoch  26
	calling Sampler:__iter__
epoch  27
	calling Sampler:__iter__
epoch  28
	calling Sampler:__iter__
epoch  29
	calling Sampler:__iter__
epoch  30
	calling Sampler:__iter__
epoch  31
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 13.346709728240967
curr_diff: 0 tensor(0.0002, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0002, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0144, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0144, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000055, Accuracy: 0.876200
batch size:: 4096
repetition 1
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.8692467212677
training time full:: 5.8692848682403564
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 600
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.016108274459839
time_baseline:: 5.034244775772095
curr_diff: 0 tensor(0.0136, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0136, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874700
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 8
delta_size:: 600
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.254317045211792
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0141, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0141, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874500
repetition 2
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.881769180297852
training time full:: 5.881809234619141
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 600
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.164291143417358
time_baseline:: 5.182219982147217
curr_diff: 0 tensor(0.0136, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0136, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874700
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 8
delta_size:: 600
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.332432270050049
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0141, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0141, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874500
repetition 3
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 6.0083794593811035
training time full:: 6.008424520492554
provenance prepare time:: 1.6689300537109375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 600
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.061774253845215
time_baseline:: 5.079280138015747
curr_diff: 0 tensor(0.0136, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0136, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874700
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 8
delta_size:: 600
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.2693777084350586
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0141, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0141, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874500
repetition 4
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.757567644119263
training time full:: 5.757603168487549
provenance prepare time:: 1.1920928955078125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 600
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.098212480545044
time_baseline:: 5.118861436843872
curr_diff: 0 tensor(0.0136, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0136, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874700
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 8
delta_size:: 600
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.2957544326782227
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0141, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0141, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874500
repetition 5
python3 benchmark_exp_lr.py 0.001 4096 8 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.350940
Train - Epoch 0, Batch: 10, Loss: 1.570816
Test Avg. Loss: 0.000394, Accuracy: 0.801500
Train - Epoch 1, Batch: 0, Loss: 1.346503
Train - Epoch 1, Batch: 10, Loss: 1.092944
Test Avg. Loss: 0.000288, Accuracy: 0.830800
Train - Epoch 2, Batch: 0, Loss: 0.998022
Train - Epoch 2, Batch: 10, Loss: 0.882155
Test Avg. Loss: 0.000239, Accuracy: 0.845200
Train - Epoch 3, Batch: 0, Loss: 0.841646
Train - Epoch 3, Batch: 10, Loss: 0.775366
Test Avg. Loss: 0.000211, Accuracy: 0.856300
Train - Epoch 4, Batch: 0, Loss: 0.734333
Train - Epoch 4, Batch: 10, Loss: 0.705106
Test Avg. Loss: 0.000192, Accuracy: 0.862200
Train - Epoch 5, Batch: 0, Loss: 0.697753
Train - Epoch 5, Batch: 10, Loss: 0.652846
Test Avg. Loss: 0.000179, Accuracy: 0.868000
Train - Epoch 6, Batch: 0, Loss: 0.640055
Train - Epoch 6, Batch: 10, Loss: 0.623329
Test Avg. Loss: 0.000170, Accuracy: 0.871400
Train - Epoch 7, Batch: 0, Loss: 0.607572
Train - Epoch 7, Batch: 10, Loss: 0.593124
Test Avg. Loss: 0.000162, Accuracy: 0.874900
training_time:: 5.859822511672974
training time full:: 5.859861850738525
provenance prepare time:: 2.1457672119140625e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 8
delta_size:: 600
max_epoch:: 8
batch_size:: 4096
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
epoch:: 2
	calling Sampler:__iter__
epoch:: 3
	calling Sampler:__iter__
epoch:: 4
	calling Sampler:__iter__
epoch:: 5
	calling Sampler:__iter__
epoch:: 6
	calling Sampler:__iter__
epoch:: 7
	calling Sampler:__iter__
training time is 5.1865925788879395
time_baseline:: 5.20377516746521
curr_diff: 0 tensor(0.0136, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0136, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874700
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 8
delta_size:: 600
max_epoch:: 8
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
epoch  2
	calling Sampler:__iter__
epoch  3
	calling Sampler:__iter__
epoch  4
	calling Sampler:__iter__
epoch  5
	calling Sampler:__iter__
epoch  6
	calling Sampler:__iter__
epoch  7
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 3.262972831726074
curr_diff: 0 tensor(0.0009, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0009, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0141, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0141, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000162, Accuracy: 0.874500
batch size:: 1024
repetition 1
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.6649820804595947
training time full:: 1.6650199890136719
provenance prepare time:: 1.1920928955078125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 600
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.3842523097991943
time_baseline:: 1.3890621662139893
curr_diff: 0 tensor(0.0138, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0138, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873100
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 2
delta_size:: 600
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.0793671607971191
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0143, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0143, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873200
repetition 2
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.6788151264190674
training time full:: 1.6788525581359863
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 600
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.3477098941802979
time_baseline:: 1.3524117469787598
curr_diff: 0 tensor(0.0138, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0138, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873100
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 2
delta_size:: 600
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.0938997268676758
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0143, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0143, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873200
repetition 3
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.747556209564209
training time full:: 1.7475907802581787
provenance prepare time:: 1.9073486328125e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 600
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.346008062362671
time_baseline:: 1.3508219718933105
curr_diff: 0 tensor(0.0138, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0138, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873100
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 2
delta_size:: 600
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.0817108154296875
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0143, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0143, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873200
repetition 4
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.6825919151306152
training time full:: 1.6826345920562744
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 600
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.3478567600250244
time_baseline:: 1.352607011795044
curr_diff: 0 tensor(0.0138, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0138, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873100
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 2
delta_size:: 600
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.0745832920074463
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0143, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0143, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873200
repetition 5
python3 benchmark_exp_lr.py 0.001 1024 2 [0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1] Logistic_regression MNIST5 0.2 0.005 1 2
3.6.9 (default, Nov  7 2019, 10:44:02) 
[GCC 8.3.0]
../../../.gitignore/
cuda:2
Train - Epoch 0, Batch: 0, Loss: 2.345990
Train - Epoch 0, Batch: 10, Loss: 1.570035
Train - Epoch 0, Batch: 20, Loss: 1.209595
Train - Epoch 0, Batch: 30, Loss: 0.998485
Train - Epoch 0, Batch: 40, Loss: 0.913253
Train - Epoch 0, Batch: 50, Loss: 0.808470
Test Avg. Loss: 0.000727, Accuracy: 0.855100
Train - Epoch 1, Batch: 0, Loss: 0.737371
Train - Epoch 1, Batch: 10, Loss: 0.694881
Train - Epoch 1, Batch: 20, Loss: 0.658711
Train - Epoch 1, Batch: 30, Loss: 0.653832
Train - Epoch 1, Batch: 40, Loss: 0.630713
Train - Epoch 1, Batch: 50, Loss: 0.601008
Test Avg. Loss: 0.000562, Accuracy: 0.873000
training_time:: 1.7113850116729736
training time full:: 1.7114217281341553
provenance prepare time:: 1.430511474609375e-06
baseline::
python3 incremental_updates_base_line_lr.py 0
max_epoch:: 2
delta_size:: 600
max_epoch:: 2
batch_size:: 1024
epoch:: 0
	calling Sampler:__iter__
epoch:: 1
	calling Sampler:__iter__
training time is 1.3446540832519531
time_baseline:: 1.349501132965088
curr_diff: 0 tensor(0.0138, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0138, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873100
incremental updates::
python3 incremental_updates_provenance3_lr.py 30 5
max_epoch:: 2
delta_size:: 600
max_epoch:: 2
epoch  0
	calling Sampler:__iter__
epoch  1
	calling Sampler:__iter__
overhead:: 0
overhead2:: 0
overhead3:: 0
time_provenance:: 1.1402599811553955
curr_diff: 0 tensor(0.0014, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0014, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
curr_diff: 0 tensor(0.0143, dtype=torch.float64, grad_fn=<NormBackward0>)
tensor(0.0143, dtype=torch.float64, grad_fn=<SqrtBackward>)
tensor(1.0000, dtype=torch.float64, grad_fn=<DivBackward0>)
Test Avg. Loss: 0.000562, Accuracy: 0.873200
